{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### analyze the spike triggered pull and gaze ditribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# force manipulation type\n",
    "# SR_bothchange: self reward, both forces changed\n",
    "# CO_bothchange: 1s cooperation, both forces changed\n",
    "# CO_A1change: 1s cooperation, animal 1 forces changed\n",
    "# CO_A2change: 1s cooperation, animal 2 forces changed\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240910_Kanga_EffortBasedMC',\n",
    "                                    '20240911_Kanga_EffortBasedMC',\n",
    "                                    '20240912_Kanga_EffortBasedSR',\n",
    "                                    '20240913_Kanga_EffortBasedSR',\n",
    "                                    '20240916_Kanga_EffortBasedMC',\n",
    "                                    '20240917_Kanga_EffortBasedSR',\n",
    "                                    '20240918_Kanga_EffortBasedMC',\n",
    "                                    '20241008_Kanga_EffortBasedMC',\n",
    "                                    '20241009_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241010_Kanga_EffortBasedMC',\n",
    "                                    '20241011_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241014_Kanga_EffortBasedMC',\n",
    "                                    '20241016_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241017_Kanga_EffortBasedMC',\n",
    "                                    '20241018_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241022_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241025_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241101_Kanga_EffortBasedSR',\n",
    "                                    '20241104_Kanga_EffortBasedSR',\n",
    "                                   ]\n",
    "        # self and other are corresponding to the recorded animals\n",
    "        task_conditions = [\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedSR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240910',\n",
    "                        '20240911',\n",
    "                        '20240912',\n",
    "                        '20240913',\n",
    "                        '20240916',\n",
    "                        '20240917',\n",
    "                        '20240918',\n",
    "                        '20241008',\n",
    "                        '20241009',\n",
    "                        '20241010',\n",
    "                        '20241011',\n",
    "                        '20241014',\n",
    "                        '20241016',\n",
    "                        '20241017',\n",
    "                        '20241018',\n",
    "                        '20241022',\n",
    "                        '20241025',\n",
    "                        '20241101',\n",
    "                        '20241104',\n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240910',\n",
    "                            '20240911',\n",
    "                            '20240912',\n",
    "                            '20240913',\n",
    "                            '20240916',\n",
    "                            '20240917',\n",
    "                            '20240918',\n",
    "                            '20241008',\n",
    "                            '20241009',\n",
    "                            '20241010',\n",
    "                            '20241011',\n",
    "                            '20241014',\n",
    "                            '20241016',\n",
    "                            '20241017',\n",
    "                            '20241018',\n",
    "                            '20241022',\n",
    "                            '20241025',\n",
    "                            '20241101',\n",
    "                            '20241104',\n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                90.1,\n",
    "                                69.5,\n",
    "                                62.5,\n",
    "                                0.00,\n",
    "                                43.5,\n",
    "                                59.6,\n",
    "                                0.00,\n",
    "                                66.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                       ]\n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        neural_record_conditions = [\n",
    "                                \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                       \n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "#\n",
    "    \n",
    "# a test case\n",
    "if 0:\n",
    "    neural_record_conditions = ['20240910_Kanga_EffortBasedMC',]\n",
    "    dates_list = ['20240910',]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['self_EffortBasedMC',]\n",
    "    session_start_times = [0.00] # in second\n",
    "    kilosortvers = [4]\n",
    "    animal1_fixedorders = ['dannon']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dannon\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events and spike aligned summarizing variables  \n",
    "animal1_name_all_dates = dict.fromkeys(dates_list, [])\n",
    "animal2_name_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "spike_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvevents_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "strategy_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "strategy_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_forceManipulation_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/animal1_name_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        animal1_name_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/animal2_name_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        animal2_name_all_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        spike_trig_events_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "      \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        #####\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_lever_reading_\" + \"*.json\") \n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            lever_reading = pd.read_json(lever_reading_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line) \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_lever_reading_\" + \"*.json\")             \n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            lever_reading = pd.read_json(lever_reading_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line) \n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "        # \n",
    "        animal1_name_all_dates[date_tgt] = animal1\n",
    "        animal2_name_all_dates[date_tgt] = animal2\n",
    "        \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        # change lever reading time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(lever_reading)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = lever_reading[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = lever_reading[ind][\"readout_timepoint\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        lever_reading[\"readout_timepoint\"] = time_points_new[\"time_points_new\"]\n",
    "        lever_reading = lever_reading[lever_reading[\"readout_timepoint\"] != 0]\n",
    "        #\n",
    "        lever1_pull = lever_reading[(lever_reading['lever_id']==1)&(lever_reading['pull_or_release']==1)]\n",
    "        lever1_release = lever_reading[(lever_reading['lever_id']==1)&(lever_reading['pull_or_release']==0)]\n",
    "        lever2_pull = lever_reading[(lever_reading['lever_id']==2)&(lever_reading['pull_or_release']==1)]\n",
    "        lever2_release = lever_reading[(lever_reading['lever_id']==2)&(lever_reading['pull_or_release']==0)]\n",
    "        #\n",
    "        if np.shape(lever1_release)[0]<np.shape(lever1_pull)[0]:\n",
    "            lever1_pull = lever1_pull.iloc[0:-1]\n",
    "        if np.shape(lever2_release)[0]<np.shape(lever2_pull)[0]:\n",
    "            lever2_pull = lever2_pull.iloc[0:-1]\n",
    "        #\n",
    "        lever1_pull_release = lever1_pull\n",
    "        lever1_pull_release['delta_timepoint'] = np.array(lever1_release['readout_timepoint'].reset_index(drop=True)-lever1_pull['readout_timepoint'].reset_index(drop=True))\n",
    "        lever1_pull_release['delta_gauge'] = np.array(lever1_release['strain_gauge'].reset_index(drop=True)-lever1_pull['strain_gauge'].reset_index(drop=True))\n",
    "        lever2_pull_release = lever2_pull\n",
    "        lever2_pull_release['delta_timepoint'] = np.array(lever2_release['readout_timepoint'].reset_index(drop=True)-lever2_pull['readout_timepoint'].reset_index(drop=True))\n",
    "        lever2_pull_release['delta_gauge'] = np.array(lever2_release['strain_gauge'].reset_index(drop=True)-lever2_pull['strain_gauge'].reset_index(drop=True))\n",
    "        \n",
    "        \n",
    "        #####\n",
    "        # load behavioral event results from the tracking analysis\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_forceManipulation_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_forceManipulation_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "\n",
    "        try:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "                            \n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,                                                                                                               sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        time_point_juice1 = output_time_points_socialgaze['time_point_juice1']\n",
    "        time_point_juice2 = output_time_points_socialgaze['time_point_juice2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "\n",
    "        # define successful pulls and failed pulls\n",
    "        trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "        bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "        #\n",
    "        time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "        time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "        time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "        time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "        #\n",
    "        trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "        bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "        #\n",
    "        time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "        time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "        time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "        time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # new total session time (instead of a fix time) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) # 30 is the fps, in the unit of second\n",
    "              \n",
    "        \n",
    "        #####     \n",
    "        # load neural recording data\n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "                  \n",
    "        # # load spike sorting results\n",
    "        print('load spike data for '+neural_record_condition)\n",
    "        if kilosortver == 2:\n",
    "            spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "            spike_time_data = np.load(spike_time_file)\n",
    "        elif kilosortver == 4:\n",
    "            spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "            spike_time_data = np.load(spike_time_file)\n",
    "        # \n",
    "        # align the FR recording time stamps\n",
    "        spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "        # down-sample the spike recording resolution to 30Hz\n",
    "        spike_time_data = spike_time_data/fs_spikes*fps\n",
    "        spike_time_data = np.round(spike_time_data)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "            spike_clusters_data = np.load(spike_clusters_file)\n",
    "            spike_channels_data = np.copy(spike_clusters_data)\n",
    "        elif kilosortver == 4:\n",
    "            spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "            spike_clusters_data = np.load(spike_clusters_file)\n",
    "            spike_channels_data = np.copy(spike_clusters_data)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "            channel_maps_data = np.load(channel_maps_file)\n",
    "        elif kilosortver == 4:\n",
    "            channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "            channel_maps_data = np.load(channel_maps_file)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "            channel_pos_data = np.load(channel_pos_file)\n",
    "        elif kilosortver == 4:\n",
    "            channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "            channel_pos_data = np.load(channel_pos_file)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "            clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "        elif kilosortver == 4:\n",
    "            clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "            clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "        #\n",
    "        # only get the spikes that are manually checked\n",
    "        try:\n",
    "            good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "        except:\n",
    "            good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "        #\n",
    "        clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "        #\n",
    "        spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "        spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "        spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "\n",
    "        #\n",
    "        nclusters = np.shape(clusters_info_data)[0]\n",
    "        #\n",
    "        for icluster in np.arange(0,nclusters,1):\n",
    "            try:\n",
    "                cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "            except:\n",
    "                cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "            spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "        # \n",
    "        # get the channel to depth information, change 2 shanks to 1 shank \n",
    "        try:\n",
    "            channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "            # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "            # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "            channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "        except:\n",
    "            channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "            # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "            channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "           \n",
    "            \n",
    "        # calculate the firing rate\n",
    "        # FR_kernel = 0.20 # in the unit of second\n",
    "        FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "        # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "\n",
    "        totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "        _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                      spike_clusters_data, spike_time_data)\n",
    "        # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "        #                                                                              spike_channels_data, spike_time_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####    \n",
    "        #####    \n",
    "        # after all the analysis and data loading, separate them based on different subblock    \n",
    "        # get task type and cooperation threshold\n",
    "        # tasktype: 1-normal SR, 2-force changed SR, 3-normal coop, 4-force changed coop\n",
    "        trialID_list = np.array(trial_record_clean['trial_number'],dtype = 'int')\n",
    "        tasktype_list = np.array(trial_record_clean['task_type'],dtype = 'int')\n",
    "        coop_thres_list = np.array(trial_record_clean['pulltime_thres'],dtype = 'int')\n",
    "        lever1force_list = np.array(trial_record_clean['lever1_force'],dtype = 'int')\n",
    "        lever2force_list = np.array(trial_record_clean['lever2_force'],dtype = 'int')\n",
    "        \n",
    "        # use the combination of lever 1/2 forces to separate trials\n",
    "        force12_uniques,indices = np.unique(np.vstack((lever1force_list,lever2force_list)),axis=1,return_index=True)\n",
    "        force12_uniques = force12_uniques[:,np.argsort(indices)]\n",
    "        ntrialtypes = np.shape(force12_uniques)[1]\n",
    "        \n",
    "        force12_uniques_names =  [f\"{force12_uniques[0][i]}&{force12_uniques[1][i]}\" for i in range(force12_uniques.shape[1])]\n",
    "\n",
    "        # \n",
    "        # initialize some variables\n",
    "        bhvevents_aligned_FR_all_dates[date_tgt] = dict.fromkeys(force12_uniques_names)\n",
    "        bhvevents_aligned_FR_allevents_all_dates[date_tgt] = dict.fromkeys(force12_uniques_names)\n",
    "        #\n",
    "        strategy_aligned_FR_all_dates[date_tgt] = dict.fromkeys(force12_uniques_names)\n",
    "        strategy_aligned_FR_allevents_all_dates[date_tgt] = dict.fromkeys(force12_uniques_names)\n",
    "        #\n",
    "        spike_trig_events_all_dates[date_tgt] = dict.fromkeys(force12_uniques_names)\n",
    "            \n",
    "        #    \n",
    "        for itrialtype in np.arange(0,ntrialtypes,1):\n",
    "            force1_unique = force12_uniques[0,itrialtype]\n",
    "            force2_unique = force12_uniques[1,itrialtype]\n",
    "            \n",
    "            force12_unique_name = str(force1_unique)+'&'+str(force2_unique)\n",
    "            \n",
    "            ind = np.isin(lever1force_list,force1_unique) & np.isin(lever2force_list,force2_unique)\n",
    "            \n",
    "            trialID_itrialtype = trialID_list[ind]\n",
    "            \n",
    "            tasktype_itrialtype = np.unique(tasktype_list[ind])\n",
    "            coop_thres_itrialtype = np.unique(coop_thres_list[ind])\n",
    "            \n",
    "            # analyze behavior results\n",
    "            bhv_data_itrialtype = bhv_data[np.isin(bhv_data['trial_number'],trialID_itrialtype)]\n",
    "            #\n",
    "            # block time\n",
    "            block_starttime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==0]['time_points'].iloc[0]\n",
    "            block_endtime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==9]['time_points'].iloc[-1]\n",
    "    \n",
    "            print(block_starttime)\n",
    "            print(block_endtime)\n",
    "            \n",
    "            # only pick time in each block\n",
    "            # for behavioral variables\n",
    "            time_point_pull1_iblock = time_point_pull1[(time_point_pull1<=block_endtime)&(time_point_pull1>=block_starttime)]\n",
    "            time_point_pull2_iblock = time_point_pull2[(time_point_pull2<=block_endtime)&(time_point_pull2>=block_starttime)]\n",
    "            oneway_gaze1_iblock = oneway_gaze1[(oneway_gaze1<=block_endtime)&(oneway_gaze1>=block_starttime)]\n",
    "            oneway_gaze2_iblock = oneway_gaze2[(oneway_gaze2<=block_endtime)&(oneway_gaze2>=block_starttime)]\n",
    "            mutual_gaze1_iblock = mutual_gaze1[(mutual_gaze1<=block_endtime)&(mutual_gaze1>=block_starttime)]\n",
    "            mutual_gaze2_iblock = mutual_gaze2[(mutual_gaze2<=block_endtime)&(mutual_gaze2>=block_starttime)]\n",
    "            #\n",
    "            time_point_pulls_succfail_iblock = time_point_pulls_succfail.copy()\n",
    "            time_point_pulls_succfail_iblock['pull1_succ'] = time_point_pulls_succfail_iblock['pull1_succ'][(time_point_pulls_succfail_iblock['pull1_succ']<=block_endtime)&(time_point_pulls_succfail_iblock['pull1_succ']>=block_starttime)]\n",
    "            time_point_pulls_succfail_iblock['pull2_succ'] = time_point_pulls_succfail_iblock['pull2_succ'][(time_point_pulls_succfail_iblock['pull2_succ']<=block_endtime)&(time_point_pulls_succfail_iblock['pull2_succ']>=block_starttime)]\n",
    "            time_point_pulls_succfail_iblock['pull1_fail'] = time_point_pulls_succfail_iblock['pull1_fail'][(time_point_pulls_succfail_iblock['pull1_fail']<=block_endtime)&(time_point_pulls_succfail_iblock['pull1_fail']>=block_starttime)]\n",
    "            time_point_pulls_succfail_iblock['pull2_fail'] = time_point_pulls_succfail_iblock['pull2_fail'][(time_point_pulls_succfail_iblock['pull2_fail']<=block_endtime)&(time_point_pulls_succfail_iblock['pull2_fail']>=block_starttime)]\n",
    "\n",
    "            # for neural data\n",
    "            spike_clusters_data_iblock = spike_clusters_data[(spike_time_data<=block_endtime*fps)&(spike_time_data>=block_starttime*fps)]\n",
    "            spike_time_data_iblock = spike_time_data[(spike_time_data<=block_endtime*fps)&(spike_time_data>=block_starttime*fps)]\n",
    "            spike_channels_data_iblock = spike_channels_data[(spike_time_data<=block_endtime*fps)&(spike_time_data>=block_starttime*fps)]\n",
    "            \n",
    "            # remove block that does not have enough variables\n",
    "            if (\n",
    "                (np.shape(time_point_pull1_iblock)[0] < 3) or\n",
    "                (np.shape(time_point_pull2_iblock)[0] < 3) or\n",
    "                (np.shape(oneway_gaze1_iblock)[0] < 3) or\n",
    "                (np.shape(oneway_gaze2_iblock)[0] < 3) or\n",
    "                (np.shape(mutual_gaze1_iblock)[0] < 3) or\n",
    "                (np.shape(mutual_gaze2_iblock)[0] < 3) or \n",
    "                (np.shape(spike_time_data_iblock)[0] < 3)\n",
    "                ):\n",
    "                continue\n",
    "\n",
    "        \n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 0: \n",
    "                print('plot event aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt+\"/\"+force12_unique_name+\"/\"\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_average_all,bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,\n",
    "                                           time_point_pull1_iblock,time_point_pull2_iblock,time_point_pulls_succfail_iblock,\n",
    "                                           oneway_gaze1_iblock,oneway_gaze2_iblock,mutual_gaze1_iblock,mutual_gaze2_iblock,\n",
    "                                           gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates[date_tgt][force12_unique_name] = bhvevents_aligned_FR_average_all\n",
    "                bhvevents_aligned_FR_allevents_all_dates[date_tgt][force12_unique_name] = bhvevents_aligned_FR_allevents_all\n",
    "\n",
    "\n",
    "            # the three strategy aligned firing rate for each unit\n",
    "            if 0: \n",
    "                print('plot strategy aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt+\"/\"+force12_unique_name+\"/\"\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                stg_twins = 3 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                strategy_aligned_FR_average_all,strategy_aligned_FR_allevents_all = plot_strategy_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,\n",
    "                                           time_point_pull1_iblock,time_point_pull2_iblock,time_point_pulls_succfail_iblock,\n",
    "                                           oneway_gaze1_iblock,oneway_gaze2_iblock,mutual_gaze1_iblock,mutual_gaze2_iblock,\n",
    "                                           gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,stg_twins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "\n",
    "                strategy_aligned_FR_all_dates[date_tgt][force12_unique_name] = strategy_aligned_FR_average_all\n",
    "                strategy_aligned_FR_allevents_all_dates[date_tgt][force12_unique_name] = strategy_aligned_FR_allevents_all\n",
    "\n",
    "\n",
    "            #\n",
    "            # Run PCA analysis\n",
    "            FR_zscore_allch_np_merged = np.array(pd.DataFrame(FR_zscore_allch).T)\n",
    "            FR_zscore_allch_np_merged = FR_zscore_allch_np_merged[~np.isnan(np.sum(FR_zscore_allch_np_merged,axis=1)),:]\n",
    "            # # run PCA on the entire session\n",
    "            pca = PCA(n_components=3)\n",
    "            FR_zscore_allch_PCs = pca.fit_transform(FR_zscore_allch_np_merged.T)\n",
    "            #\n",
    "            # # run PCA around the -PCAtwins to PCAtwins for each behavioral events\n",
    "            PCAtwins = 4 # 5 second\n",
    "            gaze_thresold = 0.5 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "            savefigs = 0 \n",
    "            if 0:\n",
    "                PCA_around_bhv_events(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                              savefigs,data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            if 0:\n",
    "                if np.isin(animal1, ['dodson','dannon']):\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1_iblock,time_point_pull2_iblock,time_point_pulls_succfail_iblock,\n",
    "                                      oneway_gaze1_iblock,oneway_gaze2_iblock,mutual_gaze1_iblock,mutual_gaze2_iblock,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "                elif np.isin(animal2, ['dodson','dannon']):\n",
    "                    time_point_pulls_succfail_rev_iblock = time_point_pulls_succfail_iblock.copy()\n",
    "                    time_point_pulls_succfail_rev_iblock['pull1_succ'] = time_point_pulls_succfail_iblock['pull2_succ']\n",
    "                    time_point_pulls_succfail_rev_iblock['pull1_fail'] = time_point_pulls_succfail_iblock['pull2_fail']\n",
    "                    time_point_pulls_succfail_rev_iblock['pull2_succ'] = time_point_pulls_succfail_iblock['pull1_succ']\n",
    "                    time_point_pulls_succfail_rev_iblock['pull2_fail'] = time_point_pulls_succfail_iblock['pull1_fail']\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull2_iblock,time_point_pull1_iblock,time_point_pulls_succfail_rev_iblock, \n",
    "                                      oneway_gaze2_iblock,oneway_gaze1_iblock,mutual_gaze2_iblock,mutual_gaze1_iblock,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "\n",
    "\n",
    "\n",
    "            # do the spike triggered average of different bhv variables, for the single camera tracking, look at the pulling and social gaze actions\n",
    "            # the goal is to get a sense for glm\n",
    "            if 0: \n",
    "                print('plot spike triggered bhv variables')\n",
    "\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt+\"/\"+force12_unique_name+\"/\"\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                do_shuffle = 0\n",
    "                #\n",
    "                min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "                #\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "\n",
    "                gaze_thresold = 0.2\n",
    "\n",
    "                stg_twins = 3 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                #\n",
    "                spike_trig_average_all =  plot_spike_triggered_singlecam_bhvevent(date_tgt,savefig,save_path, animal1, animal2, session_start_time,min_length, trig_twins,\n",
    "                                                                              stg_twins, time_point_pull1_iblock, time_point_pull2_iblock, time_point_pulls_succfail_iblock,\n",
    "                                                                              oneway_gaze1_iblock,oneway_gaze2_iblock,mutual_gaze1_iblock,mutual_gaze2_iblock,gaze_thresold,animalnames_videotrack,\n",
    "                                                                              spike_clusters_data_iblock, spike_time_data_iblock,spike_channels_data_iblock,do_shuffle)\n",
    "\n",
    "                spike_trig_events_all_dates[date_tgt][force12_unique_name] = spike_trig_average_all\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        with open(data_saved_subfolder+'/animal1_name_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(animal1_name_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/animal2_name_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(animal2_name_all_dates, f) \n",
    "                \n",
    "\n",
    "        if 0:\n",
    "            with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(spike_trig_events_all_dates, f)  \n",
    "\n",
    "            with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(bhvevents_aligned_FR_all_dates, f) \n",
    "            with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(bhvevents_aligned_FR_allevents_all_dates, f) \n",
    "\n",
    "            with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(strategy_aligned_FR_all_dates, f) \n",
    "            with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(strategy_aligned_FR_allevents_all_dates, f) \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6a57",
   "metadata": {},
   "source": [
    "## plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048173b",
   "metadata": {},
   "source": [
    "### run PCA on the neuron space, pool sessions from the same condition together\n",
    "### for the activity aligned at the different bhv events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','selfforce','partnerforce','selfforcelevel','partnerforcelevel',\n",
    "                                                                    'condition','act_animal','bhv_name','clusterID',\n",
    "                                                                    'channelID','FR_allevents'])\n",
    "bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','selfforce','partnerforce','selfforcelevel','partnerforcelevel',\n",
    "                                                          'condition','act_animal','bhv_name','clusterID',\n",
    "                                                          'channelID','FR_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "    \n",
    "    animal1_name = animal1_name_all_dates[date_tgt]\n",
    "    animal2_name = animal2_name_all_dates[date_tgt]\n",
    "    \n",
    "    #\n",
    "    # translate the absolute force to force ranks/levels   \n",
    "    force12_combines = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "    nforce12 = np.shape(force12_combines)[0]\n",
    "    #\n",
    "    # Split into two lists: force1 and force2\n",
    "    force1s, force2s = zip(*[(int(f.split('&')[0]), int(f.split('&')[1])) for f in force12_combines])\n",
    "    # Convert the tuples to lists (optional)\n",
    "    force1s = list(force1s)\n",
    "    force2s = list(force2s)\n",
    "    #\n",
    "    # only one kind of force\n",
    "    yyy = force1s\n",
    "    if np.shape(np.unique(yyy))[0] == 1:\n",
    "        yyy_quant = np.ones(np.shape(yyy))*2\n",
    "    # two kinds of force\n",
    "    elif np.shape(np.unique(yyy))[0] == 2:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = (np.ceil(ranks / len(yyy) * 2)-1)*2+1 # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles         \n",
    "    # more than two kinds of force,\n",
    "    else:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = np.ceil(ranks / len(yyy) * 3) # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles\n",
    "    force1s_quant = yyy_quant.copy()\n",
    "    #\n",
    "    # only one kind of force\n",
    "    yyy = force2s\n",
    "    if np.shape(np.unique(yyy))[0] == 1:\n",
    "        yyy_quant = np.ones(np.shape(yyy))*2\n",
    "    # two kinds of force\n",
    "    elif np.shape(np.unique(yyy))[0] == 2:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = (np.ceil(ranks / len(yyy) * 2)-1)*2+1 # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles         \n",
    "    # more than two kinds of force,\n",
    "    else:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = np.ceil(ranks / len(yyy) * 3) # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles\n",
    "    force2s_quant = yyy_quant.copy()\n",
    "\n",
    "    # #\n",
    "    for iforce12 in np.arange(0,nforce12,1):\n",
    "        \n",
    "        force12_combine = force12_combines[iforce12]\n",
    "        force1 = force1s[iforce12]\n",
    "        force2 = force2s[iforce12]\n",
    "        force1_quant = force1s_quant[iforce12]\n",
    "        force2_quant = force2s_quant[iforce12]\n",
    "    \n",
    "        try:\n",
    "            bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][force12_combine].keys())\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][force12_combine][ibhv_type].keys())\n",
    "\n",
    "            act_animal = ibhv_type.split()[0]\n",
    "            bhv_name = ibhv_type.split()[1]\n",
    "            \n",
    "            #\n",
    "            if act_animal == animal1_name:\n",
    "                selfforce = force1\n",
    "                partnerforce = force2\n",
    "                selfforcelevel = force1_quant\n",
    "                partnerforcelevel = force2_quant\n",
    "            elif act_animal == animal2_name:\n",
    "                selfforce = force2\n",
    "                partnerforce = force1\n",
    "                selfforcelevel = force2_quant\n",
    "                partnerforcelevel = force1_quant\n",
    "                \n",
    "            \n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][force12_combine][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][force12_combine][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append(\n",
    "                                                                                       {'dates': date_tgt, \n",
    "                                                                                        'selfforce':selfforce,\n",
    "                                                                                        'partnerforce':partnerforce,\n",
    "                                                                                        'selfforcelevel':selfforcelevel,\n",
    "                                                                                        'partnerforcelevel':partnerforcelevel,\n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':act_animal,\n",
    "                                                                                        'bhv_name': bhv_name,\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                #\n",
    "                ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][force12_combine][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][force12_combine][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append(\n",
    "                                                                                       {'dates': date_tgt, \n",
    "                                                                                        'selfforce':selfforce,\n",
    "                                                                                        'partnerforce':partnerforce,\n",
    "                                                                                        'selfforcelevel':selfforcelevel,\n",
    "                                                                                        'partnerforcelevel':partnerforcelevel,\n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':act_animal,\n",
    "                                                                                        'bhv_name': bhv_name,\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc007ac",
   "metadata": {},
   "source": [
    "#### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ed46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['pull','gaze']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "\n",
    "# force levels \n",
    "forcetype_to_ana = 'selfforcelevel'\n",
    "forcelevels_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[forcetype_to_ana])\n",
    "nforcelevels = np.shape(forcelevels_to_ana)[0]\n",
    "\n",
    "# task conditions \n",
    "conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "\n",
    "\n",
    "# figures\n",
    "fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "fig1.set_figheight(6*3)\n",
    "fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "#\n",
    "# 3d figure\n",
    "fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "    cond_ana = conditions_to_ana[icond_ana]\n",
    "    # ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "    ind_cond = bhvevents_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "    \n",
    "    ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "    \n",
    "    for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "        act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "        # ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "        ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "       \n",
    "        for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "            bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "            # ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "            ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "                \n",
    "            for iforcelevel in np.arange(0,nforcelevels,1):\n",
    "                forcelevel_to_ana = forcelevels_to_ana[iforcelevel]\n",
    "                \n",
    "                ind_force = bhvevents_aligned_FR_all_dates_df[forcetype_to_ana]==forcelevel_to_ana   \n",
    "    \n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_force\n",
    "\n",
    "                # bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_all_dates_df[ind_ana]\n",
    "                \n",
    "                if np.shape(bhvevents_aligned_FR_tgt)[0]==0:\n",
    "                    continue\n",
    "\n",
    "                # PCA_dataset = np.hstack(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(bhvevents_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # run PCA\n",
    "                pca = PCA(n_components=3)\n",
    "                pca.fit(PCA_dataset)\n",
    "                PCA_dataset_proj = pca.transform(PCA_dataset)\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                         color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "            \n",
    "            \n",
    "    axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "    axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "    axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "    axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "    axs1[0,icond_ana].legend()      \n",
    "    \n",
    "    axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "    axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "    axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "    axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "    axs1[1,icond_ana].legend()    \n",
    "    \n",
    "    axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "    axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "    axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "    axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "    axs1[2,icond_ana].legend()    \n",
    "    \n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2') \n",
    "    ax2.set_zlabel('PC3')    \n",
    "    ax2.set_title(cond_ana)\n",
    "    ax2.legend()    \n",
    "    ax2.view_init(elev=30, azim=-30) \n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "\n",
    "    fig1.savefig(figsavefolder+'bhvevent_aligned_PCspace_trajectory_allconditions'+'_different'+forcetype_to_ana+savefile_sufix+'_PC123separate.pdf')\n",
    "    fig2.savefig(figsavefolder+'bhvevent_aligned_PCspace_trajectory_allconditions'+'_different'+forcetype_to_ana+savefile_sufix+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f9f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216b567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "156dd4a1",
   "metadata": {},
   "source": [
    "### run PCA on the neuron space, pool sessions from the same condition together\n",
    "### for the activity aligned at the different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','selfforce','partnerforce','selfforcelevel','partnerforcelevel',\n",
    "                                                         'condition','act_animal','bhv_name','clusterID',\n",
    "                                                        'channelID','FR_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    animal1_name = animal1_name_all_dates[date_tgt]\n",
    "    animal2_name = animal2_name_all_dates[date_tgt]\n",
    "    \n",
    "    #\n",
    "    # translate the absolute force to force ranks/levels   \n",
    "    force12_combines = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "    nforce12 = np.shape(force12_combines)[0]\n",
    "    #\n",
    "    # Split into two lists: force1 and force2\n",
    "    force1s, force2s = zip(*[(int(f.split('&')[0]), int(f.split('&')[1])) for f in force12_combines])\n",
    "    # Convert the tuples to lists (optional)\n",
    "    force1s = list(force1s)\n",
    "    force2s = list(force2s)\n",
    "    #\n",
    "    # only one kind of force\n",
    "    yyy = force1s\n",
    "    if np.shape(np.unique(yyy))[0] == 1:\n",
    "        yyy_quant = np.ones(np.shape(yyy))*2\n",
    "    # two kinds of force\n",
    "    elif np.shape(np.unique(yyy))[0] == 2:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = (np.ceil(ranks / len(yyy) * 2)-1)*2+1 # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles         \n",
    "    # more than two kinds of force,\n",
    "    else:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = np.ceil(ranks / len(yyy) * 3) # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles\n",
    "    force1s_quant = yyy_quant.copy()\n",
    "    #\n",
    "    # only one kind of force\n",
    "    yyy = force2s\n",
    "    if np.shape(np.unique(yyy))[0] == 1:\n",
    "        yyy_quant = np.ones(np.shape(yyy))*2\n",
    "    # two kinds of force\n",
    "    elif np.shape(np.unique(yyy))[0] == 2:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = (np.ceil(ranks / len(yyy) * 2)-1)*2+1 # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles         \n",
    "    # more than two kinds of force,\n",
    "    else:\n",
    "        ranks = st.rankdata(yyy, method='average')  # Average ranks for ties\n",
    "        # yyy_quant = np.ceil(ranks / len(yyy) * 3) # separate into three quantiles\n",
    "        yyy_quant = (np.ceil(ranks / len(yyy) * 2)) # separate into two quantiles\n",
    "    force2s_quant = yyy_quant.copy()\n",
    "\n",
    "    # #\n",
    "    for iforce12 in np.arange(0,nforce12,1):\n",
    "        \n",
    "        force12_combine = force12_combines[iforce12]\n",
    "        force1 = force1s[iforce12]\n",
    "        force2 = force2s[iforce12]\n",
    "        force1_quant = force1s_quant[iforce12]\n",
    "        force2_quant = force2s_quant[iforce12]\n",
    "    \n",
    "        try:\n",
    "            bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt][force12_combine].keys())\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    \n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][force12_combine][ibhv_type].keys())\n",
    "\n",
    "            ibhv_type_split = ibhv_type.split()\n",
    "            if np.shape(ibhv_type_split)[0]==3:\n",
    "                ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "            act_animal = ibhv_type_split[0]\n",
    "            bhv_name = ibhv_type_split[1]\n",
    "            \n",
    "            #\n",
    "            if act_animal == animal1_name:\n",
    "                selfforce = force1\n",
    "                partnerforce = force2\n",
    "                selfforcelevel = force1_quant\n",
    "                partnerforcelevel = force2_quant\n",
    "            elif act_animal == animal2_name:\n",
    "                selfforce = force2\n",
    "                partnerforce = force1\n",
    "                selfforcelevel = force2_quant\n",
    "                partnerforcelevel = force1_quant\n",
    "                \n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                #\n",
    "                ichannelID = strategy_aligned_FR_all_dates[date_tgt][force12_combine][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_all_dates[date_tgt][force12_combine][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                            'selfforce':selfforce,\n",
    "                                                                                            'partnerforce':partnerforce,\n",
    "                                                                                            'selfforcelevel':selfforcelevel,\n",
    "                                                                                            'partnerforcelevel':partnerforcelevel,                                                                                \n",
    "                                                                                            'condition':task_condition,\n",
    "                                                                                            'act_animal':act_animal,\n",
    "                                                                                            'bhv_name': bhv_name,\n",
    "                                                                                            'clusterID':iclusterID,\n",
    "                                                                                            'channelID':ichannelID,\n",
    "                                                                                            'FR_average':iFR_average,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1630922",
   "metadata": {},
   "source": [
    "#### plot\n",
    "#### plot all bhv variables in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78897e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(strategy_aligned_FR_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(strategy_aligned_FR_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "# bhv_names_to_ana = ['not_social_attention']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "\n",
    "# force levels \n",
    "forcetype_to_ana = 'selfforcelevel'\n",
    "forcelevels_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[forcetype_to_ana])\n",
    "nforcelevels = np.shape(forcelevels_to_ana)[0]\n",
    "\n",
    "#\n",
    "conditions_to_ana = np.unique(strategy_aligned_FR_all_dates_df['condition'])\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "# figures\n",
    "fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "fig1.set_figheight(6*3)\n",
    "fig1.set_figwidth(6*nconds_to_ana)\n",
    "#\n",
    "# 3d figure\n",
    "fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "    \n",
    "\n",
    "for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "    cond_ana = conditions_to_ana[icond_ana]\n",
    "    # ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "    ind_cond = strategy_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "    ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "    # different figure for different bhv variables\n",
    "    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "\n",
    "        # ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "        ind_bhv = strategy_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "    \n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal = strategy_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "                      \n",
    "            for iforcelevel in np.arange(0,nforcelevels,1):\n",
    "                forcelevel_to_ana = forcelevels_to_ana[iforcelevel]\n",
    "                \n",
    "                ind_force = strategy_aligned_FR_all_dates_df[forcetype_to_ana]==forcelevel_to_ana   \n",
    "    \n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_force\n",
    "\n",
    "                # strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                strategy_aligned_FR_tgt = strategy_aligned_FR_all_dates_df[ind_ana]\n",
    "                \n",
    "                if np.shape(strategy_aligned_FR_tgt)[0]==0:\n",
    "                    continue\n",
    "         \n",
    "\n",
    "                # PCA_dataset = np.hstack(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(strategy_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # run PCA\n",
    "                # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "                niters = 100\n",
    "                unitsamplesizes = 100\n",
    "                #\n",
    "                nunits = np.shape(PCA_dataset)[1]\n",
    "                ntimesteps = np.shape(PCA_dataset)[0]\n",
    "                #\n",
    "                PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "                #\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "                    PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),niters)]\n",
    "                    #\n",
    "                    pca = PCA(n_components=3)\n",
    "                    pca.fit(PCA_dataset_sample)\n",
    "                    PCA_dataset_proj_allsamples[iiter,:,:] = pca.transform(PCA_dataset_sample)\n",
    "                #\n",
    "                PCA_dataset_proj = np.nanmean(PCA_dataset_proj_allsamples,axis=0)\n",
    "\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                axs1[1,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                axs1[2,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                         color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "\n",
    "    \n",
    "    axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "    axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "    axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "    axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "    axs1[0,icond_ana].legend()      \n",
    "\n",
    "    axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "    axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "    axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "    axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "    axs1[1,icond_ana].legend()    \n",
    "\n",
    "    axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "    axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "    axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "    axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "    axs1[2,icond_ana].legend()    \n",
    "\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2') \n",
    "    ax2.set_zlabel('PC3')    \n",
    "    ax2.set_title(cond_ana)\n",
    "    ax2.legend()    \n",
    "    ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "\n",
    "    fig1.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_allconditions'+'_different'+forcetype_to_ana+savefile_sufix+'_PC123separate.pdf')\n",
    "    fig2.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_allconditions'+'_different'+forcetype_to_ana+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2795fbe",
   "metadata": {},
   "source": [
    "#### plot\n",
    "#### plot all bhv variables in separate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(strategy_aligned_FR_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(strategy_aligned_FR_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "# bhv_names_to_ana = ['not_social_attention']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "\n",
    "# force levels \n",
    "forcetype_to_ana = 'selfforcelevel'\n",
    "forcelevels_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[forcetype_to_ana])\n",
    "nforcelevels = np.shape(forcelevels_to_ana)[0]\n",
    "\n",
    "#\n",
    "conditions_to_ana = np.unique(strategy_aligned_FR_all_dates_df['condition'])\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "\n",
    "# different figure for different bhv variables\n",
    "for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    # ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "    ind_bhv = strategy_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "            \n",
    "        \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        # ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = strategy_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal = strategy_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "                      \n",
    "            for iforcelevel in np.arange(0,nforcelevels,1):\n",
    "                forcelevel_to_ana = forcelevels_to_ana[iforcelevel]\n",
    "                \n",
    "                ind_force = strategy_aligned_FR_all_dates_df[forcetype_to_ana]==forcelevel_to_ana   \n",
    "    \n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_force\n",
    "\n",
    "                # strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                strategy_aligned_FR_tgt = strategy_aligned_FR_all_dates_df[ind_ana]\n",
    "                \n",
    "                if np.shape(strategy_aligned_FR_tgt)[0]==0:\n",
    "                    continue\n",
    "         \n",
    "\n",
    "                # PCA_dataset = np.hstack(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(strategy_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # run PCA\n",
    "                # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "                niters = 100\n",
    "                unitsamplesizes = 100\n",
    "                #\n",
    "                nunits = np.shape(PCA_dataset)[1]\n",
    "                ntimesteps = np.shape(PCA_dataset)[0]\n",
    "                #\n",
    "                PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "                #\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "                    PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),niters)]\n",
    "                    #\n",
    "                    pca = PCA(n_components=3)\n",
    "                    pca.fit(PCA_dataset_sample)\n",
    "                    PCA_dataset_proj_allsamples[iiter,:,:] = pca.transform(PCA_dataset_sample)\n",
    "                #\n",
    "                PCA_dataset_proj = np.nanmean(PCA_dataset_proj_allsamples,axis=0)\n",
    "\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                axs1[1,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                axs1[2,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                                       color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana+' '+forcetype_to_ana+str(int(forcelevel_to_ana)),\n",
    "                         color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana*nforcelevels+iforcelevel])\n",
    "\n",
    "    \n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "    \n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+bhvname_ana+'_stretagy_aligned_PCspace_trajectory_allconditions'+'_different'+forcetype_to_ana+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+bhvname_ana+'_stretagy_aligned_PCspace_trajectory_allconditions'+'_different'+forcetype_to_ana+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1fea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f9ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99c78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8e71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3ab50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
