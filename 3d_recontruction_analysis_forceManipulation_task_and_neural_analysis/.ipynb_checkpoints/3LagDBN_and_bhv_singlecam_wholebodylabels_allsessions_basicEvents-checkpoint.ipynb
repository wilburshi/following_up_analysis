{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the all the sessions\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)\n",
    "### separate each session based on trial types (different force levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 1*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = '_allsessions'\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# force manipulation type\n",
    "# SR_bothchange: self reward, both forces changed\n",
    "# CO_bothchange: 1s cooperation, both forces changed\n",
    "# CO_A1change: 1s cooperation, animal 1 forces changed\n",
    "# CO_A2change: 1s cooperation, animal 2 forces changed\n",
    "forceManiType = 'CO_A2change'\n",
    "\n",
    "\n",
    "    # Koala Vermelho\n",
    "if 1:\n",
    "    if do_bestsession:      \n",
    "        # both animals' lever force were changed - Self reward\n",
    "        if forceManiType == 'SR_bothchange':\n",
    "            dates_list = [ \"20240228\",\"20240229\",\"20240409\",\"20240411\",\n",
    "                           \"20240412\",\"20240416\",\"20240419\",] \n",
    "            session_start_times = [ 64.5,  73.5,  0.00,  0.00,  \n",
    "                                    0.00,  0.00,  0.00,  ] # in second\n",
    "        # both animals' lever force were changed - cooperation\n",
    "        elif forceManiType == 'CO_bothchange':\n",
    "            dates_list = [ \"20240304\", ]\n",
    "            session_start_times = [ 0.00, ] # in second\n",
    "        # Koala's lever force were changed\n",
    "        if forceManiType == 'CO_A1change':\n",
    "            dates_list = [ \"20240305\",\"20240306\",\"20240313\",\"20240318\",\"20240321\",\n",
    "                           \"20240426\",\"20240429\",\"20240430\",]\n",
    "            session_start_times = [ 62.0,  55.2,  0.00,  0.00,  0.00, \n",
    "                                    0.00,  0.00,  0.00,  ] # in second\n",
    "        # Verm's lever force were changed\n",
    "        if forceManiType == 'CO_A2change':\n",
    "            dates_list = [ \"20240307\",\"20240308\",\"20240311\",\"20240319\",\n",
    "                           \"20240320\",\"20240422\",\"20240423\",\"20240425\",\n",
    "                           \"20240621\", ]\n",
    "            session_start_times = [ 72.2,  0.00,  60.8,  0.00,  \n",
    "                                    0.00,  53.0,  0.00,  0.00, \n",
    "                                    0.00, ] # in second       \n",
    "    \n",
    "    elif not do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                    \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                               \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['koala']\n",
    "    animal2_fixedorder = ['vermelho']\n",
    "\n",
    "    animal1_filename = \"Koala\"\n",
    "    animal2_filename = \"Vermelho\"\n",
    "    \n",
    "\n",
    "# Dannon Kanga\n",
    "if 0:\n",
    "    if do_bestsession:      \n",
    "        # both animals' lever force were changed - Self reward\n",
    "        if forceManiType == 'SR_bothchange':\n",
    "            dates_list = [ \"20240912\",\"20240913\",\"20240917\",\"20241101\",\"20241104\",\n",
    "                           \"20241105\",\n",
    "                           ] \n",
    "            session_start_times = [ 0.00,  0.00, 0.00, 0.00, 0.00,\n",
    "                                    0.00,\n",
    "                                   ] # in second\n",
    "        # both animals' lever force were changed - cooperation\n",
    "        elif forceManiType == 'CO_bothchange':\n",
    "            dates_list = [  ]\n",
    "            session_start_times = [  ] # in second\n",
    "        # Dannon's lever force were changed\n",
    "        if forceManiType == 'CO_A1change':\n",
    "            dates_list = [ \"20241009\",\"20241011\",\"20241016\",\"20241018\",\"20241022\", \n",
    "                           \"20241025\", ]\n",
    "            session_start_times = [ 0.00, 0.00, 0.00, 0.00, 0.00, \n",
    "                                    0.00, ] # in second\n",
    "        # Kanga's lever force were changed\n",
    "        if forceManiType == 'CO_A2change':\n",
    "            dates_list = [ \"20240910\",\"20240911\",\"20240916\",\"20240918\",\"20240919\" ,\n",
    "                           \"20241008\",\"20241010\",\"20241014\",\"20241017\"]\n",
    "            session_start_times = [ 0.00, 0.00, 0.00, 43.5, 0.00, \n",
    "                                    59.6, 66.0, 0.00, 0.00, ] # in second       \n",
    "    \n",
    "    elif not do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                               \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#    \n",
    "# dates_list = [\"20240430\"]\n",
    "# session_start_times = [0.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables  \n",
    "# align the animal1 and animal2 across the sessions to the right animal1 and 2 fixed_order\n",
    "animal1_name_all_dates = np.empty(shape=(0,), dtype=str)\n",
    "animal2_name_all_dates = np.empty(shape=(0,), dtype=str)\n",
    "trialdates_all_dates = np.empty(shape=(0,), dtype=str)\n",
    "tasktypes_all_dates = np.zeros((0,))\n",
    "coopthres_all_dates = np.zeros((0,))\n",
    "force1_all_dates = np.zeros((0,)) \n",
    "force2_all_dates = np.zeros((0,)) \n",
    "\n",
    "subblockID_all_dates = np.zeros((0,))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((0,))\n",
    "trialnum_all_dates = np.zeros((0,))\n",
    "blocktime_all_dates = np.zeros((0,))\n",
    "\n",
    "interpullintv_all_dates = np.zeros((0,))\n",
    "pull1_IPI_all_dates = np.zeros((0,))\n",
    "pull2_IPI_all_dates = np.zeros((0,))\n",
    "pull1_IPI_std_all_dates = np.zeros((0,))\n",
    "pull2_IPI_std_all_dates = np.zeros((0,))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((0,))\n",
    "owgaze2_num_all_dates = np.zeros((0,))\n",
    "mtgaze1_num_all_dates = np.zeros((0,))\n",
    "mtgaze2_num_all_dates = np.zeros((0,))\n",
    "pull1_num_all_dates = np.zeros((0,))\n",
    "pull2_num_all_dates = np.zeros((0,))\n",
    "\n",
    "lever1_holdtime_all_dates = np.zeros((0,))\n",
    "lever2_holdtime_all_dates = np.zeros((0,))\n",
    "lever1_holdtime_std_all_dates = np.zeros((0,))\n",
    "lever2_holdtime_std_all_dates = np.zeros((0,))\n",
    "\n",
    "lever1_gauge_all_dates = np.zeros((0,))\n",
    "lever2_gauge_all_dates = np.zeros((0,))\n",
    "lever1_gauge_std_all_dates = np.zeros((0,))\n",
    "lever2_gauge_std_all_dates = np.zeros((0,))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_forceManipulation_task_data_saved/'\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    \n",
    "    print('load basic data for '+forceManiType)\n",
    "    \n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "   \n",
    "    with open(data_saved_subfolder+'/animal1_name_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        animal1_name_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/animal2_name_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        animal2_name_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialdates_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        trialdates_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/force1_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        force1_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/force2_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        force2_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/subblockID_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        subblockID_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/blocktime_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        blocktime_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_IPI_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        pull1_IPI_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_IPI_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        pull1_IPI_std_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_IPI_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        pull2_IPI_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_IPI_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        pull2_IPI_std_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)     \n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/lever1_holdtime_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever1_holdtime_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/lever1_holdtime_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever1_holdtime_std_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/lever2_holdtime_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever2_holdtime_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/lever2_holdtime_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever2_holdtime_std_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/lever1_gauge_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever1_gauge_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/lever1_gauge_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever1_gauge_std_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/lever2_gauge_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever2_gauge_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/lever2_gauge_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        lever2_gauge_std_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates for '+forceManiType)\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_lever_reading_\" + \"*.json\") \n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            lever_reading = pd.read_json(lever_reading_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_lever_reading_\" + \"*.json\")             \n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            lever_reading = pd.read_json(lever_reading_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        # change lever reading time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(lever_reading)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = lever_reading[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = lever_reading[ind][\"readout_timepoint\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        lever_reading[\"readout_timepoint\"] = time_points_new[\"time_points_new\"]\n",
    "        lever_reading = lever_reading[lever_reading[\"readout_timepoint\"] != 0]\n",
    "        #\n",
    "        lever1_pull = lever_reading[(lever_reading['lever_id']==1)&(lever_reading['pull_or_release']==1)]\n",
    "        lever1_release = lever_reading[(lever_reading['lever_id']==1)&(lever_reading['pull_or_release']==0)]\n",
    "        lever2_pull = lever_reading[(lever_reading['lever_id']==2)&(lever_reading['pull_or_release']==1)]\n",
    "        lever2_release = lever_reading[(lever_reading['lever_id']==2)&(lever_reading['pull_or_release']==0)]\n",
    "        #\n",
    "        if np.shape(lever1_release)[0]<np.shape(lever1_pull)[0]:\n",
    "            lever1_pull = lever1_pull.iloc[0:-1]\n",
    "        if np.shape(lever2_release)[0]<np.shape(lever2_pull)[0]:\n",
    "            lever2_pull = lever2_pull.iloc[0:-1]\n",
    "        #\n",
    "        lever1_pull_release = lever1_pull\n",
    "        lever1_pull_release['delta_timepoint'] = np.array(lever1_release['readout_timepoint'].reset_index(drop=True)-lever1_pull['readout_timepoint'].reset_index(drop=True))\n",
    "        lever1_pull_release['delta_gauge'] = np.array(lever1_release['strain_gauge'].reset_index(drop=True)-lever1_pull['strain_gauge'].reset_index(drop=True))\n",
    "        lever2_pull_release = lever2_pull\n",
    "        lever2_pull_release['delta_timepoint'] = np.array(lever2_release['readout_timepoint'].reset_index(drop=True)-lever2_pull['readout_timepoint'].reset_index(drop=True))\n",
    "        lever2_pull_release['delta_gauge'] = np.array(lever2_release['strain_gauge'].reset_index(drop=True)-lever2_pull['strain_gauge'].reset_index(drop=True))\n",
    "        \n",
    "        \n",
    "        # load behavioral event results from the tracking analysis\n",
    "        if 1:\n",
    "            # folder and file path\n",
    "            camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_forceManipulation_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "            camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_forceManipulation_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "\n",
    "            try:\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                try: \n",
    "                    bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                    # get the bodypart data from files\n",
    "                    bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                    video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "                except:\n",
    "                    bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                    # get the bodypart data from files\n",
    "                    bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                    video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "            except:\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "                try: \n",
    "                    bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                    # get the bodypart data from files\n",
    "                    bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                    video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "                except:\n",
    "                    bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                    # get the bodypart data from files\n",
    "                    bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                    video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "                            \n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "            except:   \n",
    "                print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                       considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                       sqr_thres_face,sqr_thres_body)\n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "\n",
    "\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "\n",
    "\n",
    "            # # plot behavioral events\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                    plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            else:\n",
    "                    plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "            #\n",
    "            # save behavioral events plot\n",
    "            if 1:\n",
    "                current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "                \n",
    "\n",
    "            # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "            # could be used for define time bin for DBN\n",
    "            if 1:\n",
    "                _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                             oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #\n",
    "                pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "                bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                                'pull_other_pooled': pull_other_pool_itv}\n",
    "\n",
    "            # plot the tracking demo video\n",
    "            if 0: \n",
    "                tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                  lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                  animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                  animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                                  video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "             \n",
    "            \n",
    "        # after all the analysis, separate them based on different subblock    \n",
    "        # get task type and cooperation threshold\n",
    "        # tasktype: 1-normal SR, 2-force changed SR, 3-normal coop, 4-force changed coop\n",
    "        trialID_list = np.array(trial_record_clean['trial_number'],dtype = 'int')\n",
    "        tasktype_list = np.array(trial_record_clean['task_type'],dtype = 'int')\n",
    "        coop_thres_list = np.array(trial_record_clean['pulltime_thres'],dtype = 'int')\n",
    "        lever1force_list = np.array(trial_record_clean['lever1_force'],dtype = 'int')\n",
    "        lever2force_list = np.array(trial_record_clean['lever2_force'],dtype = 'int')\n",
    "        \n",
    "        # use the combination of lever 1/2 forces to separate trials\n",
    "        force12_uniques,indices = np.unique(np.vstack((lever1force_list,lever2force_list)),axis=1,return_index=True)\n",
    "        force12_uniques = force12_uniques[:,np.argsort(indices)]\n",
    "        ntrialtypes = np.shape(force12_uniques)[1]\n",
    "        \n",
    "        # \n",
    "        for itrialtype in np.arange(0,ntrialtypes,1):\n",
    "            force1_unique = force12_uniques[0,itrialtype]\n",
    "            force2_unique = force12_uniques[1,itrialtype]\n",
    "\n",
    "            ind = np.isin(lever1force_list,force1_unique) & np.isin(lever2force_list,force2_unique)\n",
    "            \n",
    "            trialID_itrialtype = trialID_list[ind]\n",
    "            \n",
    "            tasktype_itrialtype = np.unique(tasktype_list[ind])\n",
    "            coop_thres_itrialtype = np.unique(coop_thres_list[ind])\n",
    "            \n",
    "            # save some simple measures\n",
    "            animal1_name_all_dates = np.append(animal1_name_all_dates,animal1)\n",
    "            animal2_name_all_dates = np.append(animal2_name_all_dates,animal2)\n",
    "            trialdates_all_dates = np.append(trialdates_all_dates,date_tgt)\n",
    "            tasktypes_all_dates = np.append(tasktypes_all_dates,tasktype_itrialtype)\n",
    "            coopthres_all_dates = np.append(coopthres_all_dates,coop_thres_itrialtype)\n",
    "            #\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                force1_all_dates = np.append(force1_all_dates,force1_unique)\n",
    "                force2_all_dates = np.append(force2_all_dates,force2_unique)\n",
    "            else:\n",
    "                force1_all_dates = np.append(force1_all_dates,force2_unique)\n",
    "                force2_all_dates = np.append(force2_all_dates,force1_unique)\n",
    "            #\n",
    "            trialnum_all_dates = np.append(trialnum_all_dates,np.sum(ind))\n",
    "            subblockID_all_dates = np.append(subblockID_all_dates,itrialtype)\n",
    "            \n",
    "            # analyze behavior results\n",
    "            bhv_data_itrialtype = bhv_data[np.isin(bhv_data['trial_number'],trialID_itrialtype)]\n",
    "            #\n",
    "            # successful rates\n",
    "            succ_rate_itrialtype = np.sum((bhv_data_itrialtype['behavior_events']==3)|(bhv_data_itrialtype['behavior_events']==4))/np.sum((bhv_data_itrialtype['behavior_events']==1)|(bhv_data_itrialtype['behavior_events']==2))\n",
    "            succ_rate_all_dates = np.append(succ_rate_all_dates,succ_rate_itrialtype)\n",
    "            #\n",
    "            # block time\n",
    "            block_starttime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==0]['time_points'].iloc[0]\n",
    "            block_endtime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==9]['time_points'].iloc[-1]\n",
    "            blocktime_all_dates = np.append(blocktime_all_dates,block_endtime-block_starttime)\n",
    "            #\n",
    "            # across animal interpull interval\n",
    "            pullid = np.array(bhv_data_itrialtype[(bhv_data_itrialtype['behavior_events']==1) | (bhv_data_itrialtype['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data_itrialtype[(bhv_data_itrialtype['behavior_events']==1) | (bhv_data_itrialtype['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<20]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates = np.append(interpullintv_all_dates,mean_interpull_intv)\n",
    "            # \n",
    "            # animal 1 and 2's pull numbers\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                pull1_num_all_dates = np.append(pull1_num_all_dates,np.sum(bhv_data_itrialtype['behavior_events']==1))\n",
    "                pull2_num_all_dates = np.append(pull2_num_all_dates,np.sum(bhv_data_itrialtype['behavior_events']==2))\n",
    "            else:\n",
    "                pull1_num_all_dates = np.append(pull1_num_all_dates,np.sum(bhv_data_itrialtype['behavior_events']==2))\n",
    "                pull2_num_all_dates = np.append(pull2_num_all_dates,np.sum(bhv_data_itrialtype['behavior_events']==1))\n",
    "            #\n",
    "            # animal 1 and 2's within animal interpull interval\n",
    "            pull1time = np.array(bhv_data_itrialtype[(bhv_data_itrialtype['behavior_events']==1)][\"time_points\"])\n",
    "            ipi_pull1 = pull1time[1:]-pull1time[0:-1]\n",
    "            ipi_pull1 = ipi_pull1[ipi_pull1<20]\n",
    "            mean_ipi_pull1 = np.nanmean(ipi_pull1)\n",
    "            std_ipi_pull1 = np.nanstd(ipi_pull1)/np.sqrt(np.shape(ipi_pull1)[0])\n",
    "            pull2time = np.array(bhv_data_itrialtype[(bhv_data_itrialtype['behavior_events']==2)][\"time_points\"])\n",
    "            ipi_pull2 = pull2time[1:]-pull2time[0:-1]\n",
    "            ipi_pull2 = ipi_pull2[ipi_pull2<20]\n",
    "            mean_ipi_pull2 = np.nanmean(ipi_pull2)\n",
    "            std_ipi_pull2 = np.nanstd(ipi_pull2)/np.sqrt(np.shape(ipi_pull2)[0])\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                pull1_IPI_all_dates = np.append(pull1_IPI_all_dates,mean_ipi_pull1)\n",
    "                pull2_IPI_all_dates = np.append(pull2_IPI_all_dates,mean_ipi_pull2)\n",
    "                pull1_IPI_std_all_dates = np.append(pull1_IPI_std_all_dates,std_ipi_pull1)\n",
    "                pull2_IPI_std_all_dates = np.append(pull2_IPI_std_all_dates,std_ipi_pull2)\n",
    "            else:\n",
    "                pull1_IPI_all_dates = np.append(pull1_IPI_all_dates,mean_ipi_pull2)\n",
    "                pull2_IPI_all_dates = np.append(pull2_IPI_all_dates,mean_ipi_pull1)\n",
    "                pull1_IPI_std_all_dates = np.append(pull1_IPI_std_all_dates,std_ipi_pull2)\n",
    "                pull2_IPI_std_all_dates = np.append(pull2_IPI_std_all_dates,std_ipi_pull1)\n",
    "            \n",
    "            \n",
    "            # lever holding time            \n",
    "            lever1_holdtimes_itrialtype = lever1_pull_release[np.isin(lever1_pull_release['trial_number'],trialID_itrialtype)]['delta_timepoint']\n",
    "            mean_lever1_holdtime = np.nanmean(lever1_holdtimes_itrialtype)           \n",
    "            std_lever1_holdtime = np.nanstd(lever1_holdtimes_itrialtype)/np.sqrt(np.shape(lever1_holdtimes_itrialtype)[0]) \n",
    "            lever2_holdtimes_itrialtype = lever2_pull_release[np.isin(lever2_pull_release['trial_number'],trialID_itrialtype)]['delta_timepoint']\n",
    "            mean_lever2_holdtime = np.nanmean(lever2_holdtimes_itrialtype)\n",
    "            std_lever2_holdtime = np.nanstd(lever2_holdtimes_itrialtype)/np.sqrt(np.shape(lever2_holdtimes_itrialtype)[0])             \n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                lever1_holdtime_all_dates = np.append(lever1_holdtime_all_dates,mean_lever1_holdtime)\n",
    "                lever2_holdtime_all_dates = np.append(lever2_holdtime_all_dates,mean_lever2_holdtime)\n",
    "                lever1_holdtime_std_all_dates = np.append(lever1_holdtime_std_all_dates,std_lever1_holdtime)\n",
    "                lever2_holdtime_std_all_dates = np.append(lever2_holdtime_std_all_dates,std_lever2_holdtime)\n",
    "            else:\n",
    "                lever1_holdtime_all_dates = np.append(lever1_holdtime_all_dates,mean_lever2_holdtime)\n",
    "                lever2_holdtime_all_dates = np.append(lever2_holdtime_all_dates,mean_lever1_holdtime)\n",
    "                lever1_holdtime_std_all_dates = np.append(lever1_holdtime_std_all_dates,std_lever2_holdtime)\n",
    "                lever2_holdtime_std_all_dates = np.append(lever2_holdtime_std_all_dates,std_lever1_holdtime)\n",
    "            \n",
    "            # strain gauge reading\n",
    "            lever1_gauge_itrialtype = lever1_pull_release[np.isin(lever1_pull_release['trial_number'],trialID_itrialtype)]['delta_gauge']\n",
    "            mean_lever1_gauge = np.nanmean(lever1_gauge_itrialtype)           \n",
    "            std_lever1_gauge = np.nanstd(lever1_gauge_itrialtype)/np.sqrt(np.shape(lever1_gauge_itrialtype)[0]) \n",
    "            lever2_gauge_itrialtype = lever2_pull_release[np.isin(lever2_pull_release['trial_number'],trialID_itrialtype)]['delta_gauge']\n",
    "            mean_lever2_gauge = np.nanmean(lever2_gauge_itrialtype)\n",
    "            std_lever2_gauge = np.nanstd(lever2_gauge_itrialtype)/np.sqrt(np.shape(lever2_gauge_itrialtype)[0])             \n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                lever1_gauge_all_dates = np.append(lever1_gauge_all_dates,mean_lever1_gauge)\n",
    "                lever2_gauge_all_dates = np.append(lever2_gauge_all_dates,mean_lever2_gauge)\n",
    "                lever1_gauge_std_all_dates = np.append(lever1_gauge_std_all_dates,std_lever1_gauge)\n",
    "                lever2_gauge_std_all_dates = np.append(lever2_gauge_std_all_dates,std_lever2_gauge)\n",
    "            else:\n",
    "                lever1_gauge_all_dates = np.append(lever1_gauge_all_dates,mean_lever2_gauge)\n",
    "                lever2_gauge_all_dates = np.append(lever2_gauge_all_dates,mean_lever1_gauge)\n",
    "                lever1_gauge_std_all_dates = np.append(lever1_gauge_std_all_dates,std_lever2_gauge)\n",
    "                lever2_gauge_std_all_dates = np.append(lever2_gauge_std_all_dates,std_lever1_gauge)\n",
    "\n",
    "            \n",
    "            # gaze number, based on the DLC tracking \n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                owgaze1_num_all_dates = np.append(owgaze1_num_all_dates,np.shape(oneway_gaze1[(oneway_gaze1<=block_endtime)&(oneway_gaze1>=block_starttime)])[0])\n",
    "                owgaze2_num_all_dates = np.append(owgaze2_num_all_dates,np.shape(oneway_gaze2[(oneway_gaze2<=block_endtime)&(oneway_gaze2>=block_starttime)])[0])\n",
    "                mtgaze1_num_all_dates = np.append(mtgaze1_num_all_dates,np.shape(mutual_gaze1[(mutual_gaze1<=block_endtime)&(mutual_gaze1>=block_starttime)])[0])\n",
    "                mtgaze2_num_all_dates = np.append(mtgaze2_num_all_dates,np.shape(mutual_gaze2[(mutual_gaze2<=block_endtime)&(mutual_gaze2>=block_starttime)])[0])\n",
    "            else:\n",
    "                owgaze1_num_all_dates = np.append(owgaze1_num_all_dates,np.shape(oneway_gaze2[(oneway_gaze2<=block_endtime)&(oneway_gaze2>=block_starttime)])[0])\n",
    "                owgaze2_num_all_dates = np.append(owgaze2_num_all_dates,np.shape(oneway_gaze1[(oneway_gaze1<=block_endtime)&(oneway_gaze1>=block_starttime)])[0])\n",
    "                mtgaze1_num_all_dates = np.append(mtgaze1_num_all_dates,np.shape(mutual_gaze2[(mutual_gaze2<=block_endtime)&(mutual_gaze2>=block_starttime)])[0])\n",
    "                mtgaze2_num_all_dates = np.append(mtgaze2_num_all_dates,np.shape(mutual_gaze1[(mutual_gaze1<=block_endtime)&(mutual_gaze1>=block_starttime)])[0])\n",
    "            \n",
    "            \n",
    "                \n",
    "    # save data\n",
    "    if 1:        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        with open(data_saved_subfolder+'/animal1_name_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(animal1_name_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/animal2_name_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(animal2_name_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialdates_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialdates_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/force1_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(force1_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/force2_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(force2_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/subblockID_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(subblockID_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/blocktime_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(blocktime_all_dates, f)\n",
    "        \n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_IPI_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_IPI_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_IPI_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_IPI_std_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_IPI_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_IPI_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_IPI_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_IPI_std_all_dates, f)\n",
    "                \n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)       \n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/lever1_holdtime_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever1_holdtime_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/lever1_holdtime_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever1_holdtime_std_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/lever2_holdtime_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever2_holdtime_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/lever2_holdtime_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever2_holdtime_std_all_dates, f)\n",
    "        \n",
    "        with open(data_saved_subfolder+'/lever1_gauge_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever1_gauge_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/lever1_gauge_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever1_gauge_std_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/lever2_gauge_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever2_gauge_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/lever2_gauge_std_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(lever2_gauge_std_all_dates, f)\n",
    "              \n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN\n",
    "#### distribution of gaze before and after pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_input_data_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# initiate the final data set\n",
    "dist_twin_range = 5\n",
    "#\n",
    "SameAnimal_gazeDist_mean_all = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "SameAnimal_gazeDist_mean_all[animal1_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "SameAnimal_gazeDist_mean_all[animal2_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "#\n",
    "AcroAnimal_gazeDist_mean_all = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "AcroAnimal_gazeDist_mean_all[animal1_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "AcroAnimal_gazeDist_mean_all[animal2_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "#\n",
    "SameAnimal_gazeDist_shuffle_all = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "SameAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "SameAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "#\n",
    "AcroAnimal_gazeDist_shuffle_all = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "AcroAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "AcroAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]] = dict.fromkeys(dates_list,[])\n",
    "\n",
    "prepare_input_data = 1\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "\n",
    "mergetempRos = 0\n",
    "doBhvitv_timebin = 0 # 1: if use the mean bhv event interval for time bin\n",
    "\n",
    "if mergetempRos:\n",
    "    temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    # use bhv event to decide temporal resolution\n",
    "    #\n",
    "    #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "    #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "#\n",
    "if doBhvitv_timebin:\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[date_tgt]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    temp_resolus = [np.nanmean(pull_other_intv_ii)]          \n",
    "#\n",
    "ntemp_reses = np.shape(temp_resolus)[0]        \n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "   \n",
    "    print('prepare DBN input data for '+forceManiType)\n",
    "    \n",
    "    # try different temporal resolutions\n",
    "    for temp_resolu in temp_resolus:\n",
    "        \n",
    "        # bhv_df = [] # combine all dates\n",
    "        \n",
    "        for idate in np.arange(0,ndates,1):\n",
    "            \n",
    "            # bhv_df = [] # combine all block in one day\n",
    "            \n",
    "            date_tgt = dates_list[idate]\n",
    "            session_start_time = session_start_times[idate]\n",
    "\n",
    "            # load behavioral results\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_lever_reading_\" + \"*.json\") \n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "                lever_reading = pd.read_json(lever_reading_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_lever_reading_\" + \"*.json\")             \n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "                lever_reading = pd.read_json(lever_reading_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # load behavioral event results\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            #\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            # align to the session start time    \n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']     \n",
    "\n",
    "\n",
    "            # after all the analysis, separate them based on different subblock    \n",
    "            # get task type and cooperation threshold\n",
    "            # tasktype: 1-normal SR, 2-force changed SR, 3-normal coop, 4-force changed coop\n",
    "            trialID_list = np.array(trial_record_clean['trial_number'],dtype = 'int')\n",
    "            tasktype_list = np.array(trial_record_clean['task_type'],dtype = 'int')\n",
    "            coop_thres_list = np.array(trial_record_clean['pulltime_thres'],dtype = 'int')\n",
    "            lever1force_list = np.array(trial_record_clean['lever1_force'],dtype = 'int')\n",
    "            lever2force_list = np.array(trial_record_clean['lever2_force'],dtype = 'int')\n",
    "\n",
    "            # use the combination of lever 1/2 forces to separate trials\n",
    "            force12_uniques,indices = np.unique(np.vstack((lever1force_list,lever2force_list)),axis=1,return_index=True)\n",
    "            force12_uniques = force12_uniques[:,np.argsort(indices)]\n",
    "            ntrialtypes = np.shape(force12_uniques)[1]\n",
    "            #\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                force12names = [str(force12_uniques[0][i])+'&'+str(force12_uniques[1][i]) for i in np.arange(0,ntrialtypes,1)]\n",
    "            else:\n",
    "                force12names = [str(force12_uniques[1][i])+'&'+str(force12_uniques[0][i]) for i in np.arange(0,ntrialtypes,1)]                \n",
    "            #\n",
    "            \n",
    "            # initialize \n",
    "            DBN_input_data_alltypes[date_tgt] = dict.fromkeys(force12names,[])\n",
    "            # initiate the final data set\n",
    "            SameAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            SameAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            AcroAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            AcroAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            #\n",
    "            SameAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            SameAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            AcroAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            AcroAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt] = dict.fromkeys(force12names,[])\n",
    "            \n",
    "            # \n",
    "            for itrialtype in np.arange(0,ntrialtypes,1):\n",
    "                \n",
    "                bhv_df = [] # separate for each block\n",
    "                \n",
    "                force1_unique = force12_uniques[0,itrialtype]\n",
    "                force2_unique = force12_uniques[1,itrialtype]\n",
    "\n",
    "                ind = np.isin(lever1force_list,force1_unique) & np.isin(lever2force_list,force2_unique)\n",
    "\n",
    "                trialID_itrialtype = trialID_list[ind]\n",
    "\n",
    "                tasktype_itrialtype = np.unique(tasktype_list[ind])\n",
    "                coop_thres_itrialtype = np.unique(coop_thres_list[ind])\n",
    "\n",
    "                # analyze behavior results\n",
    "                bhv_data_itrialtype = bhv_data[np.isin(bhv_data['trial_number'],trialID_itrialtype)]\n",
    "\n",
    "                # block time\n",
    "                block_starttime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==0]['time_points'].iloc[0]\n",
    "                block_endtime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==9]['time_points'].iloc[-1]\n",
    "\n",
    "\n",
    "                #\n",
    "                # prepare the DBN input data\n",
    "                #\n",
    "                totalsess_time_ittype = block_endtime - block_starttime\n",
    "                session_start_time_ittype = 0\n",
    "                #\n",
    "                time_point_pull1_ittype = time_point_pull1[(time_point_pull1<block_endtime)&(time_point_pull1>block_starttime)]-block_starttime\n",
    "                time_point_pull2_ittype = time_point_pull2[(time_point_pull2<block_endtime)&(time_point_pull2>block_starttime)]-block_starttime\n",
    "                oneway_gaze1_ittype = oneway_gaze1[(oneway_gaze1<block_endtime)&(oneway_gaze1>block_starttime)]-block_starttime\n",
    "                oneway_gaze2_ittype = oneway_gaze2[(oneway_gaze2<block_endtime)&(oneway_gaze2>block_starttime)]-block_starttime\n",
    "                mutual_gaze1_ittype = mutual_gaze1[(mutual_gaze1<block_endtime)&(mutual_gaze1>block_starttime)]-block_starttime\n",
    "                mutual_gaze2_ittype = mutual_gaze2[(mutual_gaze2<block_endtime)&(mutual_gaze2>block_starttime)]-block_starttime\n",
    "                \n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time_ittype, \n",
    "                                                                       session_start_time_ittype, \n",
    "                                                                       temp_resolu, \n",
    "                                                                       time_point_pull1_ittype, time_point_pull2_ittype, \n",
    "                                                                       oneway_gaze1_ittype, oneway_gaze2_ittype, \n",
    "                                                                       mutual_gaze1_ittype, mutual_gaze2_ittype)\n",
    "                else:\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time_ittype, \n",
    "                                                                       session_start_time_ittype, \n",
    "                                                                       temp_resolu, \n",
    "                                                                       time_point_pull2_ittype, time_point_pull1_ittype, \n",
    "                                                                       oneway_gaze2_ittype, oneway_gaze1_ittype, \n",
    "                                                                       mutual_gaze2_ittype, mutual_gaze1_ittype)     \n",
    "\n",
    "                if len(bhv_df)==0:\n",
    "                    bhv_df = bhv_df_itr\n",
    "                else:\n",
    "                    bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                    bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    DBN_input_data_alltypes[date_tgt][str(force1_unique)+'&'+str(force2_unique)] = bhv_df\n",
    "                else:\n",
    "                    DBN_input_data_alltypes[date_tgt][str(force2_unique)+'&'+str(force1_unique)] = bhv_df\n",
    "                    \n",
    "                    \n",
    "                # \n",
    "                # calculate the distribution\n",
    "                #\n",
    "                for idistype in np.arange(0,4,1):\n",
    "                    #\n",
    "                    if idistype == 0:\n",
    "                        # pull1_t0 and gaze1_t0\n",
    "                        xxx1 = (np.array(bhv_df['pull1_t0'])==1)*1\n",
    "                        xxx2 = (np.array(bhv_df['owgaze1_t0'])==1)*1\n",
    "                    elif idistype == 1:\n",
    "                        # pull2_t0 and gaze2_t0\n",
    "                        xxx1 = (np.array(bhv_df['pull2_t0'])==1)*1\n",
    "                        xxx2 = (np.array(bhv_df['owgaze2_t0'])==1)*1\n",
    "                    elif idistype == 2:\n",
    "                        # pull1_t0 and gaze2_t0\n",
    "                        xxx1 = (np.array(bhv_df['pull1_t0'])==1)*1\n",
    "                        xxx2 = (np.array(bhv_df['owgaze2_t0'])==1)*1\n",
    "                    elif idistype == 3:\n",
    "                        # pull2_t0 and gaze1_t0\n",
    "                        xxx1 = (np.array(bhv_df['pull2_t0'])==1)*1\n",
    "                        xxx2 = (np.array(bhv_df['owgaze1_t0'])==1)*1\n",
    "                        \n",
    "                    #   \n",
    "                    xxx1_shuffle = xxx1.copy()\n",
    "                    np.random.shuffle(xxx1_shuffle)\n",
    "                    xxx2_shuffle = xxx2.copy()\n",
    "                    np.random.shuffle(xxx2_shuffle)\n",
    "                    # pad the two sides\n",
    "                    xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                    xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                    xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                    xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                    # \n",
    "                    npulls = int(np.nansum(xxx1))\n",
    "                    pullIDs = np.where(xxx1 == 1)[0]\n",
    "                    gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                    #\n",
    "                    for ipull in np.arange(0,npulls,1):\n",
    "                        pullID = pullIDs[ipull]\n",
    "                        gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                    #\n",
    "                    if idistype == 0:\n",
    "                        # pull1_t0 and gaze1_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            SameAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            SameAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                     \n",
    "                    elif idistype == 1:\n",
    "                        # pull2_t0 and gaze2_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            SameAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            SameAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                                        \n",
    "                    elif idistype == 2:\n",
    "                        # pull1_t0 and gaze2_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            AcroAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            AcroAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_mean_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                                                     \n",
    "                    elif idistype == 3:\n",
    "                        # pull2_t0 and gaze1_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            AcroAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            AcroAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_mean_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                                                     \n",
    "                    \n",
    "                    #       \n",
    "                    # shuffle\n",
    "                    npulls = int(np.nansum(xxx1_shuffle))\n",
    "                    pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                    gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                    #\n",
    "                    for ipull in np.arange(0,npulls,1):\n",
    "                        pullID = pullIDs[ipull]\n",
    "                        gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                    \n",
    "                    #\n",
    "                    if idistype == 0:\n",
    "                        # pull1_t0 and gaze1_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            SameAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            SameAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                     \n",
    "                    elif idistype == 1:\n",
    "                        # pull2_t0 and gaze2_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            SameAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            SameAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                SameAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                                        \n",
    "                    elif idistype == 2:\n",
    "                        # pull1_t0 and gaze2_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            AcroAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            AcroAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_shuffle_all[animal2_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                                                     \n",
    "                    elif idistype == 3:\n",
    "                        # pull2_t0 and gaze1_t0\n",
    "                        if np.isin(animal1,animal1_fixedorder):\n",
    "                            AcroAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force1_unique)+'&'+str(force2_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                        else:\n",
    "                            AcroAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(xxx2)/np.nansum(xxx1))#//(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.nansum(xxx2)/np.nansum(xxx1))\n",
    "                            if npulls == 0:\n",
    "                                AcroAnimal_gazeDist_shuffle_all[animal1_fixedorder[0]][date_tgt][str(force2_unique)+'&'+str(force1_unique)]=np.ones((1,2*dist_twin_range+1))[0]*np.nan                                                     \n",
    "                                 \n",
    "            \n",
    "        # save data\n",
    "        if 1:\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "            if not mergetempRos:\n",
    "                if doBhvitv_timebin:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(DBN_input_data_alltypes, f)\n",
    "                    #\n",
    "                    with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(SameAnimal_gazeDist_mean_all, f)\n",
    "                    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(AcroAnimal_gazeDist_mean_all, f)\n",
    "                    with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(SameAnimal_gazeDist_shuffle_all, f)\n",
    "                    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(AcroAnimal_gazeDist_shuffle_all, f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(DBN_input_data_alltypes, f)\n",
    "                    #\n",
    "                    with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(SameAnimal_gazeDist_mean_all, f)\n",
    "                    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(AcroAnimal_gazeDist_mean_all, f)\n",
    "                    with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(SameAnimal_gazeDist_shuffle_all, f)\n",
    "                    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(AcroAnimal_gazeDist_shuffle_all, f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)     \n",
    "                #\n",
    "                with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(SameAnimal_gazeDist_mean_all, f)\n",
    "                with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(AcroAnimal_gazeDist_mean_all, f)\n",
    "                with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(SameAnimal_gazeDist_shuffle_all, f)\n",
    "                with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(AcroAnimal_gazeDist_shuffle_all, f)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c027c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53eeedc0",
   "metadata": {},
   "source": [
    "#### plot the  distribution of gaze before and after pulls\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze\n",
    "#### regress (>2) or connect (2) trial types in each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data for plot\n",
    "mergetempRos = 0\n",
    "doBhvitv_timebin = 0\n",
    "\n",
    "temp_resolu = 1 # temporal resolution - 1s\n",
    "\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "\n",
    "# load the gaze distribution data\n",
    "if not mergetempRos:\n",
    "    if doBhvitv_timebin:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes =pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes=pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes=pickle.load(f) \n",
    "    #\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "\n",
    "\n",
    "# initialize for plot\n",
    "animals_forplot = [animal1_fixedorder[0],animal2_fixedorder[0]]\n",
    "nanimals_forplot = np.shape(animals_forplot)[0]\n",
    "\n",
    "nplottypes = 2 # SameAnimal or AcroAnimal\n",
    "\n",
    "fig, axs = plt.subplots(nanimals_forplot*nplottypes*2,3)\n",
    "fig.set_figheight(7*nanimals_forplot*nplottypes*2)\n",
    "fig.set_figwidth(7*3)\n",
    "\n",
    "# for plot each day separately\n",
    "ind_forplot = (trialnum_all_dates>3)  # the minimal trial number in each trial type (force level combination)\n",
    "#\n",
    "trialdates_all_dates_forplot = trialdates_all_dates[ind_forplot]\n",
    "trialdates_all_dates_unique = np.unique(trialdates_all_dates_forplot)\n",
    "ntrialdates_forplot = np.shape(trialdates_all_dates_unique)[0]\n",
    "\n",
    "#\n",
    "for ianimal in np.arange(0,nanimals_forplot,1):\n",
    "    \n",
    "    animal_forplot = animals_forplot[ianimal]\n",
    "    \n",
    "    # same animal vs across animal\n",
    "    for iplottype in np.arange(0,nplottypes,1):\n",
    "            \n",
    "        if iplottype == 0:\n",
    "            target_mean_all = SameAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = SameAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Same Animal'\n",
    "        #    \n",
    "        elif iplottype == 1:\n",
    "            target_mean_all = AcroAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = AcroAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Across Animal'\n",
    "    \n",
    "        # initialize the peak time and peak distribution array for plot\n",
    "        ndatalist = np.shape(force1_all_dates)[0]\n",
    "        peaktime_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        peakdist_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        #\n",
    "        slopes1_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        slopes2_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        slopes3_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "\n",
    "        # prepare the data for distribution peak time and peak distribution\n",
    "        for idatalist in np.arange(0,ndatalist,1):\n",
    "            \n",
    "            iforce1 = int(force1_all_dates[idatalist])\n",
    "            iforce2 = int(force2_all_dates[idatalist])\n",
    "            idate = trialdates_all_dates[idatalist]\n",
    "            \n",
    "            try:\n",
    "                peaktime_all_dates[idatalist] = np.nanargmax(target_mean_all[animal_forplot][idate][str(iforce1)+'&'+str(iforce2)])\n",
    "                peakdist_all_dates[idatalist] = np.nanmax(target_mean_all[animal_forplot][idate][str(iforce1)+'&'+str(iforce2)])\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # plot         \n",
    "        # peak time or peak distibution\n",
    "        for ipeaktimedist in np.arange(0,2,1):\n",
    "        \n",
    "            isubplotID = int(str(ianimal*100+iplottype*10+ipeaktimedist),2)\n",
    "            \n",
    "            # plot, against force1, force2 and block order, separate for each date\n",
    "            # plot each day separately\n",
    "            for idate in np.arange(0,ntrialdates_forplot,1):\n",
    "                trialdate_idate = trialdates_all_dates_unique[idate]\n",
    "\n",
    "                ind_idate = np.isin(trialdates_all_dates_forplot,trialdate_idate)\n",
    "\n",
    "                if np.sum(ind_idate)>0:\n",
    "                    \n",
    "                    if ipeaktimedist == 0:\n",
    "                        yyy = peaktime_all_dates[ind_forplot]\n",
    "                        ylimmin = 0\n",
    "                        ylimmax = 12\n",
    "                        ylabelname = 'peak time'\n",
    "                    elif ipeaktimedist == 1:\n",
    "                        yyy = peakdist_all_dates[ind_forplot]\n",
    "                        ylimmin = -0.2\n",
    "                        ylimmax = 1.2\n",
    "                        ylabelname = 'peak probability'\n",
    "                    yyy = yyy[ind_idate]\n",
    "                        \n",
    "                    # subplot 1\n",
    "                    xxx = force1_all_dates[ind_forplot]\n",
    "                    xxx = xxx[ind_idate]\n",
    "                    #\n",
    "                    slope, intercept, rr, pp, std_err = st.linregress(xxx, yyy)\n",
    "                    slopes1_all_dates[idate] = slope\n",
    "                    axs[isubplotID,0].plot(xxx,yyy,'o',color = '0.75')\n",
    "                    axs[isubplotID,0].plot(np.array([xxx.min(),xxx.max()]),\n",
    "                                           np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "                    axs[isubplotID,0].set_xlabel(animal1_fixedorder[0]+' lever force (grams)',fontsize=15)\n",
    "                    axs[isubplotID,0].set_ylabel(ylabelname,fontsize=15)\n",
    "                    axs[isubplotID,0].set_ylim(ylimmin,ylimmax)\n",
    "                    axs[isubplotID,0].set_title(animal_forplot+' '+plottype, fontsize=16)\n",
    "                    axs[isubplotID,0].set_xlim(60,1020)\n",
    "                           \n",
    "                    # subplot 2\n",
    "                    xxx = force2_all_dates[ind_forplot]\n",
    "                    xxx = xxx[ind_idate]\n",
    "                    #\n",
    "                    slope, intercept, rr, pp, std_err = st.linregress(xxx, yyy)\n",
    "                    slopes2_all_dates[idate] = slope\n",
    "                    axs[isubplotID,1].plot(xxx,yyy,'o',color = '0.75')\n",
    "                    axs[isubplotID,1].plot(np.array([xxx.min(),xxx.max()]),\n",
    "                                           np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "                    axs[isubplotID,1].set_xlabel(animal2_fixedorder[0]+' lever force (grams)',fontsize=15)\n",
    "                    axs[isubplotID,1].set_ylabel(ylabelname,fontsize=15)\n",
    "                    axs[isubplotID,1].set_ylim(ylimmin,ylimmax)\n",
    "                    axs[isubplotID,1].set_title(animal_forplot+' '+plottype, fontsize=16)\n",
    "                    axs[isubplotID,1].set_xlim(60,1020)\n",
    "\n",
    "                    # subplot 3\n",
    "                    xxx = subblockID_all_dates[ind_forplot]\n",
    "                    xxx = xxx[ind_idate]\n",
    "                    #\n",
    "                    slope, intercept, rr, pp, std_err = st.linregress(xxx, yyy)\n",
    "                    slopes3_all_dates[idate] = slope\n",
    "                    axs[isubplotID,2].plot(xxx,yyy,'o',color = '0.75')\n",
    "                    axs[isubplotID,2].plot(np.array([xxx.min(),xxx.max()]),\n",
    "                                           np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "                    axs[isubplotID,2].set_xlabel('subblock order in the session',fontsize=15)\n",
    "                    axs[isubplotID,2].set_ylabel(ylabelname,fontsize=15)\n",
    "                    axs[isubplotID,2].set_ylim(ylimmin,ylimmax)\n",
    "                    axs[isubplotID,2].set_title(animal_forplot+' '+plottype, fontsize=16)\n",
    "                    axs[isubplotID,2].set_xlim(-1,5)\n",
    "           \n",
    "            # add the statitics for the slope\n",
    "            for iplot in np.arange(0,3,1):\n",
    "                try:\n",
    "                    # subplot 1\n",
    "                    if iplot == 0:\n",
    "                        pp = st.wilcoxon(slopes1_all_dates[~np.isnan(slopes1_all_dates)], zero_method='wilcox', correction=False, alternative='two-sided').pvalue\n",
    "                        axs[isubplotID,0].text(300,ylimmax*0.8,'Wilcoxon p='+\"{:.2f}\".format(pp),fontsize=10)\n",
    "                    # subplot 2\n",
    "                    elif iplot == 1:\n",
    "                        pp = st.wilcoxon(slopes2_all_dates[~np.isnan(slopes2_all_dates)], zero_method='wilcox', correction=False, alternative='two-sided').pvalue    \n",
    "                        axs[isubplotID,1].text(300,ylimmax*0.8,'Wilcoxon p='+\"{:.2f}\".format(pp),fontsize=10)\n",
    "                    # subplot 3\n",
    "                    elif iplot == 2:\n",
    "                        pp = st.wilcoxon(slopes3_all_dates[~np.isnan(slopes3_all_dates)], zero_method='wilcox', correction=False, alternative='two-sided').pvalue\n",
    "                        axs[isubplotID,2].text(2.0,ylimmax*0.8,'Wilcoxon p='+\"{:.2f}\".format(pp),fontsize=10)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"forcelevel_gazeDistAroundPullpeakProbTime_separatedays_in_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab541ea",
   "metadata": {},
   "source": [
    "#### plot the  distribution of gaze before and after pulls\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze\n",
    "#### regress against subblock ID first, and then plot the residual against force1 and force2 and separate each day (session);\n",
    "#### regress (>2) or connect (2) trial types in each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f500f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd85421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mergetempRos = 0\n",
    "doBhvitv_timebin = 0\n",
    "\n",
    "temp_resolu = 1 # temporal resolution - 1s\n",
    "\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "\n",
    "# load the gaze distribution data\n",
    "if not mergetempRos:\n",
    "    if doBhvitv_timebin:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes =pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes=pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes=pickle.load(f) \n",
    "    #\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "\n",
    "\n",
    "# initialize for plot\n",
    "animals_forplot = [animal1_fixedorder[0],animal2_fixedorder[0]]\n",
    "nanimals_forplot = np.shape(animals_forplot)[0]\n",
    "\n",
    "nplottypes = 2 # SameAnimal or AcroAnimal\n",
    "\n",
    "fig, axs = plt.subplots(nanimals_forplot*nplottypes*2,2)\n",
    "fig.set_figheight(7*nanimals_forplot*nplottypes*2)\n",
    "fig.set_figwidth(7*2)\n",
    "\n",
    "# for plot each day separately\n",
    "ind_forplot = (trialnum_all_dates>3)  # the minimal trial number in each trial type (force level combination)\n",
    "#\n",
    "trialdates_all_dates_forplot = trialdates_all_dates[ind_forplot]\n",
    "trialdates_all_dates_unique = np.unique(trialdates_all_dates_forplot)\n",
    "ntrialdates_forplot = np.shape(trialdates_all_dates_unique)[0]\n",
    "\n",
    "#\n",
    "for ianimal in np.arange(0,nanimals_forplot,1):\n",
    "    \n",
    "    animal_forplot = animals_forplot[ianimal]\n",
    "    \n",
    "    # same animal vs across animal\n",
    "    for iplottype in np.arange(0,nplottypes,1):\n",
    "            \n",
    "        if iplottype == 0:\n",
    "            target_mean_all = SameAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = SameAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Same Animal'\n",
    "        #    \n",
    "        elif iplottype == 1:\n",
    "            target_mean_all = AcroAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = AcroAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Across Animal'\n",
    "    \n",
    "        # initialize the peak time and peak distribution array for plot\n",
    "        ndatalist = np.shape(force1_all_dates)[0]\n",
    "        peaktime_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        peakdist_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        #\n",
    "        slopes1_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        slopes2_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        slopes3_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "\n",
    "        # prepare the data for distribution peak time and peak distribution\n",
    "        for idatalist in np.arange(0,ndatalist,1):\n",
    "            \n",
    "            iforce1 = int(force1_all_dates[idatalist])\n",
    "            iforce2 = int(force2_all_dates[idatalist])\n",
    "            idate = trialdates_all_dates[idatalist]\n",
    "            \n",
    "            try:\n",
    "                peaktime_all_dates[idatalist] = np.nanargmax(target_mean_all[animal_forplot][idate][str(iforce1)+'&'+str(iforce2)])\n",
    "                peakdist_all_dates[idatalist] = np.nanmax(target_mean_all[animal_forplot][idate][str(iforce1)+'&'+str(iforce2)])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # plot         \n",
    "        # peak time or peak distibution\n",
    "        for ipeaktimedist in np.arange(0,2,1):\n",
    "        \n",
    "            isubplotID = int(str(ianimal*100+iplottype*10+ipeaktimedist),2)\n",
    "            \n",
    "            if ipeaktimedist == 0:\n",
    "                yyy = peaktime_all_dates[ind_forplot]\n",
    "                ylimmin = -10\n",
    "                ylimmax = 10\n",
    "                ylabelname = 'peak time'\n",
    "            elif ipeaktimedist == 1:\n",
    "                yyy = peakdist_all_dates[ind_forplot]\n",
    "                ylimmin = -0.8\n",
    "                ylimmax = 0.8\n",
    "                ylabelname = 'peak probability'\n",
    "            \n",
    "            # regress again subblock ID first\n",
    "            xxx = subblockID_all_dates[ind_forplot]\n",
    "            slope, intercept, rr, pp, std_err = st.linregress(xxx, yyy)\n",
    "            # \n",
    "            yyy_res = yyy - (xxx*slope+intercept)\n",
    "            \n",
    "            # plot, against force1, force2 with the subblock ID regressed residue\n",
    "            # plot each day separately\n",
    "            for idate in np.arange(0,ntrialdates_forplot,1):\n",
    "                trialdate_idate = trialdates_all_dates_unique[idate]\n",
    "\n",
    "                ind_idate = np.isin(trialdates_all_dates_forplot,trialdate_idate)\n",
    "\n",
    "                if np.sum(ind_idate)>0:\n",
    "                    \n",
    "                    yyy_idate = yyy_res[ind_idate]\n",
    "                        \n",
    "                    # subplot 1\n",
    "                    xxx = force1_all_dates[ind_forplot]\n",
    "                    xxx = xxx[ind_idate]\n",
    "                    #\n",
    "                    slope, intercept, rr, pp, std_err = st.linregress(xxx, yyy_idate)\n",
    "                    slopes1_all_dates[idate] = slope\n",
    "                    axs[isubplotID,0].plot(xxx,yyy_idate,'o',color = '0.75')\n",
    "                    axs[isubplotID,0].plot(np.array([xxx.min(),xxx.max()]),\n",
    "                                           np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "                    axs[isubplotID,0].set_xlabel(animal1_fixedorder[0]+' lever force (grams)',fontsize=15)\n",
    "                    axs[isubplotID,0].set_ylabel(ylabelname,fontsize=15)\n",
    "                    axs[isubplotID,0].set_ylim(ylimmin,ylimmax)\n",
    "                    axs[isubplotID,0].set_title(animal_forplot+' '+plottype, fontsize=16)\n",
    "                    axs[isubplotID,0].set_xlim(60,1020)\n",
    "                           \n",
    "                    # subplot 2\n",
    "                    xxx = force2_all_dates[ind_forplot]\n",
    "                    xxx = xxx[ind_idate]\n",
    "                    #\n",
    "                    slope, intercept, rr, pp, std_err = st.linregress(xxx, yyy_idate)\n",
    "                    slopes2_all_dates[idate] = slope\n",
    "                    axs[isubplotID,1].plot(xxx,yyy_idate,'o',color = '0.75')\n",
    "                    axs[isubplotID,1].plot(np.array([xxx.min(),xxx.max()]),\n",
    "                                           np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "                    axs[isubplotID,1].set_xlabel(animal2_fixedorder[0]+' lever force (grams)',fontsize=15)\n",
    "                    axs[isubplotID,1].set_ylabel(ylabelname,fontsize=15)\n",
    "                    axs[isubplotID,1].set_ylim(ylimmin,ylimmax)\n",
    "                    axs[isubplotID,1].set_title(animal_forplot+' '+plottype, fontsize=16)\n",
    "                    axs[isubplotID,1].set_xlim(60,1020)\n",
    "\n",
    "                   \n",
    "           \n",
    "            # add the statitics for the slope\n",
    "            for iplot in np.arange(0,3,1):\n",
    "                try:\n",
    "                    # subplot 1\n",
    "                    if iplot == 0:\n",
    "                        pp = st.wilcoxon(slopes1_all_dates[~np.isnan(slopes1_all_dates)], zero_method='wilcox', correction=False, alternative='two-sided').pvalue\n",
    "                        axs[isubplotID,0].text(300,ylimmax*0.8,'Wilcoxon p='+\"{:.2f}\".format(pp),fontsize=10)\n",
    "                    # subplot 2\n",
    "                    elif iplot == 1:\n",
    "                        pp = st.wilcoxon(slopes2_all_dates[~np.isnan(slopes2_all_dates)], zero_method='wilcox', correction=False, alternative='two-sided').pvalue    \n",
    "                        axs[isubplotID,1].text(300,ylimmax*0.8,'Wilcoxon p='+\"{:.2f}\".format(pp),fontsize=10)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"subblockIDregressedResidual_forcelevel_gazeDistAroundPullpeakProbTime_separatedays_in_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129faecf",
   "metadata": {},
   "source": [
    "#### normalize force1 force2 and subblockID for each day (session); then do multi-variable regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc87b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data for plot\n",
    "mergetempRos = 0\n",
    "doBhvitv_timebin = 0\n",
    "\n",
    "temp_resolu = 1 # temporal resolution - 1s\n",
    "\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "\n",
    "# load the gaze distribution data\n",
    "if not mergetempRos:\n",
    "    if doBhvitv_timebin:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes =pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes=pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes=pickle.load(f) \n",
    "    #\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "\n",
    "\n",
    "# initialize for plot\n",
    "animals_forplot = [animal1_fixedorder[0],animal2_fixedorder[0]]\n",
    "nanimals_forplot = np.shape(animals_forplot)[0]\n",
    "\n",
    "nplottypes = 2 # SameAnimal or AcroAnimal\n",
    "\n",
    "fig, axs = plt.subplots(nanimals_forplot*nplottypes*2,3)\n",
    "fig.set_figheight(7*nanimals_forplot*nplottypes*2)\n",
    "fig.set_figwidth(7*3)\n",
    "\n",
    "# for plot each day separately\n",
    "ind_forplot = (trialnum_all_dates>3)  # the minimal trial number in each trial type (force level combination)\n",
    "#\n",
    "trialdates_all_dates_forplot = trialdates_all_dates[ind_forplot]\n",
    "trialdates_all_dates_unique = np.unique(trialdates_all_dates_forplot)\n",
    "ntrialdates_forplot = np.shape(trialdates_all_dates_unique)[0]\n",
    "\n",
    "#\n",
    "for ianimal in np.arange(0,nanimals_forplot,1):\n",
    "    \n",
    "    animal_forplot = animals_forplot[ianimal]\n",
    "    \n",
    "    # same animal vs across animal\n",
    "    for iplottype in np.arange(0,nplottypes,1):\n",
    "            \n",
    "        if iplottype == 0:\n",
    "            target_mean_all = SameAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = SameAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Same Animal'\n",
    "        #    \n",
    "        elif iplottype == 1:\n",
    "            target_mean_all = AcroAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = AcroAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Across Animal'\n",
    "    \n",
    "        # initialize the peak time and peak distribution array for plot\n",
    "        ndatalist = np.shape(force1_all_dates)[0]\n",
    "        peaktime_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        peakdist_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        #\n",
    "        slopes1_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        slopes2_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "        slopes3_all_dates = np.ones(np.shape(force1_all_dates))*np.nan\n",
    "\n",
    "        # prepare the data for distribution peak time and peak distribution\n",
    "        for idatalist in np.arange(0,ndatalist,1):\n",
    "            \n",
    "            iforce1 = int(force1_all_dates[idatalist])\n",
    "            iforce2 = int(force2_all_dates[idatalist])\n",
    "            idate = trialdates_all_dates[idatalist]\n",
    "            \n",
    "            try:\n",
    "                peaktime_all_dates[idatalist] = np.nanargmax(target_mean_all[animal_forplot][idate][str(iforce1)+'&'+str(iforce2)])\n",
    "                peakdist_all_dates[idatalist] = np.nanmax(target_mean_all[animal_forplot][idate][str(iforce1)+'&'+str(iforce2)])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # plot         \n",
    "        # peak time or peak distibution\n",
    "        for ipeaktimedist in np.arange(0,2,1):\n",
    "        \n",
    "            isubplotID = int(str(ianimal*100+iplottype*10+ipeaktimedist),2)\n",
    "            \n",
    "            if ipeaktimedist == 0:\n",
    "                yyy = peaktime_all_dates[ind_forplot]\n",
    "                ylimmin = -10\n",
    "                ylimmax = 10\n",
    "                ylabelname = 'peak time'\n",
    "            elif ipeaktimedist == 1:\n",
    "                yyy = peakdist_all_dates[ind_forplot]\n",
    "                ylimmin = -0.8\n",
    "                ylimmax = 0.8\n",
    "                ylabelname = 'peak probability'\n",
    "            \n",
    "            xxx1 = force1_all_dates[ind_forplot]\n",
    "            xxx2 = force2_all_dates[ind_forplot]\n",
    "            xxx3 = subblockID_all_dates[ind_forplot]\n",
    "\n",
    "            yyy_norm = yyy\n",
    "            xxx1_norm = xxx1\n",
    "            xxx2_norm = xxx2\n",
    "            xxx3_norm = xxx3\n",
    "            \n",
    "            # normalize the variables separately for each day\n",
    "            for idate in np.arange(0,ntrialdates_forplot,1):\n",
    "                trialdate_idate = trialdates_all_dates_unique[idate]\n",
    "\n",
    "                ind_idate = np.isin(trialdates_all_dates_forplot,trialdate_idate)\n",
    "\n",
    "                yyy_idate = yyy[ind_idate]\n",
    "                xxx1_idate = xxx1[ind_idate]\n",
    "                xxx2_idate = xxx2[ind_idate]\n",
    "                xxx3_idate = xxx3[ind_idate]\n",
    "                #\n",
    "                xxx1_norm[ind_idate]=(xxx1_idate-np.nanmin(xxx1_idate))/(np.nanmax(xxx1_idate)-np.nanmin(xxx1_idate))\n",
    "                xxx2_norm[ind_idate]=(xxx2_idate-np.nanmin(xxx2_idate))/(np.nanmax(xxx2_idate)-np.nanmin(xxx2_idate))\n",
    "                xxx3_norm[ind_idate]=(xxx3_idate-np.nanmin(xxx3_idate))/(np.nanmax(xxx3_idate)-np.nanmin(xxx3_idate))\n",
    "                   \n",
    "           \n",
    "            # plot three kinds of x variables\n",
    "            xlabels_all = [animal1_fixedorder[0]+' lever force (grams)',\n",
    "                           animal2_fixedorder[0]+' lever force (grams)',\n",
    "                           'subblock order in the session',\n",
    "                          ]\n",
    "\n",
    "            for isubplot in np.arange(0,3,1): \n",
    "\n",
    "                if isubplot == 0:\n",
    "                    xxx_iplot = xxx1_norm\n",
    "                elif isubplot == 1:\n",
    "                    xxx_iplot = xxx2_norm\n",
    "                elif isubplot == 2:\n",
    "                    xxx_iplot = xxx3_norm\n",
    "\n",
    "                try:\n",
    "                    # subplot 1,2,3\n",
    "                    slope, intercept, rr, pp, std_err = st.linregress(xxx_iplot[~np.isnan(xxx_iplot)], yyy_norm[[~np.isnan(xxx_iplot)]])\n",
    "                    axs[isubplotID,isubplot].text(0.4,ylimmax*0.8,'regression r='+\"{:.2f}\".format(rr),fontsize=15)\n",
    "                    axs[isubplotID,isubplot].text(0.4,ylimmax*0.7,'regression p='+\"{:.2f}\".format(pp),fontsize=15)\n",
    "                    axs[isubplotID,isubplot].plot(xxx_iplot,yyy_norm,'ko')\n",
    "                    axs[isubplotID,isubplot].plot(np.array([np.nanmin(xxx_iplot),np.nanmax(xxx_iplot)]),\n",
    "                                             np.array([np.nanmin(xxx_iplot),np.nanmax(xxx_iplot)])*slope+intercept,'k-')\n",
    "                    axs[isubplotID,isubplot].set_xlabel(xlabels_all[isubplot],fontsize=15)\n",
    "                    axs[isubplotID,isubplot].set_ylabel(ylabelname,fontsize=15)\n",
    "                    axs[isubplotID,isubplot].set_title(animal_forplot+' '+plottype, fontsize=16)\n",
    "                    axs[isubplotID,isubplot].set_ylim(ylimmin,ylimmax)\n",
    "                    axs[isubplotID,isubplot].set_xlim(-0.2,1.2)\n",
    "    \n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "fig.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"forcelevel_successfulrate_pullnumbers_normalized_in_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7561cd",
   "metadata": {},
   "source": [
    "#### for each day separate three subblocks and then average the pull aligned gaze distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data for plot\n",
    "mergetempRos = 0\n",
    "doBhvitv_timebin = 0\n",
    "\n",
    "temp_resolu = 1 # temporal resolution - 1s\n",
    "\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "\n",
    "# load the gaze distribution data\n",
    "if not mergetempRos:\n",
    "    if doBhvitv_timebin:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes =pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes=pickle.load(f)\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes=pickle.load(f) \n",
    "    #\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_mean_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_mean_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/SameAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        SameAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/AcroAnimal_gazeDist_shuffle_all_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        AcroAnimal_gazeDist_shuffle_all=pickle.load(f)\n",
    "\n",
    "\n",
    "# initialize for plot\n",
    "animals_forplot = [animal1_fixedorder[0],animal2_fixedorder[0]]\n",
    "nanimals_forplot = np.shape(animals_forplot)[0]\n",
    "\n",
    "nplottypes = 2 # SameAnimal or AcroAnimal\n",
    "\n",
    "fig, axs = plt.subplots(nanimals_forplot*nplottypes,3)\n",
    "fig.set_figheight(7*nanimals_forplot*nplottypes)\n",
    "fig.set_figwidth(7*3)\n",
    "\n",
    "# for plot each day separately\n",
    "trialdates_all_dates_unique = np.unique(trialdates_all_dates)\n",
    "ntrialdates_forplot = np.shape(trialdates_all_dates_unique)[0]\n",
    "\n",
    "#\n",
    "for ianimal in np.arange(0,nanimals_forplot,1):\n",
    "    \n",
    "    animal_forplot = animals_forplot[ianimal]\n",
    "    \n",
    "    # same animal vs across animal\n",
    "    for iplottype in np.arange(0,nplottypes,1):\n",
    "            \n",
    "        isubplotID = int(str(ianimal*10+iplottype),2)\n",
    "            \n",
    "        if iplottype == 0:\n",
    "            target_mean_all = SameAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = SameAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Same Animal'\n",
    "        #    \n",
    "        elif iplottype == 1:\n",
    "            target_mean_all = AcroAnimal_gazeDist_mean_all\n",
    "            target_shuffle_all = AcroAnimal_gazeDist_shuffle_all\n",
    "            #\n",
    "            plottype = 'Across Animal'\n",
    "    \n",
    "        # for each day, separate the blocked into three categories, for three different variables - force1, force2, subblockID\n",
    "        for ixplottype in np.arange(0,3,1):\n",
    "            \n",
    "            if ixplottype == 0:\n",
    "                xplotvalue_all_dates = force1_all_dates\n",
    "                xplotname = 'animal 1 force'\n",
    "            elif ixplottype == 1:\n",
    "                xplotvalue_all_dates = force2_all_dates\n",
    "                xplotname = 'animal 2 force'\n",
    "            elif ixplottype == 2:\n",
    "                xplotvalue_all_dates = subblockID_all_dates\n",
    "                xplotname = 'subblock number'\n",
    "            \n",
    "            # initialize, across days, separate based on the xplotvalue's quantiles\n",
    "            distarget_mean_all_q1 = np.zeros((0,))\n",
    "            distarget_mean_all_q2 = np.zeros((0,))\n",
    "            distarget_mean_all_q3 = np.zeros((0,))\n",
    "            distarget_shuffle_all_q1 = np.zeros((0,))\n",
    "            distarget_shuffle_all_q2 = np.zeros((0,))\n",
    "            distarget_shuffle_all_q3 = np.zeros((0,))\n",
    "                    \n",
    "            for idatenum in np.arange(0,ntrialdates_forplot,1):\n",
    "\n",
    "                idate = trialdates_all_dates_unique[idatenum]\n",
    "\n",
    "                ind_idate = np.isin(trialdates_all_dates,idate)    \n",
    "\n",
    "                xplotvalue_idate = xplotvalue_all_dates[ind_idate]\n",
    "\n",
    "                disttarget_mean_idate = np.vstack(list(target_mean_all[animal_forplot][idate].values()))\n",
    "                disttarget_shuffle_idate = np.vstack(list(target_mean_all[animal_forplot][idate].values()))\n",
    "\n",
    "                # separate the three quantiles\n",
    "                ind_q1 = xplotvalue_idate<=np.quantile(xplotvalue_idate,1/3)\n",
    "                ind_q2 = (xplotvalue_idate<=np.quantile(xplotvalue_idate,2/3))&(xplotvalue_idate>np.quantile(xplotvalue_idate,1/3))\n",
    "                ind_q3 = (xplotvalue_idate<=np.quantile(xplotvalue_idate,3/3))&(xplotvalue_idate>np.quantile(xplotvalue_idate,2/3))\n",
    "                #\n",
    "                if np.shape(distarget_mean_all_q1)[0]==0:\n",
    "                    distarget_mean_all_q1 = disttarget_mean_idate[ind_q1]\n",
    "                    distarget_mean_all_q2 = disttarget_mean_idate[ind_q2]\n",
    "                    distarget_mean_all_q3 = disttarget_mean_idate[ind_q3]\n",
    "                    distarget_shuffle_all_q1 = disttarget_shuffle_idate[ind_q1]\n",
    "                    distarget_shuffle_all_q2 = disttarget_shuffle_idate[ind_q2]\n",
    "                    distarget_shuffle_all_q3 = disttarget_shuffle_idate[ind_q3]\n",
    "                else:\n",
    "                    distarget_mean_all_q1 = np.vstack((distarget_mean_all_q1,disttarget_mean_idate[ind_q1]))\n",
    "                    distarget_mean_all_q2 = np.vstack((distarget_mean_all_q2,disttarget_mean_idate[ind_q2]))\n",
    "                    distarget_mean_all_q3 = np.vstack((distarget_mean_all_q3,disttarget_mean_idate[ind_q3]))\n",
    "                    distarget_shuffle_all_q1 = np.vstack((distarget_shuffle_all_q1,disttarget_shuffle_idate[ind_q1]))\n",
    "                    distarget_shuffle_all_q2 = np.vstack((distarget_shuffle_all_q2,disttarget_shuffle_idate[ind_q2]))\n",
    "                    distarget_shuffle_all_q3 = np.vstack((distarget_shuffle_all_q3,disttarget_shuffle_idate[ind_q3]))\n",
    "        \n",
    "            # for plot\n",
    "            xxx = np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "            #\n",
    "            yyy_q1 = np.nanmean(distarget_mean_all_q1,axis=0) \n",
    "            yyy_q1_se = np.nanstd(distarget_mean_all_q1,axis=0)/np.sqrt(np.shape(distarget_mean_all_q1)[0])\n",
    "            yyy_q2 = np.nanmean(distarget_mean_all_q2,axis=0) \n",
    "            yyy_q2_se = np.nanstd(distarget_mean_all_q2,axis=0)/np.sqrt(np.shape(distarget_mean_all_q2)[0])\n",
    "            yyy_q3 = np.nanmean(distarget_mean_all_q3,axis=0) \n",
    "            yyy_q3_se = np.nanstd(distarget_mean_all_q3,axis=0)/np.sqrt(np.shape(distarget_mean_all_q3)[0])\n",
    "            #\n",
    "            axs[isubplotID,ixplottype].errorbar(xxx,yyy_q1,yyy_q1_se,label='low force/value')\n",
    "            axs[isubplotID,ixplottype].errorbar(xxx,yyy_q2,yyy_q2_se,label='mid force/value')\n",
    "            axs[isubplotID,ixplottype].errorbar(xxx,yyy_q3,yyy_q3_se,label='high force/value')\n",
    "            axs[isubplotID,ixplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[isubplotID,ixplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[isubplotID,ixplottype].set_ylim(0,1)\n",
    "            axs[isubplotID,ixplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[isubplotID,ixplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "            axs[isubplotID,ixplottype].legend()   \n",
    "            axs[isubplotID,ixplottype].set_title(animal_forplot+' '+plottype+'; '+xplotname,fontsize=16) \n",
    "            \n",
    "        \n",
    "fig.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazeProb_pull_aligned_forceQuantiles_in_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(distarget_mean_all_q1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "xplotvalue_idate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "disttarget_mean_idate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a434a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a743731",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7d323",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d13d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "        if minmaxfullSampSize:\n",
    "            # key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            # key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            # key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            # min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            # min_samplesize = int(min_samplesize/100)*100\n",
    "            # max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            # max_samplesize = int(max_samplesize/100)*100\n",
    "            #samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            #samplingsizes_name = ['min_row_number','max_row_number','full_row_number']\n",
    "            samplingsizes = [np.nan]\n",
    "            samplingsizes_name = ['full_row_number']\n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        # for jj in np.arange(0,nsamplings,1):\n",
    "        for jj in np.arange(0,1,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                # get different force level pairs\n",
    "                forcepairs_idate = list(DBN_input_data_alltypes[date_tgt].keys())\n",
    "                nforcepairs = np.shape(forcepairs_idate)[0]\n",
    "                \n",
    "                DAGs_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                DAGs_shuffle_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                DAGs_scores_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                DAGs_shuffle_scores_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "\n",
    "                weighted_graphs_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                weighted_graphs_shuffled_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                sig_edges_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "            \n",
    "                for iforcepair in np.arange(0,nforcepairs,1):\n",
    "                \n",
    "                    forcepair = forcepairs_idate[iforcepair]\n",
    "                \n",
    "                    if samplingsizes_name[jj]=='full_row_number':\n",
    "                        isamplingsize = np.shape(DBN_input_data_alltypes[date_tgt][forcepair])[0]\n",
    "\n",
    "                    #\n",
    "                    bhv_df_all = DBN_input_data_alltypes[date_tgt][forcepair]\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                    colnames = list(bhv_df_all.columns)\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = np.shape(from_pops)[0]\n",
    "                    nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[date_tgt][forcepair] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[date_tgt][forcepair] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt][forcepair] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt][forcepair] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt][forcepair],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt][forcepair],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt][forcepair] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt][forcepair] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[date_tgt][forcepair] = sig_edges\n",
    "                    \n",
    "                \n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "    print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647783a",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alls = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "        if minmaxfullSampSize:\n",
    "            # key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            # key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            # key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            # min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            # min_samplesize = int(min_samplesize/100)*100\n",
    "            # max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            # max_samplesize = int(max_samplesize/100)*100\n",
    "            # # samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            # # samplingsizes_name = ['min_row_number','max_row_number','full_row_number']   \n",
    "            samplingsizes = [np.nan]\n",
    "            samplingsizes_name = ['full_row_number']\n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                # get different force level pairs\n",
    "                forcepairs_idate = list(DBN_input_data_alltypes[date_tgt].keys())\n",
    "                nforcepairs = np.shape(forcepairs_idate)[0]\n",
    "                \n",
    "                DAGs_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                DAGs_shuffle_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                DAGs_scores_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                DAGs_shuffle_scores_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "\n",
    "                weighted_graphs_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                weighted_graphs_shuffled_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "                sig_edges_alltypes[date_tgt] = dict.fromkeys(forcepairs_idate, [])\n",
    "            \n",
    "                for iforcepair in np.arange(0,nforcepairs,1):\n",
    "                \n",
    "                    forcepair = forcepairs_idate[iforcepair]\n",
    "\n",
    "                    if samplingsizes_name[jj]=='full_row_number':\n",
    "                        isamplingsize = np.shape(DBN_input_data_allsessions[date_tgt][forcepair])[0]\n",
    "\n",
    "                    # try:\n",
    "                    bhv_df_all = DBN_input_data_alltypes[date_tgt][forcepair]\n",
    "\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                    colnames = list(bhv_df_all.columns)\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = np.shape(from_pops)[0]\n",
    "                    nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[date_tgt][forcepair] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[date_tgt][forcepair] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt][forcepair] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt][forcepair] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt][forcepair],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt][forcepair],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt][forcepair] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt][forcepair] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[date_tgt][forcepair] = sig_edges\n",
    "                    \n",
    "                    # except:\n",
    "                        # DAGs_alltypes[date_tgt] = [] \n",
    "                        # DAGs_shuffle_alltypes[date_tgt] = []\n",
    "\n",
    "                        # DAGs_scores_alltypes[date_tgt] = []\n",
    "                        # DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "\n",
    "                        # weighted_graphs_alltypes[date_tgt] = []\n",
    "                        # weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                        # sig_edges_alltypes[date_tgt] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    save_data = 0\n",
    "    if save_data:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if moreSampSize:  \n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "        elif minmaxfullSampSize:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)        \n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+forceManiType+'.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb83aa",
   "metadata": {},
   "source": [
    "### plot the edges over time (session)\n",
    "#### mean edge weights of selected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30602b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(10*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        idate_name = dates_list_sorted[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_mean_forplot_all_dates,'ko',markersize=10)\n",
    "    #axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_shuffled_mean_forplot_all_dates,'bo',markersize=10)\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    axs.flatten()[i_edge].set_xlim([-0.5,ndates_sorted-0.5])\n",
    "    #\n",
    "    if i_edge > int(n_edges-1):\n",
    "        axs.flatten()[i_edge].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs.flatten()[i_edge].set_xticklabels(dates_list_sorted, rotation=90,fontsize=10)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweight_acrossAllSessions_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56642be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_graphs_diffTempRo_diffSampSize[('1','full_row_number')].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5ffd",
   "metadata": {},
   "source": [
    "#### mean edge weights of selected edges v.s. other behavioral measures\n",
    "##### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the targeted dates\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "dates_list_tgt = sorting_tgt_df['dates']\n",
    "dates_list_tgt = np.array(dates_list_tgt)\n",
    "#\n",
    "ndates_tgt = np.shape(dates_list_tgt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number'\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(5*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    if xplottype == 'succrate':\n",
    "        xxx = succ_rate_all_dates[sorting_tgt_df.index]\n",
    "    elif xplottype == 'meangazenum':   \n",
    "        xxx = gazemean_num_all_dates[sorting_tgt_df.index]\n",
    "    #     \n",
    "    yyy = edgeweight_mean_forplot_all_dates\n",
    "    #\n",
    "    rr_spe,pp_spe = scipy.stats.spearmanr(xxx, yyy)\n",
    "    slope, intercept, rr_reg, pp_reg, std_err = st.linregress(xxx.astype(float).T[0], yyy.astype(float).T[0])\n",
    "    #\n",
    "    axs.flatten()[i_edge].plot(xxx,yyy,'bo',markersize=8)\n",
    "    axs.flatten()[i_edge].plot(np.array([xxx.min(),xxx.max()]),np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    #\n",
    "    if i_edge > int(n_edges-3):\n",
    "        axs.flatten()[i_edge].set_xlabel(xplotlabel,fontsize=13)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    # axs.flatten()[i_edge].text(xxx.min(),1.0,'spearman r='+\"{:.2f}\".format(rr_spe),fontsize=10)\n",
    "    # axs.flatten()[i_edge].text(xxx.min(),0.9,'spearman p='+\"{:.2f}\".format(pp_spe),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),1.0,'regression r='+\"{:.2f}\".format(rr_reg),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.9,'regression p='+\"{:.2f}\".format(pp_reg),fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02539e5",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "####  mean edge weights of selected edges v.s. other behavioral measures\n",
    "##### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "# DBN analysis types\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meanpullnum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "# initiate the final data set\n",
    "edges_measure_slopes_all = np.zeros((nanimalpairs,n_edges))\n",
    "edges_measure_corrR_all = np.zeros((nanimalpairs,n_edges))\n",
    "edges_measure_regR_all = np.zeros((nanimalpairs,n_edges))\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    \n",
    "    # calculate the linear regression and correlation metrics for tgt edges\n",
    "    for i_edge in np.arange(0,n_edges,1):\n",
    "        #\n",
    "        edgeweight_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "        edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "        edgeweight_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "        edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "\n",
    "        edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "        fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "        toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "\n",
    "            edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "            edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "            edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "            edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "\n",
    "        # calculate correlation and linear regression\n",
    "        if xplottype == 'succrate':\n",
    "            xxx = succ_rate_all_dates[sorting_tgt_df.index]\n",
    "        elif xplottype == 'meangazenum':   \n",
    "            xxx = gazemean_num_all_dates[sorting_tgt_df.index]\n",
    "        elif xplottype == 'meanpullnum':   \n",
    "            xxx = pullmean_num_all_dates[sorting_tgt_df.index]\n",
    "        #     \n",
    "        yyy = edgeweight_mean_forplot_all_dates\n",
    "        #\n",
    "        rr_spe,pp_spe = scipy.stats.spearmanr(xxx, yyy)\n",
    "        slope, intercept, rr_reg, pp_reg, std_err = st.linregress(xxx.astype(float).T[0], yyy.astype(float).T[0])\n",
    "    \n",
    "        #\n",
    "        edges_measure_slopes_all[ianimalpair,i_edge] = slope\n",
    "        edges_measure_corrR_all[ianimalpair,i_edge] = rr_spe\n",
    "        edges_measure_regR_all[ianimalpair,i_edge] = rr_reg\n",
    "\n",
    "# plot\n",
    "edges_measure_slopes_all=np.vstack([edges_measure_slopes_all[:,np.arange(0,n_edges,2)],edges_measure_slopes_all[:,np.arange(1,n_edges,2)]])\n",
    "edges_measure_corrR_all=np.vstack([edges_measure_corrR_all[:,np.arange(0,n_edges,2)],edges_measure_corrR_all[:,np.arange(1,n_edges,2)]])\n",
    "edges_measure_regR_all=np.vstack([edges_measure_regR_all[:,np.arange(0,n_edges,2)],edges_measure_regR_all[:,np.arange(1,n_edges,2)]])\n",
    "\n",
    "# \n",
    "\n",
    "dependencytargets = ['pull-pull','within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency\n",
    "edge_measure_tgt_all = edges_measure_regR_all # regression slope or correlation R or regression R\n",
    "measure_tgt_name = 'regression_R' # 'regression_slopes' or 'correlation_R' or 'regression_R'\n",
    "# \n",
    "edge_measure_tgt_all_df = pd.DataFrame(edge_measure_tgt_all)\n",
    "edge_measure_tgt_all_df.columns = dependencytargets\n",
    "edge_measure_tgt_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([edge_measure_tgt_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].set_ylabel('edge weight v.s. '+xplotlabel,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals; '+measure_tgt_name ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[0].set_ylim([-1,1])\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "edge_measure_tgt_male_df = pd.DataFrame(edge_measure_tgt_all[[0,1,2],:])\n",
    "edge_measure_tgt_male_df.columns = dependencytargets\n",
    "edge_measure_tgt_male_df['type'] = 'male'\n",
    "#\n",
    "edge_measure_tgt_female_df = pd.DataFrame(edge_measure_tgt_all[[3,4,5,6,7],:])\n",
    "edge_measure_tgt_female_df.columns = dependencytargets\n",
    "edge_measure_tgt_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([edge_measure_tgt_male_df,edge_measure_tgt_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].set_ylabel('edge weight v.s. '+xplotlabel,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female; '+measure_tgt_name ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[1].set_ylim([-1,1])\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "edge_measure_tgt_sub_df = pd.DataFrame(edge_measure_tgt_all[[0,1,2,3],:])\n",
    "edge_measure_tgt_sub_df.columns = dependencytargets\n",
    "edge_measure_tgt_sub_df['type'] = 'subordinate'\n",
    "#\n",
    "edge_measure_tgt_dom_df = pd.DataFrame(edge_measure_tgt_all[[4,5,6,7],:])\n",
    "edge_measure_tgt_dom_df.columns = dependencytargets\n",
    "edge_measure_tgt_dom_df['type'] = 'dominant'\n",
    "#\n",
    "df_long=pd.concat([edge_measure_tgt_sub_df,edge_measure_tgt_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].set_ylabel('edge weight v.s. '+xplotlabel,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom; '+measure_tgt_name ,fontsize=24)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[2].set_ylim([-1,1])\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+measure_tgt_name+'.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f503348",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(edge_measure_tgt_dom_df['pull-pull'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_measure_tgt_sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034114a",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "####  plot the coorelation between pull time, and social gaze time\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "# initiate the final data set\n",
    "pull_gaze_time_corr_mean_all = np.zeros((nanimalpairs*2,2))\n",
    "\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    if not mergetempRos:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "            \n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    #\n",
    "    # initiate the final data set\n",
    "    within_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "    across_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "    within_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "    across_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "    \n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "\n",
    "        DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "        #\n",
    "        if 0:\n",
    "            # single behavioral events  \n",
    "            # pull1_t0 and gaze1_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "            rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "            # pull2_t0 and gaze1_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "            rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "            # pull2_t0 and gaze2_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "            rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "            # pull1_t0 and gaze2_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "            rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "        #\n",
    "        if 0:\n",
    "            # single behavioral events with synced pull\n",
    "            xxx1_1 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['pull1_t1'])==1))*1\n",
    "            xxx1_2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['pull2_t1'])==1))*1 \n",
    "            # pull1_t0 and gaze1_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "            rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "            # pull2_t0 and gaze1_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "            rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "            # pull2_t0 and gaze2_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "            rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "            # pull1_t0 and gaze2_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "            rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "        #\n",
    "        if 1:\n",
    "            # paired behavioral events\n",
    "            xxx1_1 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['pull1_t1'])==1))*1\n",
    "            xxx1_2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['pull2_t1'])==1))*1            \n",
    "            # pull1_t1 and gaze1_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "            #\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr1_spe = np.nan\n",
    "                pp1_spe = np.nan            \n",
    "            # pull2_t0 and gaze1_t1\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr2_spe = np.nan\n",
    "                pp2_spe = np.nan        \n",
    "            # pull2_t1 and gaze2_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr3_spe = np.nan\n",
    "                pp3_spe = np.nan           \n",
    "            # pull1_t0 and gaze2_t1\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr4_spe = np.nan\n",
    "                pp4_spe = np.nan\n",
    "  \n",
    "            \n",
    "        #    \n",
    "        within_pull_gaze_time_corr_all_ipair[idate_name] = [rr1_spe,rr3_spe]\n",
    "        across_pull_gaze_time_corr_all_ipair[idate_name] = [rr2_spe,rr4_spe]\n",
    "        within_pull_gaze_time_corP_all_ipair[idate_name] = [pp1_spe,pp3_spe]\n",
    "        across_pull_gaze_time_corP_all_ipair[idate_name] = [pp2_spe,pp4_spe]\n",
    "    \n",
    "    # organize the data to the summarizing mean variables\n",
    "    pull_gaze_time_corr_mean_all[[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "    pull_gaze_time_corr_mean_all[[ianimalpair*2,ianimalpair*2+1],1]=np.nanmean(pd.DataFrame(across_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "\n",
    "    \n",
    "    # plot each animal pair first\n",
    "    # figure initiate\n",
    "    fig, axs = plt.subplots(2,2)\n",
    "    fig.set_figheight(5*2)\n",
    "    fig.set_figwidth(10*2)\n",
    "    #\n",
    "    plottype_names = ['within animal gaze to pull, '+animal1_fixedorder,\n",
    "                      'across animal pull to gaze, '+animal1_fixedorder,\n",
    "                      'within animal gaze to pull, '+animal2_fixedorder,\n",
    "                      'across animal pull to gaze, '+animal2_fixedorder]\n",
    "    plotCorrs_pooled = [\n",
    "                        np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                        np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                        np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                        np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                       ]\n",
    "    #\n",
    "    for iplot in np.arange(0,4,1):\n",
    "        #\n",
    "        plottype_name = plottype_names[iplot]\n",
    "        plotCorrs = plotCorrs_pooled[iplot]\n",
    "        \n",
    "        # plot \n",
    "        axs.flatten()[iplot].plot(np.arange(0,ndates_tgt,1),plotCorrs,'ko',markersize=10)\n",
    "        #\n",
    "        axs.flatten()[iplot].set_title(plottype_name,fontsize=16)\n",
    "        axs.flatten()[iplot].set_ylabel('time coorelation with pull <-> pull',fontsize=13)\n",
    "        axs.flatten()[iplot].set_ylim([-1.1,1.1])\n",
    "        axs.flatten()[iplot].set_xlim([-0.5,ndates_tgt-0.5])\n",
    "        #\n",
    "        if iplot > 1:\n",
    "            axs.flatten()[iplot].set_xticks(np.arange(0,ndates_tgt,1))\n",
    "            axs.flatten()[iplot].set_xticklabels(dates_list_tgt, rotation=90,fontsize=10)\n",
    "        else:\n",
    "            axs.flatten()[iplot].set_xticklabels('')\n",
    "        #\n",
    "        # tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "        tasktypes = ['coop(3s)','coop(2s)','coop(1.5s)','coop(1s)']\n",
    "        taskswitches = np.where(np.array(sorting_tgt_df['coopthres'])[1:]-np.array(sorting_tgt_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs.flatten()[iplot].plot([taskswitch,taskswitch],[-1.1,1.1],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs.flatten()[iplot].text(taskswitch+0.25,-0.9,tasktypes[itaskswitch],fontsize=10)\n",
    "        axs.flatten()[iplot].plot([0,ndates_tgt],[0,0],'k--')\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'       \n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'syncedpulltime_pullgazetime_correlation_'+animal1_fixedorder+animal2_fixedorder+'.pdf')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the summarizing figure\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "dependencytargets = ['within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency\n",
    "pull_gaze_time_corr_tgt_all = pull_gaze_time_corr_mean_all \n",
    "measure_tgt_name = 'time point correlation' \n",
    "# \n",
    "pull_gaze_time_corr_tgt_all_df = pd.DataFrame(pull_gaze_time_corr_tgt_all)\n",
    "pull_gaze_time_corr_tgt_all_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals' ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[0].set_ylim([-1,1])\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "pull_gaze_time_corr_tgt_male_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[0,2,4],:])\n",
    "pull_gaze_time_corr_tgt_male_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_df['type'] = 'male'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[1,3,5,6,7],:])\n",
    "pull_gaze_time_corr_tgt_female_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_male_df,pull_gaze_time_corr_tgt_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female' ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[1].set_ylim([-1,1])\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "pull_gaze_time_corr_tgt_sub_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[0,2,4,6],:])\n",
    "pull_gaze_time_corr_tgt_sub_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_df['type'] = 'subordinate'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[1,3,5,7],:])\n",
    "pull_gaze_time_corr_tgt_dom_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_df['type'] = 'dominant'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_sub_df,pull_gaze_time_corr_tgt_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom' ,fontsize=24)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[2].set_ylim([-1,1])\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"syncedpulltime_pullgazetime_correlation_summaryplot.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fdb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22cbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299ce69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55324239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659e669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0321a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
