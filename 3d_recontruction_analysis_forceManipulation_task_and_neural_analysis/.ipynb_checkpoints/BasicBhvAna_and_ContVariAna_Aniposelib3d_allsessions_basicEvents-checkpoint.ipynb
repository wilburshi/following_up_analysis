{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea1560b",
   "metadata": {},
   "source": [
    "### This script runs some basic bhv analysis, and detailed analysis focus on the continuous behavioral variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d0681",
   "metadata": {},
   "source": [
    "#### also focus on the pull triggered average and gaze ralated continuous variable characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.plot_gaze_along_phase_of_continuous_bhv_var import plot_gaze_along_phase_of_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 1*30\n",
    "nframes = 1\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = '_allsessions'\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# a test case\n",
    "if 0:\n",
    "    dates_list = [\"20230324\"]\n",
    "    session_start_times = [5.50] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "# switch animals to make animal 1 and 2 consistent\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_succtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_errtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "gazeDist_phaseof_contbhvvar_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# where to save the demo video\n",
    "withboxCorner = 1\n",
    "video_file_dir = data_saved_folder+'/example_videos_Anipose_bhv_demo/'+animal1_filename+'_'+animal2_filename\n",
    "if not os.path.exists(video_file_dir):\n",
    "    os.makedirs(video_file_dir)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "# NOTE: THIS STEP will save the data to the combinedsession_Anipose folder, since they are the same\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    # dummy\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_succtrials_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_errtrials_all_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/gazeDist_phaseof_contbhvvar_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        gazeDist_phaseof_contbhvvar_all_dates = pickle.load(f) \n",
    "    \n",
    "    \n",
    "    print('all data from all dates are loaded')\n",
    "        \n",
    "except:\n",
    "    \n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        for imergepair in np.arange(0,np.shape(merge_campairs)[0],1):\n",
    "            \n",
    "            # should be only one merge type - \"Anipose\"\n",
    "            merge_campair = merge_campairs[imergepair]\n",
    "\n",
    "            # load camera tracking results\n",
    "            try:\n",
    "                # dummy\n",
    "                if reanalyze_video:\n",
    "                    print(\"re-analyze the data \",date_tgt)\n",
    "                    dummy\n",
    "                ## read\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                    body_part_locs_Anipose = pickle.load(f)                 \n",
    "            except:\n",
    "                print(\"did not save data for Anipose - body part tracking \"+date_tgt)\n",
    "                # analyze and save\n",
    "                Anipose_h5_file = Anipose_analyzed_path +date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "                Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "                body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                    pickle.dump(body_part_locs_Anipose, f)            \n",
    "            \n",
    "            min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "                    \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "            tasktypes_all_dates[idate] = tasktype\n",
    "            coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "            # successful trial or not\n",
    "            succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "            succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # analyze behavior results\n",
    "            # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "            succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "\n",
    "            trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "            #\n",
    "            pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<10]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "                pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "            else:\n",
    "                pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "                pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "                \n",
    "            # load behavioral event results\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                    output_key_locations = pickle.load(f)\n",
    "            except:\n",
    "                print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "                output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "               \n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_key_locations, f)\n",
    "                \n",
    "             \n",
    "            look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "            look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "            look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "            look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "            look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "            look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                               \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                               \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "            timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "            timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "            timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "                \n",
    "            # # plot behavioral events\n",
    "            if 0:\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    plot_bhv_events_levertube(date_tgt+merge_campair,animal1, animal2, session_start_time, totalsess_time, \n",
    "                                              time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2,\n",
    "                                              timepoint_lever1,timepoint_lever2,timepoint_tube1,timepoint_tube2)\n",
    "                else:\n",
    "                    plot_bhv_events_levertube(date_tgt+merge_campair,animal2, animal1, session_start_time, totalsess_time, \n",
    "                                              time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1,\n",
    "                                              timepoint_lever2,timepoint_lever1,timepoint_tube2,timepoint_tube1)\n",
    "            #\n",
    "            # save behavioral events plot\n",
    "            if 0:\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                plt.savefig(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/'+date_tgt+\"_Anipose.pdf\")\n",
    "  \n",
    "            #\n",
    "            # # old definition\n",
    "            # if np.isin(animal1,animal1_fixedorder):\n",
    "            #     owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            #     owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            #     mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            #     mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            # else:\n",
    "            #     owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            #     owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            #     mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            #     mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            #\n",
    "            # new defnition\n",
    "            # <500ms counts as one gaze, gaze number per second\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                owgaze1_num_all_dates[idate] = np.sum(oneway_gaze1[1:]-oneway_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "                owgaze2_num_all_dates[idate] = np.sum(oneway_gaze2[1:]-oneway_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze1_num_all_dates[idate] = np.sum(mutual_gaze1[1:]-mutual_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze2_num_all_dates[idate] = np.sum(mutual_gaze2[1:]-mutual_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "            else:\n",
    "                owgaze1_num_all_dates[idate] = np.sum(oneway_gaze2[1:]-oneway_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "                owgaze2_num_all_dates[idate] = np.sum(oneway_gaze1[1:]-oneway_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze1_num_all_dates[idate] = np.sum(mutual_gaze2[1:]-mutual_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze2_num_all_dates[idate] = np.sum(mutual_gaze1[1:]-mutual_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "            \n",
    "            \n",
    "            # plot key continuous behavioral variables\n",
    "            if 1:\n",
    "                filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+date_tgt+'/'\n",
    "                if not os.path.exists(filepath_cont_var):\n",
    "                    os.makedirs(filepath_cont_var)\n",
    "                \n",
    "                savefig = 0\n",
    "                pull_trig_events_summary, pull_trig_events_succtrial_summary, pull_trig_events_errtrial_summary = plot_continuous_bhv_var(filepath_cont_var+date_tgt+merge_campair,savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length,succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                pull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "                pull_trig_events_succtrials_all_dates[date_tgt] = pull_trig_events_succtrial_summary\n",
    "                pull_trig_events_errtrials_all_dates[date_tgt] = pull_trig_events_errtrial_summary\n",
    "                    \n",
    "             \n",
    "            # plot and analyze the gaze distribution along the phase of continous behavioral variables\n",
    "            if 1:\n",
    "                fig_savepath = data_saved_folder+'bhv_events_continuous_variables_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+date_tgt+'/'\n",
    "                if not os.path.exists(fig_savepath):\n",
    "                    os.makedirs(fig_savepath)\n",
    "                fig_savepath = fig_savepath + date_tgt + merge_campair\n",
    "                \n",
    "                doActvePeri = 1\n",
    "                doGazeStart = 1\n",
    "                savefig = 1\n",
    "                #\n",
    "                gazeDist_phaseof_contbhvvar_summary = plot_gaze_along_phase_of_continuous_bhv_var(fig_savepath, savefig, animal1, animal2, session_start_time, \n",
    "                                                                                succpulls_ornot, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, \n",
    "                                                                                mutual_gaze1, mutual_gaze2, animalnames_videotrack, output_look_ornot, \n",
    "                                                                                output_allvectors, output_allangles, output_key_locations, doActvePeri, doGazeStart)\n",
    "                #\n",
    "                gazeDist_phaseof_contbhvvar_all_dates[date_tgt] = gazeDist_phaseof_contbhvvar_summary\n",
    "                \n",
    "            \n",
    "            # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "            # could be used for define time bin for DBN\n",
    "            if 1:\n",
    "                _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                             oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #\n",
    "                pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "                bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                                'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        \n",
    "            \n",
    "            # plot the tracking demo video\n",
    "            if 0:      \n",
    "                video_file = video_file_dir+'/'+date_tgt+'_'+animal1_filename+'_'+animal2_filename+'_anipose_bhv_demo.mp4'\n",
    "                tracking_video_Anipose_events_demo(body_part_locs_Anipose,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                   time_point_pull1,time_point_pull2,animalnames_videotrack,bodypartnames_videotrack,\n",
    "                                                   date_tgt,animal1_filename,animal2_filename,animal1,animal2,\n",
    "                                                   session_start_time,fps,nframes,video_file,withboxCorner)\n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_succtrials_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_errtrials_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/gazeDist_phaseof_contbhvvar_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(gazeDist_phaseof_contbhvvar_all_dates, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec58897",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    minframe = 15000\n",
    "    maxframe = 17000\n",
    "    \n",
    "    gaze_thresold = 0.25\n",
    "\n",
    "    # get the discrete behavioral events\n",
    "    # aligned to the start of the video recording\n",
    "    time_point_pull1_frames = (np.array(time_point_pull1)+session_start_time)*fps \n",
    "    time_point_pull2_frames = (np.array(time_point_pull2)+session_start_time)*fps \n",
    "\n",
    "    # define gazes are all gaze entry\n",
    "    #\n",
    "    # oneway_gaze1_frames = (np.sort(np.concatenate((oneway_gaze1,mutual_gaze1)))+session_start_time)*fps \n",
    "    # oneway_gaze2_frames = (np.sort(np.concatenate((oneway_gaze2,mutual_gaze2)))+session_start_time)*fps \n",
    "\n",
    "    # define gaze as the flash gaze and gaze start\n",
    "    # get the gaze start and stop\n",
    "    #animal1_gaze = np.concatenate([oneway_gaze1, mutual_gaze1])\n",
    "    animal1_gaze = np.sort(np.concatenate((oneway_gaze1,mutual_gaze1)))\n",
    "    animal1_gaze = np.sort(np.unique(animal1_gaze))\n",
    "    animal1_gaze_stop = animal1_gaze[np.concatenate(((animal1_gaze[1:]-animal1_gaze[0:-1]>gaze_thresold)*1,[1]))==1]\n",
    "    animal1_gaze_start = np.concatenate(([animal1_gaze[0]],animal1_gaze[np.where(animal1_gaze[1:]-animal1_gaze[0:-1]>gaze_thresold)[0]+1]))\n",
    "    animal1_gaze_flash = np.intersect1d(animal1_gaze_start, animal1_gaze_stop)\n",
    "    animal1_gaze_start = animal1_gaze_start[~np.isin(animal1_gaze_start,animal1_gaze_flash)]\n",
    "    animal1_gaze_stop = animal1_gaze_stop[~np.isin(animal1_gaze_stop,animal1_gaze_flash)]\n",
    "    #\n",
    "    #animal2_gaze = np.concatenate([oneway_gaze2, mutual_gaze2])\n",
    "    animal2_gaze = np.sort(np.concatenate((oneway_gaze2,mutual_gaze2)))\n",
    "    animal2_gaze = np.sort(np.unique(animal2_gaze))\n",
    "    animal2_gaze_stop = animal2_gaze[np.concatenate(((animal2_gaze[1:]-animal2_gaze[0:-1]>gaze_thresold)*1,[1]))==1]\n",
    "    animal2_gaze_start = np.concatenate(([animal2_gaze[0]],animal2_gaze[np.where(animal2_gaze[1:]-animal2_gaze[0:-1]>gaze_thresold)[0]+1]))\n",
    "    animal2_gaze_flash = np.intersect1d(animal2_gaze_start, animal2_gaze_stop)\n",
    "    animal2_gaze_start = animal2_gaze_start[~np.isin(animal2_gaze_start,animal2_gaze_flash)]\n",
    "    animal2_gaze_stop = animal2_gaze_stop[~np.isin(animal2_gaze_stop,animal2_gaze_flash)] \n",
    "    #\n",
    "    oneway_gaze1_frames = (np.sort(np.concatenate((animal1_gaze_start,animal1_gaze_flash)))+session_start_time)*fps \n",
    "    oneway_gaze2_frames = (np.sort(np.concatenate((animal2_gaze_start,animal2_gaze_flash)))+session_start_time)*fps \n",
    "\n",
    "    #\n",
    "    a = output_key_locations['lever_loc_all_Anipose']['scorch'].transpose()\n",
    "    b = output_key_locations['meaneye_loc_all_Anipose']['scorch'].transpose()\n",
    "    a_min_b = a - b\n",
    "    animal_lever_dist = np.sqrt(np.einsum('ij,ij->j', a_min_b, a_min_b))\n",
    "    animal_lever_dist = scipy.ndimage.gaussian_filter1d(animal_lever_dist,15) \n",
    "    #\n",
    "    animal_lever_dist = (animal_lever_dist-np.nanmin(animal_lever_dist))/(np.nanmax(animal_lever_dist)-np.nanmin(animal_lever_dist))\n",
    "\n",
    "        \n",
    "    #    \n",
    "    a = output_key_locations['lever_loc_all_Anipose']['dodson'].transpose()\n",
    "    b = output_key_locations['meaneye_loc_all_Anipose']['dodson'].transpose()\n",
    "    a_min_b = a - b\n",
    "    otherani_otherlever_dist = np.sqrt(np.einsum('ij,ij->j', a_min_b, a_min_b))\n",
    "    otherani_otherlever_dist = scipy.ndimage.gaussian_filter1d(otherani_otherlever_dist,15)\n",
    "    #\n",
    "    otherani_otherlever_dist = (otherani_otherlever_dist-np.nanmin(otherani_otherlever_dist))/(np.nanmax(otherani_otherlever_dist)-np.nanmin(otherani_otherlever_dist))\n",
    "\n",
    "    oneway_gaze1_frames_forplot = oneway_gaze1_frames[(oneway_gaze1_frames<maxframe)&(oneway_gaze1_frames>minframe)]\n",
    "    oneway_gaze2_frames_forplot = oneway_gaze2_frames[(oneway_gaze2_frames<maxframe)&(oneway_gaze2_frames>minframe)]\n",
    "\n",
    "    pull1_frames_forplot = time_point_pull1_frames[(time_point_pull1_frames<maxframe)&(time_point_pull1_frames>minframe)]\n",
    "    pull2_frames_forplot = time_point_pull2_frames[(time_point_pull2_frames<maxframe)&(time_point_pull2_frames>minframe)]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for igaze in oneway_gaze2_frames_forplot:\n",
    "        if np.where(oneway_gaze2_frames_forplot==igaze)[0][0] == 0:\n",
    "            ax1.plot([igaze,igaze],[0,1],color='#7c7c7c',label = animal2+' social gaze')\n",
    "        else:\n",
    "            ax1.plot([igaze,igaze],[0,1],color='#7c7c7c')\n",
    "\n",
    "    # for ipull in pull1_frames_forplot:\n",
    "    #     ax1.plot([ipull,ipull],[0,1],color='r')\n",
    "\n",
    "    # for ipull in pull2_frames_forplot:\n",
    "    #     ax1.plot([ipull,ipull],[0,1],color='b')\n",
    "\n",
    "    ax1.plot(np.arange(minframe,maxframe,1),animal_lever_dist[minframe:maxframe],label = animal2+' distance to its lever')\n",
    "\n",
    "    ax1.plot(np.arange(minframe,maxframe,1),otherani_otherlever_dist[minframe:maxframe],label = animal1+' distance to its lever')\n",
    "\n",
    "    ax1.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deae0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(oneway_gaze2_frames_forplot==igaze)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# animal1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158391c",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb92466",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(coopthres_all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ca299",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(dates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9be41",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0aaea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['SR','MC','NV']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-5,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "    \n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(coopthres_forsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80632ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(dates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35cadc",
   "metadata": {},
   "source": [
    "### plot some other basis behavioral measures\n",
    "#### successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29625a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),succ_rate_all_dates[sorting_df.index],'o',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"successful rate\",fontsize=13)\n",
    "ax1.set_ylim([-0.1,1.1])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['SR','MC','NV']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"successfulrate_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ad0c9",
   "metadata": {},
   "source": [
    "#### animal pull numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull1_num_all_dates[sorting_df.index],'bv',markersize=5,label='animal1 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull2_num_all_dates[sorting_df.index],'rv',markersize=5,label='animal2 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pullmean_num_all_dates[sorting_df.index],'kv',markersize=8,label='mean pull #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"pull numbers\",fontsize=13)\n",
    "ax1.set_ylim([-20,240])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['SR','MC','NV']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,240],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pullnumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e48e54",
   "metadata": {},
   "source": [
    "#### gaze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "print(np.nanmax(gaze1_num_all_dates))\n",
    "print(np.nanmax(gaze2_num_all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze1_num_all_dates[sorting_df.index],'b^',markersize=5,label='animal1 gaze # per s')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze2_num_all_dates[sorting_df.index],'r^',markersize=5,label='animal2 gaze # per s')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazemean_num_all_dates[sorting_df.index],'k^',markersize=8,label='mean gaze # per s')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"social gaze number per second\",fontsize=13)\n",
    "# ax1.set_ylim([-20,1500])\n",
    "ax1.set_ylim([0,1])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    # ax1.plot([taskswitch,taskswitch],[-20,1500],'k--')\n",
    "    ax1.plot([taskswitch,taskswitch],[0,1],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,0.5,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_numbers = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)\n",
    "gaze_pull_ratios = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/(pull1_num_all_dates+pull2_num_all_dates)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "if do_trainedMCs:\n",
    "    grouptypes = ['self reward','cooperative','no-vision']\n",
    "\n",
    "gaze_numbers_groups = [np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==3)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==2)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1.5)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]]\n",
    "if do_trainedMCs:\n",
    "    gaze_numbers_groups = [np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                           np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                           np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(gaze_numbers_groups,whis=1.5, meanline=True)\n",
    "# gaze_numbers_plot = seaborn.barplot(gaze_numbers_groups)\n",
    "# seaborn.swarmplot(gaze_numbers_groups)\n",
    "\n",
    "plt.xticks(np.arange(0+1, len(grouptypes)+1, 1), grouptypes, fontsize = 14);\n",
    "# ax1.set_ylim([240/30,2100/30])\n",
    "ax1.set_ylim([0,1])\n",
    "ax1.set_ylabel(\"average social gaze number per second\",fontsize=14)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b59374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "if do_trainedMCs:\n",
    "    grouptypes = ['self reward','cooperative','no-vision']\n",
    "\n",
    "BhvIntv_groups = [pull_other_intv_mean[np.where(coopthres_forsort==100)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==3)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==2)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==1.5)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==1)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==-1)[0]]\n",
    "                 ]\n",
    "if do_trainedMCs:\n",
    "    BhvIntv_groups = [pull_other_intv_mean[np.where(coopthres_forsort==100)[0]],\n",
    "                      pull_other_intv_mean[np.where(coopthres_forsort==1)[0]],\n",
    "                      pull_other_intv_mean[np.where(coopthres_forsort==-1)[0]]\n",
    "                     ]    \n",
    "\n",
    "#BhvIntv_groups = [np.array(pd.DataFrame.stack(pull_other_intv_forplots[np.where(np.isin(\n",
    "#                  dates_list_sorted,dates_list_sorted[np.where(coopthres_forsort==100)[0]]))[0]])),\n",
    "#                  np.array(pd.DataFrame.stack(pull_other_intv_forplots[np.where(np.isin(\n",
    "#                  dates_list_sorted,dates_list_sorted[np.where(coopthres_forsort==1)[0]]))[0]])),\n",
    "#                  np.array(pd.DataFrame.stack(pull_other_intv_forplots[np.where(np.isin(\n",
    "#                  dates_list_sorted,dates_list_sorted[np.where(coopthres_forsort==-1)[0]]))[0]])),\n",
    "#                 ]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(BhvIntv_groups,whis=3, meanline=True)\n",
    "# gaze_numbers_plot = seaborn.barplot(BhvIntv_groups)\n",
    "# seaborn.swarmplot(BhvIntv_groups)\n",
    "\n",
    "plt.xticks(np.arange(0+1, len(grouptypes)+1, 1), grouptypes, fontsize = 14);\n",
    "#ax1.set_ylim([250,2000])\n",
    "ax1.set_ylabel(\"average behavioral event intervals (s)\",fontsize=14)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_bhvIntv_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39ed5c",
   "metadata": {},
   "source": [
    "### plot the gaze distribution along the continuous variables\n",
    "#### only focus on \"otherani_otherlever_dist\", plot the ratio of number of 'decrasing' phase vs 'increasing' phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2,1)\n",
    "# fig.set_figheight(10)\n",
    "# fig.set_figwidth(10*2)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "type_tgt = 'otherani_otherlever_dist'\n",
    "\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "gazeDist_ratios_forplot_animal1 = np.ones(np.shape(dates_list_sorted))*np.nan\n",
    "gazeDist_ratios_forplot_animal2 = np.ones(np.shape(dates_list_sorted))*np.nan\n",
    "\n",
    "for idate in np.arange(0,ndates_sorted,1):\n",
    "    \n",
    "    date_i = dates_list_sorted[idate]\n",
    "    \n",
    "    # for animal 1\n",
    "    gazeDist_tgt = gazeDist_phaseof_contbhvvar_all_dates[date_i][animal1_fixedorder[0]][type_tgt]\n",
    "    gateDist_ratio_tgt = np.sum(gazeDist_tgt['phase']=='decreasing')/np.sum(gazeDist_tgt['phase']=='increasing')\n",
    "    gazeDist_ratios_forplot_animal1[idate] = gateDist_ratio_tgt\n",
    "    \n",
    "    # for animal 2\n",
    "    gazeDist_tgt = gazeDist_phaseof_contbhvvar_all_dates[date_i][animal2_fixedorder[0]][type_tgt]\n",
    "    gateDist_ratio_tgt = np.sum(gazeDist_tgt['phase']=='decreasing')/np.sum(gazeDist_tgt['phase']=='increasing')\n",
    "    gazeDist_ratios_forplot_animal2[idate] = gateDist_ratio_tgt\n",
    "    \n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazeDist_ratios_forplot_animal1,'b^',markersize=5,label=animal1_fixedorder[0])\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazeDist_ratios_forplot_animal2,'r^',markersize=5,label=animal2_fixedorder[0])\n",
    "ax1.plot([-0.5,ndates_sorted-0.5],[1,1],'k--')\n",
    "ax1.legend()\n",
    "\n",
    "#\n",
    "ax1.set_ylabel('ratio',fontsize=13)\n",
    "ax1.set_title(\"social gaze on the decreasing phase / inecreasing phase of \"+type_tgt)\n",
    "# ax1.set_ylim([-20,1500])\n",
    "ax1.set_ylim([0,2])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['self','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    # ax1.plot([taskswitch,taskswitch],[-20,1500],'k--')\n",
    "    ax1.plot([taskswitch,taskswitch],[0,2],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,0.5,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazenumberratio_decreasingphaseVSincreasingphase\"+type_tgt+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f202d5",
   "metadata": {},
   "source": [
    "### plot pull triggered event related plot\n",
    "#### plot averaged trace for all pulls, seprating into different training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "varis_ylims = [[0,np.pi],[0,np.pi],[0,20],[0,1]]\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,ngroups)\n",
    "fig.set_figheight(10*nvaris*2)\n",
    "fig.set_figwidth(10*ngroups)\n",
    "\n",
    "# set the x axis info, make sure they are the same as in plot_continuous_bhv_var.py\n",
    "trig_twin = [-6,6] #s\n",
    "xxx_trigevent = np.arange(trig_twin[0],trig_twin[1],1/fps) \n",
    "\n",
    "for igroup in np.arange(0,ngroups,1):\n",
    "    \n",
    "    igroup_typename = group_typenames[igroup] \n",
    "    igroup_typeID =  group_typeIDs[igroup] \n",
    "    igroup_cothres = group_coopthres[igroup]\n",
    "    igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "    igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "       \n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "        tracecolor = varis_tracecolor[ivari]\n",
    "\n",
    "        pull1_trig_events_toplot = []\n",
    "        pull2_trig_events_toplot = []\n",
    "\n",
    "        for date_tgt in igroup_dates_tgt:\n",
    "\n",
    "            # get the mean trigger events for all pulls - animal 1\n",
    "            pull1_trig_events_toplot = np.nanmean(pull_trig_events_all_dates[date_tgt][(animal1_toplot,vari_toplot)],axis=0)\n",
    "            axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "            #\n",
    "            axs[ivari,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari,igroup].set_xticks([])\n",
    "            axs[ivari,igroup].set_xticklabels([])\n",
    "            axs[ivari,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari,igroup].set_title(animal1_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari,igroup].set_yticks([])\n",
    "                axs[ivari,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "                \n",
    "        \n",
    "            # get the mean trigger events for all pulls - animal 2\n",
    "            pull2_trig_events_toplot = np.nanmean(pull_trig_events_all_dates[date_tgt][(animal2_toplot,vari_toplot)],axis=0)\n",
    "            axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "            #\n",
    "            axs[ivari+nvaris,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari+nvaris,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari+nvaris,igroup].set_xticks([])\n",
    "            axs[ivari+nvaris,igroup].set_xticklabels([])\n",
    "            axs[ivari+nvaris,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari+nvaris,igroup].set_title(animal2_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari+nvaris,igroup].set_yticks([])\n",
    "                axs[ivari+nvaris,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari+nvaris,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "            \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averagePullTrigEventTraces_summary_allpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5730f",
   "metadata": {},
   "source": [
    "#### plot averaged trace for SUCCESSFUL pulls, seprating into different training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "varis_ylims = [[0,np.pi],[0,np.pi],[0,20],[0,1]]\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,ngroups)\n",
    "fig.set_figheight(10*nvaris*2)\n",
    "fig.set_figwidth(10*ngroups)\n",
    "\n",
    "# set the x axis info, make sure they are the same as in plot_continuous_bhv_var.py\n",
    "trig_twin = [-6,6] #s\n",
    "xxx_trigevent = np.arange(trig_twin[0],trig_twin[1],1/fps) \n",
    "\n",
    "for igroup in np.arange(0,ngroups,1):\n",
    "    \n",
    "    igroup_typename = group_typenames[igroup] \n",
    "    igroup_typeID =  group_typeIDs[igroup] \n",
    "    igroup_cothres = group_coopthres[igroup]\n",
    "    igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "    igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "       \n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "        tracecolor = varis_tracecolor[ivari]\n",
    "\n",
    "        pull1_trig_events_toplot = []\n",
    "        pull2_trig_events_toplot = []\n",
    "\n",
    "        for date_tgt in igroup_dates_tgt:\n",
    "\n",
    "            # get the mean trigger events for all pulls - animal 1\n",
    "            try:\n",
    "                pull1_trig_events_toplot = np.nanmean(pull_trig_events_succtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull1_trig_events_toplot = pull_trig_events_succtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "                    axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "        \n",
    "            axs[ivari,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari,igroup].set_xticks([])\n",
    "            axs[ivari,igroup].set_xticklabels([])\n",
    "            axs[ivari,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari,igroup].set_title(animal1_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari,igroup].set_yticks([])\n",
    "                axs[ivari,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "                \n",
    "        \n",
    "            # get the mean trigger events for all pulls - animal 2\n",
    "            try:\n",
    "                pull2_trig_events_toplot = np.nanmean(pull_trig_events_succtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull2_trig_events_toplot = pull_trig_events_succtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "                    axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "            axs[ivari+nvaris,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari+nvaris,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari+nvaris,igroup].set_xticks([])\n",
    "            axs[ivari+nvaris,igroup].set_xticklabels([])\n",
    "            axs[ivari+nvaris,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari+nvaris,igroup].set_title(animal2_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari+nvaris,igroup].set_yticks([])\n",
    "                axs[ivari+nvaris,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari+nvaris,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "            \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averagePullTrigEventTraces_summary_succpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367a0c1",
   "metadata": {},
   "source": [
    "#### plot averaged trace for FAILED pulls, seprating into different training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "varis_ylims = [[0,np.pi],[0,np.pi],[0,20],[0,1]]\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,ngroups)\n",
    "fig.set_figheight(10*nvaris*2)\n",
    "fig.set_figwidth(10*ngroups)\n",
    "\n",
    "# set the x axis info, make sure they are the same as in plot_continuous_bhv_var.py\n",
    "trig_twin = [-6,6] #s\n",
    "xxx_trigevent = np.arange(trig_twin[0],trig_twin[1],1/fps) \n",
    "\n",
    "for igroup in np.arange(0,ngroups,1):\n",
    "    \n",
    "    igroup_typename = group_typenames[igroup] \n",
    "    igroup_typeID =  group_typeIDs[igroup] \n",
    "    igroup_cothres = group_coopthres[igroup]\n",
    "    igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "    igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "       \n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "        tracecolor = varis_tracecolor[ivari]\n",
    "\n",
    "        pull1_trig_events_toplot = []\n",
    "        pull2_trig_events_toplot = []\n",
    "\n",
    "        for date_tgt in igroup_dates_tgt:\n",
    "\n",
    "            # get the mean trigger events for all pulls - animal 1\n",
    "            try:\n",
    "                pull1_trig_events_toplot = np.nanmean(pull_trig_events_errtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull1_trig_events_toplot = pull_trig_events_errtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "                    axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "        \n",
    "            axs[ivari,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari,igroup].set_xticks([])\n",
    "            axs[ivari,igroup].set_xticklabels([])\n",
    "            axs[ivari,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari,igroup].set_title(animal1_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari,igroup].set_yticks([])\n",
    "                axs[ivari,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "                \n",
    "        \n",
    "            # get the mean trigger events for all pulls - animal 2\n",
    "            try:\n",
    "                pull2_trig_events_toplot = np.nanmean(pull_trig_events_errtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull2_trig_events_toplot = pull_trig_events_errtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "                    axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "            axs[ivari+nvaris,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari+nvaris,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari+nvaris,igroup].set_xticks([])\n",
    "            axs[ivari+nvaris,igroup].set_xticklabels([])\n",
    "            axs[ivari+nvaris,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari+nvaris,igroup].set_title(animal2_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari+nvaris,igroup].set_yticks([])\n",
    "                axs[ivari+nvaris,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari+nvaris,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "            \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averagePullTrigEventTraces_summary_failedpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70ddf0",
   "metadata": {},
   "source": [
    "### plot the correlation among pull trigger traces - \n",
    "#### all pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "    varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "    varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "    nvaris = np.shape(varis_toplot)[0]\n",
    "    animal1_toplot = animal1_fixedorder[0]\n",
    "    animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "    # sort the data based on task type and dates\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "    ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "    fig, axs = plt.subplots(nvaris*2,1)\n",
    "    fig.set_figheight(5*nvaris*2)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "\n",
    "    # \n",
    "    tracecorr1_allpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "    tracecorr2_allpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "\n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "\n",
    "        # step 1 - prepare the data\n",
    "\n",
    "        tracecorr1_all_dates = dict.fromkeys(dates_list,[])\n",
    "        tracecorr2_all_dates = dict.fromkeys(dates_list,[])\n",
    "        tracecorr1_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "        tracecorr2_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "\n",
    "        for idate in np.arange(0,ndates_sorted,1):\n",
    "\n",
    "            date_tgt = dates_list_sorted[idate]\n",
    "\n",
    "            # \n",
    "            # animal 1\n",
    "            pull1_trig_events_idate = pull_trig_events_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "            ntrigs = np.shape(pull1_trig_events_idate)[0]\n",
    "            #\n",
    "            pull1_trig_events_corr_idates = []\n",
    "            #\n",
    "            for itrig in np.arange(0,ntrigs-1,1):\n",
    "                for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                    # corr_tracepair = np.corrcoef(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig])[0,1]\n",
    "                    corr_tracepair,pp = scipy.stats.spearmanr(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                    if pp>0.05:\n",
    "                        corr_tracepair = np.nan\n",
    "                    corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                    # \n",
    "                    if (itrig==0) & (jtrig==0):\n",
    "                        pull1_trig_events_corr_idates = corr_tracepair\n",
    "                    else:\n",
    "                        pull1_trig_events_corr_idates = np.append(pull1_trig_events_corr_idates,corr_tracepair)\n",
    "            #\n",
    "            tracecorr1_all_dates[date_tgt]=pd.Series(pull1_trig_events_corr_idates)\n",
    "            tracecorr1_mean_all_dates[idate] = np.nanmean(pull1_trig_events_corr_idates)\n",
    "\n",
    "            # animal 2\n",
    "            pull2_trig_events_idate = pull_trig_events_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "            ntrigs = np.shape(pull2_trig_events_idate)[0]\n",
    "            #\n",
    "            pull2_trig_events_corr_idates = []\n",
    "            #\n",
    "            for itrig in np.arange(0,ntrigs-1,1):\n",
    "                for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                    # corr_tracepair = np.corrcoef(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig])[0,1]\n",
    "                    corr_tracepair,pp = scipy.stats.spearmanr(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                    if pp>0.05:\n",
    "                        corr_tracepair = np.nan\n",
    "                    corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                    # \n",
    "                    if (itrig==0) & (jtrig==0):\n",
    "                        pull2_trig_events_corr_idates = corr_tracepair\n",
    "                    else:\n",
    "                        pull2_trig_events_corr_idates = np.append(pull2_trig_events_corr_idates,corr_tracepair)\n",
    "            #\n",
    "            tracecorr2_all_dates[date_tgt]=pd.Series(pull2_trig_events_corr_idates)\n",
    "            tracecorr2_mean_all_dates[idate] = np.nanmean(pull2_trig_events_corr_idates)     \n",
    "        # \n",
    "        tracecorr1_allpull_trigevent_sum[vari_toplot] = tracecorr1_all_dates\n",
    "        tracecorr2_allpull_trigevent_sum[vari_toplot] = tracecorr2_all_dates\n",
    "\n",
    "        # step 2 - plot\n",
    "        # animal 1\n",
    "        #\n",
    "        tracecorr1_all_dates = pd.DataFrame(tracecorr1_all_dates)\n",
    "        #\n",
    "        tracecorr1_all_dates.plot(kind = 'box',ax=axs[ivari*2], positions=np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2].plot(np.arange(0,ndates_sorted,1),tracecorr1_mean_all_dates,'r*',markersize=10)\n",
    "        #\n",
    "        axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2].set_ylim([-0.2,1.2])\n",
    "        #\n",
    "        axs[ivari*2].set_xticklabels([])\n",
    "        #\n",
    "        axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "        # animal 2\n",
    "        #\n",
    "        tracecorr2_all_dates = pd.DataFrame(tracecorr2_all_dates)\n",
    "        #\n",
    "        tracecorr2_all_dates.plot(kind = 'box',ax=axs[ivari*2+1], positions=np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2+1].plot(np.arange(0,ndates_sorted,1),tracecorr2_mean_all_dates,'r*',markersize=10)\n",
    "        #\n",
    "        axs[ivari*2+1].set_ylabel(\"paiwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "        #\n",
    "        if ivari == nvaris-1:\n",
    "            axs[ivari*2+1].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "            axs[ivari*2+1].set_xticklabels(dates_list_sorted,rotation=90, fontsize=10)\n",
    "        else:\n",
    "            axs[ivari*2+1].set_xticklabels([])\n",
    "        #\n",
    "        axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2+1].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2+1].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_allpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c803b4e",
   "metadata": {},
   "source": [
    "#### plot the correlation among pull trigger traces - successful pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "    varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "    varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "    nvaris = np.shape(varis_toplot)[0]\n",
    "    animal1_toplot = animal1_fixedorder[0]\n",
    "    animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "    # sort the data based on task type and dates\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "    ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "    fig, axs = plt.subplots(nvaris*2,1)\n",
    "    fig.set_figheight(5*nvaris*2)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "\n",
    "    # \n",
    "    tracecorr1_succpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "    tracecorr2_succpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "\n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "\n",
    "        # step 1 - prepare the data\n",
    "\n",
    "        tracecorr1_all_dates = dict.fromkeys(dates_list,[])\n",
    "        tracecorr2_all_dates = dict.fromkeys(dates_list,[])\n",
    "        tracecorr1_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "        tracecorr2_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "\n",
    "        for idate in np.arange(0,ndates_sorted,1):\n",
    "\n",
    "            date_tgt = dates_list_sorted[idate]\n",
    "\n",
    "            # \n",
    "            # animal 1\n",
    "            pull1_trig_events_idate = pull_trig_events_succtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "            ntrigs = np.shape(pull1_trig_events_idate)[0]\n",
    "            #\n",
    "            pull1_trig_events_corr_idates = []\n",
    "            #\n",
    "            for itrig in np.arange(0,ntrigs-1,1):\n",
    "                for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                    # corr_tracepair = np.corrcoef(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig])[0,1]\n",
    "                    corr_tracepair,pp = scipy.stats.spearmanr(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                    if pp>0.05:\n",
    "                        corr_tracepair = np.nan\n",
    "                    corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                    # \n",
    "                    if (itrig==0) & (jtrig==0):\n",
    "                        pull1_trig_events_corr_idates = corr_tracepair\n",
    "                    else:\n",
    "                        pull1_trig_events_corr_idates = np.append(pull1_trig_events_corr_idates,corr_tracepair)\n",
    "            #\n",
    "            tracecorr1_all_dates[date_tgt]=pd.Series(pull1_trig_events_corr_idates)\n",
    "            tracecorr1_mean_all_dates[idate] = np.nanmean(pull1_trig_events_corr_idates)\n",
    "\n",
    "            # animal 2\n",
    "            pull2_trig_events_idate = pull_trig_events_succtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "            ntrigs = np.shape(pull2_trig_events_idate)[0]\n",
    "            #\n",
    "            pull2_trig_events_corr_idates = []\n",
    "            #\n",
    "            for itrig in np.arange(0,ntrigs-1,1):\n",
    "                for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                    # corr_tracepair = np.corrcoef(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig])[0,1]\n",
    "                    corr_tracepair,pp = scipy.stats.spearmanr(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                    if pp>0.05:\n",
    "                        corr_tracepair = np.nan\n",
    "                    corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                    # \n",
    "                    if (itrig==0) & (jtrig==0):\n",
    "                        pull2_trig_events_corr_idates = corr_tracepair\n",
    "                    else:\n",
    "                        pull2_trig_events_corr_idates = np.append(pull2_trig_events_corr_idates,corr_tracepair)\n",
    "            #\n",
    "            tracecorr2_all_dates[date_tgt]=pd.Series(pull2_trig_events_corr_idates)\n",
    "            tracecorr2_mean_all_dates[idate] = np.nanmean(pull2_trig_events_corr_idates)\n",
    "        # \n",
    "        tracecorr1_succpull_trigevent_sum[vari_toplot] = tracecorr1_all_dates\n",
    "        tracecorr2_succpull_trigevent_sum[vari_toplot] = tracecorr2_all_dates              \n",
    "\n",
    "        # step 2 - plot\n",
    "        # animal 1\n",
    "        #\n",
    "        tracecorr1_all_dates = pd.DataFrame(tracecorr1_all_dates)\n",
    "        #\n",
    "        tracecorr1_all_dates.plot(kind = 'box',ax=axs[ivari*2], positions=np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2].plot(np.arange(0,ndates_sorted,1),tracecorr1_mean_all_dates,'r*',markersize=10)\n",
    "        #\n",
    "        axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2].set_ylim([-0.2,1.2])\n",
    "        #\n",
    "        axs[ivari*2].set_xticklabels([])\n",
    "        #\n",
    "        axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "        # animal 2\n",
    "        #\n",
    "        tracecorr2_all_dates = pd.DataFrame(tracecorr2_all_dates)\n",
    "        #\n",
    "        tracecorr2_all_dates.plot(kind = 'box',ax=axs[ivari*2+1], positions=np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2+1].plot(np.arange(0,ndates_sorted,1),tracecorr2_mean_all_dates,'r*',markersize=10)\n",
    "        #\n",
    "        axs[ivari*2+1].set_ylabel(\"paiwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "        #\n",
    "        if ivari == nvaris-1:\n",
    "            axs[ivari*2+1].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "            axs[ivari*2+1].set_xticklabels(dates_list_sorted,rotation=90, fontsize=10)\n",
    "        else:\n",
    "            axs[ivari*2+1].set_xticklabels([])\n",
    "        #\n",
    "        axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2+1].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2+1].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_succpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0ab79",
   "metadata": {},
   "source": [
    "#### plot the correlation among pull trigger traces - failed pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "    varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "    varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "    nvaris = np.shape(varis_toplot)[0]\n",
    "    animal1_toplot = animal1_fixedorder[0]\n",
    "    animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "    # sort the data based on task type and dates\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "    ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "    fig, axs = plt.subplots(nvaris*2,1)\n",
    "    fig.set_figheight(5*nvaris*2)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "\n",
    "    # \n",
    "    tracecorr1_errpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "    tracecorr2_errpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "\n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "\n",
    "        # step 1 - prepare the data\n",
    "\n",
    "        tracecorr1_all_dates = dict.fromkeys(dates_list,[])\n",
    "        tracecorr2_all_dates = dict.fromkeys(dates_list,[])\n",
    "        tracecorr1_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "        tracecorr2_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "\n",
    "        for idate in np.arange(0,ndates_sorted,1):\n",
    "\n",
    "            date_tgt = dates_list_sorted[idate]\n",
    "\n",
    "            # \n",
    "            # animal 1\n",
    "            pull1_trig_events_idate = pull_trig_events_errtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "            ntrigs = np.shape(pull1_trig_events_idate)[0]\n",
    "            #\n",
    "            pull1_trig_events_corr_idates = []\n",
    "            #\n",
    "            for itrig in np.arange(0,ntrigs-1,1):\n",
    "                for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                    # corr_tracepair = np.corrcoef(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig])[0,1]\n",
    "                    corr_tracepair,pp = scipy.stats.spearmanr(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                    if pp>0.05:\n",
    "                        corr_tracepair = np.nan\n",
    "                    corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                    # \n",
    "                    if (itrig==0) & (jtrig==0):\n",
    "                        pull1_trig_events_corr_idates = corr_tracepair\n",
    "                    else:\n",
    "                        pull1_trig_events_corr_idates = np.append(pull1_trig_events_corr_idates,corr_tracepair)\n",
    "            #\n",
    "            tracecorr1_all_dates[date_tgt]=pd.Series(pull1_trig_events_corr_idates)\n",
    "            tracecorr1_mean_all_dates[idate] = np.nanmean(pull1_trig_events_corr_idates)\n",
    "\n",
    "            # animal 2\n",
    "            pull2_trig_events_idate = pull_trig_events_errtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "            ntrigs = np.shape(pull2_trig_events_idate)[0]\n",
    "            #\n",
    "            pull2_trig_events_corr_idates = []\n",
    "            #\n",
    "            for itrig in np.arange(0,ntrigs-1,1):\n",
    "                for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                    # corr_tracepair = np.corrcoef(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig])[0,1]\n",
    "                    corr_tracepair,pp = scipy.stats.spearmanr(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                    if pp>0.05:\n",
    "                        corr_tracepair = np.nan\n",
    "                    corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                    # \n",
    "                    if (itrig==0) & (jtrig==0):\n",
    "                        pull2_trig_events_corr_idates = corr_tracepair\n",
    "                    else:\n",
    "                        pull2_trig_events_corr_idates = np.append(pull2_trig_events_corr_idates,corr_tracepair)\n",
    "            #\n",
    "            tracecorr2_all_dates[date_tgt]=pd.Series(pull2_trig_events_corr_idates)\n",
    "            tracecorr2_mean_all_dates[idate] = np.nanmean(pull2_trig_events_corr_idates)\n",
    "        # \n",
    "        tracecorr1_errpull_trigevent_sum[vari_toplot] = tracecorr1_all_dates\n",
    "        tracecorr2_errpull_trigevent_sum[vari_toplot] = tracecorr2_all_dates                \n",
    "\n",
    "        # step 2 - plot\n",
    "        # animal 1\n",
    "        #\n",
    "        tracecorr1_all_dates = pd.DataFrame(tracecorr1_all_dates)\n",
    "        #\n",
    "        tracecorr1_all_dates.plot(kind = 'box',ax=axs[ivari*2], positions=np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2].plot(np.arange(0,ndates_sorted,1),tracecorr1_mean_all_dates,'r*',markersize=10)\n",
    "        #\n",
    "        axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2].set_ylim([-0.2,1.2])\n",
    "        #\n",
    "        axs[ivari*2].set_xticklabels([])\n",
    "        #\n",
    "        axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "        # animal 2\n",
    "        #\n",
    "        tracecorr2_all_dates = pd.DataFrame(tracecorr2_all_dates)\n",
    "        #\n",
    "        tracecorr2_all_dates.plot(kind = 'box',ax=axs[ivari*2+1], positions=np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2+1].plot(np.arange(0,ndates_sorted,1),tracecorr2_mean_all_dates,'r*',markersize=10)\n",
    "        #\n",
    "        axs[ivari*2+1].set_ylabel(\"paiwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "        #\n",
    "        if ivari == nvaris-1:\n",
    "            axs[ivari*2+1].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "            axs[ivari*2+1].set_xticklabels(dates_list_sorted,rotation=90, fontsize=10)\n",
    "        else:\n",
    "            axs[ivari*2+1].set_xticklabels([])\n",
    "        #\n",
    "        axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2+1].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[ivari*2+1].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_failedpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82e86a",
   "metadata": {},
   "source": [
    "#### put the correlation result together in one figure - average across sessions within training condition; all pulls, add statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "    nvaris = np.shape(varis_toplot)[0]\n",
    "    animal1_toplot = animal1_fixedorder[0]\n",
    "    animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "    # different session conditions\n",
    "    #group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    #group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "    #group_coopthres = [0,3,2,1.5,1,0]\n",
    "    #group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "    group_typenames = ['self','coop(1s)','no-vision']\n",
    "    group_typeIDs  =  [1,3,5]\n",
    "    group_coopthres = [0,1,0]\n",
    "    group_sorting_df_coopthres = [100,1,-1]\n",
    "    ngroups = np.shape(group_typenames)[0]\n",
    "    #\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "    fig, axs = plt.subplots(nvaris*2,1)\n",
    "    fig.set_figheight(5*nvaris*2)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "\n",
    "        tracecorr1_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "        tracecorr2_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "\n",
    "        tracecorr1_signiP_matrx = np.ones((ngroups,ngroups))\n",
    "        tracecorr2_signiP_matrx = np.ones((ngroups,ngroups))\n",
    "\n",
    "        for igroup in np.arange(0,ngroups,1):\n",
    "\n",
    "            igroup_typename = group_typenames[igroup] \n",
    "            igroup_typeID =  group_typeIDs[igroup] \n",
    "            igroup_cothres = group_coopthres[igroup]\n",
    "            igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "            igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "\n",
    "            #\n",
    "            tracecorr1_allpull_trigevent_ivari = tracecorr1_allpull_trigevent_sum[vari_toplot]\n",
    "            tracecorr2_allpull_trigevent_ivari = tracecorr2_allpull_trigevent_sum[vari_toplot]\n",
    "            #\n",
    "            mergeddata = [list(tracecorr1_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr1_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "            mergeddata = [list(tracecorr2_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr2_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "\n",
    "        # test the significance\n",
    "        for igroup in np.arange(0,ngroups,1):\n",
    "            igroup_typename = group_typenames[igroup] \n",
    "            for jgroup in np.arange(0,ngroups,1):\n",
    "                jgroup_typename = group_typenames[jgroup] \n",
    "                #\n",
    "                xxx = tracecorr1_allpull_trigevent_ivari_allgroups[igroup_typename]\n",
    "                xxx = np.array(xxx[~pd.isna(xxx)])\n",
    "                yyy = tracecorr1_allpull_trigevent_ivari_allgroups[jgroup_typename]\n",
    "                yyy = np.array(yyy[~pd.isna(yyy)])\n",
    "\n",
    "                _,pp = scipy.stats.ranksums(xxx,yyy)\n",
    "                #\n",
    "                tracecorr1_signiP_matrx[igroup,jgroup]=pp\n",
    "                #\n",
    "                xxx = tracecorr2_allpull_trigevent_ivari_allgroups[igroup_typename]\n",
    "                xxx = np.array(xxx[~pd.isna(xxx)])\n",
    "                yyy = tracecorr2_allpull_trigevent_ivari_allgroups[jgroup_typename]   \n",
    "                yyy = np.array(yyy[~pd.isna(yyy)])\n",
    "                _,pp = scipy.stats.ranksums(xxx,yyy)\n",
    "                #\n",
    "                tracecorr2_signiP_matrx[igroup,jgroup]=pp\n",
    "\n",
    "\n",
    "        # do the plot \n",
    "        # animal 1\n",
    "        tracecorr1_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_allpull_trigevent_ivari_allgroups)\n",
    "        #\n",
    "        bx1 = tracecorr1_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                    positions=np.arange(0,ngroups,1),color='r')\n",
    "        # axs[ivari*2].legend([bx1,bx2,bx3],['all pulls','successful pulls','failed pulls'])    \n",
    "        axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2].set_ylim([-0.2,1])\n",
    "        axs[ivari*2].set_xticklabels([])\n",
    "        axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "        #\n",
    "        print(np.round(tracecorr1_signiP_matrx,3))\n",
    "\n",
    "        # animal 2\n",
    "        tracecorr2_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_allpull_trigevent_ivari_allgroups)\n",
    "        #\n",
    "        bx1 = tracecorr2_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                     positions=np.arange(0,ngroups,1),color='r')\n",
    "        # axs[ivari*2+1].legend(['all pulls','successful pulls','failed pulls'])    \n",
    "        axs[ivari*2+1].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "        axs[ivari*2+1].set_xticklabels(group_typenames)\n",
    "        axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "        #\n",
    "        print(np.round(tracecorr2_signiP_matrx,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39aa15a",
   "metadata": {},
   "source": [
    "#### put the correlation result together in one figure - average across sessions within training condition; put the all successful and failed pulls in one plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17672eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "    nvaris = np.shape(varis_toplot)[0]\n",
    "    animal1_toplot = animal1_fixedorder[0]\n",
    "    animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "    # different session conditions\n",
    "    #group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    #group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "    #group_coopthres = [0,3,2,1.5,1,0]\n",
    "    #group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "    group_typenames = ['self','coop(1s)','no-vision']\n",
    "    group_typeIDs  =  [1,3,5]\n",
    "    group_coopthres = [0,1,0]\n",
    "    group_sorting_df_coopthres = [100,1,-1]\n",
    "    ngroups = np.shape(group_typenames)[0]\n",
    "    #\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "    fig, axs = plt.subplots(nvaris*2,1)\n",
    "    fig.set_figheight(5*nvaris*2)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "\n",
    "        tracecorr1_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "        tracecorr2_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "        tracecorr1_succpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "        tracecorr2_succpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "        tracecorr1_errpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "        tracecorr2_errpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "\n",
    "        for igroup in np.arange(0,ngroups,1):\n",
    "\n",
    "            igroup_typename = group_typenames[igroup] \n",
    "            igroup_typeID =  group_typeIDs[igroup] \n",
    "            igroup_cothres = group_coopthres[igroup]\n",
    "            igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "            igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "\n",
    "            #\n",
    "            tracecorr1_allpull_trigevent_ivari = tracecorr1_allpull_trigevent_sum[vari_toplot]\n",
    "            tracecorr2_allpull_trigevent_ivari = tracecorr2_allpull_trigevent_sum[vari_toplot]\n",
    "            tracecorr1_succpull_trigevent_ivari = tracecorr1_succpull_trigevent_sum[vari_toplot]\n",
    "            tracecorr2_succpull_trigevent_ivari = tracecorr2_succpull_trigevent_sum[vari_toplot]\n",
    "            tracecorr1_errpull_trigevent_ivari = tracecorr1_errpull_trigevent_sum[vari_toplot]\n",
    "            tracecorr2_errpull_trigevent_ivari = tracecorr2_errpull_trigevent_sum[vari_toplot]\n",
    "\n",
    "            mergeddata = [list(tracecorr1_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr1_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "            mergeddata = [list(tracecorr2_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr2_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "            mergeddata = [list(tracecorr1_succpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr1_succpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "            mergeddata = [list(tracecorr2_succpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr2_succpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "            mergeddata = [list(tracecorr1_errpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr1_errpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "            mergeddata = [list(tracecorr2_errpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "            tracecorr2_errpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "\n",
    "        # do the plot \n",
    "        # animal 1\n",
    "        tracecorr1_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_allpull_trigevent_ivari_allgroups)\n",
    "        tracecorr1_succpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_succpull_trigevent_ivari_allgroups)\n",
    "        tracecorr1_errpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_errpull_trigevent_ivari_allgroups)\n",
    "        #\n",
    "        bx1 = tracecorr1_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                    positions=np.arange(0,ngroups*4,4),color='r')\n",
    "        axs[ivari*2].set_xticks([])\n",
    "        bx2 = tracecorr1_succpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                    positions=np.arange(1,ngroups*4+1,4),color='b')\n",
    "        axs[ivari*2].set_xticks([])\n",
    "        bx3 = tracecorr1_errpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                    positions=np.arange(2,ngroups*4+2,4),color='k')\n",
    "        axs[ivari*2].set_xticks(np.arange(1,ngroups*4+1,4))\n",
    "        # axs[ivari*2].legend([bx1,bx2,bx3],['all pulls','successful pulls','failed pulls'])    \n",
    "        axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2].set_ylim([-0.2,1])\n",
    "        axs[ivari*2].set_xticklabels([])\n",
    "        axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "\n",
    "        # animal 2\n",
    "        tracecorr2_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_allpull_trigevent_ivari_allgroups)\n",
    "        tracecorr2_succpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_succpull_trigevent_ivari_allgroups)\n",
    "        tracecorr2_errpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_errpull_trigevent_ivari_allgroups)\n",
    "        #\n",
    "        bx1 = tracecorr2_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                     positions=np.arange(0,ngroups*4,4),color='r')\n",
    "        axs[ivari*2+1].set_xticks([])\n",
    "        bx2 = tracecorr2_succpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                    positions=np.arange(1,ngroups*4+1,4),color='b')\n",
    "        axs[ivari*2+1].set_xticks([])\n",
    "        bx3 = tracecorr2_errpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                    positions=np.arange(2,ngroups*4+2,4),color='k')\n",
    "        axs[ivari*2+1].set_xticks(np.arange(1,ngroups*4+1,4))\n",
    "        # axs[ivari*2+1].legend(['all pulls','successful pulls','failed pulls'])    \n",
    "        axs[ivari*2+1].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "        axs[ivari*2+1].set_ylim([-0.2,1])\n",
    "        axs[ivari*2+1].set_xticklabels(group_typenames)\n",
    "        axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_summary_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99967b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69edc36a",
   "metadata": {},
   "source": [
    "## plot that includes all the animals - final summarizing plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6b72b",
   "metadata": {},
   "source": [
    "#### plot the gaze numbers for all individuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_onlyLearningsess = 1 # only consider pairs from the learning analysis\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "if do_onlyLearningsess:\n",
    "    animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_2',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "if do_trainedMCs:\n",
    "    grouptypes = ['self reward','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 1, -1]\n",
    "ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "gazenum_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "gazenum_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "#\n",
    "malenames = ['eddie','dodson','dannon','vermelho']\n",
    "femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "if do_onlyLearningsess:\n",
    "    malenames = ['eddie','dodson',]\n",
    "    femalenames = ['sparkle','scorch','kanga_2',]\n",
    "gazenum_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "gazenum_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "#\n",
    "subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "if do_onlyLearningsess:\n",
    "    subnames = ['eddie','dodson','ginger',]\n",
    "    domnames = ['sparkle','scorch','kanga_2',]\n",
    "gazenum_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "gazenum_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "#\n",
    "for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "    \n",
    "    grouptype = grouptypes[igrouptype]\n",
    "    coopthres_ID = coopthres_IDs[igrouptype]\n",
    "    \n",
    "    gazenum_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "\n",
    "    #\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1 = animal1_fixedorders[ianimalpair]\n",
    "        animal2 = animal2_fixedorders[ianimalpair]\n",
    "        \n",
    "        if (animal2 == 'kanga_1') | (animal2 == 'kanga_2'):\n",
    "            animal2_filename = 'kanga'\n",
    "        else:\n",
    "            animal2_filename = animal2\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1+animal2_filename+'/'\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            owgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            owgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            mtgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            mtgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            pull1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "        # combine owgaze and mtgaze\n",
    "        gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "        gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "\n",
    "        #\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "        # \n",
    "        gazenum_foreachgroup_foreachAni[grouptype][animal1] = gaze1_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "        gazenum_foreachgroup_foreachAni[grouptype][animal2] = gaze2_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "        \n",
    "    \n",
    "    # combine across all animals\n",
    "    gazenum_foreachgroup_all[grouptype] = np.hstack(list(gazenum_foreachgroup_foreachAni[grouptype].values()))\n",
    "    \n",
    "    # combine across male and female\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[malenames]\n",
    "    gazenum_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[femalenames]\n",
    "    gazenum_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "    \n",
    "    # combine across sub and dom\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[subnames]\n",
    "    gazenum_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[domnames]\n",
    "    gazenum_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "    \n",
    "    \n",
    "        \n",
    "# box plot \n",
    "fig, axs = plt.subplots(3,1)\n",
    "fig.set_figheight(5*3)\n",
    "fig.set_figwidth(3*2)\n",
    "\n",
    "# subplot 1 - all animals\n",
    "gazenum_foreachgroup_all_df = pd.DataFrame.from_dict(gazenum_foreachgroup_all,orient='index')\n",
    "gazenum_foreachgroup_all_df = gazenum_foreachgroup_all_df.transpose()\n",
    "gazenum_foreachgroup_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([gazenum_foreachgroup_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "# seaborn.boxplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type')\n",
    "seaborn.violinplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].set_xticklabels('')\n",
    "axs[0].xaxis.set_tick_params(labelsize=15)\n",
    "axs[0].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[0].set_title('all animals' ,fontsize=24)\n",
    "axs[0].set_ylim([-0.1,0.7])\n",
    "axs[0].legend(fontsize=18)\n",
    "\n",
    "# subplot 2 - male and female animals\n",
    "gazenum_foreachgroup_male_df = pd.DataFrame.from_dict(gazenum_foreachgroup_male,orient='index')\n",
    "gazenum_foreachgroup_male_df = gazenum_foreachgroup_male_df.transpose()\n",
    "gazenum_foreachgroup_male_df['type'] = 'male'\n",
    "gazenum_foreachgroup_female_df = pd.DataFrame.from_dict(gazenum_foreachgroup_female,orient='index')\n",
    "gazenum_foreachgroup_female_df = gazenum_foreachgroup_female_df.transpose()\n",
    "gazenum_foreachgroup_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([gazenum_foreachgroup_male_df,gazenum_foreachgroup_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "seaborn.violinplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs[1].set_xlabel('')\n",
    "axs[1].set_xticklabels('')\n",
    "axs[1].xaxis.set_tick_params(labelsize=15)\n",
    "axs[1].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[1].set_title('male and female' ,fontsize=24)\n",
    "axs[1].set_ylim([-0.1,0.7])\n",
    "axs[1].legend(fontsize=18)\n",
    "\n",
    "# subplot 3 - dom and sub animals\n",
    "gazenum_foreachgroup_sub_df = pd.DataFrame.from_dict(gazenum_foreachgroup_sub,orient='index')\n",
    "gazenum_foreachgroup_sub_df = gazenum_foreachgroup_sub_df.transpose()\n",
    "gazenum_foreachgroup_sub_df['type'] = 'sub'\n",
    "gazenum_foreachgroup_dom_df = pd.DataFrame.from_dict(gazenum_foreachgroup_dom,orient='index')\n",
    "gazenum_foreachgroup_dom_df = gazenum_foreachgroup_dom_df.transpose()\n",
    "gazenum_foreachgroup_dom_df['type'] = 'dom'\n",
    "#\n",
    "df_long=pd.concat([gazenum_foreachgroup_sub_df,gazenum_foreachgroup_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "seaborn.boxplot(ax=axs[2],data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs[2].set_xlabel('')\n",
    "axs[2].set_xticklabels(axs[2].get_xticklabels(),rotation=45)\n",
    "axs[2].xaxis.set_tick_params(labelsize=15)\n",
    "axs[2].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[2].set_title('sub and dom' ,fontsize=24)\n",
    "axs[2].set_ylim([-0.1,0.7])\n",
    "axs[2].legend(fontsize=18)\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+'averaged_gazenumbers_acrossAllAnimals.pdf')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c986ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the anova on all animals\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "gazenum_foreachgroup_all_df = pd.DataFrame.from_dict(gazenum_foreachgroup_all,orient='index')\n",
    "gazenum_foreachgroup_all_df = gazenum_foreachgroup_all_df.transpose()\n",
    "gazenum_foreachgroup_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([gazenum_foreachgroup_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "\n",
    "# anova\n",
    "cw_lm=ols('value ~ condition', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition'], alpha=0.05)\n",
    "print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the anova on male and female\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "gazenum_foreachgroup_male_df = pd.DataFrame.from_dict(gazenum_foreachgroup_male,orient='index')\n",
    "gazenum_foreachgroup_male_df = gazenum_foreachgroup_male_df.transpose()\n",
    "gazenum_foreachgroup_male_df['type'] = 'male'\n",
    "gazenum_foreachgroup_female_df = pd.DataFrame.from_dict(gazenum_foreachgroup_female,orient='index')\n",
    "gazenum_foreachgroup_female_df = gazenum_foreachgroup_female_df.transpose()\n",
    "gazenum_foreachgroup_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([gazenum_foreachgroup_sub_df,gazenum_foreachgroup_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "\n",
    "# anova\n",
    "cw_lm=ols('value ~ type + condition + type:condition', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition']+df_long2['type'], alpha=0.05)\n",
    "print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the anova on sub and dom\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "gazenum_foreachgroup_sub_df = pd.DataFrame.from_dict(gazenum_foreachgroup_sub,orient='index')\n",
    "gazenum_foreachgroup_sub_df = gazenum_foreachgroup_sub_df.transpose()\n",
    "gazenum_foreachgroup_sub_df['type'] = 'sub'\n",
    "gazenum_foreachgroup_dom_df = pd.DataFrame.from_dict(gazenum_foreachgroup_dom,orient='index')\n",
    "gazenum_foreachgroup_dom_df = gazenum_foreachgroup_dom_df.transpose()\n",
    "gazenum_foreachgroup_dom_df['type'] = 'dom'\n",
    "#\n",
    "df_long=pd.concat([gazenum_foreachgroup_male_df,gazenum_foreachgroup_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "\n",
    "# anova\n",
    "cw_lm=ols('value ~ type + condition + type:condition', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition']+df_long2['type'], alpha=0.05)\n",
    "print(tukey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650e101",
   "metadata": {},
   "source": [
    "#### plot the correlation between gaze number and sucessful rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "if do_onlyLearningsess:\n",
    "    animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_2',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "if do_trainedMCs:\n",
    "    grouptypes = ['self reward','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 1, -1]\n",
    "ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "gazenum_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "gazenum_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "succrate_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "succrate_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "#\n",
    "malenames = ['eddie','dodson','dannon','vermelho']\n",
    "femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "if do_onlyLearningsess:\n",
    "    malenames = ['eddie','dodson',]\n",
    "    femalenames = ['sparkle','scorch','kanga_2','ginger',]\n",
    "gazenum_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "gazenum_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "succrate_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "succrate_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "#\n",
    "subnames = ['eddie','dodson','dannon','ginger','vermelho']\n",
    "domnames = ['sparkle','scorch','kanga_1','kanga_2','koala']\n",
    "if do_onlyLearningsess:\n",
    "    subnames = ['eddie','dodson','ginger',]\n",
    "    domnames = ['sparkle','scorch','kanga_2',]\n",
    "gazenum_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "gazenum_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "succrate_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "succrate_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "#\n",
    "for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "    \n",
    "    grouptype = grouptypes[igrouptype]\n",
    "    coopthres_ID = coopthres_IDs[igrouptype]\n",
    "    \n",
    "    gazenum_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    succrate_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "\n",
    "    #\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1 = animal1_fixedorders[ianimalpair]\n",
    "        animal2 = animal2_fixedorders[ianimalpair]\n",
    "        \n",
    "        if (animal2 == 'kanga_1') | (animal2 == 'kanga_2'):\n",
    "            animal2_filename = 'kanga'\n",
    "        else:\n",
    "            animal2_filename = animal2\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1+animal2_filename+'/'\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            owgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            owgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            mtgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            mtgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            pull1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "        # combine owgaze and mtgaze\n",
    "        gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "        gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "\n",
    "        #\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "        # \n",
    "        gazenum_foreachgroup_foreachAni[grouptype][animal1] = gaze1_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "        gazenum_foreachgroup_foreachAni[grouptype][animal2] = gaze2_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "        succrate_foreachgroup_foreachAni[grouptype][animal1] = succ_rate_all_dates[coopthres_forsort==coopthres_ID]\n",
    "        succrate_foreachgroup_foreachAni[grouptype][animal2] = succ_rate_all_dates[coopthres_forsort==coopthres_ID]\n",
    "    \n",
    "    # combine across all animals\n",
    "    gazenum_foreachgroup_all[grouptype] = np.hstack(list(gazenum_foreachgroup_foreachAni[grouptype].values()))\n",
    "    succrate_foreachgroup_all[grouptype] = np.hstack(list(succrate_foreachgroup_foreachAni[grouptype].values()))\n",
    "    \n",
    "    # combine across male and female\n",
    "    # gaze number\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[malenames]\n",
    "    gazenum_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[femalenames]\n",
    "    gazenum_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "    # successful rate\n",
    "    df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[malenames]\n",
    "    succrate_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[femalenames]\n",
    "    succrate_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "    \n",
    "    # combine across sub and dom\n",
    "    # gaze number\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[subnames]\n",
    "    gazenum_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[domnames]\n",
    "    gazenum_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "    # successful rate\n",
    "    df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[subnames]\n",
    "    succrate_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[domnames]\n",
    "    succrate_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "    \n",
    "        \n",
    "# scatter plot + correlation line \n",
    "fig, axs = plt.subplots(1,4)\n",
    "fig.set_figheight(5*1)\n",
    "fig.set_figwidth(5*4)\n",
    "\n",
    "#  condtypes_forplot = ['3s threshold','2s threshold','1.5s threshold','1s threshold']\n",
    "condtypes_forplot = ['1s threshold']\n",
    "# condtypes_forplot = ['novision']\n",
    "condtypes_filename = '1scoop'\n",
    "# condtypes_filename = 'novision'\n",
    "\n",
    "# subplot 1 - all animals\n",
    "xxx = np.hstack([succrate_foreachgroup_all[condname] for condname in condtypes_forplot])\n",
    "yyy = np.hstack([gazenum_foreachgroup_all[condname] for condname in condtypes_forplot])\n",
    "p_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').pvalue\n",
    "r_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').rvalue\n",
    "# \n",
    "seaborn.regplot(ax=axs[0], x=xxx, y=yyy,label=condtypes_forplot[0])\n",
    "axs[0].set_title('all animals' ,fontsize=17)\n",
    "axs[0].set_xlabel('success rate',fontsize=15)\n",
    "axs[0].set_xlim([-0.05,0.8])\n",
    "axs[0].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[0].set_ylim([-0.05,0.45])\n",
    "axs[0].legend()\n",
    "axs[0].text(0.27,0.38,'regression r='+\"{:.2f}\".format(r_reg),fontsize=10)\n",
    "axs[0].text(0.27,0.36,'regression p='+\"{:.2f}\".format(p_reg),fontsize=10)\n",
    "\n",
    "# subplot 2 - male and female\n",
    "xxx_m = np.hstack([succrate_foreachgroup_male[condname] for condname in condtypes_forplot])\n",
    "yyy_m = np.hstack([gazenum_foreachgroup_male[condname] for condname in condtypes_forplot])\n",
    "ind_good = ~np.isnan(xxx_m) & ~np.isnan(yyy_m)\n",
    "xxx_m = xxx_m[ind_good]\n",
    "yyy_m = yyy_m[ind_good]\n",
    "dfm = pd.DataFrame({'succrate':xxx_m,'gazenum':yyy_m})\n",
    "dfm['condtype'] = 'male'\n",
    "p_reg_m = scipy.stats.linregress(xxx_m, yyy_m, alternative='two-sided').pvalue\n",
    "r_reg_m = scipy.stats.linregress(xxx_m, yyy_m, alternative='two-sided').rvalue\n",
    "#\n",
    "xxx_f = np.hstack([succrate_foreachgroup_female[condname] for condname in condtypes_forplot])\n",
    "yyy_f = np.hstack([gazenum_foreachgroup_female[condname] for condname in condtypes_forplot])\n",
    "ind_good = ~np.isnan(xxx_f) & ~np.isnan(yyy_f)\n",
    "xxx_f = xxx_f[ind_good]\n",
    "yyy_f = yyy_f[ind_good]\n",
    "dff = pd.DataFrame({'succrate':xxx_f,'gazenum':yyy_f})\n",
    "dff['condtype'] = 'female'\n",
    "p_reg_f = scipy.stats.linregress(xxx_f, yyy_f, alternative='two-sided').pvalue\n",
    "r_reg_f = scipy.stats.linregress(xxx_f, yyy_f, alternative='two-sided').rvalue\n",
    "# \n",
    "dfmf = pd.concat([dfm,dff]).reset_index(drop=True)\n",
    "model_interaction = sm.formula.ols('gazenum ~ succrate + condtype + succrate*condtype', data=dfmf).fit()\n",
    "p_slopediff = model_interaction.pvalues['succrate:condtype[T.male]']\n",
    "p_slopeboth = model_interaction.pvalues['succrate']\n",
    "#\n",
    "seaborn.regplot(ax=axs[1], x=xxx_m, y=yyy_m,label='male')\n",
    "seaborn.regplot(ax=axs[1], x=xxx_f, y=yyy_f,label='female')\n",
    "axs[1].set_title('male and female' ,fontsize=17)\n",
    "axs[1].set_xlabel('success rate',fontsize=15)\n",
    "axs[1].set_xlim([-0.05,0.8])\n",
    "axs[1].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[1].set_ylim([-0.05,0.45])\n",
    "axs[1].legend()\n",
    "axs[1].text(0.27,0.38,'male reg r='+\"{:.2f}\".format(r_reg_m),fontsize=10)\n",
    "axs[1].text(0.27,0.36,'male reg p='+\"{:.2f}\".format(p_reg_m),fontsize=10)\n",
    "axs[1].text(0.27,0.34,'female reg r='+\"{:.2f}\".format(r_reg_f),fontsize=10)\n",
    "axs[1].text(0.27,0.32,'female reg p='+\"{:.2f}\".format(p_reg_f),fontsize=10)\n",
    "axs[1].text(0.27,0.30,'slope diff ANCOVA p='+\"{:.2f}\".format(p_slopediff),fontsize=10)\n",
    "axs[1].text(0.27,0.28,'both slope ANCOVA p='+\"{:.2f}\".format(p_slopeboth),fontsize=10)\n",
    "\n",
    "# subplot 3 - sub and dom\n",
    "xxx_s = np.hstack([succrate_foreachgroup_sub[condname] for condname in condtypes_forplot])\n",
    "yyy_s = np.hstack([gazenum_foreachgroup_sub[condname] for condname in condtypes_forplot])\n",
    "ind_good = ~np.isnan(xxx_s) & ~np.isnan(yyy_s)\n",
    "xxx_s = xxx_s[ind_good]\n",
    "yyy_s = yyy_s[ind_good]\n",
    "dfs = pd.DataFrame({'succrate':xxx_s,'gazenum':yyy_s})\n",
    "dfs['condtype'] = 'sub'\n",
    "p_reg_s = scipy.stats.linregress(xxx_s, yyy_s, alternative='two-sided').pvalue\n",
    "r_reg_s = scipy.stats.linregress(xxx_s, yyy_s, alternative='two-sided').rvalue\n",
    "#\n",
    "xxx_d = np.hstack([succrate_foreachgroup_dom[condname] for condname in condtypes_forplot])\n",
    "yyy_d = np.hstack([gazenum_foreachgroup_dom[condname] for condname in condtypes_forplot])\n",
    "ind_good = ~np.isnan(xxx_d) & ~np.isnan(yyy_d)\n",
    "xxx_d = xxx_d[ind_good]\n",
    "yyy_d = yyy_d[ind_good]\n",
    "dfd = pd.DataFrame({'succrate':xxx_d,'gazenum':yyy_d})\n",
    "dfd['condtype'] = 'dom'\n",
    "p_reg_d = scipy.stats.linregress(xxx_d, yyy_d, alternative='two-sided').pvalue\n",
    "r_reg_d = scipy.stats.linregress(xxx_d, yyy_d, alternative='two-sided').rvalue\n",
    "# \n",
    "dfsd = pd.concat([dfs,dfd]).reset_index(drop=True)\n",
    "model_interaction = sm.formula.ols('gazenum ~ succrate + condtype + succrate*condtype', data=dfsd).fit()\n",
    "p_slopediff = model_interaction.pvalues['succrate:condtype[T.sub]']\n",
    "p_slopeboth = model_interaction.pvalues['succrate']\n",
    "# \n",
    "seaborn.regplot(ax=axs[2], x=xxx_s, y=yyy_s,label='subordinate')\n",
    "seaborn.regplot(ax=axs[2], x=xxx_d, y=yyy_d,label='dominant')\n",
    "axs[2].set_title('sub and dom' ,fontsize=17)\n",
    "axs[2].set_xlabel('success rate',fontsize=15)\n",
    "axs[2].set_xlim([-0.05,0.8])\n",
    "axs[2].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[2].set_ylim([-0.05,0.45])\n",
    "axs[2].legend()\n",
    "axs[2].text(0.27,0.38,'sub reg r='+\"{:.2f}\".format(r_reg_s),fontsize=10)\n",
    "axs[2].text(0.27,0.36,'sub reg p='+\"{:.2f}\".format(p_reg_s),fontsize=10)\n",
    "axs[2].text(0.27,0.34,'dom reg r='+\"{:.2f}\".format(r_reg_d),fontsize=10)\n",
    "axs[2].text(0.27,0.32,'dom reg p='+\"{:.2f}\".format(p_reg_d),fontsize=10)\n",
    "axs[2].text(0.27,0.30,'slope diff ANCOVA p='+\"{:.2f}\".format(p_slopediff),fontsize=10)\n",
    "axs[2].text(0.27,0.28,'both slope ANCOVA p='+\"{:.2f}\".format(p_slopeboth),fontsize=10)\n",
    "\n",
    "    \n",
    "# ancova comparison of regression slopes (between cooperation and NV)\n",
    "#\n",
    "# condtypes_forplot = ['3s threshold','2s threshold','1.5s threshold','1s threshold']\n",
    "condtype1_forplot = ['1s threshold']\n",
    "condtype2_forplot = ['novision']\n",
    "#\n",
    "xxx1 = np.hstack([succrate_foreachgroup_all[condname] for condname in condtype1_forplot])\n",
    "yyy1 = np.hstack([gazenum_foreachgroup_all[condname] for condname in condtype1_forplot])\n",
    "df1 = pd.DataFrame({'succrate':xxx1,'gazenum':yyy1})\n",
    "df1['condtype'] = 'coop'\n",
    "#\n",
    "xxx2 = np.hstack([succrate_foreachgroup_all[condname] for condname in condtype2_forplot])\n",
    "yyy2 = np.hstack([gazenum_foreachgroup_all[condname] for condname in condtype2_forplot])\n",
    "df2 = pd.DataFrame({'succrate':xxx2,'gazenum':yyy2})\n",
    "df2['condtype'] = 'nov'\n",
    "#\n",
    "df12 = pd.concat([df1,df2]).reset_index(drop=True)\n",
    "#\n",
    "model_interaction = sm.formula.ols('gazenum ~ succrate + condtype + succrate*condtype', data=df12).fit()\n",
    "p_slopediff = model_interaction.pvalues['succrate:condtype[T.nov]']\n",
    "p_slopeboth = model_interaction.pvalues['succrate']\n",
    "#\n",
    "seaborn.regplot(ax=axs[3], data = df12[df12['condtype']=='coop'], x='succrate', y='gazenum',label='MC')\n",
    "seaborn.regplot(ax=axs[3], data = df12[df12['condtype']=='nov'], x='succrate', y='gazenum',label = 'NV')\n",
    "#\n",
    "axs[3].set_title('all animals' ,fontsize=17)\n",
    "axs[3].set_xlabel('success rate',fontsize=15)\n",
    "axs[3].set_xlim([-0.05,0.90])\n",
    "axs[3].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "axs[3].set_ylim([-0.05,0.45])\n",
    "axs[3].legend()\n",
    "axs[3].text(0.00,0.36,'slope diff ANCOVA p='+\"{:.2f}\".format(p_slopediff),fontsize=10)\n",
    "axs[3].text(0.00,0.34,'both slope ANCOVA p='+\"{:.2f}\".format(p_slopeboth),fontsize=10)\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+condtypes_filename+'_gazenumbers_succrate_correlation_acrossAllAnimals.pdf')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.hstack([succrate_foreachgroup_all[condname] for condname in condtypes_forplot])\n",
    "np.shape(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7b3c8",
   "metadata": {},
   "source": [
    "####  plot the gaze distribution along the continuous variables - pool across animals  and sesisons\n",
    "#### only focus on \"otherani_otherlever_dist\", plot the ratio of number of 'decrasing' phase vs 'increasing' phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a72e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "# animal1_fixedorders = ['eddie','dodson','dannon','ginger',]\n",
    "# animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "# type_tgt = 'otherani_otherlever_dist'\n",
    "type_tgt = 'animal_lever_dist'\n",
    "# type_tgt = 'animal_animal_dist'\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "if do_trainedMCs:\n",
    "    grouptypes = ['self reward','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 1, -1]\n",
    "ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "gazeDist_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "gazeDist_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "#\n",
    "malenames = ['eddie','dodson','dannon','vermelho']\n",
    "femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "# malenames = ['eddie','dodson','dannon',]\n",
    "# femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger',]\n",
    "gazeDist_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "gazeDist_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "#\n",
    "subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "# subnames = ['eddie','dodson','dannon','ginger',]\n",
    "# domnames = ['sparkle','scorch','kanga_1','kanga_2',]\n",
    "gazeDist_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "gazeDist_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "#\n",
    "for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "    \n",
    "    grouptype = grouptypes[igrouptype]\n",
    "    coopthres_ID = coopthres_IDs[igrouptype]\n",
    "    \n",
    "    gazeDist_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    \n",
    "    #\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1 = animal1_fixedorders[ianimalpair]\n",
    "        animal2 = animal2_fixedorders[ianimalpair]\n",
    "        \n",
    "        if (animal2 == 'kanga_1') | (animal2 == 'kanga_2'):\n",
    "            animal2_filename = 'kanga'\n",
    "        else:\n",
    "            animal2_filename = animal2\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1+animal2_filename+'/'\n",
    "        \n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/gazeDist_phaseof_contbhvvar_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "            gazeDist_phaseof_contbhvvar_all_dates = pickle.load(f) \n",
    "            \n",
    "        #\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "        \n",
    "            \n",
    "        dates_all_dates = np.array(list(gazeDist_phaseof_contbhvvar_all_dates.keys()))\n",
    "        dates_igrouptype = dates_all_dates[coopthres_forsort.transpose()[0]==coopthres_ID]\n",
    "        ndates = np.shape(dates_igrouptype)[0]\n",
    "        \n",
    "        #\n",
    "        gazeDist_ratios_animal1 = np.ones(np.shape(dates_igrouptype))*np.nan\n",
    "        gazeDist_ratios_animal2 = np.ones(np.shape(dates_igrouptype))*np.nan\n",
    "        \n",
    "        for idate in np.arange(0,ndates,1):     \n",
    "            # \n",
    "            # for animal 1\n",
    "            gazeDist_idate = gazeDist_phaseof_contbhvvar_all_dates[dates_igrouptype[idate]][animal1][type_tgt]\n",
    "            gateDist_ratio_idate = np.sum(gazeDist_idate['phase']=='decreasing')/np.sum(gazeDist_idate['phase']=='increasing')\n",
    "            gazeDist_ratios_animal1[idate] = gateDist_ratio_idate\n",
    "\n",
    "            # for animal 2\n",
    "            gazeDist_idate = gazeDist_phaseof_contbhvvar_all_dates[dates_igrouptype[idate]][animal2_filename][type_tgt]\n",
    "            gateDist_ratio_idate = np.sum(gazeDist_idate['phase']=='decreasing')/np.sum(gazeDist_idate['phase']=='increasing')\n",
    "            gazeDist_ratios_animal2[idate] = gateDist_ratio_idate\n",
    "    \n",
    "        # \n",
    "        gazeDist_foreachgroup_foreachAni[grouptype][animal1] = gazeDist_ratios_animal1\n",
    "        gazeDist_foreachgroup_foreachAni[grouptype][animal2] = gazeDist_ratios_animal2\n",
    "        \n",
    "    \n",
    "    # combine across all animals\n",
    "    gazeDist_foreachgroup_all[grouptype] = np.hstack(list(gazeDist_foreachgroup_foreachAni[grouptype].values()))\n",
    "    \n",
    "    # combine across male and female\n",
    "    df = pd.DataFrame.from_dict(gazeDist_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[malenames]\n",
    "    gazeDist_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(gazeDist_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[femalenames]\n",
    "    gazeDist_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "    \n",
    "    # combine across sub and dom\n",
    "    df = pd.DataFrame.from_dict(gazeDist_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[subnames]\n",
    "    gazeDist_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "    #\n",
    "    df = pd.DataFrame.from_dict(gazeDist_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "    df = df.transpose()[domnames]\n",
    "    gazeDist_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "    \n",
    "    \n",
    "        \n",
    "# box plot \n",
    "fig, axs = plt.subplots(3,1)\n",
    "fig.set_figheight(5*3)\n",
    "fig.set_figwidth(3*2)\n",
    "\n",
    "# subplot 1 - all animals\n",
    "gazeDist_foreachgroup_all_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_all,orient='index')\n",
    "gazeDist_foreachgroup_all_df = gazeDist_foreachgroup_all_df.transpose()\n",
    "gazeDist_foreachgroup_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([gazeDist_foreachgroup_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "seaborn.boxplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs[0].plot([-0.5,5.5],[1,1],'k--')\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].set_xticklabels('')\n",
    "axs[0].xaxis.set_tick_params(labelsize=15)\n",
    "axs[0].set_ylabel(\"ratio\",fontsize=15)\n",
    "axs[0].set_title(\"social gaze on the decreasing phase / inecreasing phase of \"+type_tgt)\n",
    "axs[0].set_ylim([-0.1,2.7])\n",
    "axs[0].legend(fontsize=18)\n",
    "\n",
    "# subplot 2 - male and female animals\n",
    "gazeDist_foreachgroup_male_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_male,orient='index')\n",
    "gazeDist_foreachgroup_male_df = gazeDist_foreachgroup_male_df.transpose()\n",
    "gazeDist_foreachgroup_male_df['type'] = 'male'\n",
    "gazeDist_foreachgroup_female_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_female,orient='index')\n",
    "gazeDist_foreachgroup_female_df = gazeDist_foreachgroup_female_df.transpose()\n",
    "gazeDist_foreachgroup_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([gazeDist_foreachgroup_male_df,gazeDist_foreachgroup_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "seaborn.boxplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs[1].plot([-0.5,5.5],[1,1],'k--')\n",
    "axs[1].set_xlabel('')\n",
    "axs[1].set_xticklabels('')\n",
    "axs[1].xaxis.set_tick_params(labelsize=15)\n",
    "axs[1].set_ylabel(\"ratio\",fontsize=15)\n",
    "# axs[1].set_title('male and female' ,fontsize=24)\n",
    "axs[1].set_ylim([-0.1,2.7])\n",
    "axs[1].legend(fontsize=18)\n",
    "\n",
    "# subplot 3 - dom and sub animals\n",
    "gazeDist_foreachgroup_sub_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_sub,orient='index')\n",
    "gazeDist_foreachgroup_sub_df = gazeDist_foreachgroup_sub_df.transpose()\n",
    "gazeDist_foreachgroup_sub_df['type'] = 'sub'\n",
    "gazeDist_foreachgroup_dom_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_dom,orient='index')\n",
    "gazeDist_foreachgroup_dom_df = gazeDist_foreachgroup_dom_df.transpose()\n",
    "gazeDist_foreachgroup_dom_df['type'] = 'dom'\n",
    "#\n",
    "df_long=pd.concat([gazeDist_foreachgroup_sub_df,gazeDist_foreachgroup_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "seaborn.boxplot(ax=axs[2],data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs[2].plot([-0.5,5.5],[1,1],'k--')\n",
    "axs[2].set_xlabel('')\n",
    "axs[2].set_xticklabels(axs[2].get_xticklabels(),rotation=45)\n",
    "axs[2].xaxis.set_tick_params(labelsize=15)\n",
    "axs[2].set_ylabel(\"ratio\",fontsize=15)\n",
    "# axs[2].set_title('sub and dom' ,fontsize=24)\n",
    "axs[2].set_ylim([-0.1,2.7])\n",
    "axs[2].legend(fontsize=18)\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumberratio_decreasingphaseVSincreasingphase\"+type_tgt+\"_\"+'acrossAllAnimals.pdf')\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eda78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeDist_phaseof_contbhvvar_all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the anova on all animals\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "gazeDist_foreachgroup_all_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_all,orient='index')\n",
    "gazeDist_foreachgroup_all_df = gazeDist_foreachgroup_all_df.transpose()\n",
    "gazeDist_foreachgroup_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([gazeDist_foreachgroup_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "\n",
    "# Remove any NaN or infinite values\n",
    "df_long2 = df_long2.replace([np.inf, -np.inf], np.nan)\n",
    "df_long2 = df_long2.dropna(subset=['value', 'condition'])\n",
    "\n",
    "# anova\n",
    "cw_lm=ols('value ~ condition', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition'], alpha=0.05)\n",
    "print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the anova on male and female\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "gazeDist_foreachgroup_male_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_male,orient='index')\n",
    "gazeDist_foreachgroup_male_df = gazeDist_foreachgroup_male_df.transpose()\n",
    "gazeDist_foreachgroup_male_df['type'] = 'male'\n",
    "gazeDist_foreachgroup_female_df = pd.DataFrame.from_dict(gazeDist_foreachgroup_female,orient='index')\n",
    "gazeDist_foreachgroup_female_df = gazeDist_foreachgroup_female_df.transpose()\n",
    "gazeDist_foreachgroup_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([gazeDist_foreachgroup_male_df,gazeDist_foreachgroup_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "\n",
    "# Remove any NaN or infinite values\n",
    "df_long2 = df_long2.replace([np.inf, -np.inf], np.nan)\n",
    "df_long2 = df_long2.dropna(subset=['value', 'condition'])\n",
    "\n",
    "# anova\n",
    "cw_lm=ols('value ~ type + condition + type:condition', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition']+df_long2['type'], alpha=0.05)\n",
    "print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# xxx = gazeDist_foreachgroup_all_df['1s threshold']\n",
    "# xxx = gazeDist_foreachgroup_dom_df['1s threshold']\n",
    "xxx = gazeDist_foreachgroup_male_df['novision']\n",
    "xxx_clean = xxx[~np.isnan(xxx) & ~np.isinf(xxx)]\n",
    "\n",
    "st.ttest_1samp(xxx_clean,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86494ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50925eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6379d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ec914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aaf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e995eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
