{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### use GLM model to analyze spike count trains, based on discrete behavioral variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - make force quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.make_force_quantiles import make_force_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - GLM fitting for spike trains based on the discrete variables from single camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.singlecam_bhv_var_neuralGLM_fitting import get_singlecam_bhv_var_for_neuralGLM_fitting\n",
    "from ana_functions.singlecam_bhv_var_neuralGLM_fitting import neuralGLM_fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240910_Kanga_EffortBasedMC',\n",
    "                                    '20240911_Kanga_EffortBasedMC',\n",
    "                                    '20240912_Kanga_EffortBasedSR',\n",
    "                                    '20240913_Kanga_EffortBasedSR',\n",
    "                                    '20240916_Kanga_EffortBasedMC',\n",
    "                                    '20240917_Kanga_EffortBasedSR',\n",
    "                                    '20240918_Kanga_EffortBasedMC',\n",
    "                                    '20241008_Kanga_EffortBasedMC',\n",
    "                                    '20241009_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241010_Kanga_EffortBasedMC',\n",
    "                                    '20241011_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241014_Kanga_EffortBasedMC',\n",
    "                                    '20241016_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241017_Kanga_EffortBasedMC',\n",
    "                                    '20241018_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241022_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241025_Kanga_DannonEffortBasedMC',\n",
    "                                    '20241101_Kanga_EffortBasedSR',\n",
    "                                    '20241104_Kanga_EffortBasedSR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'other_EffortBasedMC',\n",
    "                            'self_EffortBasedSR',\n",
    "                            'self_EffortBasedSR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240910',\n",
    "                        '20240911',\n",
    "                        '20240912',\n",
    "                        '20240913',\n",
    "                        '20240916',\n",
    "                        '20240917',\n",
    "                        '20240918',\n",
    "                        '20241008',\n",
    "                        '20241009',\n",
    "                        '20241010',\n",
    "                        '20241011',\n",
    "                        '20241014',\n",
    "                        '20241016',\n",
    "                        '20241017',\n",
    "                        '20241018',\n",
    "                        '20241022',\n",
    "                        '20241025',\n",
    "                        '20241101',\n",
    "                        '20241104',\n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240910',\n",
    "                            '20240911',\n",
    "                            '20240912',\n",
    "                            '20240913',\n",
    "                            '20240916',\n",
    "                            '20240917',\n",
    "                            '20240918',\n",
    "                            '20241008',\n",
    "                            '20241009',\n",
    "                            '20241010',\n",
    "                            '20241011',\n",
    "                            '20241014',\n",
    "                            '20241016',\n",
    "                            '20241017',\n",
    "                            '20241018',\n",
    "                            '20241022',\n",
    "                            '20241025',\n",
    "                            '20241101',\n",
    "                            '20241104',\n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                90.1,\n",
    "                                69.5,\n",
    "                                62.5,\n",
    "                                0.00,\n",
    "                                43.5,\n",
    "                                59.6,\n",
    "                                0.00,\n",
    "                                66.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                       ]\n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        neural_record_conditions = [\n",
    "                                \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                       \n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "#\n",
    "    \n",
    "# a test case\n",
    "if 0:\n",
    "    neural_record_conditions = ['20240910_Kanga_EffortBasedMC',]\n",
    "    dates_list = ['20240910',]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['self_EffortBasedMC',]\n",
    "    session_start_times = [0.00] # in second\n",
    "    kilosortvers = [4]\n",
    "    animal1_fixedorders = ['dannon']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dannon\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables  \n",
    "\n",
    "# GLM related variables\n",
    "Kernel_coefs_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_spikehist_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_selfforce_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_partnerforce_all_dates = dict.fromkeys(dates_list, [])\n",
    "#\n",
    "Kernel_coefs_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_spikehist_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_selfforce_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_partnerforce_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "Kernel_coefs_stretagy_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_spikehist_stretagy_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_selfforce_stretagy_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_partnerforce_stretagy_all_dates = dict.fromkeys(dates_list, [])\n",
    "#\n",
    "Kernel_coefs_stretagy_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_spikehist_stretagy_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_selfforce_stretagy_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_partnerforce_stretagy_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_forceManipulation_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9718c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    #\n",
    "    with open(data_saved_subfolder+'/Kernel_coefs_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_coefs_all_dates = pickle.load(f)         \n",
    "    with open(data_saved_subfolder+'/Kernel_spikehist_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_spikehist_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_coefs_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_coefs_all_shuffled_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_spikehist_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_spikehist_all_shuffled_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/Kernel_selfforce_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_selfforce_all_dates = pickle.load(f)         \n",
    "    with open(data_saved_subfolder+'/Kernel_partnerforce_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_partnerforce_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_selfforce_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_selfforce_all_shuffled_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_partnerforce_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_partnerforce_all_shuffled_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/Kernel_coefs_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_coefs_stretagy_all_dates = pickle.load(f)         \n",
    "    with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_spikehist_stretagy_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_coefs_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_coefs_stretagy_all_shuffled_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_spikehist_stretagy_all_shuffled_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_selfforce_stretagy_all_dates = pickle.load(f)         \n",
    "    with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_partnerforce_stretagy_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_selfforce_stretagy_all_shuffled_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        Kernel_partnerforce_stretagy_all_shuffled_dates = pickle.load(f) \n",
    "    \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "      \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        #####\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_lever_reading_\" + \"*.json\") \n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            lever_reading = pd.read_json(lever_reading_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line) \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            lever_reading_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_lever_reading_\" + \"*.json\")             \n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            lever_reading = pd.read_json(lever_reading_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line) \n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        # change lever reading time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(lever_reading)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = lever_reading[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = lever_reading[ind][\"readout_timepoint\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        lever_reading[\"readout_timepoint\"] = time_points_new[\"time_points_new\"]\n",
    "        lever_reading = lever_reading[lever_reading[\"readout_timepoint\"] != 0]\n",
    "        #\n",
    "        lever1_pull = lever_reading[(lever_reading['lever_id']==1)&(lever_reading['pull_or_release']==1)]\n",
    "        lever1_release = lever_reading[(lever_reading['lever_id']==1)&(lever_reading['pull_or_release']==0)]\n",
    "        lever2_pull = lever_reading[(lever_reading['lever_id']==2)&(lever_reading['pull_or_release']==1)]\n",
    "        lever2_release = lever_reading[(lever_reading['lever_id']==2)&(lever_reading['pull_or_release']==0)]\n",
    "        #\n",
    "        if np.shape(lever1_release)[0]<np.shape(lever1_pull)[0]:\n",
    "            lever1_pull = lever1_pull.iloc[0:-1]\n",
    "        if np.shape(lever2_release)[0]<np.shape(lever2_pull)[0]:\n",
    "            lever2_pull = lever2_pull.iloc[0:-1]\n",
    "        #\n",
    "        lever1_pull_release = lever1_pull\n",
    "        lever1_pull_release['delta_timepoint'] = np.array(lever1_release['readout_timepoint'].reset_index(drop=True)-lever1_pull['readout_timepoint'].reset_index(drop=True))\n",
    "        lever1_pull_release['delta_gauge'] = np.array(lever1_release['strain_gauge'].reset_index(drop=True)-lever1_pull['strain_gauge'].reset_index(drop=True))\n",
    "        lever2_pull_release = lever2_pull\n",
    "        lever2_pull_release['delta_timepoint'] = np.array(lever2_release['readout_timepoint'].reset_index(drop=True)-lever2_pull['readout_timepoint'].reset_index(drop=True))\n",
    "        lever2_pull_release['delta_gauge'] = np.array(lever2_release['strain_gauge'].reset_index(drop=True)-lever2_pull['strain_gauge'].reset_index(drop=True))\n",
    "        \n",
    "        \n",
    "        #####\n",
    "        # load behavioral event results from the tracking analysis\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_forceManipulation_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_forceManipulation_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "\n",
    "        try:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "                            \n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,                                                                                                               sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        time_point_juice1 = output_time_points_socialgaze['time_point_juice1']\n",
    "        time_point_juice2 = output_time_points_socialgaze['time_point_juice2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "\n",
    "        # define successful pulls and failed pulls\n",
    "        trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "        bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "        #\n",
    "        time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "        time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "        time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "        time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "        #\n",
    "        trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "        bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "        #\n",
    "        time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "        time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "        time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "        time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # new total session time (instead of a fix time) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) # 30 is the fps, in the unit of second\n",
    "              \n",
    "        \n",
    "        #####     \n",
    "        # load neural recording data\n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "                  \n",
    "        # # load spike sorting results\n",
    "        print('load spike data for '+neural_record_condition)\n",
    "        if kilosortver == 2:\n",
    "            spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "            spike_time_data = np.load(spike_time_file)\n",
    "        elif kilosortver == 4:\n",
    "            spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "            spike_time_data = np.load(spike_time_file)\n",
    "        # \n",
    "        # align the FR recording time stamps\n",
    "        spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "        # down-sample the spike recording resolution to 30Hz\n",
    "        spike_time_data = spike_time_data/fs_spikes*fps\n",
    "        spike_time_data = np.round(spike_time_data)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "            spike_clusters_data = np.load(spike_clusters_file)\n",
    "            spike_channels_data = np.copy(spike_clusters_data)\n",
    "        elif kilosortver == 4:\n",
    "            spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "            spike_clusters_data = np.load(spike_clusters_file)\n",
    "            spike_channels_data = np.copy(spike_clusters_data)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "            channel_maps_data = np.load(channel_maps_file)\n",
    "        elif kilosortver == 4:\n",
    "            channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "            channel_maps_data = np.load(channel_maps_file)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "            channel_pos_data = np.load(channel_pos_file)\n",
    "        elif kilosortver == 4:\n",
    "            channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "            channel_pos_data = np.load(channel_pos_file)\n",
    "        #\n",
    "        if kilosortver == 2:\n",
    "            clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "            clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "        elif kilosortver == 4:\n",
    "            clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "            clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "        #\n",
    "        # only get the spikes that are manually checked\n",
    "        try:\n",
    "            good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "        except:\n",
    "            good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "        #\n",
    "        clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "        #\n",
    "        spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "        spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "        spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "\n",
    "        #\n",
    "        nclusters = np.shape(clusters_info_data)[0]\n",
    "        #\n",
    "        for icluster in np.arange(0,nclusters,1):\n",
    "            try:\n",
    "                cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "            except:\n",
    "                cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "            spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "        # \n",
    "        # get the channel to depth information, change 2 shanks to 1 shank \n",
    "        try:\n",
    "            channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "            # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "            # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "            channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "        except:\n",
    "            channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "            # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "            channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "\n",
    "            \n",
    "            \n",
    "        #####    \n",
    "        #####    \n",
    "        # after all the analysis and data loading, separate them based on different subblock    \n",
    "        # get task type and cooperation threshold\n",
    "        # tasktype: 1-normal SR, 2-force changed SR, 3-normal coop, 4-force changed coop\n",
    "        trialID_list = np.array(trial_record_clean['trial_number'],dtype = 'int')\n",
    "        tasktype_list = np.array(trial_record_clean['task_type'],dtype = 'int')\n",
    "        coop_thres_list = np.array(trial_record_clean['pulltime_thres'],dtype = 'int')\n",
    "        lever1force_list = np.array(trial_record_clean['lever1_force'],dtype = 'int')\n",
    "        lever2force_list = np.array(trial_record_clean['lever2_force'],dtype = 'int')\n",
    "        \n",
    "        # use the combination of lever 1/2 forces to separate trials\n",
    "        force12_uniques,indices = np.unique(np.vstack((lever1force_list,lever2force_list)),axis=1,return_index=True)\n",
    "        force12_uniques = force12_uniques[:,np.argsort(indices)]\n",
    "        ntrialtypes = np.shape(force12_uniques)[1]\n",
    "        \n",
    "        # \n",
    "        # put them in the summarizing result, organized for each day\n",
    "        force1_all_idate = np.zeros((0,)) \n",
    "        force2_all_idate = np.zeros((0,)) \n",
    "\n",
    "        subblockID_all_idate = np.zeros((0,))\n",
    "\n",
    "        succ_rate_all_idate = np.zeros((0,))\n",
    "        trialnum_all_idate = np.zeros((0,))\n",
    "        blockstarttime_all_idate = np.zeros((0,))\n",
    "        blockendtime_all_idate = np.zeros((0,))\n",
    "         \n",
    "        #    \n",
    "        for itrialtype in np.arange(0,ntrialtypes,1):\n",
    "            force1_unique = force12_uniques[0,itrialtype]\n",
    "            force2_unique = force12_uniques[1,itrialtype]\n",
    "\n",
    "            ind = np.isin(lever1force_list,force1_unique) & np.isin(lever2force_list,force2_unique)\n",
    "            \n",
    "            trialID_itrialtype = trialID_list[ind]\n",
    "            \n",
    "            tasktype_itrialtype = np.unique(tasktype_list[ind])\n",
    "            coop_thres_itrialtype = np.unique(coop_thres_list[ind])\n",
    "            \n",
    "            # save some simple measures\n",
    "            force1_all_idate = np.append(force1_all_idate,force1_unique)\n",
    "            force2_all_idate = np.append(force2_all_idate,force2_unique)\n",
    "            #\n",
    "            trialnum_all_idate = np.append(trialnum_all_idate,np.sum(ind))\n",
    "            subblockID_all_idate = np.append(subblockID_all_idate,itrialtype)\n",
    "            \n",
    "            # analyze behavior results\n",
    "            bhv_data_itrialtype = bhv_data[np.isin(bhv_data['trial_number'],trialID_itrialtype)]\n",
    "            \n",
    "            #\n",
    "            # successful rates\n",
    "            succ_rate_itrialtype = np.sum((bhv_data_itrialtype['behavior_events']==3)|(bhv_data_itrialtype['behavior_events']==4))/np.sum((bhv_data_itrialtype['behavior_events']==1)|(bhv_data_itrialtype['behavior_events']==2))\n",
    "            succ_rate_all_idate = np.append(succ_rate_all_idate,succ_rate_itrialtype)\n",
    "            #\n",
    "            # block time\n",
    "            block_starttime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==0]['time_points'].iloc[0]\n",
    "            blockstarttime_all_idate = np.append(blockstarttime_all_idate,block_starttime)\n",
    "            block_endtime = bhv_data_itrialtype[bhv_data_itrialtype['behavior_events']==9]['time_points'].iloc[-1]\n",
    "            blockendtime_all_idate = np.append(blockendtime_all_idate,block_endtime)\n",
    "        \n",
    "        \n",
    "        # translate force level to quantiles\n",
    "        force1_all_idate = make_force_quantiles(force1_all_idate)\n",
    "        force2_all_idate = make_force_quantiles(force2_all_idate)\n",
    "        \n",
    "        \n",
    "        # get the dataset for GLM and run GLM\n",
    "        starttime = bhv_data[bhv_data['behavior_events']==0]['time_points'].iloc[0]\n",
    "        endtime = bhv_data[bhv_data['behavior_events']==9]['time_points'].iloc[-1]\n",
    "        # \n",
    "        gaze_threshold = 0.5 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "\n",
    "        #\n",
    "        stg_twins = 3 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "\n",
    "        # get the organized data for GLM\n",
    "        print('get '+neural_record_condition+' data for single camera GLM fitting')\n",
    "        #\n",
    "        data_summary, data_summary_names, spiketrain_summary = get_singlecam_bhv_var_for_neuralGLM_fitting(animal1, animal2, animalnames_videotrack, \n",
    "                                                                session_start_time, starttime, endtime, totalsess_time, \n",
    "                                                                blockstarttime_all_idate, blockendtime_all_idate, force1_all_idate, force2_all_idate,\n",
    "                                                                stg_twins, time_point_pull1, time_point_pull2, time_point_juice1, time_point_juice2,\n",
    "                                                                time_point_pulls_succfail, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, gaze_threshold, \n",
    "                                                                spike_clusters_data, spike_time_data, spike_channels_data)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # GLM to behavioral events (actions)\n",
    "        if 1:\n",
    "            print('do single camera GLM fitting (behavioral events) for '+neural_record_condition)\n",
    "\n",
    "            # load saved data\n",
    "            try:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'+date_tgt+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/Kernel_coefs_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_coefs_allboots_allcells = pickle.load(f)         \n",
    "                with open(data_saved_subfolder+'/Kernel_spikehist_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_spikehist_allboots_allcells = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_coefs_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_coefs_allboots_allcells_shf = pickle.load(f)    \n",
    "                with open(data_saved_subfolder+'/Kernel_spikehist_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_spikehist_allboots_allcells_shf = pickle.load(f)  \n",
    "\n",
    "                with open(data_saved_subfolder+'/Kernel_selfforce_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_selfforce_allboots_allcells = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_partnerforce_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_partnerforce_allboots_allcells = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_selfforce_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_selfforce_allboots_allcells_shf = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_partnerforce_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_partnerforce_allboots_allcells_shf = pickle.load(f)   \n",
    "                \n",
    "                Kernel_coefs_all_dates[date_tgt] = Kernel_coefs_allboots_allcells\n",
    "                Kernel_spikehist_all_dates[date_tgt] = Kernel_spikehist_allboots_allcells\n",
    "                Kernel_selfforce_all_dates[date_tgt] = Kernel_selfforce_allboots_allcells\n",
    "                Kernel_partnerforce_all_dates[date_tgt] = Kernel_partnerforce_allboots_allcells\n",
    "                Kernel_coefs_all_shuffled_dates[date_tgt] = Kernel_coefs_allboots_allcells_shf\n",
    "                Kernel_spikehist_all_shuffled_dates[date_tgt] = Kernel_spikehist_allboots_allcells_shf\n",
    "                Kernel_selfforce_all_shuffled_dates[date_tgt] = Kernel_selfforce_allboots_allcells_shf\n",
    "                Kernel_partnerforce_all_shuffled_dates[date_tgt] = Kernel_partnerforce_allboots_allcells_shf\n",
    "                    \n",
    "                print('analyzed and loaded single camera GLM fitting (behavioral events) for '+neural_record_condition)\n",
    "            \n",
    "            except:\n",
    "            \n",
    "                nbootstraps = 20 # cannot be 1, will introduce error\n",
    "                traintestperc = 0.6\n",
    "\n",
    "                # select the behavioral variables that want to be in the GLM\n",
    "                dostrategies = 0\n",
    "                bhvvaris_toGLM = ['self leverpull_prob', 'self socialgaze_prob', 'self juice_prob', \n",
    "                                  'other leverpull_prob', 'other socialgaze_prob', 'other juice_prob', ]\n",
    "                # bhvvaris_toGLM = ['self leverpull_prob', 'self socialgaze_prob', ]\n",
    "\n",
    "                # the time window for behavioral variables, 0 means the spike time\n",
    "                trig_twin = [-4,4] # in the unit of second\n",
    "\n",
    "                # if consider the spike history\n",
    "                dospikehist = 1\n",
    "                spikehist_twin = 3 # the length the spike history to consider, in the unit of second; # has to smallerthan trig_twin\n",
    "\n",
    "                doforcelevel = 1\n",
    "\n",
    "                # if do a spike time shuffle to generate null distribution \n",
    "                donullshuffle = 1\n",
    "\n",
    "                doplots = 0 # plot the kernel for each variables and each cell clusters\n",
    "                savefig = 0 # save the plotted kernel function\n",
    "                #\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_singlecam/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "\n",
    "                (Kernel_coefs_allboots_allcells, Kernel_spikehist_allboots_allcells,\n",
    "                Kernel_selfforce_allboots_allcells, Kernel_partnerforce_allboots_allcells, \n",
    "                Kernel_coefs_allboots_allcells_shf, Kernel_spikehist_allboots_allcells_shf, \n",
    "                Kernel_selfforce_allboots_allcells_shf, Kernel_partnerforce_allboots_allcells_shf)  = neuralGLM_fitting(animal1, animal2, data_summary_names, data_summary, spiketrain_summary, \n",
    "                                                                   bhvvaris_toGLM, nbootstraps, traintestperc, trig_twin, dospikehist, spikehist_twin,\n",
    "                                                                   doplots, date_tgt, savefig, save_path, dostrategies, donullshuffle, doforcelevel)\n",
    "\n",
    "                Kernel_coefs_all_dates[date_tgt] = Kernel_coefs_allboots_allcells\n",
    "                Kernel_spikehist_all_dates[date_tgt] = Kernel_spikehist_allboots_allcells\n",
    "                Kernel_selfforce_all_dates[date_tgt] = Kernel_selfforce_allboots_allcells\n",
    "                Kernel_partnerforce_all_dates[date_tgt] = Kernel_partnerforce_allboots_allcells\n",
    "                Kernel_coefs_all_shuffled_dates[date_tgt] = Kernel_coefs_allboots_allcells_shf\n",
    "                Kernel_spikehist_all_shuffled_dates[date_tgt] = Kernel_spikehist_allboots_allcells_shf\n",
    "                Kernel_selfforce_all_shuffled_dates[date_tgt] = Kernel_selfforce_allboots_allcells_shf\n",
    "                Kernel_partnerforce_all_shuffled_dates[date_tgt] = Kernel_partnerforce_allboots_allcells_shf\n",
    "\n",
    "                # save the data for each date\n",
    "                if 0:\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'+date_tgt+'/'\n",
    "                    if not os.path.exists(data_saved_subfolder):\n",
    "                        os.makedirs(data_saved_subfolder)\n",
    "                    # GLM to behavioral events (actions)\n",
    "                    with open(data_saved_subfolder+'/Kernel_coefs_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_coefs_allboots_allcells, f)    \n",
    "                    with open(data_saved_subfolder+'/Kernel_spikehist_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_spikehist_allboots_allcells, f)    \n",
    "                    with open(data_saved_subfolder+'/Kernel_coefs_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_coefs_allboots_allcells_shf, f)    \n",
    "                    with open(data_saved_subfolder+'/Kernel_spikehist_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_spikehist_allboots_allcells_shf, f) \n",
    "\n",
    "                    with open(data_saved_subfolder+'/Kernel_selfforce_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_selfforce_allboots_allcells, f)  \n",
    "                    with open(data_saved_subfolder+'/Kernel_partnerforce_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_partnerforce_allboots_allcells, f) \n",
    "                    with open(data_saved_subfolder+'/Kernel_selfforce_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_selfforce_allboots_allcells_shf, f)  \n",
    "                    with open(data_saved_subfolder+'/Kernel_partnerforce_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_partnerforce_allboots_allcells_shf, f) \n",
    "\n",
    "        \n",
    "\n",
    "        # GLM to strategies (pairs of actions)\n",
    "        if 1:\n",
    "            print('do single camera GLM fitting (strategies) for '+neural_record_condition)\n",
    "\n",
    "            # load saved data\n",
    "            try:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'+date_tgt+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/Kernel_coefs_stretagy_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_coefs_stretagy_allboots_allcells = pickle.load(f)         \n",
    "                with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_spikehist_stretagy_allboots_allcells = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_coefs_stretagy_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_coefs_stretagy_allboots_allcells_shf = pickle.load(f)    \n",
    "                with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_spikehist_stretagy_allboots_allcells_shf = pickle.load(f)  \n",
    "\n",
    "                with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_selfforce_stretagy_allboots_allcells = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_allboots_allcells.pkl', 'rb') as f:\n",
    "                    Kernel_partnerforce_stretagy_allboots_allcells = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_selfforce_stretagy_allboots_allcells_shf = pickle.load(f)  \n",
    "                with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_allboots_allcells_shf.pkl', 'rb') as f:\n",
    "                    Kernel_partnerforce_stretagy_allboots_allcells_shf = pickle.load(f)   \n",
    "                \n",
    "                Kernel_coefs_stretagy_all_dates[date_tgt] = Kernel_coefs_stretagy_allboots_allcells\n",
    "                Kernel_spikehist_stretagy_all_dates[date_tgt] = Kernel_spikehist_stretagy_allboots_allcells\n",
    "                Kernel_selfforce_stretagy_all_dates[date_tgt] = Kernel_selfforce_stretagy_allboots_allcells\n",
    "                Kernel_partnerforce_stretagy_all_dates[date_tgt] = Kernel_partnerforce_stretagy_allboots_allcells\n",
    "\n",
    "                Kernel_coefs_stretagy_all_shuffled_dates[date_tgt] = Kernel_coefs_stretagy_allboots_allcells_shf\n",
    "                Kernel_spikehist_stretagy_all_shuffled_dates[date_tgt] = Kernel_spikehist_stretagy_allboots_allcells_shf\n",
    "                Kernel_selfforce_stretagy_all_shuffled_dates[date_tgt] = Kernel_selfforce_stretagy_allboots_allcells_shf\n",
    "                Kernel_partnerforce_stretagy_all_shuffled_dates[date_tgt] = Kernel_partnerforce_stretagy_allboots_allcells_shf        \n",
    "                    \n",
    "                print('analyzed and loaded single camera GLM fitting (strategies) for '+neural_record_condition)\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                nbootstraps = 20 # cannot be 1, will introduce error\n",
    "                traintestperc = 0.6\n",
    "\n",
    "                # select the behavioral variables that want to be in the GLM\n",
    "                dostrategies = 1\n",
    "                bhvvaris_toGLM = ['self sync_pull_prob', 'self gaze_lead_pull_prob', 'self social_attention_prob', \n",
    "                                 ]\n",
    "\n",
    "                # the time window for behavioral variables, 0 means the spike time\n",
    "                trig_twin = [-4,4] # in the unit of second\n",
    "\n",
    "                # if consider the spike history\n",
    "                dospikehist = 1\n",
    "                spikehist_twin = 3 # the length the spike history to consider, in the unit of second; # has to smallerthan trig_twin\n",
    "\n",
    "                # if consider the force\n",
    "                doconsiderforce = 1\n",
    "\n",
    "                # if do a spike time shuffle to generate null distribution \n",
    "                donullshuffle = 1\n",
    "\n",
    "                doplots = 0 # plot the kernel for each variables and each cell clusters\n",
    "                savefig = 0 # save the plotted kernel function\n",
    "                #\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_singlecam/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "\n",
    "                (Kernel_coefs_stretagy_allboots_allcells, Kernel_spikehist_stretagy_allboots_allcells,\n",
    "                Kernel_selfforce_stretagy_allboots_allcells, Kernel_partnerforce_stretagy_allboots_allcells,\n",
    "                Kernel_coefs_stretagy_allboots_allcells_shf, Kernel_spikehist_stretagy_allboots_allcells_shf,\n",
    "                Kernel_selfforce_stretagy_allboots_allcells_shf, Kernel_partnerforce_stretagy_allboots_allcells_shf) = neuralGLM_fitting(animal1, animal2, data_summary_names, data_summary, spiketrain_summary, \n",
    "                                                                   bhvvaris_toGLM, nbootstraps, traintestperc, trig_twin, dospikehist, spikehist_twin,\n",
    "                                                                   doplots, date_tgt, savefig, save_path, dostrategies, donullshuffle, doforcelevel)\n",
    "\n",
    "                Kernel_coefs_stretagy_all_dates[date_tgt] = Kernel_coefs_stretagy_allboots_allcells\n",
    "                Kernel_spikehist_stretagy_all_dates[date_tgt] = Kernel_spikehist_stretagy_allboots_allcells\n",
    "                Kernel_selfforce_stretagy_all_dates[date_tgt] = Kernel_selfforce_stretagy_allboots_allcells\n",
    "                Kernel_partnerforce_stretagy_all_dates[date_tgt] = Kernel_partnerforce_stretagy_allboots_allcells\n",
    "\n",
    "                Kernel_coefs_stretagy_all_shuffled_dates[date_tgt] = Kernel_coefs_stretagy_allboots_allcells_shf\n",
    "                Kernel_spikehist_stretagy_all_shuffled_dates[date_tgt] = Kernel_spikehist_stretagy_allboots_allcells_shf\n",
    "                Kernel_selfforce_stretagy_all_shuffled_dates[date_tgt] = Kernel_selfforce_stretagy_allboots_allcells_shf\n",
    "                Kernel_partnerforce_stretagy_all_shuffled_dates[date_tgt] = Kernel_partnerforce_stretagy_allboots_allcells_shf        \n",
    "\n",
    "                # save the data for each date\n",
    "                if 0:\n",
    "\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'+date_tgt+'/'\n",
    "                    if not os.path.exists(data_saved_subfolder):\n",
    "                        os.makedirs(data_saved_subfolder)\n",
    "\n",
    "                    # GLM to strategies (pairs of actions)\n",
    "                    with open(data_saved_subfolder+'/Kernel_coefs_stretagy_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_coefs_stretagy_allboots_allcells, f)    \n",
    "                    with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_spikehist_stretagy_allboots_allcells, f)    \n",
    "                    with open(data_saved_subfolder+'/Kernel_coefs_stretagy_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_coefs_stretagy_allboots_allcells_shf, f)    \n",
    "                    with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_spikehist_stretagy_allboots_allcells_shf, f) \n",
    "\n",
    "                    with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_selfforce_stretagy_allboots_allcells, f)  \n",
    "                    with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_allboots_allcells.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_partnerforce_stretagy_allboots_allcells, f) \n",
    "                    with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_selfforce_stretagy_allboots_allcells_shf, f)  \n",
    "                    with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_allboots_allcells_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(Kernel_partnerforce_stretagy_allboots_allcells_shf, f) \n",
    "\n",
    "            \n",
    "            \n",
    "    # save the final data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # GLM to behavioral events (actions)\n",
    "        if 0:\n",
    "            with open(data_saved_subfolder+'/Kernel_coefs_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_coefs_all_dates, f)    \n",
    "            with open(data_saved_subfolder+'/Kernel_spikehist_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_spikehist_all_dates, f)    \n",
    "            with open(data_saved_subfolder+'/Kernel_coefs_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_coefs_all_shuffled_dates, f)    \n",
    "            with open(data_saved_subfolder+'/Kernel_spikehist_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_spikehist_all_shuffled_dates, f) \n",
    "                \n",
    "        if 1:\n",
    "            with open(data_saved_subfolder+'/Kernel_selfforce_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_selfforce_all_dates, f)  \n",
    "            with open(data_saved_subfolder+'/Kernel_partnerforce_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_partnerforce_all_dates, f) \n",
    "            with open(data_saved_subfolder+'/Kernel_selfforce_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_selfforce_all_shuffled_dates, f)  \n",
    "            with open(data_saved_subfolder+'/Kernel_partnerforce_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_partnerforce_all_shuffled_dates, f) \n",
    "                \n",
    "           \n",
    "        # GLM to strategies (pairs of actions)\n",
    "        if 0:\n",
    "            with open(data_saved_subfolder+'/Kernel_coefs_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_coefs_stretagy_all_dates, f)    \n",
    "            with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_spikehist_stretagy_all_dates, f)    \n",
    "            with open(data_saved_subfolder+'/Kernel_coefs_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_coefs_stretagy_all_shuffled_dates, f)    \n",
    "            with open(data_saved_subfolder+'/Kernel_spikehist_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_spikehist_stretagy_all_shuffled_dates, f) \n",
    "        \n",
    "        if 1:\n",
    "            with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_selfforce_stretagy_all_dates, f)  \n",
    "            with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_partnerforce_stretagy_all_dates, f) \n",
    "            with open(data_saved_subfolder+'/Kernel_selfforce_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_selfforce_stretagy_all_shuffled_dates, f)  \n",
    "            with open(data_saved_subfolder+'/Kernel_partnerforce_stretagy_all_shuffled_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(Kernel_partnerforce_stretagy_all_shuffled_dates, f) \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284a691",
   "metadata": {},
   "source": [
    "## plot - for individual animal\n",
    "### prepare the summarizing data set and run population level analysis such as PCA\n",
    "### plot the kernel defined based on the stretagy (pair of action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d16671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "Kernel_coefs_stretagy_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                          'Kernel_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    # make sure to be the same as the bhvvaris_toGLM\n",
    "    bhv_types = ['self sync_pull_prob', 'self gaze_lead_pull_prob', 'self social_attention_prob',]\n",
    "    nbhv_types = np.shape(bhv_types)[0]\n",
    "\n",
    "    for ibhv_type in np.arange(0,nbhv_types,1):\n",
    "        \n",
    "        bhv_type = bhv_types[ibhv_type]\n",
    "\n",
    "        clusterIDs = Kernel_coefs_stretagy_all_dates[date_tgt].keys()\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            kernel_ibhv = Kernel_coefs_stretagy_all_dates[date_tgt][iclusterID][:,ibhv_type,:]\n",
    "            \n",
    "            kernel_ibhv_average = np.nanmean(kernel_ibhv,axis = 0)\n",
    "\n",
    "            Kernel_coefs_stretagy_all_dates_df = Kernel_coefs_stretagy_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':bhv_type.split()[0],\n",
    "                                                                                    'bhv_name': bhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'Kernel_average':kernel_ibhv_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            \n",
    "            \n",
    "# only focus on the certain act animal and certain bhv_name\n",
    "# act_animals_all = np.unique(Kernel_coefs_stretagy_all_dates_df['act_animal'])\n",
    "act_animals_all = ['self']\n",
    "bhv_names_all = np.unique(Kernel_coefs_stretagy_all_dates_df['bhv_name'])\n",
    "# bhv_names_all = ['sync_pull_prob']\n",
    "conditions_all = np.unique(Kernel_coefs_stretagy_all_dates_df['condition'])\n",
    "\n",
    "nact_animals = np.shape(act_animals_all)[0]\n",
    "nbhv_names = np.shape(bhv_names_all)[0]\n",
    "nconditions = np.shape(conditions_all)[0]\n",
    "\n",
    "\n",
    "# run PCA and plot\n",
    "for ianimal in np.arange(0,nact_animals,1):\n",
    "    \n",
    "    act_animal = act_animals_all[ianimal]\n",
    "    \n",
    "    for icondition in np.arange(0,nconditions,1):\n",
    "        \n",
    "        task_condition = conditions_all[icondition]\n",
    "        \n",
    "        # set up for plotting\n",
    "        nPC_toplot = 4\n",
    "\n",
    "        # Create a figure with GridSpec, specifying height_ratios\n",
    "        fig = plt.figure(figsize=(nPC_toplot*2,6*nbhv_names))\n",
    "\n",
    "        # Define a grid with 4*nbhv_names rows in the left and nbhv_names rows in the right, \n",
    "        # but scale the right column's height by 3\n",
    "        gs = GridSpec(nPC_toplot*nbhv_names, 2, height_ratios=[1] * nPC_toplot*nbhv_names)\n",
    "\n",
    "        # Left column (4*nbhv_names rows, 1 column) for PC 1 to 4 traces\n",
    "        ax_left = [fig.add_subplot(gs[i, 0]) for i in range(nPC_toplot*nbhv_names)]  # Access all 4*nbhv_names rows in the left column\n",
    "\n",
    "        # Right column (nbhv_names rows, 1 column, scaling the height by using multiple rows for each plot)\n",
    "        # for the variance explanation\n",
    "        ax_right = [fig.add_subplot(gs[nPC_toplot * i:nPC_toplot * i + nPC_toplot, 1]) for i in range(nbhv_names)]  # Group 4 rows for each of the 3 subplots on the right\n",
    "\n",
    "\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (Kernel_coefs_stretagy_all_dates_df['act_animal']==act_animal)&(Kernel_coefs_stretagy_all_dates_df['bhv_name']==bhv_name)&(Kernel_coefs_stretagy_all_dates_df['condition']==task_condition)\n",
    "\n",
    "            Kernel_coefs_stretagy_tgt = np.vstack(list(Kernel_coefs_stretagy_all_dates_df[ind]['Kernel_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(Kernel_coefs_stretagy_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            Kernel_coefs_stretagy_tgt = Kernel_coefs_stretagy_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            Kernel_coefs_stretagy_pca = pca.fit_transform(Kernel_coefs_stretagy_tgt.transpose())\n",
    "\n",
    "            # Get the explained variance ratio\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "            # Calculate the cumulative explained variance\n",
    "            cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "            # Plot the cumulative explained variance\n",
    "            ax_right[ibhvname].plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', linestyle='--', color='blue', alpha=0.7)\n",
    "            ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "            ax_right[ibhvname].set_ylabel('Cumulative Percentage of Variance Explained')\n",
    "            ax_right[ibhvname].set_title(bhv_name)\n",
    "            ax_right[ibhvname].set_xticks(np.arange(1, len(cumulative_variance) + 1, 1))\n",
    "            ax_right[ibhvname].grid(True)\n",
    "            # \n",
    "            # if ibhvname == nbhv_names - 1:\n",
    "            #     ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "\n",
    "            # plot the PCs\n",
    "            for iPC_toplot in np.arange(0,nPC_toplot,1):\n",
    "\n",
    "                PCtoplot = Kernel_coefs_stretagy_pca[:,iPC_toplot]\n",
    "\n",
    "                trig_twin = [-4,4] # in the unit of second\n",
    "                xxx = np.arange(trig_twin[0]*fps,trig_twin[1]*fps,1)\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot(xxx, PCtoplot, 'k')\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot([0,0],[np.nanmin(PCtoplot)*1.1,np.nanmax(PCtoplot)*1.1],'k--')\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].set_title(bhv_name+' kernel PC'+str(iPC_toplot+1))\n",
    "\n",
    "                if iPC_toplot == nPC_toplot - 1:\n",
    "                    ax_left[iPC_toplot+ibhvname*nPC_toplot].set_xlabel('time (s)')\n",
    "\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()        \n",
    "        \n",
    "        if (animal1_filenames[0] == 'Kanga') | (animal2_filenames[0] == 'Kanga'):\n",
    "            recordedAnimal = 'Kanga'\n",
    "        elif (animal1_filenames[0] == 'Dodson') | (animal2_filenames[0] == 'Dodson'):\n",
    "            recordedAnimal = 'Dodson'\n",
    "\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_singlecam/\"+cameraID+\"/\"+recordedAnimal+\"_neuralGLM/\"\n",
    "\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            fig.savefig(figsavefolder+'stretagy_kernel_coefs_pca_patterns_all_dates'+savefile_sufix+'_'+act_animal+'action_in'+task_condition+'.pdf')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b8788",
   "metadata": {},
   "source": [
    "### prepare the summarizing data set and run population level analysis such as PCA\n",
    "### plot the kernel defined based on the single actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "Kernel_coefs_action_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                          'Kernel_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    # make sure to be the same as the bhvvaris_toGLM\n",
    "    bhv_types = ['self leverpull_prob', 'self socialgaze_prob', 'self juice_prob', \n",
    "                 'other leverpull_prob', 'other socialgaze_prob', 'other juice_prob', ]\n",
    "    nbhv_types = np.shape(bhv_types)[0]\n",
    "\n",
    "    for ibhv_type in np.arange(0,nbhv_types,1):\n",
    "        \n",
    "        bhv_type = bhv_types[ibhv_type]\n",
    "\n",
    "        clusterIDs = Kernel_coefs_all_dates[date_tgt].keys()\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            kernel_ibhv = Kernel_coefs_all_dates[date_tgt][iclusterID][:,ibhv_type,:]\n",
    "            \n",
    "            kernel_ibhv_average = np.nanmean(kernel_ibhv,axis = 0)\n",
    "\n",
    "            Kernel_coefs_action_all_dates_df = Kernel_coefs_action_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':bhv_type.split()[0],\n",
    "                                                                                    'bhv_name': bhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'Kernel_average':kernel_ibhv_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            \n",
    "            \n",
    "# only focus on the certain act animal and certain bhv_name\n",
    "act_animals_all = np.unique(Kernel_coefs_action_all_dates_df['act_animal'])\n",
    "# act_animals_all = ['self']\n",
    "bhv_names_all = np.unique(Kernel_coefs_action_all_dates_df['bhv_name'])\n",
    "# bhv_names_all = ['leverpull_prob']\n",
    "conditions_all = np.unique(Kernel_coefs_action_all_dates_df['condition'])\n",
    "\n",
    "nact_animals = np.shape(act_animals_all)[0]\n",
    "nbhv_names = np.shape(bhv_names_all)[0]\n",
    "nconditions = np.shape(conditions_all)[0]\n",
    "\n",
    "\n",
    "# run PCA and plot\n",
    "for ianimal in np.arange(0,nact_animals,1):\n",
    "    \n",
    "    act_animal = act_animals_all[ianimal]\n",
    "    \n",
    "    for icondition in np.arange(0,nconditions,1):\n",
    "        \n",
    "        task_condition = conditions_all[icondition]\n",
    "        \n",
    "        # set up for plotting\n",
    "        nPC_toplot = 4\n",
    "\n",
    "        # Create a figure with GridSpec, specifying height_ratios\n",
    "        fig = plt.figure(figsize=(nPC_toplot*2,6*nbhv_names))\n",
    "\n",
    "        # Define a grid with 4*nbhv_names rows in the left and nbhv_names rows in the right, \n",
    "        # but scale the right column's height by 3\n",
    "        gs = GridSpec(nPC_toplot*nbhv_names, 2, height_ratios=[1] * nPC_toplot*nbhv_names)\n",
    "\n",
    "        # Left column (4*nbhv_names rows, 1 column) for PC 1 to 4 traces\n",
    "        ax_left = [fig.add_subplot(gs[i, 0]) for i in range(nPC_toplot*nbhv_names)]  # Access all 4*nbhv_names rows in the left column\n",
    "\n",
    "        # Right column (nbhv_names rows, 1 column, scaling the height by using multiple rows for each plot)\n",
    "        # for the variance explanation\n",
    "        ax_right = [fig.add_subplot(gs[nPC_toplot * i:nPC_toplot * i + nPC_toplot, 1]) for i in range(nbhv_names)]  # Group 4 rows for each of the 3 subplots on the right\n",
    "\n",
    "\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (Kernel_coefs_action_all_dates_df['act_animal']==act_animal)&(Kernel_coefs_action_all_dates_df['bhv_name']==bhv_name)&(Kernel_coefs_action_all_dates_df['condition']==task_condition)\n",
    "\n",
    "            Kernel_coefs_action_tgt = np.vstack(list(Kernel_coefs_action_all_dates_df[ind]['Kernel_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(Kernel_coefs_action_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            Kernel_coefs_action_tgt = Kernel_coefs_action_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            Kernel_coefs_action_pca = pca.fit_transform(Kernel_coefs_action_tgt.transpose())\n",
    "\n",
    "            # Get the explained variance ratio\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "            # Calculate the cumulative explained variance\n",
    "            cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "            # Plot the cumulative explained variance\n",
    "            ax_right[ibhvname].plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', linestyle='--', color='blue', alpha=0.7)\n",
    "            ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "            ax_right[ibhvname].set_ylabel('Cumulative Percentage of Variance Explained')\n",
    "            ax_right[ibhvname].set_title(bhv_name)\n",
    "            ax_right[ibhvname].set_xticks(np.arange(1, len(cumulative_variance) + 1, 1))\n",
    "            ax_right[ibhvname].grid(True)\n",
    "            # \n",
    "            # if ibhvname == nbhv_names - 1:\n",
    "            #     ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "\n",
    "            # plot the PCs\n",
    "            for iPC_toplot in np.arange(0,nPC_toplot,1):\n",
    "\n",
    "                PCtoplot = Kernel_coefs_action_pca[:,iPC_toplot]\n",
    "\n",
    "                trig_twin = [-4,4] # in the unit of second\n",
    "                xxx = np.arange(trig_twin[0]*fps,trig_twin[1]*fps,1)\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot(xxx, PCtoplot, 'k')\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot([0,0],[np.nanmin(PCtoplot)*1.1,np.nanmax(PCtoplot)*1.1],'k--')\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].set_title(bhv_name+' kernel PC'+str(iPC_toplot+1))\n",
    "\n",
    "                if iPC_toplot == nPC_toplot - 1:\n",
    "                    ax_left[iPC_toplot+ibhvname*nPC_toplot].set_xlabel('time (s)')\n",
    "\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()        \n",
    "        \n",
    "        if (animal1_filenames[0] == 'Kanga') | (animal2_filenames[0] == 'Kanga'):\n",
    "            recordedAnimal = 'Kanga'\n",
    "        elif (animal1_filenames[0] == 'Dodson') | (animal2_filenames[0] == 'Dodson'):\n",
    "            recordedAnimal = 'Dodson'\n",
    "\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_singlecam/\"+cameraID+\"/\"+recordedAnimal+\"_neuralGLM/\"\n",
    "\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            fig.savefig(figsavefolder+'action_kernel_coefs_pca_patterns_all_dates'+savefile_sufix+'_'+act_animal+'action_in'+task_condition+'.pdf')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2725f",
   "metadata": {},
   "source": [
    "### prepare the summarizing data set and run population level analysis\n",
    "### plot the coefficient for self and partner force level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "Kernel_coefs_force_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                          'Kernel_average','datatype'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    # make sure to be the same as the bhvvaris_toGLM\n",
    "    bhv_types = ['self self_force', 'self partner_force', ]\n",
    "    nbhv_types = np.shape(bhv_types)[0]\n",
    "\n",
    "    for ibhv_type in np.arange(0,nbhv_types,1):\n",
    "        \n",
    "        bhv_type = bhv_types[ibhv_type]\n",
    "\n",
    "        if ibhv_type == 0:\n",
    "            Kernel_force_stretagy_all_dates = Kernel_selfforce_stretagy_all_dates\n",
    "            Kernel_force_stretagy_shf_all_dates = Kernel_selfforce_stretagy_all_shuffled_dates\n",
    "        elif ibhv_type == 1:\n",
    "            Kernel_force_stretagy_all_dates = Kernel_partnerforce_stretagy_all_dates\n",
    "            Kernel_force_stretagy_shf_all_dates = Kernel_partnerforce_stretagy_all_shuffled_dates\n",
    "        \n",
    "        clusterIDs = Kernel_force_stretagy_all_dates[date_tgt].keys()\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            kernel_ibhv = Kernel_force_stretagy_all_dates[date_tgt][iclusterID]\n",
    "            kernel_ibhv_average = np.nanmean(kernel_ibhv,axis = 0)[0]\n",
    "            \n",
    "            kernel_ibhv_shuffled = Kernel_force_stretagy_shf_all_dates[date_tgt][iclusterID]\n",
    "            kernel_ibhv_average_shuffled = np.nanmean(kernel_ibhv_shuffled,axis = 0)[0]\n",
    "\n",
    "            Kernel_coefs_force_all_dates_df = Kernel_coefs_force_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':bhv_type.split()[0],\n",
    "                                                                                    'bhv_name': bhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'Kernel_average':kernel_ibhv_average,\n",
    "                                                                                    'datatype': 'realdata',\n",
    "                                                                                   }, ignore_index=True)\n",
    "            \n",
    "            Kernel_coefs_force_all_dates_df = Kernel_coefs_force_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':bhv_type.split()[0],\n",
    "                                                                                    'bhv_name': bhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'Kernel_average':kernel_ibhv_average_shuffled,\n",
    "                                                                                    'datatype': 'shuffleddata',\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            \n",
    "#           \n",
    "bhv_names_all = np.unique(Kernel_coefs_force_all_dates_df['bhv_name'])\n",
    "nbhv_names = np.shape(bhv_names_all)[0]\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(nbhv_names, 1)\n",
    "fig.set_figheight(5*nbhv_names)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "\n",
    "for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "    bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "    ind = (Kernel_coefs_force_all_dates_df['bhv_name']==bhv_name)\n",
    "\n",
    "    Kernel_coefs_force_tgt = Kernel_coefs_force_all_dates_df[ind]\n",
    "            \n",
    "    seaborn.violinplot(ax=axs[ibhvname],data=Kernel_coefs_force_tgt,\n",
    "                       x='condition',y='Kernel_average',hue='datatype')\n",
    "    axs[ibhvname].set_title(bhv_name)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kernel_coefs_force_all_dates_df[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ea6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhv_names_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a5e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fc4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
