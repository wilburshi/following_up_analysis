{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the all the sessions\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 35*30 # second*30fps\n",
    "# nframes = 1\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "            \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "            \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          # \"20220912\",\n",
    "                          \"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                          \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                          \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                          \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                          \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                          \"20230117\",\"20230118\",\"20230124\",\n",
    "                          # \"20230126\",\n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    # 18.10, \n",
    "                                     0.00, 33.03,  6.50,  0.00, \n",
    "                                     2.80, 27.80, 27.90, 27.00,  \n",
    "                                    51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                    26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                     0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                     0.00,  0.00,  0.00, \n",
    "                                     # 0.00,  \n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20220915\",\"20220920\",\"20221010\",\"20230208\", # SR\n",
    "                          \n",
    "                          \"20230321\",\"20230322\",\"20230323\",\"20230324\",\"20230412\",\"20230413\", # trained MC\n",
    "                          \n",
    "                          \"20230117\",\"20230118\",\"20230124\", # NV \n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                     0.00, 33.03,  6.50,  0.00, \n",
    "                                     \n",
    "                                     20.5,  21.4,  21.0,  24.5,  20.5,  26.6,\n",
    "                    \n",
    "                                     0.00,  0.00,  0.00,  \n",
    "                                  ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "    \n",
    "# eddie sparkle\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20221122\",  \"20221125\",  \n",
    "                          \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                          \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                          \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                          \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                          \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                      8.00,  38.00, \n",
    "                                      9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                      9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                      8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                      7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                      4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20221122\",  \"20221125\",  # sr\n",
    "                \n",
    "                          \"20230410\",  \"20230411\",  \"20230412\",  \"20230413\",  \"20230616\", # trained MC\n",
    "                \n",
    "                          \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\", # nv\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                      8.00, 38.00, \n",
    "                \n",
    "                                      23.2,  23.0,  21.2,  25.0,  23.0,   \n",
    "                \n",
    "                                      4.50,  9.30, 25.50, 20.40, 21.30,\n",
    "                \n",
    "                                  ] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "    \n",
    "# ginger kanga\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second \n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          #\"20230213\",\n",
    "                          \"20230214\",\"20230216\",\n",
    "                          \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                          \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                          \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                          \"20230323\",\"20230412\",\"20230413\",\"20230517\",\n",
    "                          \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                    # 0.00, \n",
    "                                     0.00, 48.00, \n",
    "                                    23.00, 28.50, 34.00, 25.50, \n",
    "                                    25.50, 31.50, 28.00, 30.50,\n",
    "                                     0.00,  0.00,  0.00,  0.00, \n",
    "                                     0.00,  0.00,  0.00,  0.00, \n",
    "                                     0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                  ] # in second \n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20230214\",   \"20230216\",  # SR\n",
    "                          \n",
    "                          \"20230614\",   \"20230615\",  \"20230711\",\"20230712\", # trained MC\n",
    "                \n",
    "                          \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\", # nv  \n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                     0.00, 48.00, \n",
    "                                    \n",
    "                                     0.00,  0.00,  54.5,  24.7,\n",
    "                \n",
    "                                     0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                  ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                              \n",
    "                              ] # in second \n",
    "    elif do_bestsession: \n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20230718\",\"20230720\",\"20230914\",\"20230726\",\"20230727\",\"20230809\",\n",
    "                          \"20230810\",\"20230811\",\"20230814\",\"20230816\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                          \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                          \"20231013\",\"20231020\",\"20231024\",\"20231025\",\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                        0,    0,    0, 32.2, 27.2, 37.5,\n",
    "                                     21.0, 21.5, 19.8, 32.0,    0,    0,   0, \n",
    "                                        0,    0,    0,    0,    0,    0,\n",
    "                                        0,    0,    0,    0, \n",
    "                                  ] # in second \n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20230718\",\"20230720\",\"20230914\", # sr\n",
    "                \n",
    "                          \"20231030\",\"20231031\",\"20231101\",\"20231102\",\"20240304\",\"20240305\", # trained MC\n",
    "                \n",
    "                          \"20231011\",\"20231013\",\"20231020\",\"20231024\",\"20231025\", # nv\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                       0,    0,    0,\n",
    "                \n",
    "                                    18.2, 14.0, 15.8, 15.2, 16.3, 37.9,\n",
    "                \n",
    "                                       0,    0,    0,    0,    0, \n",
    "                                  ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "# Koala Vermelho\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                     \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                               \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20231222\",\"20231226\",\"20231227\",  \"20231229\",\"20231230\",\n",
    "                          \"20231231\",\"20240102\",\"20240104-2\",\"20240105\",\"20240108\",\n",
    "                          \"20240109\",\"20240115\",\"20240116\",  \"20240117\",\"20240118\",\"20240119\",\n",
    "                          \"20240207\",\"20240208\",\"20240209\",  \"20240212\",\"20240213\",\n",
    "                          \"20240214\",\"20240215\",\"20240216\",  \n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    21.5,  0.00,  0.00,  0.00,  0.00, \n",
    "                                    0.00,  12.2,  0.00,  18.8,  31.2,  \n",
    "                                    32.5,  0.00,  50.0,  0.00,  37.5,  29.5,\n",
    "                                    58.5,  72.0,  0.00,  71.5,  70.5,\n",
    "                                    86.8,  94.0,  65.0, \n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20231222\",\"20231226\",\"20231227\", # SR\n",
    "                          \n",
    "                          \"20240220\",\"20240222\",\"20240223\",\"20240226\", # trained MC\n",
    "                 \n",
    "                          \"20240214\",\"20240215\",\"20240216\",  # NV\n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    21.5,  0.00,  0.00, \n",
    "                                    \n",
    "                                    68.8,  43.8,  13.2,  47.5,\n",
    "                \n",
    "                                    86.8,  94.0,  65.0, \n",
    "                                  ] # in second\n",
    "\n",
    "    animal1_fixedorder = ['koala']\n",
    "    animal2_fixedorder = ['vermelho']\n",
    "\n",
    "    animal1_filename = \"Koala\"\n",
    "    animal2_filename = \"Vermelho\"\n",
    "    \n",
    "# a test case\n",
    "if 0:\n",
    "    dates_list = [\"20230327\"]\n",
    "    session_start_times = [2*60+30] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "    \n",
    "#    \n",
    "# dates_list = [\"20221128\"]\n",
    "# session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "sess_videotimes_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# save the session start time\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "if not os.path.exists(data_saved_subfolder):\n",
    "    os.makedirs(data_saved_subfolder)\n",
    "#\n",
    "with open(data_saved_subfolder+'sessstart_time_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "    pickle.dump(session_start_times, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        sess_videotimes_all_dates = pickle.load(f)    \n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        try:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        min_length = np.min(list(bodyparts_locs_camI.values())[0].shape[0])\n",
    "        \n",
    "        sess_videotimes_all_dates[idate] = min_length/fps\n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']+11-session_start_time\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']+11-session_start_time\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        # # old definition\n",
    "        # if np.isin(animal1,animal1_fixedorder):\n",
    "        #     owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]#/(min_length/fps)\n",
    "        #     owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]#/(min_length/fps)\n",
    "        #     mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]#/(min_length/fps)\n",
    "        #     mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]#/(min_length/fps)\n",
    "        # else:\n",
    "        #     owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]#/(min_length/fps)\n",
    "         #    owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]#/(min_length/fps)\n",
    "          #   mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]#/(min_length/fps)\n",
    "           #  mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]#/(min_length/fps)\n",
    "        #\n",
    "        # new defnition\n",
    "        # <500ms counts as one gaze, gaze number per second\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.sum(oneway_gaze1[1:]-oneway_gaze1[:-1]>=0.5)# /(min_length/fps)\n",
    "            owgaze2_num_all_dates[idate] = np.sum(oneway_gaze2[1:]-oneway_gaze2[:-1]>=0.5)# /(min_length/fps)\n",
    "            mtgaze1_num_all_dates[idate] = np.sum(mutual_gaze1[1:]-mutual_gaze1[:-1]>=0.5)# /(min_length/fps)\n",
    "            mtgaze2_num_all_dates[idate] = np.sum(mutual_gaze2[1:]-mutual_gaze2[:-1]>=0.5)# /(min_length/fps)\n",
    "        else:\n",
    "            owgaze1_num_all_dates[idate] = np.sum(oneway_gaze2[1:]-oneway_gaze2[:-1]>=0.5)# /(min_length/fps)\n",
    "            owgaze2_num_all_dates[idate] = np.sum(oneway_gaze1[1:]-oneway_gaze1[:-1]>=0.5)# /(min_length/fps)\n",
    "            mtgaze1_num_all_dates[idate] = np.sum(mutual_gaze2[1:]-mutual_gaze2[:-1]>=0.5)# /(min_length/fps)\n",
    "            mtgaze2_num_all_dates[idate] = np.sum(mutual_gaze1[1:]-mutual_gaze1[:-1]>=0.5)# /(min_length/fps)\n",
    "        \n",
    "        \n",
    "\n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 0:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 1: \n",
    "            tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                              lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                              animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                              animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                              video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sess_videotimes_all_dates, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "succ_rate_all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aada694",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900d890",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b179dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['SR','MC','NV']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-5,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "\n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b01345",
   "metadata": {},
   "source": [
    "### plot some other basis behavioral measures\n",
    "#### successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e545fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),succ_rate_all_dates[sorting_df.index],'o',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"successful rate\",fontsize=13)\n",
    "ax1.set_ylim([-0.1,1.1])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['SR','MC','NV']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"successfulrate_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b5e6d",
   "metadata": {},
   "source": [
    "#### animal pull numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e99c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull1_num_all_dates[sorting_df.index],'bv',markersize=5,label='animal1 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull2_num_all_dates[sorting_df.index],'rv',markersize=5,label='animal2 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pullmean_num_all_dates[sorting_df.index],'kv',markersize=8,label='mean pull #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"pull numbers\",fontsize=13)\n",
    "ax1.set_ylim([-20,240])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "if do_trainedMCs:\n",
    "    tasktypes = ['SR','MC','NV']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,240],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pullnumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba624af5",
   "metadata": {},
   "source": [
    "#### gaze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20149789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "print(np.nanmax(gaze1_num_all_dates))\n",
    "print(np.nanmax(gaze2_num_all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze1_num_all_dates[sorting_df.index],'b^',markersize=5,label='animal1 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze2_num_all_dates[sorting_df.index],'r^',markersize=5,label='animal2 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazemean_num_all_dates[sorting_df.index],'k^',markersize=8,label='mean gaze #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"social gaze number\",fontsize=13)\n",
    "ax1.set_ylim([-20,1500])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,1500],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_numbers = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/30\n",
    "gaze_pull_ratios = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/(pull1_num_all_dates+pull2_num_all_dates)/30\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "if do_trainedMCs:\n",
    "    grouptypes = ['SR','MC','NV']\n",
    "\n",
    "gaze_numbers_groups = [np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==3)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==2)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1.5)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]]\n",
    "if do_trainedMCs:\n",
    "    gaze_numbers_groups = [np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                           np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                           np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(gaze_numbers_groups)\n",
    "\n",
    "plt.xticks(np.arange(1, len(grouptypes)+1, 1), grouptypes, fontsize = 12);\n",
    "ax1.set_ylim([-30/30,5400/30])\n",
    "ax1.set_ylabel(\"average social gaze numbers\",fontsize=13)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240c38b",
   "metadata": {},
   "source": [
    "#### plot the gaze numbers for all individuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56538078",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    do_onlyLearningsess = 1 # only consider pairs from the learning analysis\n",
    "\n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 0 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_2',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['SR','MC','NV']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    gazenum_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "    gazenum_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_2',]\n",
    "    gazenum_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "    gazenum_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    # subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    # domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    subnames = ['eddie','dodson','dannon','ginger','vermelho']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_2',]\n",
    "    gazenum_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "    gazenum_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        gazenum_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "\n",
    "        #\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1 = animal1_fixedorders[ianimalpair]\n",
    "            animal2 = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2 == 'kanga_1') | (animal2 == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1+animal2_filename+'/'\n",
    "            with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                owgaze1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                owgaze2_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                mtgaze1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                mtgaze2_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                pull1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                succ_rate_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                interpullintv_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                trialnum_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                bhv_intv_all_dates = pickle.load(f)\n",
    "            \n",
    "            with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                sess_videotimes_all_dates = pickle.load(f)\n",
    "\n",
    "            # combine owgaze and mtgaze\n",
    "            gaze1_num_all_dates = (owgaze1_num_all_dates + mtgaze1_num_all_dates)/sess_videotimes_all_dates\n",
    "            gaze2_num_all_dates = (owgaze2_num_all_dates + mtgaze2_num_all_dates)/sess_videotimes_all_dates\n",
    "\n",
    "            #\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "            # \n",
    "            gazenum_foreachgroup_foreachAni[grouptype][animal1] = gaze1_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            gazenum_foreachgroup_foreachAni[grouptype][animal2] = gaze2_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "\n",
    "\n",
    "        # combine across all animals\n",
    "        gazenum_foreachgroup_all[grouptype] = np.hstack(list(gazenum_foreachgroup_foreachAni[grouptype].values()))\n",
    "\n",
    "        # combine across male and female\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[malenames]\n",
    "        gazenum_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "        #\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[femalenames]\n",
    "        gazenum_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "\n",
    "        # combine across sub and dom\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[subnames]\n",
    "        gazenum_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "        #\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[domnames]\n",
    "        gazenum_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "    # box plot \n",
    "    fig, axs = plt.subplots(3,1)\n",
    "    fig.set_figheight(5*3)\n",
    "    fig.set_figwidth(3*2)\n",
    "\n",
    "    # subplot 1 - all animals\n",
    "    gazenum_foreachgroup_all_df = pd.DataFrame.from_dict(gazenum_foreachgroup_all,orient='index')\n",
    "    gazenum_foreachgroup_all_df = gazenum_foreachgroup_all_df.transpose()\n",
    "    gazenum_foreachgroup_all_df['type'] = 'all'\n",
    "    #\n",
    "    df_long=pd.concat([gazenum_foreachgroup_all_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "    # \n",
    "    # barplot ans swarmplot\n",
    "    seaborn.boxplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type')\n",
    "    # seaborn.swarmplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs[0].set_xlabel('')\n",
    "    axs[0].set_xticklabels('')\n",
    "    axs[0].xaxis.set_tick_params(labelsize=15)\n",
    "    axs[0].set_ylabel(\"social gaze number\",fontsize=15)\n",
    "    axs[0].set_title('all animals' ,fontsize=24)\n",
    "    # axs[0].set_ylim([0,2000])\n",
    "    axs[0].set_ylim([-0.1,0.7])\n",
    "    axs[0].legend(fontsize=18)\n",
    "\n",
    "    # subplot 2 - male and female animals\n",
    "    gazenum_foreachgroup_male_df = pd.DataFrame.from_dict(gazenum_foreachgroup_male,orient='index')\n",
    "    gazenum_foreachgroup_male_df = gazenum_foreachgroup_male_df.transpose()\n",
    "    gazenum_foreachgroup_male_df['type'] = 'male'\n",
    "    gazenum_foreachgroup_female_df = pd.DataFrame.from_dict(gazenum_foreachgroup_female,orient='index')\n",
    "    gazenum_foreachgroup_female_df = gazenum_foreachgroup_female_df.transpose()\n",
    "    gazenum_foreachgroup_female_df['type'] = 'female'\n",
    "    #\n",
    "    df_long=pd.concat([gazenum_foreachgroup_male_df,gazenum_foreachgroup_female_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "    # \n",
    "    # barplot ans swarmplot\n",
    "    # seaborn.boxplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type')\n",
    "    seaborn.violinplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type')\n",
    "    # seaborn.swarmplot(ax=axs[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs[1].set_xlabel('')\n",
    "    axs[1].set_xticklabels('')\n",
    "    axs[1].xaxis.set_tick_params(labelsize=15)\n",
    "    axs[1].set_ylabel(\"social gaze number\",fontsize=15)\n",
    "    axs[1].set_title('male and female' ,fontsize=24)\n",
    "    # axs[1].set_ylim([0,2000])\n",
    "    axs[1].set_ylim([-0.1,0.7])\n",
    "    axs[1].legend(fontsize=18)\n",
    "\n",
    "    # subplot 3 - dom and sub animals\n",
    "    gazenum_foreachgroup_sub_df = pd.DataFrame.from_dict(gazenum_foreachgroup_sub,orient='index')\n",
    "    gazenum_foreachgroup_sub_df = gazenum_foreachgroup_sub_df.transpose()\n",
    "    gazenum_foreachgroup_sub_df['type'] = 'sub'\n",
    "    gazenum_foreachgroup_dom_df = pd.DataFrame.from_dict(gazenum_foreachgroup_dom,orient='index')\n",
    "    gazenum_foreachgroup_dom_df = gazenum_foreachgroup_dom_df.transpose()\n",
    "    gazenum_foreachgroup_dom_df['type'] = 'dom'\n",
    "    #\n",
    "    df_long=pd.concat([gazenum_foreachgroup_sub_df,gazenum_foreachgroup_dom_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "    # \n",
    "    # barplot ans swarmplot\n",
    "    seaborn.violinplot(ax=axs[2],data=df_long2,x='condition',y='value',hue='type')\n",
    "    # seaborn.swarmplot(ax=axs[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs[2].set_xlabel('')\n",
    "    axs[2].set_xticklabels(axs[2].get_xticklabels(),rotation=45)\n",
    "    axs[2].xaxis.set_tick_params(labelsize=15)\n",
    "    axs[2].set_ylabel(\"social gaze number\",fontsize=15)\n",
    "    axs[2].set_title('sub and dom' ,fontsize=24)\n",
    "    # axs[2].set_ylim([0,2000])\n",
    "    axs[2].set_ylim([-0.1,0.7])\n",
    "    axs[2].legend(fontsize=18)\n",
    "\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'averaged_gazenumbers_acrossAllAnimals.pdf')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f440b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # perform the anova on all animals\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "    gazenum_foreachgroup_all_df = pd.DataFrame.from_dict(gazenum_foreachgroup_all,orient='index')\n",
    "    gazenum_foreachgroup_all_df = gazenum_foreachgroup_all_df.transpose()\n",
    "    gazenum_foreachgroup_all_df['type'] = 'all'\n",
    "    #\n",
    "    df_long=pd.concat([gazenum_foreachgroup_all_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "    df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "\n",
    "    # anova\n",
    "    cw_lm=ols('value ~ condition', data=df_long2).fit() #Specify C for Categorical\n",
    "    print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "    # post hoc test \n",
    "    tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition'], alpha=0.05)\n",
    "    print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e676972",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # perform the anova on male and female\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "    gazenum_foreachgroup_male_df = pd.DataFrame.from_dict(gazenum_foreachgroup_male,orient='index')\n",
    "    gazenum_foreachgroup_male_df = gazenum_foreachgroup_male_df.transpose()\n",
    "    gazenum_foreachgroup_male_df['type'] = 'male'\n",
    "    gazenum_foreachgroup_female_df = pd.DataFrame.from_dict(gazenum_foreachgroup_female,orient='index')\n",
    "    gazenum_foreachgroup_female_df = gazenum_foreachgroup_female_df.transpose()\n",
    "    gazenum_foreachgroup_female_df['type'] = 'female'\n",
    "    #\n",
    "    df_long=pd.concat([gazenum_foreachgroup_male_df,gazenum_foreachgroup_female_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "    df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "\n",
    "    # anova\n",
    "    cw_lm=ols('value ~ type + condition + type:condition', data=df_long2).fit() #Specify C for Categorical\n",
    "    print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "    # post hoc test \n",
    "    tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition']+df_long2['type'], alpha=0.05)\n",
    "    print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # perform the anova on male and female\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "    gazenum_foreachgroup_sub_df = pd.DataFrame.from_dict(gazenum_foreachgroup_sub,orient='index')\n",
    "    gazenum_foreachgroup_sub_df = gazenum_foreachgroup_sub_df.transpose()\n",
    "    gazenum_foreachgroup_sub_df['type'] = 'sub'\n",
    "    gazenum_foreachgroup_dom_df = pd.DataFrame.from_dict(gazenum_foreachgroup_dom,orient='index')\n",
    "    gazenum_foreachgroup_dom_df = gazenum_foreachgroup_dom_df.transpose()\n",
    "    gazenum_foreachgroup_dom_df['type'] = 'dom'\n",
    "    #\n",
    "    df_long=pd.concat([gazenum_foreachgroup_sub_df,gazenum_foreachgroup_dom_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=grouptypes,var_name='condition', value_name='value')\n",
    "    df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "\n",
    "    # anova\n",
    "    cw_lm=ols('value ~ type + condition + type:condition', data=df_long2).fit() #Specify C for Categorical\n",
    "    print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "    # post hoc test \n",
    "    tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition']+df_long2['type'], alpha=0.05)\n",
    "    print(tukey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31aebca",
   "metadata": {},
   "source": [
    "#### plot the correlation between gaze number and sucessful rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 0 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_2',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    gazenum_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "    gazenum_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "    succrate_foreachgroup_foreachAni = dict.fromkeys(grouptypes,[])\n",
    "    succrate_foreachgroup_all = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_2','ginger',]\n",
    "    gazenum_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "    gazenum_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "    succrate_foreachgroup_male = dict.fromkeys(grouptypes,[])\n",
    "    succrate_foreachgroup_female = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_2',]\n",
    "    gazenum_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "    gazenum_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "    succrate_foreachgroup_sub = dict.fromkeys(grouptypes,[])\n",
    "    succrate_foreachgroup_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        gazenum_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        succrate_foreachgroup_foreachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "\n",
    "        #\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1 = animal1_fixedorders[ianimalpair]\n",
    "            animal2 = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2 == 'kanga_1') | (animal2 == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1+animal2_filename+'/'\n",
    "            with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                owgaze1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                owgaze2_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                mtgaze1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                mtgaze2_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                pull1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                succ_rate_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                interpullintv_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                trialnum_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "            with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1+animal2_filename+'.pkl', 'rb') as f:\n",
    "                sess_videotimes_all_dates = pickle.load(f)\n",
    "\n",
    "            # combine owgaze and mtgaze\n",
    "            gaze1_num_all_dates = (owgaze1_num_all_dates + mtgaze1_num_all_dates)/sess_videotimes_all_dates\n",
    "            gaze2_num_all_dates = (owgaze2_num_all_dates + mtgaze2_num_all_dates)/sess_videotimes_all_dates\n",
    "\n",
    "            #\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "            # \n",
    "            gazenum_foreachgroup_foreachAni[grouptype][animal1] = gaze1_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            gazenum_foreachgroup_foreachAni[grouptype][animal2] = gaze2_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            succrate_foreachgroup_foreachAni[grouptype][animal1] = succ_rate_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            succrate_foreachgroup_foreachAni[grouptype][animal2] = succ_rate_all_dates[coopthres_forsort==coopthres_ID]\n",
    "\n",
    "        # combine across all animals\n",
    "        gazenum_foreachgroup_all[grouptype] = np.hstack(list(gazenum_foreachgroup_foreachAni[grouptype].values()))\n",
    "        succrate_foreachgroup_all[grouptype] = np.hstack(list(succrate_foreachgroup_foreachAni[grouptype].values()))\n",
    "\n",
    "        # combine across male and female\n",
    "        # gaze number\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[malenames]\n",
    "        gazenum_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "        #\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[femalenames]\n",
    "        gazenum_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "        # successful rate\n",
    "        df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[malenames]\n",
    "        succrate_foreachgroup_male[grouptype] = df.values.ravel()\n",
    "        #\n",
    "        df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[femalenames]\n",
    "        succrate_foreachgroup_female[grouptype] = df.values.ravel()\n",
    "\n",
    "        # combine across sub and dom\n",
    "        # gaze number\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[subnames]\n",
    "        gazenum_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "        #\n",
    "        df = pd.DataFrame.from_dict(gazenum_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[domnames]\n",
    "        gazenum_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "        # successful rate\n",
    "        df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[subnames]\n",
    "        succrate_foreachgroup_sub[grouptype] = df.values.ravel()\n",
    "        #\n",
    "        df = pd.DataFrame.from_dict(succrate_foreachgroup_foreachAni[grouptype],orient='index')\n",
    "        df = df.transpose()[domnames]\n",
    "        succrate_foreachgroup_dom[grouptype] = df.values.ravel()\n",
    "\n",
    "\n",
    "    # scatter plot + correlation line \n",
    "    fig, axs = plt.subplots(1,4)\n",
    "    fig.set_figheight(5*1)\n",
    "    fig.set_figwidth(5*4)\n",
    "\n",
    "    # condtypes_forplot = ['3s threshold','2s threshold','1.5s threshold','1s threshold']\n",
    "    condtypes_forplot = ['1s threshold']\n",
    "    # condtypes_forplot = ['novision']\n",
    "    condtypes_filename = '1scoop'\n",
    "    # condtypes_filename = 'novision'\n",
    "\n",
    "    # subplot 1 - all animals\n",
    "    xxx = np.hstack([succrate_foreachgroup_all[condname] for condname in condtypes_forplot])\n",
    "    yyy = np.hstack([gazenum_foreachgroup_all[condname] for condname in condtypes_forplot])\n",
    "    p_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').pvalue\n",
    "    r_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').rvalue\n",
    "    # \n",
    "    seaborn.regplot(ax=axs[0], x=xxx, y=yyy,label=condtypes_forplot[0])\n",
    "    axs[0].set_title('all animals' ,fontsize=17)\n",
    "    axs[0].set_xlabel('success rate',fontsize=15)\n",
    "    axs[0].set_xlim([-0.05,0.8])\n",
    "    axs[0].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "    axs[0].set_ylim([-0.05,0.45])\n",
    "    axs[0].legend()\n",
    "    axs[0].text(0.27,0.38,'regression r='+\"{:.2f}\".format(r_reg),fontsize=10)\n",
    "    axs[0].text(0.27,0.36,'regression p='+\"{:.2f}\".format(p_reg),fontsize=10)\n",
    "\n",
    "\n",
    "    # subplot 2 - male and female\n",
    "    xxx_m = np.hstack([succrate_foreachgroup_male[condname] for condname in condtypes_forplot])\n",
    "    yyy_m = np.hstack([gazenum_foreachgroup_male[condname] for condname in condtypes_forplot])\n",
    "    ind_good = ~np.isnan(xxx_m) & ~np.isnan(yyy_m)\n",
    "    xxx_m = xxx_m[ind_good]\n",
    "    yyy_m = yyy_m[ind_good]\n",
    "    dfm = pd.DataFrame({'succrate':xxx_m,'gazenum':yyy_m})\n",
    "    dfm['condtype'] = 'male'\n",
    "    p_reg_m = scipy.stats.linregress(xxx_m, yyy_m, alternative='two-sided').pvalue\n",
    "    r_reg_m = scipy.stats.linregress(xxx_m, yyy_m, alternative='two-sided').rvalue\n",
    "    #\n",
    "    xxx_f = np.hstack([succrate_foreachgroup_female[condname] for condname in condtypes_forplot])\n",
    "    yyy_f = np.hstack([gazenum_foreachgroup_female[condname] for condname in condtypes_forplot])\n",
    "    ind_good = ~np.isnan(xxx_f) & ~np.isnan(yyy_f)\n",
    "    xxx_f = xxx_f[ind_good]\n",
    "    yyy_f = yyy_f[ind_good]\n",
    "    dff = pd.DataFrame({'succrate':xxx_f,'gazenum':yyy_f})\n",
    "    dff['condtype'] = 'female'\n",
    "    p_reg_f = scipy.stats.linregress(xxx_f, yyy_f, alternative='two-sided').pvalue\n",
    "    r_reg_f = scipy.stats.linregress(xxx_f, yyy_f, alternative='two-sided').rvalue\n",
    "    # \n",
    "    dfmf = pd.concat([dfm,dff]).reset_index(drop=True)\n",
    "    model_interaction = sm.formula.ols('gazenum ~ succrate + condtype + succrate*condtype', data=dfmf).fit()\n",
    "    p_slopediff = model_interaction.pvalues['succrate:condtype[T.male]']\n",
    "    p_slopeboth = model_interaction.pvalues['succrate']\n",
    "    #\n",
    "    seaborn.regplot(ax=axs[1], x=xxx_m, y=yyy_m,label='male')\n",
    "    seaborn.regplot(ax=axs[1], x=xxx_f, y=yyy_f,label='female')\n",
    "    axs[1].set_title('male and female' ,fontsize=17)\n",
    "    axs[1].set_xlabel('success rate',fontsize=15)\n",
    "    axs[1].set_xlim([-0.05,0.8])\n",
    "    axs[1].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "    axs[1].set_ylim([-0.05,0.45])\n",
    "    axs[1].legend()\n",
    "    axs[1].text(0.27,0.38,'male reg r='+\"{:.2f}\".format(r_reg_m),fontsize=10)\n",
    "    axs[1].text(0.27,0.36,'male reg p='+\"{:.2f}\".format(p_reg_m),fontsize=10)\n",
    "    axs[1].text(0.27,0.34,'female reg r='+\"{:.2f}\".format(r_reg_f),fontsize=10)\n",
    "    axs[1].text(0.27,0.32,'female reg p='+\"{:.2f}\".format(p_reg_f),fontsize=10)\n",
    "    axs[1].text(0.27,0.30,'slope diff ANCOVA p='+\"{:.2f}\".format(p_slopediff),fontsize=10)\n",
    "    axs[1].text(0.27,0.28,'both slope ANCOVA p='+\"{:.2f}\".format(p_slopeboth),fontsize=10)\n",
    "\n",
    "    # subplot 3 - sub and dom\n",
    "    xxx_s = np.hstack([succrate_foreachgroup_sub[condname] for condname in condtypes_forplot])\n",
    "    yyy_s = np.hstack([gazenum_foreachgroup_sub[condname] for condname in condtypes_forplot])\n",
    "    ind_good = ~np.isnan(xxx_s) & ~np.isnan(yyy_s)\n",
    "    xxx_s = xxx_s[ind_good]\n",
    "    yyy_s = yyy_s[ind_good]\n",
    "    dfs = pd.DataFrame({'succrate':xxx_s,'gazenum':yyy_s})\n",
    "    dfs['condtype'] = 'sub'\n",
    "    p_reg_s = scipy.stats.linregress(xxx_s, yyy_s, alternative='two-sided').pvalue\n",
    "    r_reg_s = scipy.stats.linregress(xxx_s, yyy_s, alternative='two-sided').rvalue\n",
    "    #\n",
    "    xxx_d = np.hstack([succrate_foreachgroup_dom[condname] for condname in condtypes_forplot])\n",
    "    yyy_d = np.hstack([gazenum_foreachgroup_dom[condname] for condname in condtypes_forplot])\n",
    "    ind_good = ~np.isnan(xxx_d) & ~np.isnan(yyy_d)\n",
    "    xxx_d = xxx_d[ind_good]\n",
    "    yyy_d = yyy_d[ind_good]\n",
    "    dfd = pd.DataFrame({'succrate':xxx_d,'gazenum':yyy_d})\n",
    "    dfd['condtype'] = 'dom'\n",
    "    p_reg_d = scipy.stats.linregress(xxx_d, yyy_d, alternative='two-sided').pvalue\n",
    "    r_reg_d = scipy.stats.linregress(xxx_d, yyy_d, alternative='two-sided').rvalue\n",
    "    # \n",
    "    dfsd = pd.concat([dfs,dfd]).reset_index(drop=True)\n",
    "    model_interaction = sm.formula.ols('gazenum ~ succrate + condtype + succrate*condtype', data=dfsd).fit()\n",
    "    p_slopediff = model_interaction.pvalues['succrate:condtype[T.sub]']\n",
    "    p_slopeboth = model_interaction.pvalues['succrate']\n",
    "    # \n",
    "    seaborn.regplot(ax=axs[2], x=xxx_s, y=yyy_s,label='subordinate')\n",
    "    seaborn.regplot(ax=axs[2], x=xxx_d, y=yyy_d,label='dominant')\n",
    "    axs[2].set_title('sub and dom' ,fontsize=17)\n",
    "    axs[2].set_xlabel('success rate',fontsize=15)\n",
    "    axs[2].set_xlim([-0.05,0.8])\n",
    "    axs[2].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "    axs[2].set_ylim([-0.05,0.45])\n",
    "    axs[2].legend()\n",
    "    axs[2].text(0.27,0.38,'sub reg r='+\"{:.2f}\".format(r_reg_s),fontsize=10)\n",
    "    axs[2].text(0.27,0.36,'sub reg p='+\"{:.2f}\".format(p_reg_s),fontsize=10)\n",
    "    axs[2].text(0.27,0.34,'dom reg r='+\"{:.2f}\".format(r_reg_d),fontsize=10)\n",
    "    axs[2].text(0.27,0.32,'dom reg p='+\"{:.2f}\".format(p_reg_d),fontsize=10)\n",
    "    axs[2].text(0.27,0.30,'slope diff ANCOVA p='+\"{:.2f}\".format(p_slopediff),fontsize=10)\n",
    "    axs[2].text(0.27,0.28,'both slope ANCOVA p='+\"{:.2f}\".format(p_slopeboth),fontsize=10)\n",
    "\n",
    "\n",
    "    # ancova comparison of regression slopes (between cooperation and NV)\n",
    "    #\n",
    "    # condtypes_forplot = ['3s threshold','2s threshold','1.5s threshold','1s threshold']\n",
    "    condtype1_forplot = ['1s threshold']\n",
    "    condtype2_forplot = ['novision']\n",
    "    #\n",
    "    xxx1 = np.hstack([succrate_foreachgroup_all[condname] for condname in condtype1_forplot])\n",
    "    yyy1 = np.hstack([gazenum_foreachgroup_all[condname] for condname in condtype1_forplot])\n",
    "    df1 = pd.DataFrame({'succrate':xxx1,'gazenum':yyy1})\n",
    "    df1['condtype'] = 'coop'\n",
    "    #\n",
    "    xxx2 = np.hstack([succrate_foreachgroup_all[condname] for condname in condtype2_forplot])\n",
    "    yyy2 = np.hstack([gazenum_foreachgroup_all[condname] for condname in condtype2_forplot])\n",
    "    df2 = pd.DataFrame({'succrate':xxx2,'gazenum':yyy2})\n",
    "    df2['condtype'] = 'nov'\n",
    "    #\n",
    "    df12 = pd.concat([df1,df2]).reset_index(drop=True)\n",
    "    #\n",
    "    model_interaction = sm.formula.ols('gazenum ~ succrate + condtype + succrate*condtype', data=df12).fit()\n",
    "    p_slopediff = model_interaction.pvalues['succrate:condtype[T.nov]']\n",
    "    p_slopeboth = model_interaction.pvalues['succrate']\n",
    "    #\n",
    "    seaborn.regplot(ax=axs[3], data = df12[df12['condtype']=='coop'], x='succrate', y='gazenum',label='MC')\n",
    "    seaborn.regplot(ax=axs[3], data = df12[df12['condtype']=='nov'], x='succrate', y='gazenum',label = 'NV')\n",
    "    #\n",
    "    axs[3].set_title('all animals' ,fontsize=17)\n",
    "    axs[3].set_xlabel('success rate',fontsize=15)\n",
    "    axs[3].set_xlim([-0.05,0.90])\n",
    "    axs[3].set_ylabel(\"social gaze number per second\",fontsize=15)\n",
    "    axs[3].set_ylim([-0.05,0.45])\n",
    "    axs[3].legend()\n",
    "    axs[3].text(0.00,0.36,'slope diff ANCOVA p='+\"{:.2f}\".format(p_slopediff),fontsize=10)\n",
    "    axs[3].text(0.00,0.34,'both slope ANCOVA p='+\"{:.2f}\".format(p_slopeboth),fontsize=10)\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+condtypes_filename+'_gazenumbers_succrate_correlation_acrossAllAnimals.pdf')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5aef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal1_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_input_data_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "doBhvitv_timebin = 0 # 1: if use the mean bhv event interval for time bin\n",
    "\n",
    "prepare_input_data = 0\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "        except:    \n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            \n",
    "        # get animal info\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "        \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "            \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "\n",
    "        # load behavioral event results\n",
    "        print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "            output_look_ornot = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "            output_allvectors = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "            output_allangles = pickle.load(f)  \n",
    "        #\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # redefine the totalsess_time for the length of each recording (NOT! remove the session_start_time)\n",
    "        totalsess_time = int(np.ceil(np.shape(look_at_other_or_not_merge['dodson'])[0]/fps))\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']   \n",
    "\n",
    "        \n",
    "\n",
    "        if mergetempRos:\n",
    "            temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            # use bhv event to decide temporal resolution\n",
    "            #\n",
    "            #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "        #\n",
    "        if doBhvitv_timebin:\n",
    "            pull_other_intv_ii = pd.Series(bhv_intv_all_dates[date_tgt]['pull_other_pooled'])\n",
    "            # remove the interval that is too large\n",
    "            pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "            # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "            temp_resolus = [np.nanmean(pull_other_intv_ii)]          \n",
    "        #\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "        \n",
    "        # try different temporal resolutions\n",
    "        for temp_resolu in temp_resolus:\n",
    "            bhv_df = []\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            else:\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "            if len(bhv_df)==0:\n",
    "                bhv_df = bhv_df_itr\n",
    "            else:\n",
    "                bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "            DBN_input_data_alltypes[date_tgt] = bhv_df\n",
    "            \n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b853146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(np.ceil(np.shape(look_at_other_or_not_merge['dodson'])[0]/fps-session_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb30f8",
   "metadata": {},
   "source": [
    "#### plot the gaze distribution around pulls, analysis is based on the DBN_input_data all session format\n",
    "#### similar plot was in \"3LagDBN_and_SuccAndFailedPull_singlecam_wholebodylabels_allsessions_basicEvents\" looking at the difference between successful and failed pulls\n",
    "#### pool across all animals, compared self reward, 3s to 1s cooperation and no vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    # animal1_fixedorders = ['eddie']\n",
    "    # animal2_fixedorders = ['sparkle']\n",
    "    # \n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    # initiate the final data set\n",
    "    SameAnimal_gazeDist_mean_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_mean_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    # shuffle both the pull and gaze time stamp\n",
    "    SameAnimal_gazeDist_shuffle_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_shuffle_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    SameAnimal_gazeDist_mean_all = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_mean_all = dict.fromkeys(grouptypes,[])\n",
    "    # shuffle both the pull and gaze time stamp\n",
    "    SameAnimal_gazeDist_shuffle_all = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_shuffle_all = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    SameAnimal_gazeDist_mean_male = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_mean_male = dict.fromkeys(grouptypes,[])\n",
    "    SameAnimal_gazeDist_mean_female = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_mean_female = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "    SameAnimal_gazeDist_mean_sub = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_mean_sub = dict.fromkeys(grouptypes,[])\n",
    "    SameAnimal_gazeDist_mean_dom = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_gazeDist_mean_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        SameAnimal_gazeDist_mean_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        AcroAnimal_gazeDist_mean_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        SameAnimal_gazeDist_shuffle_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        AcroAnimal_gazeDist_shuffle_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])   \n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # initiate the final data set\n",
    "            SameAnimal_gazeDist_mean_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            SameAnimal_gazeDist_mean_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_gazeDist_mean_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_gazeDist_mean_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            #\n",
    "            SameAnimal_gazeDist_shuffle_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            SameAnimal_gazeDist_shuffle_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "\n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "\n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "\n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "\n",
    "\n",
    "        # combine across all animals\n",
    "        df = pd.DataFrame([SameAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        SameAnimal_gazeDist_mean_all[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        AcroAnimal_gazeDist_mean_all[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "        # combine across al animals for shuffle\n",
    "        df = pd.DataFrame([SameAnimal_gazeDist_shuffle_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        SameAnimal_gazeDist_shuffle_all[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_gazeDist_shuffle_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        AcroAnimal_gazeDist_shuffle_all[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "        # combine across male and female\n",
    "        df = pd.DataFrame([SameAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in malenames])\n",
    "        SameAnimal_gazeDist_mean_male[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in malenames])\n",
    "        AcroAnimal_gazeDist_mean_male[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([SameAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in femalenames])\n",
    "        SameAnimal_gazeDist_mean_female[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in femalenames])\n",
    "        AcroAnimal_gazeDist_mean_female[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "        # combine across sub and dom\n",
    "        df = pd.DataFrame([SameAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in subnames])\n",
    "        SameAnimal_gazeDist_mean_sub[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in subnames])\n",
    "        AcroAnimal_gazeDist_mean_sub[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([SameAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in domnames])\n",
    "        SameAnimal_gazeDist_mean_dom[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_gazeDist_mean_forEachAni[grouptype][name] for name in domnames])\n",
    "        AcroAnimal_gazeDist_mean_dom[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "\n",
    "    #\n",
    "    if 1:\n",
    "\n",
    "        xxx = np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "        fig, axs = plt.subplots(3, 2)\n",
    "        fig.set_figheight(5*3)\n",
    "        fig.set_figwidth(7*2)   \n",
    "\n",
    "        # plot the summarizing figure\n",
    "        # plot the within animal and across animal distribution\n",
    "\n",
    "        for iplottype in np.arange(0,2,1):\n",
    "            # \n",
    "            # plot, all animals\n",
    "            conds_forplot = ['self reward','1s threshold','novision']\n",
    "            # conds_forplot = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "            gazeDist_average_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_std_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_average_shf_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_std_shf_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            for cond_forplot in conds_forplot:\n",
    "                if iplottype == 0:\n",
    "                    gazeDist_average_forplot[cond_forplot] = np.nanmean(SameAnimal_gazeDist_mean_all[cond_forplot],axis=0)\n",
    "                    gazeDist_std_forplot[cond_forplot] = np.nanstd(SameAnimal_gazeDist_mean_all[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_gazeDist_mean_all[cond_forplot])[0])\n",
    "                    #\n",
    "                    gazeDist_average_shf_forplot[cond_forplot] = np.nanmean(SameAnimal_gazeDist_shuffle_all[cond_forplot],axis=0)\n",
    "                    gazeDist_std_shf_forplot[cond_forplot] = np.nanstd(SameAnimal_gazeDist_shuffle_all[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_gazeDist_shuffle_all[cond_forplot])[0])\n",
    "                elif iplottype == 1:\n",
    "                    gazeDist_average_forplot[cond_forplot] = np.nanmean(AcroAnimal_gazeDist_mean_all[cond_forplot],axis=0)\n",
    "                    gazeDist_std_forplot[cond_forplot] = np.nanstd(AcroAnimal_gazeDist_mean_all[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_gazeDist_mean_all[cond_forplot])[0])\n",
    "                    #\n",
    "                    gazeDist_average_shf_forplot[cond_forplot] = np.nanmean(AcroAnimal_gazeDist_shuffle_all[cond_forplot],axis=0)\n",
    "                    gazeDist_std_shf_forplot[cond_forplot] = np.nanstd(AcroAnimal_gazeDist_shuffle_all[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_gazeDist_shuffle_all[cond_forplot])[0])\n",
    "                #\n",
    "                axs[0,iplottype].errorbar(xxx,gazeDist_average_forplot[cond_forplot],\n",
    "                                gazeDist_std_forplot[cond_forplot],label=cond_forplot)\n",
    "                # axs[0,iplottype].errorbar(xxx,gazeDist_average_shf_forplot[cond_forplot],\n",
    "                #                 gazeDist_std_shf_forplot[cond_forplot],label=\"shuffled \"+cond_forplot)\n",
    "            axs[0,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[0,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[0,iplottype].set_ylim(0,0.3)\n",
    "            # axs[0,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[0,iplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "            axs[0,iplottype].legend()   \n",
    "            if iplottype == 0:\n",
    "                axs[0,iplottype].set_title('within animal: all animals',fontsize=16)   \n",
    "            elif iplottype == 1:\n",
    "                axs[0,iplottype].set_title('across animal: all animals',fontsize=16)\n",
    "\n",
    "            # plot, male and female\n",
    "            conds_forplot = ['1s threshold']\n",
    "            gazeDist_average_male_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_std_male_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_average_female_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_std_female_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            for cond_forplot in conds_forplot:\n",
    "                if iplottype == 0:\n",
    "                    gazeDist_average_male_forplot[cond_forplot] = np.nanmean(SameAnimal_gazeDist_mean_male[cond_forplot],axis=0)\n",
    "                    gazeDist_std_male_forplot[cond_forplot] = np.nanstd(SameAnimal_gazeDist_mean_male[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_gazeDist_mean_male[cond_forplot])[0])\n",
    "                    #\n",
    "                    gazeDist_average_female_forplot[cond_forplot] = np.nanmean(SameAnimal_gazeDist_mean_female[cond_forplot],axis=0)\n",
    "                    gazeDist_std_female_forplot[cond_forplot] = np.nanstd(SameAnimal_gazeDist_mean_female[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_gazeDist_mean_female[cond_forplot])[0])\n",
    "                elif iplottype == 1:\n",
    "                    gazeDist_average_male_forplot[cond_forplot] = np.nanmean(AcroAnimal_gazeDist_mean_male[cond_forplot],axis=0)\n",
    "                    gazeDist_std_male_forplot[cond_forplot] = np.nanstd(AcroAnimal_gazeDist_mean_male[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_gazeDist_mean_male[cond_forplot])[0])\n",
    "                    #\n",
    "                    gazeDist_average_female_forplot[cond_forplot] = np.nanmean(AcroAnimal_gazeDist_mean_female[cond_forplot],axis=0)\n",
    "                    gazeDist_std_female_forplot[cond_forplot] = np.nanstd(AcroAnimal_gazeDist_mean_female[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_gazeDist_mean_female[cond_forplot])[0])\n",
    "                #\n",
    "                axs[1,iplottype].errorbar(xxx,gazeDist_average_male_forplot[cond_forplot],\n",
    "                                gazeDist_std_male_forplot[cond_forplot],label='male '+cond_forplot)\n",
    "                axs[1,iplottype].errorbar(xxx,gazeDist_average_female_forplot[cond_forplot],\n",
    "                                gazeDist_std_female_forplot[cond_forplot],label='female '+cond_forplot)\n",
    "            axs[1,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[1,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[1,iplottype].set_ylim(0,0.3)\n",
    "            # axs[1,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[1,iplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "            axs[1,iplottype].legend()   \n",
    "            if iplottype == 0:\n",
    "                axs[1,iplottype].set_title('within animal: male and female',fontsize=16) \n",
    "            elif iplottype == 1:\n",
    "                axs[1,iplottype].set_title('across animal: male and female',fontsize=16) \n",
    "\n",
    "            # plot, sub and dom\n",
    "            conds_forplot = ['1s threshold']\n",
    "            gazeDist_average_dom_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_std_dom_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_average_sub_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            gazeDist_std_sub_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            for cond_forplot in conds_forplot:\n",
    "                if iplottype == 0:\n",
    "                    gazeDist_average_sub_forplot[cond_forplot] = np.nanmean(SameAnimal_gazeDist_mean_sub[cond_forplot],axis=0)\n",
    "                    gazeDist_std_sub_forplot[cond_forplot] = np.nanstd(SameAnimal_gazeDist_mean_sub[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_gazeDist_mean_sub[cond_forplot])[0]) \n",
    "                    #\n",
    "                    gazeDist_average_dom_forplot[cond_forplot] = np.nanmean(SameAnimal_gazeDist_mean_dom[cond_forplot],axis=0)\n",
    "                    gazeDist_std_dom_forplot[cond_forplot] = np.nanstd(SameAnimal_gazeDist_mean_dom[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_gazeDist_mean_dom[cond_forplot])[0])\n",
    "                elif iplottype == 1:\n",
    "                    gazeDist_average_sub_forplot[cond_forplot] = np.nanmean(AcroAnimal_gazeDist_mean_sub[cond_forplot],axis=0)\n",
    "                    gazeDist_std_sub_forplot[cond_forplot] = np.nanstd(AcroAnimal_gazeDist_mean_sub[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_gazeDist_mean_sub[cond_forplot])[0]) \n",
    "                    #\n",
    "                    gazeDist_average_dom_forplot[cond_forplot] = np.nanmean(AcroAnimal_gazeDist_mean_dom[cond_forplot],axis=0)\n",
    "                    gazeDist_std_dom_forplot[cond_forplot] = np.nanstd(AcroAnimal_gazeDist_mean_dom[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_gazeDist_mean_dom[cond_forplot])[0])\n",
    "                #\n",
    "                axs[2,iplottype].errorbar(xxx,gazeDist_average_sub_forplot[cond_forplot],\n",
    "                                gazeDist_std_sub_forplot[cond_forplot],label='sub '+cond_forplot)\n",
    "                axs[2,iplottype].errorbar(xxx,gazeDist_average_dom_forplot[cond_forplot],\n",
    "                                gazeDist_std_dom_forplot[cond_forplot],label='dom '+cond_forplot)\n",
    "            axs[2,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[2,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[2,iplottype].set_ylim(0,0.3)\n",
    "            axs[2,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[2,iplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "            axs[2,iplottype].legend()   \n",
    "            if iplottype == 0:\n",
    "                axs[2,iplottype].set_title('within animal: subordinate and dominant',fontsize=16) \n",
    "            elif iplottype == 1:\n",
    "                axs[2,iplottype].set_title('across animal: subordinate and dominant',fontsize=16) \n",
    "\n",
    "        savefigs = 1\n",
    "        if savefigs:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            plt.savefig(figsavefolder+\"socialgaze_distribution_summaryplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # t-test for each time point\n",
    "    xxx1 = AcroAnimal_gazeDist_mean_all['1s threshold']\n",
    "    xxx2 = AcroAnimal_gazeDist_mean_all['self reward']\n",
    "    xxx3 = AcroAnimal_gazeDist_mean_all['novision']\n",
    "    #\n",
    "    ntimepoints = np.shape(xxx1)[1]\n",
    "    pvalues12_all = np.ones((1,ntimepoints))[0]\n",
    "    pvalues13_all = np.ones((1,ntimepoints))[0]\n",
    "    pvalues23_all = np.ones((1,ntimepoints))[0]\n",
    "    #\n",
    "    for itimepoint in np.arange(0,ntimepoints,1):\n",
    "        pvalues12_all[itimepoint] = st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint]).pvalue\n",
    "        pvalues12_all[itimepoint] = round(pvalues12_all[itimepoint]*1000)/1000\n",
    "    print(pvalues12_all)\n",
    "    #\n",
    "    for itimepoint in np.arange(0,ntimepoints,1):\n",
    "        pvalues13_all[itimepoint] = st.ttest_ind(xxx1[:,itimepoint],xxx3[:,itimepoint]).pvalue\n",
    "        pvalues13_all[itimepoint] = round(pvalues13_all[itimepoint]*1000)/1000\n",
    "    print(pvalues13_all)\n",
    "    #\n",
    "    for itimepoint in np.arange(0,ntimepoints,1):\n",
    "        pvalues23_all[itimepoint] = st.ttest_ind(xxx2[:,itimepoint],xxx3[:,itimepoint]).pvalue\n",
    "        pvalues23_all[itimepoint] = round(pvalues23_all[itimepoint]*1000)/1000\n",
    "    print(pvalues23_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # t-test for each time point\n",
    "    xxx1 = AcroAnimal_gazeDist_mean_dom['1s threshold']\n",
    "    xxx2 = AcroAnimal_gazeDist_mean_sub['1s threshold']\n",
    "    #\n",
    "    ntimepoints = np.shape(xxx1)[1]\n",
    "    pvalues_all = np.ones((1,ntimepoints))[0]\n",
    "    #\n",
    "    for itimepoint in np.arange(0,ntimepoints,1):\n",
    "        st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint])\n",
    "        pvalues_all[itimepoint] = st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint]).pvalue\n",
    "        pvalues_all[itimepoint] = round(pvalues_all[itimepoint]*1000)/1000\n",
    "    print(pvalues_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39116ea6",
   "metadata": {},
   "source": [
    "#### get the half (max - min) width for selected conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splrep, sproot, splev\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.optimize import curve_fit \n",
    "\n",
    "class MultiplePeaks(Exception): pass\n",
    "class NoPeaksFound(Exception): pass\n",
    "\n",
    "def fwhm(x, y, k=10):\n",
    "    \"\"\"\n",
    "    Determine full-with-half-maximum of a peaked set of points, x and y.\n",
    "\n",
    "    Assumes that there is only one peak present in the datasset.  The function\n",
    "    uses a spline interpolation of order k.\n",
    "    \"\"\"\n",
    "\n",
    "    half_max = max(y)/2.0\n",
    "    # half_max = y[round(np.shape(y)[0]/2)-1]\n",
    "    s = splrep(x, y - half_max, k=k)\n",
    "    roots = sproot(s)\n",
    "\n",
    "    if len(roots) > 2:\n",
    "    #     raise MultiplePeaks(\"The dataset appears to have multiple peaks, and \"\n",
    "    #             \"thus the FWHM can't be determined.\")\n",
    "        # return np.nan\n",
    "        return abs(roots[1] - roots[0])\n",
    "    elif len(roots) < 2:\n",
    "    #     raise NoPeaksFound(\"No proper peaks were found in the data set; likely \"\n",
    "    #             \"the dataset is flat (e.g. all zeros).\")\n",
    "        # return np.max(x)-np.min(x)\n",
    "        return np.nan\n",
    "    else:\n",
    "        return abs(roots[1] - roots[0])\n",
    "        \n",
    "        \n",
    "#\n",
    "# Define the Gaussian function \n",
    "def Gauss(x, A, B): \n",
    "    y = A*np.exp(-1*B*x**2) \n",
    "    return y \n",
    "\n",
    "# Define the Gaussian function\n",
    "def gaussian(x, A, B, C):\n",
    "    y = A*np.exp(-1*B*(x-C)**2) \n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    x =  np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "    conditions = list(AcroAnimal_gazeDist_mean_all.keys())\n",
    "    nconds = np.shape(conditions)[0]\n",
    "\n",
    "    halfwidth_all = dict.fromkeys(conditions)\n",
    "\n",
    "    for icond in np.arange(0,nconds,1):\n",
    "\n",
    "        condname = conditions[icond]\n",
    "\n",
    "        y_allsess = AcroAnimal_gazeDist_mean_all[condname]\n",
    "        nsess = np.shape(y_allsess)[0]\n",
    "\n",
    "        halfwidth_all[condname] = np.ones((1,nsess))[0]*np.nan\n",
    "\n",
    "        for isess in np.arange(0,nsess,1):\n",
    "\n",
    "            try:\n",
    "                y =  y_allsess[isess]\n",
    "                y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))      \n",
    "\n",
    "                # parameters, covariance = curve_fit(Gauss, x, y) \n",
    "                parameters, covariance = curve_fit(gaussian, x, y) \n",
    "                #\n",
    "                fit_A = parameters[0] \n",
    "                fit_B = parameters[1] \n",
    "                fit_C = parameters[2] \n",
    "                #\n",
    "                # fit_y = Gauss(x, fit_A, fit_B, fit_C) \n",
    "                fit_y = gaussian(x,fit_A,fit_B,fit_C)\n",
    "                y = (fit_y-np.nanmin(fit_y))/(np.nanmax(fit_y)-np.nanmin(fit_y)) \n",
    "\n",
    "                halfwidth_all[condname][isess] = fwhm(x, y, k=3)\n",
    "\n",
    "            except:\n",
    "                halfwidth_all[condname][isess] = np.nan\n",
    "\n",
    "    # box plot \n",
    "    fig, axs = plt.subplots(1,1)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "\n",
    "    # subplot 1 - all animals\n",
    "    halfwidth_all_df = pd.DataFrame.from_dict(halfwidth_all,orient='index')\n",
    "    halfwidth_all_df = halfwidth_all_df.transpose()\n",
    "    halfwidth_all_df['type'] = 'all'\n",
    "    #\n",
    "    df_long=pd.concat([halfwidth_all_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=conditions,var_name='condition', value_name='value')\n",
    "    # \n",
    "    # barplot ans swarmplot\n",
    "    #$  seaborn.boxplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "    seaborn.violinplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "    # seaborn.swarmplot(ax=axs,data=df_long2,x='condition',y='value',hue='type',\n",
    "    #                   alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs.set_xlabel('')\n",
    "    axs.set_xticklabels(conditions)\n",
    "    axs.xaxis.set_tick_params(labelsize=15,rotation=45)\n",
    "    axs.set_ylabel(\"half max width\",fontsize=15)\n",
    "    axs.set_title('all animals' ,fontsize=24)\n",
    "    axs.set_ylim([-5,15])\n",
    "    axs.legend(fontsize=18)\n",
    "\n",
    "    savefigs = 0\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        plt.savefig(figsavefolder+\"socialgaze_distribution_summaryplot_halfmaxWitdh.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    df_long2 = df_long2[~np.isnan(df_long2.value)]\n",
    "    # anova\n",
    "    cw_lm=ols('value ~ condition', data=df_long2).fit() #Specify C for Categorical\n",
    "    print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "    # post hoc test \n",
    "    tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition'], alpha=0.05)\n",
    "    print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# male female\n",
    "if 1:\n",
    "    \n",
    "    x =  np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "    conditions = list(AcroAnimal_gazeDist_mean_all.keys())\n",
    "    nconds = np.shape(conditions)[0]\n",
    "\n",
    "    halfwidth_male = dict.fromkeys(conditions)\n",
    "    \n",
    "    for icond in np.arange(0,nconds,1):\n",
    "\n",
    "        condname = conditions[icond]\n",
    "\n",
    "        y_allsess = AcroAnimal_gazeDist_mean_male[condname]\n",
    "        nsess = np.shape(y_allsess)[0]\n",
    "\n",
    "        halfwidth_male[condname] = np.ones((1,nsess))[0]*np.nan\n",
    "\n",
    "        for isess in np.arange(0,nsess,1):\n",
    "\n",
    "            try:\n",
    "                y =  y_allsess[isess]\n",
    "                y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))      \n",
    "\n",
    "                # parameters, covariance = curve_fit(Gauss, x, y) \n",
    "                parameters, covariance = curve_fit(gaussian, x, y) \n",
    "                #\n",
    "                fit_A = parameters[0] \n",
    "                fit_B = parameters[1] \n",
    "                fit_C = parameters[2] \n",
    "                #\n",
    "                # fit_y = Gauss(x, fit_A, fit_B, fit_C) \n",
    "                fit_y = gaussian(x,fit_A,fit_B,fit_C)\n",
    "                y = (fit_y-np.nanmin(fit_y))/(np.nanmax(fit_y)-np.nanmin(fit_y)) \n",
    "\n",
    "                halfwidth_male[condname][isess] = fwhm(x, y, k=3)\n",
    "\n",
    "            except:\n",
    "                halfwidth_male[condname][isess] = np.nan\n",
    "\n",
    "    halfwidth_female = dict.fromkeys(conditions)\n",
    "    \n",
    "    for icond in np.arange(0,nconds,1):\n",
    "\n",
    "        condname = conditions[icond]\n",
    "\n",
    "        y_allsess = AcroAnimal_gazeDist_mean_female[condname]\n",
    "        nsess = np.shape(y_allsess)[0]\n",
    "\n",
    "        halfwidth_female[condname] = np.ones((1,nsess))[0]*np.nan\n",
    "\n",
    "        for isess in np.arange(0,nsess,1):\n",
    "\n",
    "            try:\n",
    "                y =  y_allsess[isess]\n",
    "                y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))      \n",
    "\n",
    "                # parameters, covariance = curve_fit(Gauss, x, y) \n",
    "                parameters, covariance = curve_fit(gaussian, x, y) \n",
    "                #\n",
    "                fit_A = parameters[0] \n",
    "                fit_B = parameters[1] \n",
    "                fit_C = parameters[2] \n",
    "                #\n",
    "                # fit_y = Gauss(x, fit_A, fit_B, fit_C) \n",
    "                fit_y = gaussian(x,fit_A,fit_B,fit_C)\n",
    "                y = (fit_y-np.nanmin(fit_y))/(np.nanmax(fit_y)-np.nanmin(fit_y)) \n",
    "\n",
    "                halfwidth_female[condname][isess] = fwhm(x, y, k=3)\n",
    "\n",
    "            except:\n",
    "                halfwidth_female[condname][isess] = np.nan  \n",
    "    \n",
    "    # box plot \n",
    "    fig, axs = plt.subplots(1,1)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "\n",
    "    # subplot 1 - all animals\n",
    "    halfwidth_male_df = pd.DataFrame.from_dict(halfwidth_male,orient='index')\n",
    "    halfwidth_male_df = halfwidth_male_df.transpose()\n",
    "    halfwidth_male_df['type'] = 'male'\n",
    "    halfwidth_female_df = pd.DataFrame.from_dict(halfwidth_female,orient='index')\n",
    "    halfwidth_female_df = halfwidth_female_df.transpose()\n",
    "    halfwidth_female_df['type'] = 'female'\n",
    "    \n",
    "    #\n",
    "    df_long=pd.concat([halfwidth_male_df,halfwidth_female_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=conditions,var_name='condition', value_name='value')\n",
    "    # \n",
    "    # barplot ans swarmplot\n",
    "    #$  seaborn.boxplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "    seaborn.violinplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "    # seaborn.swarmplot(ax=axs,data=df_long2,x='condition',y='value',hue='type',\n",
    "    #                   alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs.set_xlabel('')\n",
    "    axs.set_xticklabels(conditions)\n",
    "    axs.xaxis.set_tick_params(labelsize=15,rotation=45)\n",
    "    axs.set_ylabel(\"half max width\",fontsize=15)\n",
    "    axs.set_title('all animals' ,fontsize=24)\n",
    "    axs.set_ylim([-5,15])\n",
    "    axs.legend(fontsize=18)\n",
    "\n",
    "    savefigs = 0\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        plt.savefig(figsavefolder+\"socialgaze_distribution_summaryplot_halfmaxWitdh.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4b378",
   "metadata": {},
   "source": [
    "#### plot the pull distribution around pulls, analysis is based on the DBN_input_data all session format\n",
    "#### pool across all animals, compared self reward, 3s to 1s cooperation and no vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f65662",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    \n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    # initiate the final data set\n",
    "    SameAnimal_pullDist_mean_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_mean_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    # shuffle both the pull and gaze time stamp\n",
    "    SameAnimal_pullDist_shuffle_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_shuffle_forEachAni = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    SameAnimal_pullDist_mean_all = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_mean_all = dict.fromkeys(grouptypes,[])\n",
    "    # shuffle both the pull and gaze time stamp\n",
    "    SameAnimal_pullDist_shuffle_all = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_shuffle_all = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    SameAnimal_pullDist_mean_male = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_mean_male = dict.fromkeys(grouptypes,[])\n",
    "    SameAnimal_pullDist_mean_female = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_mean_female = dict.fromkeys(grouptypes,[])\n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "    SameAnimal_pullDist_mean_sub = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_mean_sub = dict.fromkeys(grouptypes,[])\n",
    "    SameAnimal_pullDist_mean_dom = dict.fromkeys(grouptypes,[])\n",
    "    AcroAnimal_pullDist_mean_dom = dict.fromkeys(grouptypes,[])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        SameAnimal_pullDist_mean_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        AcroAnimal_pullDist_mean_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        SameAnimal_pullDist_shuffle_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "        AcroAnimal_pullDist_shuffle_forEachAni[grouptype] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])   \n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # initiate the final data set\n",
    "            SameAnimal_pullDist_mean_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            SameAnimal_pullDist_mean_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_pullDist_mean_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_pullDist_mean_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            #\n",
    "            SameAnimal_pullDist_shuffle_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            SameAnimal_pullDist_shuffle_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_pullDist_shuffle_forEachAni[grouptype][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "            AcroAnimal_pullDist_shuffle_forEachAni[grouptype][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # pull1_t0 and pull1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_pullDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_pullDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_pullDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_pullDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "\n",
    "                # pull2_t0 and pull2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_pullDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_pullDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_pullDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_pullDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "\n",
    "                # pull1_t0 and pull2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_pullDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_pullDist_mean_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_pullDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_pullDist_shuffle_forEachAni[grouptype][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "\n",
    "                # pull2_t0 and pull1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_pullDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_pullDist_mean_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_pullDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_pullDist_shuffle_forEachAni[grouptype][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "\n",
    "\n",
    "        # combine across all animals\n",
    "        df = pd.DataFrame([SameAnimal_pullDist_mean_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        SameAnimal_pullDist_mean_all[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_pullDist_mean_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        AcroAnimal_pullDist_mean_all[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "        # combine across al animals for shuffle\n",
    "        df = pd.DataFrame([SameAnimal_pullDist_shuffle_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        SameAnimal_pullDist_shuffle_all[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_pullDist_shuffle_forEachAni[grouptype][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "        AcroAnimal_pullDist_shuffle_all[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "        # combine across male and female\n",
    "        df = pd.DataFrame([SameAnimal_pullDist_mean_forEachAni[grouptype][name] for name in malenames])\n",
    "        SameAnimal_pullDist_mean_male[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_pullDist_mean_forEachAni[grouptype][name] for name in malenames])\n",
    "        AcroAnimal_pullDist_mean_male[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([SameAnimal_pullDist_mean_forEachAni[grouptype][name] for name in femalenames])\n",
    "        SameAnimal_pullDist_mean_female[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_pullDist_mean_forEachAni[grouptype][name] for name in femalenames])\n",
    "        AcroAnimal_pullDist_mean_female[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "        # combine across sub and dom\n",
    "        df = pd.DataFrame([SameAnimal_pullDist_mean_forEachAni[grouptype][name] for name in subnames])\n",
    "        SameAnimal_pullDist_mean_sub[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_pullDist_mean_forEachAni[grouptype][name] for name in subnames])\n",
    "        AcroAnimal_pullDist_mean_sub[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([SameAnimal_pullDist_mean_forEachAni[grouptype][name] for name in domnames])\n",
    "        SameAnimal_pullDist_mean_dom[grouptype] = np.vstack(df.stack().values)\n",
    "        df = pd.DataFrame([AcroAnimal_pullDist_mean_forEachAni[grouptype][name] for name in domnames])\n",
    "        AcroAnimal_pullDist_mean_dom[grouptype] = np.vstack(df.stack().values)\n",
    "\n",
    "\n",
    "    #\n",
    "    if 1:\n",
    "\n",
    "        xxx = np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "        fig, axs = plt.subplots(3, 2)\n",
    "        fig.set_figheight(5*3)\n",
    "        fig.set_figwidth(7*2)   \n",
    "\n",
    "        # plot the summarizing figure\n",
    "        # plot the within animal and across animal distribution\n",
    "\n",
    "        for iplottype in np.arange(0,2,1):\n",
    "            # \n",
    "            # plot, all animals\n",
    "            conds_forplot = ['self reward','1s threshold','novision']\n",
    "            # conds_forplot = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "            pullDist_average_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_std_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_average_shf_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_std_shf_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            for cond_forplot in conds_forplot:\n",
    "                if iplottype == 0:\n",
    "                    pullDist_average_forplot[cond_forplot] = np.nanmean(SameAnimal_pullDist_mean_all[cond_forplot],axis=0)\n",
    "                    pullDist_std_forplot[cond_forplot] = np.nanstd(SameAnimal_pullDist_mean_all[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_pullDist_mean_all[cond_forplot])[0])\n",
    "                    #\n",
    "                    pullDist_average_shf_forplot[cond_forplot] = np.nanmean(SameAnimal_pullDist_shuffle_all[cond_forplot],axis=0)\n",
    "                    pullDist_std_shf_forplot[cond_forplot] = np.nanstd(SameAnimal_pullDist_shuffle_all[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_pullDist_shuffle_all[cond_forplot])[0])\n",
    "                elif iplottype == 1:\n",
    "                    pullDist_average_forplot[cond_forplot] = np.nanmean(AcroAnimal_pullDist_mean_all[cond_forplot],axis=0)\n",
    "                    pullDist_std_forplot[cond_forplot] = np.nanstd(AcroAnimal_pullDist_mean_all[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_pullDist_mean_all[cond_forplot])[0])\n",
    "                    #\n",
    "                    pullDist_average_shf_forplot[cond_forplot] = np.nanmean(AcroAnimal_pullDist_shuffle_all[cond_forplot],axis=0)\n",
    "                    pullDist_std_shf_forplot[cond_forplot] = np.nanstd(AcroAnimal_pullDist_shuffle_all[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_pullDist_shuffle_all[cond_forplot])[0])\n",
    "                #\n",
    "                axs[0,iplottype].errorbar(xxx,pullDist_average_forplot[cond_forplot],\n",
    "                                pullDist_std_forplot[cond_forplot],label=cond_forplot)\n",
    "                # axs[0,iplottype].errorbar(xxx,pullDist_average_shf_forplot[cond_forplot],\n",
    "                #                 pullDist_std_shf_forplot[cond_forplot],label=\"shuffled \"+cond_forplot)\n",
    "            axs[0,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[0,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[0,iplottype].set_ylim(0,0.3)\n",
    "            # axs[0,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[0,iplottype].set_ylabel('pull probability',fontsize=15)\n",
    "            axs[0,iplottype].legend()   \n",
    "            if iplottype == 0:\n",
    "                axs[0,iplottype].set_title('within animal: all animals',fontsize=16)   \n",
    "            elif iplottype == 1:\n",
    "                axs[0,iplottype].set_title('across animal: all animals',fontsize=16)\n",
    "\n",
    "            # plot, male and female\n",
    "            conds_forplot = ['1s threshold']\n",
    "            pullDist_average_male_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_std_male_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_average_female_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_std_female_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            for cond_forplot in conds_forplot:\n",
    "                if iplottype == 0:\n",
    "                    pullDist_average_male_forplot[cond_forplot] = np.nanmean(SameAnimal_pullDist_mean_male[cond_forplot],axis=0)\n",
    "                    pullDist_std_male_forplot[cond_forplot] = np.nanstd(SameAnimal_pullDist_mean_male[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_pullDist_mean_male[cond_forplot])[0])\n",
    "                    #\n",
    "                    pullDist_average_female_forplot[cond_forplot] = np.nanmean(SameAnimal_pullDist_mean_female[cond_forplot],axis=0)\n",
    "                    pullDist_std_female_forplot[cond_forplot] = np.nanstd(SameAnimal_pullDist_mean_female[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_pullDist_mean_female[cond_forplot])[0])\n",
    "                elif iplottype == 1:\n",
    "                    pullDist_average_male_forplot[cond_forplot] = np.nanmean(AcroAnimal_pullDist_mean_male[cond_forplot],axis=0)\n",
    "                    pullDist_std_male_forplot[cond_forplot] = np.nanstd(AcroAnimal_pullDist_mean_male[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_pullDist_mean_male[cond_forplot])[0])\n",
    "                    #\n",
    "                    pullDist_average_female_forplot[cond_forplot] = np.nanmean(AcroAnimal_pullDist_mean_female[cond_forplot],axis=0)\n",
    "                    pullDist_std_female_forplot[cond_forplot] = np.nanstd(AcroAnimal_pullDist_mean_female[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_pullDist_mean_female[cond_forplot])[0])\n",
    "                #\n",
    "                axs[1,iplottype].errorbar(xxx,pullDist_average_male_forplot[cond_forplot],\n",
    "                                pullDist_std_male_forplot[cond_forplot],label='male '+cond_forplot)\n",
    "                axs[1,iplottype].errorbar(xxx,pullDist_average_female_forplot[cond_forplot],\n",
    "                                pullDist_std_female_forplot[cond_forplot],label='female '+cond_forplot)\n",
    "            axs[1,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[1,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[1,iplottype].set_ylim(0,0.3)\n",
    "            # axs[1,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[1,iplottype].set_ylabel('pull probability',fontsize=15)\n",
    "            axs[1,iplottype].legend()   \n",
    "            if iplottype == 0:\n",
    "                axs[1,iplottype].set_title('within animal: male and female',fontsize=16) \n",
    "            elif iplottype == 1:\n",
    "                axs[1,iplottype].set_title('across animal: male and female',fontsize=16) \n",
    "\n",
    "            # plot, sub and dom\n",
    "            conds_forplot = ['1s threshold']\n",
    "            pullDist_average_dom_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_std_dom_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_average_sub_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            pullDist_std_sub_forplot = dict.fromkeys(conds_forplot,[])\n",
    "            for cond_forplot in conds_forplot:\n",
    "                if iplottype == 0:\n",
    "                    pullDist_average_sub_forplot[cond_forplot] = np.nanmean(SameAnimal_pullDist_mean_sub[cond_forplot],axis=0)\n",
    "                    pullDist_std_sub_forplot[cond_forplot] = np.nanstd(SameAnimal_pullDist_mean_sub[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_pullDist_mean_sub[cond_forplot])[0]) \n",
    "                    #\n",
    "                    pullDist_average_dom_forplot[cond_forplot] = np.nanmean(SameAnimal_pullDist_mean_dom[cond_forplot],axis=0)\n",
    "                    pullDist_std_dom_forplot[cond_forplot] = np.nanstd(SameAnimal_pullDist_mean_dom[cond_forplot],axis=0)/np.sqrt(np.shape(SameAnimal_pullDist_mean_dom[cond_forplot])[0])\n",
    "                elif iplottype == 1:\n",
    "                    pullDist_average_sub_forplot[cond_forplot] = np.nanmean(AcroAnimal_pullDist_mean_sub[cond_forplot],axis=0)\n",
    "                    pullDist_std_sub_forplot[cond_forplot] = np.nanstd(AcroAnimal_pullDist_mean_sub[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_pullDist_mean_sub[cond_forplot])[0]) \n",
    "                    #\n",
    "                    pullDist_average_dom_forplot[cond_forplot] = np.nanmean(AcroAnimal_pullDist_mean_dom[cond_forplot],axis=0)\n",
    "                    pullDist_std_dom_forplot[cond_forplot] = np.nanstd(AcroAnimal_pullDist_mean_dom[cond_forplot],axis=0)/np.sqrt(np.shape(AcroAnimal_pullDist_mean_dom[cond_forplot])[0])\n",
    "                #\n",
    "                axs[2,iplottype].errorbar(xxx,pullDist_average_sub_forplot[cond_forplot],\n",
    "                                pullDist_std_sub_forplot[cond_forplot],label='sub '+cond_forplot)\n",
    "                axs[2,iplottype].errorbar(xxx,pullDist_average_dom_forplot[cond_forplot],\n",
    "                                pullDist_std_dom_forplot[cond_forplot],label='dom '+cond_forplot)\n",
    "            axs[2,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "            axs[2,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "            axs[2,iplottype].set_ylim(0,0.3)\n",
    "            axs[2,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "            axs[2,iplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "            axs[2,iplottype].legend()   \n",
    "            if iplottype == 0:\n",
    "                axs[2,iplottype].set_title('within animal: subordinate and dominant',fontsize=16) \n",
    "            elif iplottype == 1:\n",
    "                axs[2,iplottype].set_title('across animal: subordinate and dominant',fontsize=16) \n",
    "\n",
    "        savefigs = 0\n",
    "        if savefigs:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            plt.savefig(figsavefolder+\"pull_distribution_summaryplot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590c485",
   "metadata": {},
   "source": [
    "#### plot the pull auto-correlation\n",
    "#### similar as the previous code, but use higher resolution and calculate auto-correlation\n",
    "#### pool across all animals, compared self reward, 3s to 1s cooperation and no vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_autocorrelation(time_points, time_window_ms=100, max_lag_s=5):\n",
    "    \"\"\"\n",
    "    Compute the auto-correlation for given time point data.\n",
    "\n",
    "    Args:\n",
    "        time_points (np.array): Array of time points in seconds.\n",
    "        time_window_ms (int): Time window for the binary time series in milliseconds.\n",
    "        max_lag_s (int): Maximum lag to be shown in the plot in seconds.\n",
    "\n",
    "    Returns:\n",
    "        lags (np.array): Lag time points in seconds (with zero lag as NaN).\n",
    "        autocorrelation (np.array): Auto-correlation values at each lag (with zero lag as NaN).\n",
    "    \"\"\"\n",
    "    # Determine the maximum time range for the binary series\n",
    "    max_time = np.ceil(np.max(time_points)) + 1  # Adding extra second to cover the last interval\n",
    "    max_index = int(max_time * (1000 // time_window_ms))  # Convert to number of time windows\n",
    "\n",
    "    # Create binary time series\n",
    "    binary_series = np.zeros(max_index)\n",
    "    indices = (time_points * (1000 // time_window_ms)).astype(int)\n",
    "    binary_series[indices] = 1\n",
    "\n",
    "    # Calculate auto-correlation\n",
    "    def autocorrelation_with_step(data, step=1):\n",
    "        n = len(data)\n",
    "        lags = np.arange(0, n-1, step)\n",
    "        autocorr_values = np.correlate(data, data, mode='full')[len(data)-1:]\n",
    "        return lags, autocorr_values[lags]\n",
    "\n",
    "    lags, autocorr = autocorrelation_with_step(binary_series)\n",
    "    lags = lags * (time_window_ms / 1000)  # Convert lags to seconds\n",
    "\n",
    "    autocorr = autocorr / autocorr.max()  # Normalize the auto-correlation\n",
    "    # autocorr[0] = np.nan  # Replace zero lag auto-correlation with NaN\n",
    "\n",
    "    # Limit auto-correlation to max lag\n",
    "    max_lag_index = int(max_lag_s / (time_window_ms / 1000))  # Convert to number of time windows\n",
    "    return lags[:max_lag_index], autocorr[:max_lag_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    #\n",
    "    animal1_filenames = ['Eddie','Dodson','Dannon','Ginger','Koala']\n",
    "    animal2_filenames = ['Sparkle','Scorch','Kanga','Kanga','Vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "        #\n",
    "        animal1_filenames = ['Eddie','Dodson','Ginger',]\n",
    "        animal2_filenames = ['Sparkle','Scorch','Kanga',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    BhvAutoCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                     'bhvname','lags','autoCorr'])\n",
    "    \n",
    "    #\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "        animal1_filename = animal1_filenames[ianimalpair]\n",
    "        animal2_filename = animal2_filenames[ianimalpair]\n",
    "\n",
    "        print('organize data for '+animal1_fixedorder+' '+animal2_fixedorder)\n",
    "\n",
    "        # load the basic behavioral measures\n",
    "        # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_filename.lower()+animal2_filename.lower()+'/'\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sessstart_time_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            sessstart_time_all_dates = pickle.load(f)\n",
    "\n",
    "        # load the DBN related analysis\n",
    "        # load data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_filename.lower()+animal2_filename.lower()+'/'\n",
    "        #\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        #\n",
    "        # load data for successful and failed pulls\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_filename.lower()+animal2_filename.lower()+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "        #\n",
    "        # re-organize the target dates\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "        #\n",
    "        # sort the data based on task type and dates\n",
    "        dates_list = list(DBN_input_data_alltypes.keys())\n",
    "        sorting_df = pd.DataFrame({'dates':dates_list,'coopthres':coopthres_forsort.ravel(),'sessstarttime':sessstart_time_all_dates}, columns=['dates', 'coopthres','sessstarttime'])\n",
    "        sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "        #\n",
    "        # only select the targeted dates\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]        \n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "        sorting_tgt_df = sorting_df\n",
    "        dates_list_tgt = sorting_tgt_df['dates']\n",
    "        dates_list_tgt = np.array(dates_list_tgt)\n",
    "        session_start_times = np.array(sorting_tgt_df['sessstarttime'])\n",
    "        #\n",
    "        ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            grouptype = grouptypes[np.where(np.array(coopthres_IDs)==sorting_df['coopthres'][idate])[0][0]]\n",
    "            \n",
    "            print('organize data for '+idate_name)\n",
    "\n",
    "            # load behavioral results\n",
    "            trial_record = []\n",
    "            try:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info from the session information\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # load the raw gaze time \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_filename.lower()+animal2_filename.lower()+\"/\"+cameraID+'/'+idate_name+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_filename.lower()+animal2_filename.lower()+\"/\"+cameraID+'/'+idate_name+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_filename.lower()+animal2_filename.lower()+\"/\"+cameraID+'/'+idate_name+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            if animal1 == animal1_fixedorder:\n",
    "                time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "                time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "                oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "                oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "                mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "                mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            elif animal1 == animal2_fixedorder:\n",
    "                time_point_pull2 = output_time_points_socialgaze['time_point_pull1']\n",
    "                time_point_pull1 = output_time_points_socialgaze['time_point_pull2']\n",
    "                oneway_gaze2 = output_time_points_socialgaze['oneway_gaze1']\n",
    "                oneway_gaze1 = output_time_points_socialgaze['oneway_gaze2']\n",
    "                mutual_gaze2 = output_time_points_socialgaze['mutual_gaze1']\n",
    "                mutual_gaze1 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            #\n",
    "            time_point_pull1 = np.array(time_point_pull1)\n",
    "            time_point_pull2 = np.array(time_point_pull2)\n",
    "            time_point_gaze1 = np.sort(np.hstack((oneway_gaze1,mutual_gaze1)))\n",
    "            time_point_gaze2 = np.sort(np.hstack((oneway_gaze2,mutual_gaze2)))\n",
    "            # \n",
    "            # change social gaze as the start of a period of social gaze\n",
    "            if 1:\n",
    "                ind_gazestart = np.hstack(([1],(time_point_gaze1[1:]-time_point_gaze1[:-1])>=0.5))\n",
    "                ind_gazestart = ind_gazestart.astype(bool)\n",
    "                time_point_gaze1 = time_point_gaze1[ind_gazestart]\n",
    "                ind_gazestart = np.hstack(([1],(time_point_gaze2[1:]-time_point_gaze2[:-1])>=0.5))\n",
    "                ind_gazestart = ind_gazestart.astype(bool)\n",
    "                time_point_gaze2 = time_point_gaze2[ind_gazestart]\n",
    "                \n",
    "            # Compute auto-correlation\n",
    "            time_window_ms=500 # the size of the sliding time window; in the unit of ms\n",
    "            max_lag_s=10 # the max lag for saving output; in the unit of s\n",
    "            \n",
    "            lags, pull1_autocorr = compute_autocorrelation(time_point_pull1, time_window_ms, max_lag_s)\n",
    "            _, pull2_autocorr = compute_autocorrelation(time_point_pull2, time_window_ms, max_lag_s)\n",
    "            _, gaze1_autocorr = compute_autocorrelation(time_point_gaze1, time_window_ms, max_lag_s)\n",
    "            _, gaze2_autocorr = compute_autocorrelation(time_point_gaze2, time_window_ms, max_lag_s)\n",
    "            \n",
    "            # put the data together\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal1_fixedorder,\n",
    "                                                                        'bhvname':'pull',\n",
    "                                                                        'lags':lags,\n",
    "                                                                        'autoCorr':np.array(pull1_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "            #\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal2_fixedorder,\n",
    "                                                                        'bhvname':'pull',\n",
    "                                                                        'lags':lags,\n",
    "                                                                        'autoCorr':np.array(pull2_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "            #\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal1_fixedorder,\n",
    "                                                                        'bhvname':'gaze',\n",
    "                                                                        'lags':lags,\n",
    "                                                                        'autoCorr':np.array(gaze1_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "            #\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal2_fixedorder,\n",
    "                                                                        'bhvname':'gaze',\n",
    "                                                                        'lags':lags,\n",
    "                                                                        'autoCorr':np.array(gaze2_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # for plot the previous code\n",
    "    import math\n",
    "\n",
    "    lags = BhvAutoCorr_all_dates_df['lags'][0]\n",
    "\n",
    "    condition_toplot = 'self reward'\n",
    "    ind_cond = BhvAutoCorr_all_dates_df['condition']==condition_toplot\n",
    "\n",
    "    bhv_toplot = 'pull'\n",
    "    ind_bhv = BhvAutoCorr_all_dates_df['bhvname']==bhv_toplot\n",
    "\n",
    "    animal1_toplot = 'ginger'\n",
    "    ind_ani1 = BhvAutoCorr_all_dates_df['act_animal'] == animal1_toplot\n",
    "\n",
    "    animal2_toplot = 'kanga_2'\n",
    "    ind_ani2 = BhvAutoCorr_all_dates_df['act_animal'] == animal2_toplot\n",
    "\n",
    "    ind_plot1 = ind_cond & ind_bhv & ind_ani1\n",
    "    ind_plot2 = ind_cond & ind_bhv & ind_ani2\n",
    "\n",
    "    BhvAutoCorr_all_animal1 = BhvAutoCorr_all_dates_df[ind_plot1].reset_index(drop=True)\n",
    "    BhvAutoCorr_all_animal2 = BhvAutoCorr_all_dates_df[ind_plot2].reset_index(drop=True)\n",
    "    \n",
    "    BhvAutoCorr_all = BhvAutoCorr_all_dates_df[ind_cond & ind_bhv].reset_index(drop=True)\n",
    "\n",
    "    # plot for each date\n",
    "\n",
    "    # Determine the number of subplots needed\n",
    "    num_plots = len(BhvAutoCorr_all_animal1)\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(num_plots / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "    axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "    for i, row in BhvAutoCorr_all_animal1.iterrows():\n",
    "        axes[i].plot(row[\"lags\"], row[\"autoCorr\"], marker='o')\n",
    "    #    \n",
    "    for i, row in BhvAutoCorr_all_animal2.iterrows():\n",
    "        axes[i].plot(row[\"lags\"], row[\"autoCorr\"], marker='o')\n",
    "        axes[i].set_title(f\"Date: {row['dates']}\"+\" \"+animal1_toplot+\" \"+animal2_toplot)\n",
    "        axes[i].set_xlabel(\"Lags (s)\")\n",
    "        axes[i].set_ylabel(\"Autocorrelation\")\n",
    "        axes[i].grid()\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # averaging across animals\n",
    "    autoCorr = np.array(BhvAutoCorr_all['autoCorr'])\n",
    "    # Compute mean and standard error\n",
    "    mean_autoCorr = np.mean(autoCorr, axis=0)\n",
    "    mean_autoCorr[0] = np.nan\n",
    "    sem_autoCorr = np.std(autoCorr, axis=0)  / np.sqrt(autoCorr.shape[0])\n",
    "    # Plot with shaded error bars\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    seaborn.lineplot(x=lags, y=mean_autoCorr, label='Mean AutoCorr')\n",
    "    plt.fill_between(lags, mean_autoCorr - sem_autoCorr, mean_autoCorr + sem_autoCorr, alpha=0.3)\n",
    "    # Labels and title\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title('Mean Autocorrelation with SEM')\n",
    "    plt.ylim([0,0.15])\n",
    "    plt.axvline(0, color='black', linestyle='--', alpha=0.7)  # Mark zero lag\n",
    "    plt.legend()\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        plt.savefig(figsavefolder+\"self_\"+bhv_toplot+\"_autoCorr_allanimals_\"+condition_toplot+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoCorr = np.array(BhvAutoCorr_all['autoCorr'])\n",
    "# Compute mean and standard error\n",
    "mean_autoCorr = np.mean(autoCorr, axis=0)\n",
    "mean_autoCorr[0] = np.nan\n",
    "sem_autoCorr = np.std(autoCorr, axis=0) #  / np.sqrt(autoCorr.shape[0])\n",
    "# Plot with shaded error bars\n",
    "plt.figure(figsize=(8, 5))\n",
    "seaborn.lineplot(x=lags, y=mean_autoCorr, label='Mean AutoCorr')\n",
    "plt.fill_between(lags, mean_autoCorr - sem_autoCorr, mean_autoCorr + sem_autoCorr, alpha=0.3)\n",
    "# Labels and title\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Mean Autocorrelation with SEM')\n",
    "plt.axvline(0, color='black', linestyle='--', alpha=0.7)  # Mark zero lag\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883bb8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fc49df9",
   "metadata": {},
   "source": [
    "#### plot the pull fourier transform\n",
    "#### similar concept of the previous code, the goal is to identify the rhythmic pattern in the action time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80717686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_fft_analysis(event_times, bin_size):\n",
    "    \"\"\"\n",
    "    Converts event timestamps into a binary time series and computes its FFT power spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "    - event_times: List of timestamps when events occurred (e.g., pull actions).\n",
    "    - session_duration: Total duration of the session in seconds.\n",
    "    - bin_size: Time bin size in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - freqs: Frequencies corresponding to the FFT output.\n",
    "    - power: Power spectrum of the event series.\n",
    "    \"\"\"\n",
    "    session_duration = np.ceil(np.max(event_times)) + 1  # Adding extra second to cover the last interval\n",
    "    \n",
    "    # Step 1: Convert events to a binary time series\n",
    "    num_bins = int(np.ceil(session_duration / bin_size))  # Total number of bins\n",
    "    binary_series = np.zeros(num_bins, dtype=int)  # Initialize binary array\n",
    "\n",
    "    bin_indices = (np.array(event_times) / bin_size).astype(int)  # Convert times to bin indices\n",
    "    bin_indices = bin_indices[bin_indices < num_bins]  # Ensure indices are within range\n",
    "    binary_series[bin_indices] = 1  # Mark event occurrences\n",
    "\n",
    "    # Step 2: Compute FFT\n",
    "    sampling_rate = 1 / bin_size  # Convert bin size to sampling rate\n",
    "    n = len(binary_series)\n",
    "    freqs = np.fft.rfftfreq(n, d=1/sampling_rate)  # Frequency axis\n",
    "    fft_values = np.fft.rfft(binary_series)  # Compute FFT\n",
    "    power = np.abs(fft_values) ** 2  # Power spectrum\n",
    "\n",
    "    return freqs, power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    #\n",
    "    animal1_filenames = ['Eddie','Dodson','Dannon','Ginger','Koala']\n",
    "    animal2_filenames = ['Sparkle','Scorch','Kanga','Kanga','Vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "        #\n",
    "        animal1_filenames = ['Eddie','Dodson','Ginger',]\n",
    "        animal2_filenames = ['Sparkle','Scorch','Kanga',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    BhvAutoCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                     'bhvname','lags','autoCorr'])\n",
    "    \n",
    "    #\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "        animal1_filename = animal1_filenames[ianimalpair]\n",
    "        animal2_filename = animal2_filenames[ianimalpair]\n",
    "\n",
    "        print('organize data for '+animal1_fixedorder+' '+animal2_fixedorder)\n",
    "\n",
    "        # load the basic behavioral measures\n",
    "        # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_filename.lower()+animal2_filename.lower()+'/'\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sessstart_time_all_dates_'+animal1_filename.lower()+animal2_filename.lower()+'.pkl', 'rb') as f:\n",
    "            sessstart_time_all_dates = pickle.load(f)\n",
    "\n",
    "        # load the DBN related analysis\n",
    "        # load data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_filename.lower()+animal2_filename.lower()+'/'\n",
    "        #\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        #\n",
    "        # load data for successful and failed pulls\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_filename.lower()+animal2_filename.lower()+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename.lower()+animal2_filename.lower()+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "        #\n",
    "        # re-organize the target dates\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "        #\n",
    "        # sort the data based on task type and dates\n",
    "        dates_list = list(DBN_input_data_alltypes.keys())\n",
    "        sorting_df = pd.DataFrame({'dates':dates_list,'coopthres':coopthres_forsort.ravel(),'sessstarttime':sessstart_time_all_dates}, columns=['dates', 'coopthres','sessstarttime'])\n",
    "        sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "        #\n",
    "        # only select the targeted dates\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]        \n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "        sorting_tgt_df = sorting_df\n",
    "        dates_list_tgt = sorting_tgt_df['dates']\n",
    "        dates_list_tgt = np.array(dates_list_tgt)\n",
    "        session_start_times = np.array(sorting_tgt_df['sessstarttime'])\n",
    "        #\n",
    "        ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            grouptype = grouptypes[np.where(np.array(coopthres_IDs)==sorting_df['coopthres'][idate])[0][0]]\n",
    "            \n",
    "            print('organize data for '+idate_name)\n",
    "\n",
    "            # load behavioral results\n",
    "            trial_record = []\n",
    "            try:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info from the session information\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # load the raw gaze time \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_filename.lower()+animal2_filename.lower()+\"/\"+cameraID+'/'+idate_name+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_filename.lower()+animal2_filename.lower()+\"/\"+cameraID+'/'+idate_name+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_filename.lower()+animal2_filename.lower()+\"/\"+cameraID+'/'+idate_name+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            if animal1 == animal1_fixedorder:\n",
    "                time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "                time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "                oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "                oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "                mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "                mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            elif animal1 == animal2_fixedorder:\n",
    "                time_point_pull2 = output_time_points_socialgaze['time_point_pull1']\n",
    "                time_point_pull1 = output_time_points_socialgaze['time_point_pull2']\n",
    "                oneway_gaze2 = output_time_points_socialgaze['oneway_gaze1']\n",
    "                oneway_gaze1 = output_time_points_socialgaze['oneway_gaze2']\n",
    "                mutual_gaze2 = output_time_points_socialgaze['mutual_gaze1']\n",
    "                mutual_gaze1 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            #\n",
    "            time_point_pull1 = np.array(time_point_pull1)\n",
    "            time_point_pull2 = np.array(time_point_pull2)\n",
    "            time_point_gaze1 = np.sort(np.hstack((oneway_gaze1,mutual_gaze1)))\n",
    "            time_point_gaze2 = np.sort(np.hstack((oneway_gaze2,mutual_gaze2)))\n",
    "            # \n",
    "            # change social gaze as the start of a period of social gaze\n",
    "            if 1:\n",
    "                ind_gazestart = np.hstack(([1],(time_point_gaze1[1:]-time_point_gaze1[:-1])>=0.5))\n",
    "                ind_gazestart = ind_gazestart.astype(bool)\n",
    "                time_point_gaze1 = time_point_gaze1[ind_gazestart]\n",
    "                ind_gazestart = np.hstack(([1],(time_point_gaze2[1:]-time_point_gaze2[:-1])>=0.5))\n",
    "                ind_gazestart = ind_gazestart.astype(bool)\n",
    "                time_point_gaze2 = time_point_gaze2[ind_gazestart]\n",
    "                \n",
    "            # Compute auto-correlation\n",
    "            bin_size = 0.1 # 100ms\n",
    "            \n",
    "            freqs_pull1, pull1_autocorr = event_fft_analysis(time_point_pull1, bin_size)\n",
    "            pull1_autocorr[0] = np.nan\n",
    "            freqs_pull2, pull2_autocorr = event_fft_analysis(time_point_pull2, bin_size)\n",
    "            pull2_autocorr[0] = np.nan\n",
    "            freqs_gaze1, gaze1_autocorr = event_fft_analysis(time_point_gaze1, bin_size)\n",
    "            gaze1_autocorr[0] = np.nan\n",
    "            freqs_gaze2, gaze2_autocorr = event_fft_analysis(time_point_gaze2, bin_size)\n",
    "            gaze2_autocorr[0] = np.nan\n",
    "            \n",
    "            # put the data together\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal1_fixedorder,\n",
    "                                                                        'bhvname':'pull',\n",
    "                                                                        'lags':freqs_pull1,\n",
    "                                                                        'autoCorr':np.array(pull1_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "            #\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal2_fixedorder,\n",
    "                                                                        'bhvname':'pull',\n",
    "                                                                        'lags':freqs_pull2,\n",
    "                                                                        'autoCorr':np.array(pull2_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "            #\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal1_fixedorder,\n",
    "                                                                        'bhvname':'gaze',\n",
    "                                                                        'lags':freqs_gaze1,\n",
    "                                                                        'autoCorr':np.array(gaze1_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "            #\n",
    "            BhvAutoCorr_all_dates_df = BhvAutoCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                        'condition':grouptype,\n",
    "                                                                        'act_animal':animal2_fixedorder,\n",
    "                                                                        'bhvname':'gaze',\n",
    "                                                                        'lags':freqs_gaze2,\n",
    "                                                                        'autoCorr':np.array(gaze2_autocorr),\n",
    "                                                                       }, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # for plot the previous code\n",
    "    import math\n",
    "\n",
    "    lags = BhvAutoCorr_all_dates_df['lags'][0]\n",
    "\n",
    "    condition_toplot = '1s threshold'\n",
    "    ind_cond = BhvAutoCorr_all_dates_df['condition']==condition_toplot\n",
    "\n",
    "    bhv_toplot = 'pull'\n",
    "    ind_bhv = BhvAutoCorr_all_dates_df['bhvname']==bhv_toplot\n",
    "\n",
    "    animal1_toplot = 'ginger'\n",
    "    ind_ani1 = BhvAutoCorr_all_dates_df['act_animal'] == animal1_toplot\n",
    "\n",
    "    animal2_toplot = 'kanga_2'\n",
    "    ind_ani2 = BhvAutoCorr_all_dates_df['act_animal'] == animal2_toplot\n",
    "\n",
    "    ind_plot1 = ind_cond & ind_bhv & ind_ani1\n",
    "    ind_plot2 = ind_cond & ind_bhv & ind_ani2\n",
    "\n",
    "    BhvAutoCorr_all_animal1 = BhvAutoCorr_all_dates_df[ind_plot1].reset_index(drop=True)\n",
    "    BhvAutoCorr_all_animal2 = BhvAutoCorr_all_dates_df[ind_plot2].reset_index(drop=True)\n",
    "\n",
    "    # plot for each date\n",
    "\n",
    "    # Determine the number of subplots needed\n",
    "    num_plots = len(BhvAutoCorr_all_animal1)\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(num_plots / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "    axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "    for i, row in BhvAutoCorr_all_animal1.iterrows():\n",
    "        axes[i].plot(row[\"lags\"], row[\"autoCorr\"], marker='o')\n",
    "    #    \n",
    "    for i, row in BhvAutoCorr_all_animal2.iterrows():\n",
    "        axes[i].plot(row[\"lags\"], row[\"autoCorr\"], marker='o')\n",
    "        axes[i].set_title(f\"Date: {row['dates']}\"+\" \"+animal1_toplot+\" \"+animal2_toplot)\n",
    "        axes[i].set_xlabel(\"Lags (s)\")\n",
    "        axes[i].set_ylabel(\"Autocorrelation\")\n",
    "        axes[i].grid()\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4273ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f363e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c28e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf92d0c",
   "metadata": {},
   "source": [
    "#### moving time window to analyze the correlation among bhv events, analysis is based on the DBN_input_data all session format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79803412",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # Define function to compute max cross-correlation\n",
    "    def max_cross_correlation(series1, series2):\n",
    "        \"\"\"\n",
    "        Compute the maximum absolute cross-correlation value between two time series.\n",
    "        \"\"\"\n",
    "        valid_mask = ~np.isnan(series1) & ~np.isnan(series2)  # Remove NaNs\n",
    "        series1, series2 = series1[valid_mask], series2[valid_mask]\n",
    "\n",
    "        if len(series1) == 0 or len(series2) == 0:\n",
    "            return np.nan  # Return NaN if no valid data points\n",
    "\n",
    "        cross_corr = np.correlate(series1 - np.mean(series1), series2 - np.mean(series2), mode=\"full\")\n",
    "        return np.max(np.abs(cross_corr)) / (np.std(series1) * np.std(series2) * len(series1))  # Normalize by standard deviations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    \n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    rollingCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                     'corr1_name','corr2_name','rollingCorr'])\n",
    "    #\n",
    "    CorrOfrollingCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                           'Rollingcorr1_name','Rollingcorr2_name',\n",
    "                                                           'CorrOfrollingCorr'])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # load all the variables \n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx3 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx4 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                df = pd.DataFrame({'xxx1': xxx1, \n",
    "                                   'xxx2': xxx2, \n",
    "                                   'xxx3': xxx3,\n",
    "                                   'xxx4': xxx4, })\n",
    "\n",
    "                # Compute rolling correlation with a window of rollingTW\n",
    "                rollingTW =10\n",
    "                # simple correlation\n",
    "                if 1:\n",
    "                    rolling_corr12 = df['xxx1'].rolling(window=rollingTW).corr(df['xxx2'])\n",
    "                    rolling_corr13 = df['xxx1'].rolling(window=rollingTW).corr(df['xxx3'])\n",
    "                    rolling_corr14 = df['xxx1'].rolling(window=rollingTW).corr(df['xxx4'])\n",
    "                    rolling_corr23 = df['xxx2'].rolling(window=rollingTW).corr(df['xxx3'])\n",
    "                    rolling_corr24 = df['xxx2'].rolling(window=rollingTW).corr(df['xxx4'])\n",
    "                    rolling_corr34 = df['xxx3'].rolling(window=rollingTW).corr(df['xxx4'])\n",
    "                # max cross correlation value\n",
    "                if 0:\n",
    "                    # Compute rolling max cross-correlation\n",
    "                    rolling_max_corr12 = [max_cross_correlation(df['xxx1'][i:i+rollingTW], df['xxx2'][i:i+rollingTW]) for i in range(len(df)-rollingTW+1)]\n",
    "                    rolling_max_corr13 = [max_cross_correlation(df['xxx1'][i:i+rollingTW], df['xxx3'][i:i+rollingTW]) for i in range(len(df)-rollingTW+1)]\n",
    "                    rolling_max_corr14 = [max_cross_correlation(df['xxx1'][i:i+rollingTW], df['xxx4'][i:i+rollingTW]) for i in range(len(df)-rollingTW+1)] \n",
    "                    rolling_max_corr23 = [max_cross_correlation(df['xxx2'][i:i+rollingTW], df['xxx3'][i:i+rollingTW]) for i in range(len(df)-rollingTW+1)]\n",
    "                    rolling_max_corr24 = [max_cross_correlation(df['xxx2'][i:i+rollingTW], df['xxx4'][i:i+rollingTW]) for i in range(len(df)-rollingTW+1)]\n",
    "                    rolling_max_corr34 = [max_cross_correlation(df['xxx3'][i:i+rollingTW], df['xxx4'][i:i+rollingTW]) for i in range(len(df)-rollingTW+1)]\n",
    "                    # Convert to Pandas Series and align with original index\n",
    "                    rolling_corr12 = pd.Series(rolling_max_corr12, index=df.index[rollingTW-1:])\n",
    "                    rolling_corr13 = pd.Series(rolling_max_corr13, index=df.index[rollingTW-1:])\n",
    "                    rolling_corr14 = pd.Series(rolling_max_corr14, index=df.index[rollingTW-1:])\n",
    "                    rolling_corr23 = pd.Series(rolling_max_corr23, index=df.index[rollingTW-1:])\n",
    "                    rolling_corr24 = pd.Series(rolling_max_corr24, index=df.index[rollingTW-1:])\n",
    "                    rolling_corr34 = pd.Series(rolling_max_corr34, index=df.index[rollingTW-1:])\n",
    "                \n",
    "                # for all the rolling corr, replace the nan with zero\n",
    "                if 0:\n",
    "                    rolling_corr12 = rolling_corr12.fillna(0)\n",
    "                    rolling_corr13 = rolling_corr13.fillna(0)\n",
    "                    rolling_corr14 = rolling_corr14.fillna(0)\n",
    "                    rolling_corr23 = rolling_corr23.fillna(0)\n",
    "                    rolling_corr24 = rolling_corr24.fillna(0)\n",
    "                    rolling_corr34 = rolling_corr34.fillna(0)\n",
    "                \n",
    "                # data for animal1\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'corr1_name':'self_pull',\n",
    "                                                                            'corr2_name':'self_gaze',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr12),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'corr1_name':'self_pull',\n",
    "                                                                            'corr2_name':'partner_pull',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr13),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'corr1_name':'self_gaze',\n",
    "                                                                            'corr2_name':'partner_pull',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr23),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'corr1_name':'self_gaze',\n",
    "                                                                            'corr2_name':'partner_gaze',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr24),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # data for animal2\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'corr1_name':'self_pull',\n",
    "                                                                            'corr2_name':'self_gaze',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr34),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'corr1_name':'self_pull',\n",
    "                                                                            'corr2_name':'partner_pull',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr13),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'corr1_name':'self_gaze',\n",
    "                                                                            'corr2_name':'partner_pull',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr14),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                rollingCorr_all_dates_df = rollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'corr1_name':'self_gaze',\n",
    "                                                                            'corr2_name':'partner_gaze',\n",
    "                                                                            'rollingCorr':np.array(rolling_corr24),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                \n",
    "                # compute the correlation between rolling correlation\n",
    "                valid_indices = rolling_corr13.dropna().index.intersection(rolling_corr12.dropna().index)\n",
    "                rolling_corr13_clean = rolling_corr13.loc[valid_indices]\n",
    "                rolling_corr12_clean = rolling_corr12.loc[valid_indices]\n",
    "                correlation13vs12 = rolling_corr13_clean.corr(rolling_corr12_clean)\n",
    "                #\n",
    "                valid_indices = rolling_corr13.dropna().index.intersection(rolling_corr23.dropna().index)\n",
    "                rolling_corr13_clean = rolling_corr13.loc[valid_indices]\n",
    "                rolling_corr23_clean = rolling_corr23.loc[valid_indices]\n",
    "                correlation13vs23 = rolling_corr13_clean.corr(rolling_corr23_clean)\n",
    "                #\n",
    "                valid_indices = rolling_corr13.dropna().index.intersection(rolling_corr34.dropna().index)\n",
    "                rolling_corr13_clean = rolling_corr13.loc[valid_indices]\n",
    "                rolling_corr34_clean = rolling_corr34.loc[valid_indices]\n",
    "                correlation13vs34 = rolling_corr13_clean.corr(rolling_corr34_clean)\n",
    "                #\n",
    "                valid_indices = rolling_corr13.dropna().index.intersection(rolling_corr14.dropna().index)\n",
    "                rolling_corr13_clean = rolling_corr13.loc[valid_indices]\n",
    "                rolling_corr14_clean = rolling_corr14.loc[valid_indices]\n",
    "                correlation13vs14 = rolling_corr13_clean.corr(rolling_corr14_clean)\n",
    "                #\n",
    "                valid_indices = rolling_corr12.dropna().index.intersection(rolling_corr23.dropna().index)\n",
    "                rolling_corr12_clean = rolling_corr12.loc[valid_indices]\n",
    "                rolling_corr23_clean = rolling_corr23.loc[valid_indices]\n",
    "                correlation12vs23 = rolling_corr12_clean.corr(rolling_corr23_clean)\n",
    "                #\n",
    "                valid_indices = rolling_corr14.dropna().index.intersection(rolling_corr34.dropna().index)\n",
    "                rolling_corr14_clean = rolling_corr14.loc[valid_indices]\n",
    "                rolling_corr34_clean = rolling_corr34.loc[valid_indices]\n",
    "                correlation14vs34 = rolling_corr14_clean.corr(rolling_corr34_clean)\n",
    "                \n",
    "                # data for animal1\n",
    "                CorrOfrollingCorr_all_dates_df = CorrOfrollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'Rollingcorr1_name':'sync_pull',\n",
    "                                                                            'Rollingcorr2_name':'gaze_lead_pull',\n",
    "                                                                            'CorrOfrollingCorr':correlation13vs12,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                CorrOfrollingCorr_all_dates_df = CorrOfrollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'Rollingcorr1_name':'sync_pull',\n",
    "                                                                            'Rollingcorr2_name':'social_attention',\n",
    "                                                                            'CorrOfrollingCorr':correlation13vs23,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                CorrOfrollingCorr_all_dates_df = CorrOfrollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal1_fixedorder,\n",
    "                                                                            'Rollingcorr1_name':'gaze_lead_pull',\n",
    "                                                                            'Rollingcorr2_name':'social_attention',\n",
    "                                                                            'CorrOfrollingCorr':correlation12vs23,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                \n",
    "                # data for animal2\n",
    "                CorrOfrollingCorr_all_dates_df = CorrOfrollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'Rollingcorr1_name':'sync_pull',\n",
    "                                                                            'Rollingcorr2_name':'gaze_lead_pull',\n",
    "                                                                            'CorrOfrollingCorr':correlation13vs34,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                CorrOfrollingCorr_all_dates_df = CorrOfrollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'Rollingcorr1_name':'sync_pull',\n",
    "                                                                            'Rollingcorr2_name':'social_attention',\n",
    "                                                                            'CorrOfrollingCorr':correlation13vs14,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                #\n",
    "                CorrOfrollingCorr_all_dates_df = CorrOfrollingCorr_all_dates_df.append({'dates': idate_name, \n",
    "                                                                            'condition':grouptype,\n",
    "                                                                            'act_animal':animal2_fixedorder,\n",
    "                                                                            'Rollingcorr1_name':'gaze_lead_pull',\n",
    "                                                                            'Rollingcorr2_name':'social_attention',\n",
    "                                                                            'CorrOfrollingCorr':correlation14vs34,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "    # plot\n",
    "    CorrOfrollingCorr_all_dates_df[\"Rollingcorr_pair\"] = CorrOfrollingCorr_all_dates_df[\"Rollingcorr1_name\"] + \" vs \" + CorrOfrollingCorr_all_dates_df[\"Rollingcorr2_name\"]\n",
    "    # Get the unique Rollingcorr_pair values\n",
    "    unique_pairs = CorrOfrollingCorr_all_dates_df[\"Rollingcorr_pair\"].unique()\n",
    "    \n",
    "    npairs = np.shape(unique_pairs)[0]\n",
    "    \n",
    "    # figure initiate\n",
    "    fig, axs = plt.subplots(1,1)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "     \n",
    "    seaborn.boxplot(ax=axs,data=CorrOfrollingCorr_all_dates_df,\n",
    "                       x='Rollingcorr_pair',y='CorrOfrollingCorr',hue='condition')\n",
    "    # seaborn.violinplot(ax=axs,data=CorrOfrollingCorr_all_dates_df,\n",
    "    #                    x='Rollingcorr_pair',y='CorrOfrollingCorr',hue='condition')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984c440",
   "metadata": {},
   "source": [
    "#### similar analysis as the previous one, quantify the number of pull events that comes from the two stretagies\n",
    "#### generate the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    \n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "    \n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # load all the variables \n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx3 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx4 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                \n",
    "                # Initialize arrays to store results\n",
    "                pull2_to_pull1 = np.zeros_like(xxx1)  # focus on pull1\n",
    "                pull1_to_pull2 = np.zeros_like(xxx3)  # focus on pull2\n",
    "                gaze1_to_pull1 = np.zeros_like(xxx1)  # focus on pull1\n",
    "                gaze2_to_pull2 = np.zeros_like(xxx3)  # focus on pull2\n",
    "                pull1_to_gaze2 = np.zeros_like(xxx1)  # focus on pull1\n",
    "                pull2_to_gaze1 = np.zeros_like(xxx3)  # focus on pull2\n",
    "\n",
    "                # Populate the arrays \n",
    "                surround_steps = 1\n",
    "                #\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1 and any(xxx3[max(0, i-surround_steps):i+1]):\n",
    "                        pull2_to_pull1[i] = 1\n",
    "                    if xxx1[i] == 1 and any(xxx2[max(0, i-surround_steps):i+1]):\n",
    "                        gaze1_to_pull1[i] = 1\n",
    "                    if xxx1[i] == 1 and any(xxx4[i:min(i+surround_steps+1, len(xxx4))]):\n",
    "                        pull1_to_gaze2[i] = 1\n",
    "\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1 and any(xxx1[max(0, i-surround_steps):i+1]):\n",
    "                        pull1_to_pull2[i] = 1\n",
    "                    if xxx3[i] == 1 and any(xxx4[max(0, i-surround_steps):i+1]):\n",
    "                        gaze2_to_pull2[i] = 1\n",
    "                    if xxx3[i] == 1 and any(xxx2[i:min(i+surround_steps+1, len(xxx2))]):\n",
    "                        pull2_to_gaze1[i] = 1\n",
    "\n",
    "                # Stack the six binary time series into a 2D array (each row is a time step)\n",
    "                data_matrix = np.vstack([\n",
    "                    pull2_to_pull1,\n",
    "                    pull1_to_pull2,\n",
    "                    gaze1_to_pull1,\n",
    "                    gaze2_to_pull2,\n",
    "                    # pull1_to_gaze2,\n",
    "                    # pull2_to_gaze1\n",
    "                ]).T  # Transpose to get time steps as rows\n",
    "\n",
    "                # Compute the co-occurrence matrix (dot product of the binary matrix)\n",
    "                co_occurrence_matrix = np.dot(data_matrix.T, data_matrix)\n",
    "\n",
    "                # Calculate unique events (events that do not co-occur with any other event)\n",
    "                unique_events = np.array([np.sum((data_matrix[:, i] == 1) & (np.sum(data_matrix[:, np.arange(data_matrix.shape[1]) != i], axis=1) == 0)) \n",
    "                                          for i in range(data_matrix.shape[1])])\n",
    "\n",
    "                # Add the unique events as a new row and a new column\n",
    "                co_occurrence_matrix_with_unique = np.vstack([\n",
    "                    np.hstack([co_occurrence_matrix, unique_events.reshape(-1, 1)]),\n",
    "                    np.append(unique_events, np.sum(unique_events))  # Add sum of unique events as a bottom-right corner element\n",
    "                ])\n",
    "\n",
    "                # Create a DataFrame for better visualization\n",
    "                column_names = [\"pull2_to_pull1\", \"pull1_to_pull2\", \n",
    "                                \"gaze1_to_pull1\",\"gaze2_to_pull2\", \n",
    "                                # \"pull1_to_gaze2\", \"pull2_to_gaze1\", \n",
    "                                \"Unique Events\"]\n",
    "                df_cooccurrence = pd.DataFrame(co_occurrence_matrix_with_unique, columns=column_names, index=column_names)\n",
    "\n",
    "                # Create the heatmap\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                seaborn.heatmap(df_cooccurrence, annot=True, cmap=\"Blues\", fmt=\"d\", linewidths=0.5)\n",
    "\n",
    "                # Formatting\n",
    "                plt.title(\"Co-Occurrence Matrix Heatmap with Unique Events\\n\"\n",
    "                          +animal1_fixedorder+' '+animal2_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.yticks(rotation=0)\n",
    "                plt.show()\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb034f7f",
   "metadata": {},
   "source": [
    "#### similar analysis as the previous one, quantify the number of pull events that is around gaze or not \n",
    "#### generate the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a989a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    do_succpull = 0\n",
    "    do_failpull = 0\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    \n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    rollingCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                     'corr1_name','corr2_name','rollingCorr'])\n",
    "    #\n",
    "    CorrOfrollingCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                           'Rollingcorr1_name','Rollingcorr2_name',\n",
    "                                                           'CorrOfrollingCorr'])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            # load data for successful and failed pulls\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "            if do_succpull:\n",
    "                DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['succpull']\n",
    "            if do_failpull:\n",
    "                DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['failedpull']\n",
    "                    \n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # load all the variables \n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx3 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx4 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                \n",
    "                \n",
    "\n",
    "                # Initialize arrays to store results\n",
    "                pull1_surrounded_by_pull2 = np.zeros_like(xxx1)  # pull1 surrounded by pull2\n",
    "                pull1_surrounded_by_gaze1 = np.zeros_like(xxx1)  # pull1 surrounded by gaze1\n",
    "                pull1_surrounded_by_gaze2 = np.zeros_like(xxx1)  # pull1 surrounded by gaze2\n",
    "\n",
    "                pull2_surrounded_by_pull1 = np.zeros_like(xxx3)  # pull2 surrounded by pull1\n",
    "                pull2_surrounded_by_gaze1 = np.zeros_like(xxx3)  # pull2 surrounded by gaze1\n",
    "                pull2_surrounded_by_gaze2 = np.zeros_like(xxx3)  # pull2 surrounded by gaze2\n",
    "\n",
    "                #\n",
    "                surround_steps = 1\n",
    "                # pull1 surrounded by pull2\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx3[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_pull2[i] = 1  # Mark as surrounded by pull2\n",
    "\n",
    "                # pull1 surrounded by gaze1\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                # pull1 surrounded by gaze2\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "\n",
    "                # pull2 surrounded by pull1\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx1[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_pull1[i] = 1  # Mark as surrounded by pull1\n",
    "\n",
    "                # pull2 surrounded by gaze1\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                # pull2 surrounded by gaze2\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "                \n",
    "                # put two animals together in one matrix\n",
    "                # Stack the surrounded variables into a 2D array (each row is a time step)\n",
    "                data_matrix = np.vstack([\n",
    "                    pull1_surrounded_by_pull2,\n",
    "                    pull1_surrounded_by_gaze1,\n",
    "                    pull1_surrounded_by_gaze2,\n",
    "                    pull2_surrounded_by_pull1,\n",
    "                    pull2_surrounded_by_gaze2,\n",
    "                    pull2_surrounded_by_gaze1\n",
    "                ]).T  # Transpose to get time steps as rows\n",
    "\n",
    "                # Compute the co-occurrence matrix (dot product of the binary matrix)\n",
    "                co_occurrence_matrix = np.dot(data_matrix.T, data_matrix)\n",
    "\n",
    "                # Calculate unique events (events that do not co-occur with any other event)\n",
    "                unique_events = np.array([np.sum((data_matrix[:, i] == 1) & (np.sum(data_matrix[:, np.arange(data_matrix.shape[1]) != i], axis=1) == 0)) \n",
    "                                          for i in range(data_matrix.shape[1])])\n",
    "\n",
    "                # Add the unique events as a new row and a new column\n",
    "                co_occurrence_matrix_with_unique = np.vstack([\n",
    "                    np.hstack([co_occurrence_matrix, unique_events.reshape(-1, 1)]),\n",
    "                    np.append(unique_events, np.sum(unique_events))  # Add sum of unique events as a bottom-right corner element\n",
    "                ])\n",
    "\n",
    "                # Create a DataFrame for better visualization\n",
    "                column_names = [\"pull1_surrounded_by_pull2\", \n",
    "                                \"pull1_surrounded_by_gaze1\", \n",
    "                                \"pull1_surrounded_by_gaze2\",\n",
    "                                \"pull2_surrounded_by_pull1\", \n",
    "                                \"pull2_surrounded_by_gaze2\", \n",
    "                                \"pull2_surrounded_by_gaze1\", \n",
    "                                \"Unique Events\"]\n",
    "                df_cooccurrence = pd.DataFrame(co_occurrence_matrix_with_unique, columns=column_names, index=column_names)\n",
    "\n",
    "\n",
    "                # Create the heatmap\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                seaborn.heatmap(df_cooccurrence, annot=True, cmap=\"Blues\", fmt=\"d\", linewidths=0.5)\n",
    "\n",
    "                # Formatting\n",
    "                plt.title(\"Co-Occurrence Matrix Heatmap with Unique Events\\n\"\n",
    "                          +animal1_fixedorder+' '+animal2_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.yticks(rotation=0)\n",
    "                plt.show()\n",
    "\n",
    "                # separate the two animals\n",
    "                # Create the heatmap\n",
    "                fig, axs = plt.subplots(1,2)\n",
    "                fig.set_figheight(5)\n",
    "                fig.set_figwidth(5*2)\n",
    "\n",
    "                for ianimal in np.arange(0,2,1):\n",
    "                    if ianimal == 0:\n",
    "                        # Stack the surrounded variables into a 2D array (each row is a time step)\n",
    "                        data_matrix = np.vstack([\n",
    "                            pull1_surrounded_by_pull2,\n",
    "                            pull1_surrounded_by_gaze1,\n",
    "                            pull1_surrounded_by_gaze2,\n",
    "                        ]).T  # Transpose to get time steps as rows\n",
    "                        # Create a DataFrame for better visualization\n",
    "                        column_names = [\"pull1_surrounded_by_pull2\", \n",
    "                                        \"pull1_surrounded_by_gaze1\", \n",
    "                                        \"pull1_surrounded_by_gaze2\",\n",
    "                                        \"Unique Events\"]\n",
    "                    elif ianimal == 1:\n",
    "                        # Stack the surrounded variables into a 2D array (each row is a time step)\n",
    "                        data_matrix = np.vstack([\n",
    "                            pull2_surrounded_by_pull1,\n",
    "                            pull2_surrounded_by_gaze2,\n",
    "                            pull2_surrounded_by_gaze1\n",
    "                        ]).T  # Transpose to get time steps as rows\n",
    "                        # Create a DataFrame for better visualization\n",
    "                        column_names = [\"pull2_surrounded_by_pull1\", \n",
    "                                        \"pull2_surrounded_by_gaze2\", \n",
    "                                        \"pull2_surrounded_by_gaze1\",\n",
    "                                        \"Unique Events\"]\n",
    "\n",
    "                    # Compute the co-occurrence matrix (dot product of the binary matrix)\n",
    "                    co_occurrence_matrix = np.dot(data_matrix.T, data_matrix)\n",
    "\n",
    "                    # Calculate unique events (events that do not co-occur with any other event)\n",
    "                    unique_events = np.array([np.sum((data_matrix[:, i] == 1) & (np.sum(data_matrix[:, np.arange(data_matrix.shape[1]) != i], axis=1) == 0)) \n",
    "                                              for i in range(data_matrix.shape[1])])\n",
    "\n",
    "                    # Add the unique events as a new row and a new column\n",
    "                    co_occurrence_matrix_with_unique = np.vstack([\n",
    "                        np.hstack([co_occurrence_matrix, unique_events.reshape(-1, 1)]),\n",
    "                        np.append(unique_events, np.sum(unique_events))  # Add sum of unique events as a bottom-right corner element\n",
    "                    ])\n",
    "\n",
    "                    df_cooccurrence = pd.DataFrame(co_occurrence_matrix_with_unique, columns=column_names, index=column_names)\n",
    "                    \n",
    "                    # calculate the ratio over all pull numbers\n",
    "                    if ianimal == 0:\n",
    "                        df_cooccurrence = df_cooccurrence/np.sum(xxx1)\n",
    "                    elif ianimal == 1:\n",
    "                        df_cooccurrence = df_cooccurrence/np.sum(xxx3)\n",
    "                    \n",
    "                    # Create the heatmap\n",
    "                    seaborn.heatmap(df_cooccurrence, annot=True, \n",
    "                                    cmap=\"Blues\", linewidths=0.5,ax=axs[ianimal])\n",
    "\n",
    "                    # Formatting\n",
    "                    if ianimal == 0:\n",
    "                        # axs[ianimal].set_title(\"Co-Occurrence Matrix Heatmap with Unique Events\\n\"\n",
    "                        #           +animal1_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                        axs[ianimal].set_title(\"Co-Occurrence Ratio Matrix Heatmap with Unique Events\\n\"\n",
    "                                   +animal1_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                    elif ianimal == 1:\n",
    "                        # axs[ianimal].set_title(\"Co-Occurrence Matrix Heatmap with Unique Events\\n\"\n",
    "                        #           +animal2_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                        axs[ianimal].set_title(\"Co-Occurrence Ratio Matrix Heatmap with Unique Events\\n\"\n",
    "                                  +animal2_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                    axs[ianimal].set_xticklabels(axs[ianimal].get_xticklabels(), rotation=45)\n",
    "                    \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff90c4b",
   "metadata": {},
   "source": [
    "#### similar analysis as the previous one, quantify the number of pull events that is around gaze or not \n",
    "#### concatenate all the sessions, and add a bootstrap for statisitcs\n",
    "#### generate the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106cf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    do_succpull = 0\n",
    "    do_failpull = 0\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    # animal1_fixedorders = ['koala',]\n",
    "    # animal2_fixedorders = ['vermelho',]\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    # grouptypes = ['1s threshold',]\n",
    "    # coopthres_IDs = [1,]\n",
    "    if do_trainedMCs:\n",
    "        # grouptypes = ['self reward','1s threshold','novision']\n",
    "        # coopthres_IDs = [100, 1, -1]\n",
    "        grouptypes = ['1s threshold',]\n",
    "        coopthres_IDs = [1,]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    \n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "\n",
    "    #\n",
    "    animaltype_tgt = 'female' # 'male','female','sub','dom'\n",
    "    # tgt_animalnames = animal1_fixedorders + animal2_fixedorders\n",
    "    tgt_animalnames = femalenames\n",
    "    \n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "        \n",
    "        # initialize the array for concatenation for each task condition\n",
    "        pull_surrounded_by_otherpull = np.array([])\n",
    "        pull_surrounded_by_othergaze = np.array([])\n",
    "        pull_surrounded_by_selfgaze  = np.array([])\n",
    "        pull_only = np.array([])\n",
    "        gaze_only = np.array([])\n",
    "        #\n",
    "        pull_surrounded_by_otherpull_male = np.array([])\n",
    "        pull_surrounded_by_othergaze_male = np.array([])\n",
    "        pull_surrounded_by_selfgaze_male  = np.array([])\n",
    "        pull_only_male = np.array([])\n",
    "        gaze_only_male = np.array([])\n",
    "        #\n",
    "        pull_surrounded_by_otherpull_female = np.array([])\n",
    "        pull_surrounded_by_othergaze_female = np.array([])\n",
    "        pull_surrounded_by_selfgaze_female  = np.array([])\n",
    "        pull_only_female = np.array([])\n",
    "        gaze_only_female = np.array([])\n",
    "        #\n",
    "        pull_surrounded_by_otherpull_sub = np.array([])\n",
    "        pull_surrounded_by_othergaze_sub = np.array([])\n",
    "        pull_surrounded_by_selfgaze_sub  = np.array([])\n",
    "        pull_only_sub = np.array([])\n",
    "        gaze_only_sub = np.array([])\n",
    "        #\n",
    "        pull_surrounded_by_otherpull_dom = np.array([])\n",
    "        pull_surrounded_by_othergaze_dom = np.array([])\n",
    "        pull_surrounded_by_selfgaze_dom  = np.array([])\n",
    "        pull_only_dom = np.array([])\n",
    "        gaze_only_dom = np.array([])\n",
    "\n",
    "        \n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            # load data for successful and failed pulls\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "            if do_succpull:\n",
    "                DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['succpull']\n",
    "            if do_failpull:\n",
    "                DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['failedpull']\n",
    "                    \n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # load all the variables \n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx3 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx4 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                \n",
    "                \n",
    "                # Initialize arrays to store results\n",
    "                pull1_surrounded_by_pull2 = np.zeros_like(xxx1)  # pull1 surrounded by pull2\n",
    "                pull1_surrounded_by_gaze1 = np.zeros_like(xxx1)  # pull1 surrounded by gaze1\n",
    "                pull1_surrounded_by_gaze2 = np.zeros_like(xxx1)  # pull1 surrounded by gaze2\n",
    "\n",
    "                pull2_surrounded_by_pull1 = np.zeros_like(xxx3)  # pull2 surrounded by pull1\n",
    "                pull2_surrounded_by_gaze1 = np.zeros_like(xxx3)  # pull2 surrounded by gaze1\n",
    "                pull2_surrounded_by_gaze2 = np.zeros_like(xxx3)  # pull2 surrounded by gaze2\n",
    "\n",
    "                #\n",
    "                surround_steps = 3\n",
    "                # pull1 surrounded by pull2\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx3[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_pull2[i] = 1  # Mark as surrounded by pull2\n",
    "\n",
    "                # pull1 surrounded by gaze1\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                # pull1 surrounded by gaze2\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "\n",
    "                # pull2 surrounded by pull1\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx1[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_pull1[i] = 1  # Mark as surrounded by pull1\n",
    "\n",
    "                # pull2 surrounded by gaze1\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                # pull2 surrounded by gaze2\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "                \n",
    "                # concatenate the data\n",
    "                if np.isin(animal1_fixedorder,tgt_animalnames):\n",
    "                    pull_surrounded_by_otherpull = np.concatenate((pull_surrounded_by_otherpull,pull1_surrounded_by_pull2))\n",
    "                    pull_surrounded_by_othergaze = np.concatenate((pull_surrounded_by_othergaze,pull1_surrounded_by_gaze2))\n",
    "                    pull_surrounded_by_selfgaze = np.concatenate((pull_surrounded_by_selfgaze,pull1_surrounded_by_gaze1))\n",
    "                    pull_only = np.concatenate((pull_only,xxx1))\n",
    "                    gaze_only = np.concatenate((gaze_only,xxx2))\n",
    "                #\n",
    "                if np.isin(animal2_fixedorder,tgt_animalnames):\n",
    "                    pull_surrounded_by_otherpull = np.concatenate((pull_surrounded_by_otherpull,pull2_surrounded_by_pull1))\n",
    "                    pull_surrounded_by_othergaze = np.concatenate((pull_surrounded_by_othergaze,pull2_surrounded_by_gaze1))\n",
    "                    pull_surrounded_by_selfgaze = np.concatenate((pull_surrounded_by_selfgaze,pull2_surrounded_by_gaze2))\n",
    "                    pull_only = np.concatenate((pull_only,xxx3))\n",
    "                    gaze_only = np.concatenate((gaze_only,xxx4))\n",
    "                \n",
    "                \n",
    "        # put two animals together in one matrix\n",
    "        # Stack the surrounded variables into a 2D array (each row is a time step)\n",
    "        data_matrix = np.vstack([\n",
    "            pull_surrounded_by_otherpull,\n",
    "            # pull_surrounded_by_othergaze,\n",
    "            pull_surrounded_by_selfgaze,\n",
    "\n",
    "        ]).T  # Transpose to get time steps as rows\n",
    "\n",
    "        # Compute the co-occurrence matrix (dot product of the binary matrix)\n",
    "        co_occurrence_matrix = np.dot(data_matrix.T, data_matrix)\n",
    "\n",
    "        # Calculate unique events (events that do not co-occur with any other event)\n",
    "        unique_events = np.array([np.sum((data_matrix[:, i] == 1) & (np.sum(data_matrix[:, np.arange(data_matrix.shape[1]) != i], axis=1) == 0)) \n",
    "                                  for i in range(data_matrix.shape[1])])\n",
    "\n",
    "        # Add the unique events as a new row and a new column\n",
    "        co_occurrence_matrix_with_unique = np.vstack([\n",
    "            np.hstack([co_occurrence_matrix, unique_events.reshape(-1, 1)]),\n",
    "            np.append(unique_events, np.sum(unique_events))  # Add sum of unique events as a bottom-right corner element\n",
    "        ])\n",
    "\n",
    "        # Create a DataFrame for better visualization\n",
    "        column_names = [\"pull_surrounded_by_otherpull\", \n",
    "                        # \"pull_surrounded_by_othergaze\", \n",
    "                        \"pull_surrounded_by_selfgaze\", \n",
    "                        \"Unique Events\"]\n",
    "        df_cooccurrence = pd.DataFrame(co_occurrence_matrix_with_unique, columns=column_names, index=column_names)\n",
    "        #\n",
    "        # change to ratio\n",
    "        df_cooccurrence = df_cooccurrence/np.sum(pull_only)\n",
    "        df_cooccurrence_true = df_cooccurrence\n",
    "        \n",
    "        \n",
    "        # plot for the distribution of pull_to_pull without social gaze interference\n",
    "        # Step 1: Remove all rows where all three variables are 0\n",
    "        # valid_indices = ~((pull_surrounded_by_otherpull == 0) & (pull_surrounded_by_othergaze == 0) & (pull_surrounded_by_selfgaze == 0)) \n",
    "        valid_indices = ~((pull_surrounded_by_otherpull == 0)  & (pull_surrounded_by_selfgaze == 0) )\n",
    "        # valid_indices = ~((pull_surrounded_by_otherpull == 0)  & (gaze_only == 0) )\n",
    "        # Filter the arrays to exclude those time steps\n",
    "        filtered_pull = pull_surrounded_by_otherpull[valid_indices]\n",
    "        # filtered_gaze1 = pull_surrounded_by_othergaze[valid_indices]\n",
    "        filtered_gaze2 = pull_surrounded_by_selfgaze[valid_indices]\n",
    "        # filtered_gaze2 = gaze_only[valid_indices]\n",
    "        # Apply the new mask: Set filtered_pull to 0 where gaze1 or gaze2 is 1\n",
    "        # filtered_pull[(filtered_gaze1 == 1) | (filtered_gaze2 == 1)] = 0\n",
    "        filtered_pull[(filtered_gaze2 == 1)] = 0\n",
    "        # Step 2: Find consecutive 1s in pull_surrounded_by_otherpull\n",
    "        consecutive_lengths = []\n",
    "        current_length = 0\n",
    "        #\n",
    "        for val in filtered_pull:\n",
    "            if val == 1:\n",
    "                current_length += 1  # Count length of consecutive 1s\n",
    "            elif current_length > 0:\n",
    "                consecutive_lengths.append(current_length)  # Store sequence length\n",
    "                current_length = 0  # Reset counter\n",
    "        # Capture last sequence if the series ends with 1s\n",
    "        if current_length > 0:\n",
    "            consecutive_lengths.append(current_length)\n",
    "        # Capture the last sequence if it ended at the last element\n",
    "        if current_length > 0:\n",
    "            consecutive_lengths.append(current_length)\n",
    "        # Step 3: Calculate the mean and median\n",
    "        mean_ones = np.mean(consecutive_lengths)\n",
    "        median_ones = np.median(consecutive_lengths)\n",
    "        # Step 4: Plot distribution of consecutive 1s\n",
    "        plt.figure(figsize=(7,5))\n",
    "        # Compute the relative frequencies by dividing counts by the sum of filtered_pull\n",
    "        bin_counts, bins = np.histogram(consecutive_lengths, bins=np.arange(1, max(consecutive_lengths, default=1) + 2) - 0.5)\n",
    "        relative_frequencies = bin_counts / np.sum(pull_only)  # Divide by total valid entries\n",
    "        # Plot the relative frequency as a bar plot\n",
    "        plt.bar(bins[:-1], relative_frequencies, width=1, color='blue', alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel(\"Length of Consecutive 1s in pull_surrounded_by_otherpull\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Distribution of Consecutive 1s in pull_surrounded_by_otherpull (without gaze inteference)\\n\"+\n",
    "                 animaltype_tgt+' animals in all sessions of '+grouptype)\n",
    "        # plt.xticks(range(1, max(consecutive_lengths, default=1) + 1),rotation=45)\n",
    "        plt.xticks(range(1,15,5),rotation=45)\n",
    "        plt.xlim([0,15])\n",
    "        # Step 5: Add lines for mean and median\n",
    "        plt.axvline(mean_ones, color='green', linestyle='--', label=f'Mean: {mean_ones:.2f}')\n",
    "        plt.axvline(median_ones, color='red', linestyle='-', label=f'Median: {median_ones:.2f}')\n",
    "        # Step 6: Add legend\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        #\n",
    "        consecutive_lengths_true = consecutive_lengths\n",
    "\n",
    "        #\n",
    "        #\n",
    "        # repeat for bootstrap to create null hypothesis\n",
    "        nbootstrap = 50\n",
    "        co_occurrence_list = []\n",
    "        #\n",
    "        for iboot in np.arange(0,nbootstrap,1):\n",
    "            # initialize the array for concatenation for each task condition\n",
    "            pull_surrounded_by_otherpull = np.array([])\n",
    "            pull_surrounded_by_othergaze = np.array([])\n",
    "            pull_surrounded_by_selfgaze  = np.array([])\n",
    "            pull_only = np.array([])\n",
    "            gaze_only = np.array([])\n",
    "            #\n",
    "            pull_surrounded_by_otherpull_male = np.array([])\n",
    "            pull_surrounded_by_othergaze_male = np.array([])\n",
    "            pull_surrounded_by_selfgaze_male  = np.array([])\n",
    "            pull_only_male = np.array([])\n",
    "            gaze_only_male = np.array([])\n",
    "            #\n",
    "            pull_surrounded_by_otherpull_female = np.array([])\n",
    "            pull_surrounded_by_othergaze_female = np.array([])\n",
    "            pull_surrounded_by_selfgaze_female  = np.array([])\n",
    "            pull_only_female = np.array([])\n",
    "            gaze_only_female = np.array([])\n",
    "            #\n",
    "            pull_surrounded_by_otherpull_sub = np.array([])\n",
    "            pull_surrounded_by_othergaze_sub = np.array([])\n",
    "            pull_surrounded_by_selfgaze_sub  = np.array([])\n",
    "            pull_only_sub = np.array([])\n",
    "            gaze_only_sub = np.array([])\n",
    "            #\n",
    "            pull_surrounded_by_otherpull_dom = np.array([])\n",
    "            pull_surrounded_by_othergaze_dom = np.array([])\n",
    "            pull_surrounded_by_selfgaze_dom  = np.array([])\n",
    "            pull_only_dom = np.array([])\n",
    "            gaze_only_dom = np.array([])\n",
    "\n",
    "\n",
    "            for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "                animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "                animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "                if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                    animal2_filename = 'kanga'\n",
    "                else:\n",
    "                    animal2_filename = animal2_fixedorder\n",
    "\n",
    "                # load the basic behavioral measures\n",
    "                # load saved data\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                    tasktypes_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                    coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "                #     \n",
    "                # load the DBN related analysis\n",
    "                # load data\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "                #\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                # load data for successful and failed pulls\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "                if do_succpull:\n",
    "                    DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['succpull']\n",
    "                if do_failpull:\n",
    "                    DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['failedpull']\n",
    "\n",
    "                #\n",
    "                # re-organize the target dates\n",
    "                # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "                tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "                coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "                coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "                #\n",
    "                # sort the data based on task type and dates\n",
    "                dates_list = list(DBN_input_data_alltypes.keys())\n",
    "                sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "                sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "                #\n",
    "                # only select the targeted dates\n",
    "                sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "                dates_list_tgt = sorting_tgt_df['dates']\n",
    "                dates_list_tgt = np.array(dates_list_tgt)\n",
    "                #\n",
    "                ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "                #\n",
    "                # \n",
    "                for idate in np.arange(0,ndates_tgt,1):\n",
    "                    idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                    DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                    # load all the variables \n",
    "                    xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                    xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                    xxx3 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                    xxx4 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                    \n",
    "                    # randomize for each bootstrap shuffle\n",
    "                    np.random.seed(iboot)\n",
    "                    xxx1 = np.random.permutation(xxx1)\n",
    "                    xxx2 = np.random.permutation(xxx2)\n",
    "                    xxx3 = np.random.permutation(xxx3)\n",
    "                    xxx4 = np.random.permutation(xxx4)\n",
    "                    \n",
    "\n",
    "                    # Initialize arrays to store results\n",
    "                    pull1_surrounded_by_pull2 = np.zeros_like(xxx1)  # pull1 surrounded by pull2\n",
    "                    pull1_surrounded_by_gaze1 = np.zeros_like(xxx1)  # pull1 surrounded by gaze1\n",
    "                    pull1_surrounded_by_gaze2 = np.zeros_like(xxx1)  # pull1 surrounded by gaze2\n",
    "\n",
    "                    pull2_surrounded_by_pull1 = np.zeros_like(xxx3)  # pull2 surrounded by pull1\n",
    "                    pull2_surrounded_by_gaze1 = np.zeros_like(xxx3)  # pull2 surrounded by gaze1\n",
    "                    pull2_surrounded_by_gaze2 = np.zeros_like(xxx3)  # pull2 surrounded by gaze2\n",
    "\n",
    "                    #\n",
    "                    surround_steps = 3\n",
    "                    # pull1 surrounded by pull2\n",
    "                    for i in range(len(xxx1)):\n",
    "                        if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                            if any(xxx3[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                                pull1_surrounded_by_pull2[i] = 1  # Mark as surrounded by pull2\n",
    "\n",
    "                    # pull1 surrounded by gaze1\n",
    "                    for i in range(len(xxx1)):\n",
    "                        if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                            if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                                pull1_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                    # pull1 surrounded by gaze2\n",
    "                    for i in range(len(xxx1)):\n",
    "                        if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                            if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                                pull1_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "\n",
    "                    # pull2 surrounded by pull1\n",
    "                    for i in range(len(xxx3)):\n",
    "                        if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                            if any(xxx1[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                                pull2_surrounded_by_pull1[i] = 1  # Mark as surrounded by pull1\n",
    "\n",
    "                    # pull2 surrounded by gaze1\n",
    "                    for i in range(len(xxx3)):\n",
    "                        if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                            if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                                pull2_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                    # pull2 surrounded by gaze2\n",
    "                    for i in range(len(xxx3)):\n",
    "                        if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                            if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                                pull2_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "\n",
    "                    # concatenate the data\n",
    "                    if np.isin(animal1_fixedorder,tgt_animalnames):\n",
    "                        pull_surrounded_by_otherpull = np.concatenate((pull_surrounded_by_otherpull,pull1_surrounded_by_pull2))\n",
    "                        pull_surrounded_by_othergaze = np.concatenate((pull_surrounded_by_othergaze,pull1_surrounded_by_gaze2))\n",
    "                        pull_surrounded_by_selfgaze = np.concatenate((pull_surrounded_by_selfgaze,pull1_surrounded_by_gaze1))\n",
    "                        pull_only = np.concatenate((pull_only,xxx1))\n",
    "                        gaze_only = np.concatenate((gaze_only,xxx2))\n",
    "                    #\n",
    "                    if np.isin(animal2_fixedorder,tgt_animalnames):\n",
    "                        pull_surrounded_by_otherpull = np.concatenate((pull_surrounded_by_otherpull,pull2_surrounded_by_pull1))\n",
    "                        pull_surrounded_by_othergaze = np.concatenate((pull_surrounded_by_othergaze,pull2_surrounded_by_gaze1))\n",
    "                        pull_surrounded_by_selfgaze = np.concatenate((pull_surrounded_by_selfgaze,pull2_surrounded_by_gaze2))\n",
    "                        pull_only = np.concatenate((pull_only,xxx3))\n",
    "                        gaze_only = np.concatenate((gaze_only,xxx4))\n",
    "\n",
    "\n",
    "            # put two animals together in one matrix\n",
    "            # Stack the surrounded variables into a 2D array (each row is a time step)\n",
    "            data_matrix = np.vstack([\n",
    "                pull_surrounded_by_otherpull,\n",
    "                # pull_surrounded_by_othergaze,\n",
    "                pull_surrounded_by_selfgaze,\n",
    "\n",
    "            ]).T  # Transpose to get time steps as rows\n",
    "\n",
    "            # Compute the co-occurrence matrix (dot product of the binary matrix)\n",
    "            co_occurrence_matrix = np.dot(data_matrix.T, data_matrix)\n",
    "\n",
    "            # Calculate unique events (events that do not co-occur with any other event)\n",
    "            unique_events = np.array([np.sum((data_matrix[:, i] == 1) & (np.sum(data_matrix[:, np.arange(data_matrix.shape[1]) != i], axis=1) == 0)) \n",
    "                                      for i in range(data_matrix.shape[1])])\n",
    "\n",
    "            # Add the unique events as a new row and a new column\n",
    "            co_occurrence_matrix_with_unique = np.vstack([\n",
    "                np.hstack([co_occurrence_matrix, unique_events.reshape(-1, 1)]),\n",
    "                np.append(unique_events, np.sum(unique_events))  # Add sum of unique events as a bottom-right corner element\n",
    "            ])\n",
    "\n",
    "            # Create a DataFrame for better visualization\n",
    "            column_names = [\"pull_surrounded_by_otherpull\", \n",
    "                            # \"pull_surrounded_by_othergaze\", \n",
    "                            \"pull_surrounded_by_selfgaze\", \n",
    "                            \"Unique Events\"]\n",
    "            df_cooccurrence = pd.DataFrame(co_occurrence_matrix_with_unique, columns=column_names, index=column_names)\n",
    "            #\n",
    "            # change to ratio\n",
    "            df_cooccurrence = df_cooccurrence/np.sum(pull_only)\n",
    "        \n",
    "            co_occurrence_list.append(df_cooccurrence)\n",
    "        \n",
    "        #\n",
    "        # for a shuffled data \n",
    "        #\n",
    "        # plot for the distribution of pull_to_pull without social gaze interference\n",
    "        # Step 1: Remove all rows where all three variables are 0\n",
    "        # valid_indices = ~((pull_surrounded_by_otherpull == 0) & (pull_surrounded_by_othergaze == 0) & (pull_surrounded_by_selfgaze == 0)) \n",
    "        valid_indices = ~((pull_surrounded_by_otherpull == 0)  & (pull_surrounded_by_selfgaze == 0) )\n",
    "        # valid_indices = ~((pull_surrounded_by_otherpull == 0)  & (gaze_only == 0) )\n",
    "        # Filter the arrays to exclude those time steps\n",
    "        filtered_pull = pull_surrounded_by_otherpull[valid_indices]\n",
    "        # filtered_gaze1 = pull_surrounded_by_othergaze[valid_indices]\n",
    "        filtered_gaze2 = pull_surrounded_by_selfgaze[valid_indices]\n",
    "        # filtered_gaze2 = gaze_only[valid_indices]\n",
    "        # Apply the new mask: Set filtered_pull to 0 where gaze1 or gaze2 is 1\n",
    "        # filtered_pull[(filtered_gaze1 == 1) | (filtered_gaze2 == 1)] = 0\n",
    "        filtered_pull[(filtered_gaze2 == 1)] = 0\n",
    "        # Step 2: Find consecutive 1s in pull_surrounded_by_otherpull\n",
    "        consecutive_lengths = []\n",
    "        current_length = 0\n",
    "        #\n",
    "        for val in filtered_pull:\n",
    "            if val == 1:\n",
    "                current_length += 1  # Count length of consecutive 1s\n",
    "            elif current_length > 0:\n",
    "                consecutive_lengths.append(current_length)  # Store sequence length\n",
    "                current_length = 0  # Reset counter\n",
    "        # Capture last sequence if the series ends with 1s\n",
    "        if current_length > 0:\n",
    "            consecutive_lengths.append(current_length)\n",
    "        # Capture the last sequence if it ended at the last element\n",
    "        if current_length > 0:\n",
    "            consecutive_lengths.append(current_length)\n",
    "        # Step 3: Calculate the mean and median\n",
    "        mean_ones = np.mean(consecutive_lengths)\n",
    "        median_ones = np.median(consecutive_lengths)\n",
    "        # Step 4: Plot distribution of consecutive 1s\n",
    "        plt.figure(figsize=(7,5))\n",
    "        # Compute the relative frequencies by dividing counts by the sum of filtered_pull\n",
    "        bin_counts, bins = np.histogram(consecutive_lengths, bins=np.arange(1, max(consecutive_lengths, default=1) + 2) - 0.5)\n",
    "        relative_frequencies = bin_counts / np.sum(pull_only)  # Divide by total valid entries\n",
    "        # Plot the relative frequency as a bar plot\n",
    "        plt.bar(bins[:-1], relative_frequencies, width=1, color='blue', alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel(\"Length of Consecutive 1s in pull_surrounded_by_otherpull\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Distribution of Consecutive 1s in pull_surrounded_by_otherpull (without self gaze inteference)\\n\"+\n",
    "                 animaltype_tgt+' animals in all sessions of '+grouptype+' SHUFFLED')\n",
    "        # plt.xticks(range(1, max(consecutive_lengths, default=1) + 1),rotation=45)\n",
    "        plt.xticks(range(1, 15,5),rotation=45)\n",
    "        plt.xlim([0,15])\n",
    "        # Step 5: Add lines for mean and median\n",
    "        plt.axvline(mean_ones, color='green', linestyle='--', label=f'Mean: {mean_ones:.2f}')\n",
    "        plt.axvline(median_ones, color='red', linestyle='-', label=f'Median: {median_ones:.2f}')\n",
    "        # Step 6: Add legend\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        #\n",
    "        consecutive_lengths_shf = consecutive_lengths\n",
    "        #\n",
    "        stat, p_value = st.mannwhitneyu(consecutive_lengths_true, consecutive_lengths_shf, alternative='two-sided')\n",
    "        print(\"consecutive lentgh true vs shuffled \"+f\"Wilcoxon Test Statistic: {stat}\")\n",
    "        print(\"consecutive lentgh true vs shuffled \"+f\"P-value: {p_value}\")\n",
    "        \n",
    "        # for plotting\n",
    "        # Compute the 2.5th and 97.5th percentiles for significance threshold\n",
    "        lower_bound = np.percentile(co_occurrence_list, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(co_occurrence_list, 97.5, axis=0)\n",
    "\n",
    "        # Determine significance: If outside the confidence interval\n",
    "        significance_mask = (df_cooccurrence_true < lower_bound) | (df_cooccurrence_true > upper_bound)\n",
    "\n",
    "        # Create a new matrix for colors: Blue for smaller, Red for larger\n",
    "        color_matrix = np.where(df_cooccurrence_true < lower_bound, \"blue\",\n",
    "                                np.where(df_cooccurrence_true > upper_bound, \"red\", \"black\"))\n",
    "\n",
    "        # Create the annotation matrix with \"*\"\n",
    "        annot_matrix = df_cooccurrence_true.round(3).astype(str)  # Convert values to string for annotation\n",
    "        for i in range(df_cooccurrence_true.shape[0]):\n",
    "            for j in range(df_cooccurrence_true.shape[1]):\n",
    "                if significance_mask.iloc[i, j]:\n",
    "                    annot_matrix.iloc[i, j] += \" *\"  # Append \"*\" to significant values\n",
    "\n",
    "        # Plot the heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = seaborn.heatmap(df_cooccurrence_true, annot=annot_matrix.values,\n",
    "                             cmap=\"Blues\", fmt=\"s\", linewidths=0.5)\n",
    "\n",
    "        # Adjust text color based on significance (blue/red/black)\n",
    "        for text, color in zip(ax.texts, color_matrix.flatten()):\n",
    "            text.set_color(color)\n",
    "            \n",
    "        # Formatting\n",
    "        plt.title(\"Co-Occurrence Ratio Matrix Heatmap with Unique Events\\n\"\n",
    "                  +grouptype+' concatenated '+animaltype_tgt+' animals all sessions')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.show()\n",
    "        \n",
    "        # \n",
    "        # \n",
    "        #\n",
    "        # Plot the distribution of unique pull_sounded_by_other_pull\n",
    "        iind = np.where(np.array(column_names) == 'pull_surrounded_by_otherpull')[0][0]\n",
    "        jind = np.where(np.array(column_names) == 'Unique Events')[0][0]\n",
    "        # Extract the relevant data from co_occurrence_list (shuffled distribution)\n",
    "        shuffled_data = np.array(co_occurrence_list)[:, iind, jind]\n",
    "        # Extract the true value from df_cooccurrence_true\n",
    "        true_value = df_cooccurrence_true.iloc[iind, jind]\n",
    "        \n",
    "        # Plot the histogram for the shuffled distribution\n",
    "        plt.hist(shuffled_data, bins=10, color='blue', edgecolor='black', alpha=0.6, label='Shuffled')\n",
    "        # Overlay the true value as a vertical line\n",
    "        plt.axvline(x=true_value, color='red', linestyle='--', label=f'True Value: {true_value:.3f}')\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(\"Distribution of the unique 'pull_surrounded_by_otherpull'\\n\"+\n",
    "                 animaltype_tgt+' animals in all sessions of '+grouptype)\n",
    "        plt.xlabel(\"occurance ratio (pair of events/total pull number)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda9e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2fa2be",
   "metadata": {},
   "source": [
    "#### similar analysis as the previous one, quantify the number of pull events that is *around* gaze or not \n",
    "#### generate the confusion matrix  of the correlation coeffients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "    \n",
    "    do_succpull = 0\n",
    "    do_failpull = 1\n",
    "    \n",
    "    # session list options\n",
    "    do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "    do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "    if do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            savefile_sufix = '_bestsessions'\n",
    "        elif do_trainedMCs:\n",
    "            savefile_sufix = '_trainedMCsessions'\n",
    "    else:\n",
    "        savefile_sufix = ''\n",
    "    \n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    #\n",
    "    animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "        animal2_fixedorders = ['sparkle','scorch','kanga_1',]\n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "\n",
    "    grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "    coopthres_IDs = [100, 3, 2, 1.5, 1, -1]\n",
    "    if do_trainedMCs:\n",
    "        grouptypes = ['self reward','1s threshold','novision']\n",
    "        coopthres_IDs = [100, 1, -1]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    #\n",
    "    malenames = ['eddie','dodson','dannon','vermelho']\n",
    "    femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "    if do_onlyLearningsess:\n",
    "        malenames = ['eddie','dodson',]\n",
    "        femalenames = ['sparkle','scorch','kanga_1','ginger',]\n",
    "    \n",
    "    #\n",
    "    subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "    domnames = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "    if do_onlyLearningsess:\n",
    "        subnames = ['eddie','dodson','ginger',]\n",
    "        domnames = ['sparkle','scorch','kanga_1',]\n",
    "    \n",
    "    # initialize the dataframe\n",
    "    rollingCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                     'corr1_name','corr2_name','rollingCorr'])\n",
    "    #\n",
    "    CorrOfrollingCorr_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',\n",
    "                                                           'Rollingcorr1_name','Rollingcorr2_name',\n",
    "                                                           'CorrOfrollingCorr'])\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "                animal2_filename = 'kanga'\n",
    "            else:\n",
    "                animal2_filename = animal2_fixedorder\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            # load data for successful and failed pulls\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "            if do_succpull:\n",
    "                DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['succpull']\n",
    "            if do_failpull:\n",
    "                DBN_input_data_alltypes = DBN_input_data_alltypes_succfail['failedpull']\n",
    "                    \n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            #\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # load all the variables \n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx3 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx4 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                \n",
    "                \n",
    "\n",
    "                # Initialize arrays to store results\n",
    "                pull1_surrounded_by_pull2 = np.zeros_like(xxx1)  # pull1 surrounded by pull2\n",
    "                pull1_surrounded_by_gaze1 = np.zeros_like(xxx1)  # pull1 surrounded by gaze1\n",
    "                pull1_surrounded_by_gaze2 = np.zeros_like(xxx1)  # pull1 surrounded by gaze2\n",
    "\n",
    "                pull2_surrounded_by_pull1 = np.zeros_like(xxx3)  # pull2 surrounded by pull1\n",
    "                pull2_surrounded_by_gaze1 = np.zeros_like(xxx3)  # pull2 surrounded by gaze1\n",
    "                pull2_surrounded_by_gaze2 = np.zeros_like(xxx3)  # pull2 surrounded by gaze2\n",
    "\n",
    "                #\n",
    "                surround_steps = 1\n",
    "                # pull1 surrounded by pull2\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx3[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_pull2[i] = 1  # Mark as surrounded by pull2\n",
    "\n",
    "                # pull1 surrounded by gaze1\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                # pull1 surrounded by gaze2\n",
    "                for i in range(len(xxx1)):\n",
    "                    if xxx1[i] == 1:  # Check if pull1 = 1\n",
    "                        if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull1_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "\n",
    "                # pull2 surrounded by pull1\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx1[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_pull1[i] = 1  # Mark as surrounded by pull1\n",
    "\n",
    "                # pull2 surrounded by gaze1\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx2[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_gaze1[i] = 1  # Mark as surrounded by gaze1\n",
    "\n",
    "                # pull2 surrounded by gaze2\n",
    "                for i in range(len(xxx3)):\n",
    "                    if xxx3[i] == 1:  # Check if pull2 = 1\n",
    "                        if any(xxx4[max(0, i-surround_steps):i+surround_steps+1]):  # Check up to 3 steps before and after\n",
    "                            pull2_surrounded_by_gaze2[i] = 1  # Mark as surrounded by gaze2\n",
    "                \n",
    "                # put two animals together in one matrix\n",
    "                # Stack the surrounded variables into a 2D array (each row is a time step)\n",
    "                data_matrix = np.vstack([\n",
    "                    pull1_surrounded_by_pull2,\n",
    "                    pull1_surrounded_by_gaze1,\n",
    "                    pull1_surrounded_by_gaze2,\n",
    "                    pull2_surrounded_by_pull1,\n",
    "                    pull2_surrounded_by_gaze2,\n",
    "                    pull2_surrounded_by_gaze1\n",
    "                ]).T  # Transpose to get time steps as rows\n",
    "\n",
    "                # Compute the correlation matrix using pandas DataFrame\n",
    "                df_data = pd.DataFrame(data_matrix, columns=[\"pull1_surrounded_by_pull2\",\n",
    "                                                             \"pull1_surrounded_by_gaze1\",\n",
    "                                                             \"pull1_surrounded_by_gaze2\",\n",
    "                                                             \"pull2_surrounded_by_pull1\",\n",
    "                                                             \"pull2_surrounded_by_gaze2\",\n",
    "                                                             \"pull2_surrounded_by_gaze1\"])\n",
    "\n",
    "                correlation_matrix = df_data.corr(method='spearman')\n",
    "\n",
    "                # Create the heatmap\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                seaborn.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "                # Formatting\n",
    "                plt.title(\"Correlation Matrix Heatmap\\n\"\n",
    "                          +animal1_fixedorder+' '+animal2_fixedorder+'; '+idate_name+' '+grouptype)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.yticks(rotation=0)\n",
    "                plt.show()\n",
    "\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b2c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a743731",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7d323",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d13d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "        if minmaxfullSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            #samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            #samplingsizes_name = ['min_row_number','max_row_number','full_row_number']\n",
    "            samplingsizes = [np.nan]\n",
    "            samplingsizes_name = ['full_row_number']\n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        # for jj in np.arange(0,nsamplings,1):\n",
    "        for jj in np.arange(0,1,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                if samplingsizes_name[jj]=='full_row_number':\n",
    "                    isamplingsize = np.shape(DBN_input_data_alltypes[date_tgt])[0]\n",
    "\n",
    "                try:\n",
    "                    bhv_df_all = DBN_input_data_alltypes[date_tgt]\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                    colnames = list(bhv_df_all.columns)\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = np.shape(from_pops)[0]\n",
    "                    nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[date_tgt] = sig_edges\n",
    "                    \n",
    "                except:\n",
    "                    DAGs_alltypes[date_tgt] = [] \n",
    "                    DAGs_shuffle_alltypes[date_tgt] = []\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt] = []\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt] = []\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                    sig_edges_alltypes[date_tgt] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "    print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647783a",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alls = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "        if minmaxfullSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            # samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            # samplingsizes_name = ['min_row_number','max_row_number','full_row_number']   \n",
    "            samplingsizes = [np.nan]\n",
    "            samplingsizes_name = ['full_row_number']\n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                if samplingsizes_name[jj]=='full_row_number':\n",
    "                    isamplingsize = np.shape(DBN_input_data_allsessions[date_tgt])[0]\n",
    "\n",
    "                # try:\n",
    "                bhv_df_all = DBN_input_data_alltypes[date_tgt]\n",
    "\n",
    "\n",
    "                # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                colnames = list(bhv_df_all.columns)\n",
    "                eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                nevents = np.size(eventnames)\n",
    "\n",
    "                all_pops = list(bhv_df_all.columns)\n",
    "                from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                nFromNodes = np.shape(from_pops)[0]\n",
    "                nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                score_randstart = np.zeros((num_starting_points))\n",
    "                score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                # step 1: randomize the starting point for num_starting_points times\n",
    "                for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                    # try different down/re-sampling size\n",
    "                    bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                    aic = AicScore(bhv_df)\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                    aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                    np.random.seed(istarting_points)\n",
    "                    random.seed(istarting_points)\n",
    "                    starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                    starting_graph = DAG()\n",
    "                    starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                    starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                    best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                    DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                    DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                    score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                    # step 2: add the shffled data results\n",
    "                    # shuffled bhv_df\n",
    "                    best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                    DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                    DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                    score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                sig_edges_alltypes[date_tgt] = sig_edges\n",
    "                    \n",
    "                # except:\n",
    "                #     DAGs_alltypes[date_tgt] = [] \n",
    "                #     DAGs_shuffle_alltypes[date_tgt] = []\n",
    "                # \n",
    "                #     DAGs_scores_alltypes[date_tgt] = []\n",
    "                #     DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "                # \n",
    "                #     weighted_graphs_alltypes[date_tgt] = []\n",
    "                #     weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                #     sig_edges_alltypes[date_tgt] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    savedata = 0\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if moreSampSize:  \n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "        elif minmaxfullSampSize:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)        \n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb83aa",
   "metadata": {},
   "source": [
    "### plot the edges over time (session)\n",
    "#### mean edge weights of selected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30602b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(10*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        idate_name = dates_list_sorted[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_mean_forplot_all_dates,'ko',markersize=10)\n",
    "    #axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_shuffled_mean_forplot_all_dates,'bo',markersize=10)\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    axs.flatten()[i_edge].set_xlim([-0.5,ndates_sorted-0.5])\n",
    "    #\n",
    "    if i_edge > int(n_edges-1):\n",
    "        axs.flatten()[i_edge].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs.flatten()[i_edge].set_xticklabels(dates_list_sorted, rotation=90,fontsize=10)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweight_acrossAllSessions_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56642be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_graphs_diffTempRo_diffSampSize[('1','full_row_number')].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5ffd",
   "metadata": {},
   "source": [
    "#### mean edge weights of selected edges v.s. other behavioral measures\n",
    "##### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the targeted dates\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "dates_list_tgt = sorting_tgt_df['dates']\n",
    "dates_list_tgt = np.array(dates_list_tgt)\n",
    "#\n",
    "ndates_tgt = np.shape(dates_list_tgt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number'\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(5*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    if xplottype == 'succrate':\n",
    "        xxx = succ_rate_all_dates[sorting_tgt_df.index]\n",
    "    elif xplottype == 'meangazenum':   \n",
    "        xxx = gazemean_num_all_dates[sorting_tgt_df.index]\n",
    "    #     \n",
    "    yyy = edgeweight_mean_forplot_all_dates\n",
    "    #\n",
    "    rr_spe,pp_spe = scipy.stats.spearmanr(xxx, yyy)\n",
    "    slope, intercept, rr_reg, pp_reg, std_err = st.linregress(xxx.astype(float).T[0], yyy.astype(float).T[0])\n",
    "    #\n",
    "    axs.flatten()[i_edge].plot(xxx,yyy,'bo',markersize=8)\n",
    "    axs.flatten()[i_edge].plot(np.array([xxx.min(),xxx.max()]),np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    #\n",
    "    if i_edge > int(n_edges-3):\n",
    "        axs.flatten()[i_edge].set_xlabel(xplotlabel,fontsize=13)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    axs.flatten()[i_edge].text(xxx.min(),1.0,'spearman r='+\"{:.2f}\".format(rr_spe),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.9,'spearman p='+\"{:.2f}\".format(pp_spe),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.8,'regression r='+\"{:.2f}\".format(rr_reg),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.7,'regression p='+\"{:.2f}\".format(pp_reg),fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec6a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "np.array(fromNodesIDs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02539e5",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "###  mean edge weights of selected edges v.s. other behavioral measures\n",
    "### run the correlation with each animal, then pool them together\n",
    "#### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b918ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "# DBN analysis types\n",
    "# \n",
    "# 1s time lag\n",
    "timelagtype = 1 # 1, 2, 3, 12(12lagmerged), 0(all merged)\n",
    "if timelagtype == 1:\n",
    "    edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                          ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                          ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "    fromNodesIDs = [[ 9, 8],\n",
    "                    [10,11],\n",
    "                    [ 9, 8],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '1slag'\n",
    "# 2s time lag\n",
    "elif timelagtype == 2:\n",
    "    edges_target_names = [['2slag_pull2_pull1','2slag_pull1_pull2'],\n",
    "                          ['2slag_gaze1_pull1','2slag_gaze2_pull2'],\n",
    "                          ['2slag_pull2_gaze1','2slag_pull1_gaze2'],]\n",
    "    fromNodesIDs = [[ 5, 4],\n",
    "                    [ 6, 7],\n",
    "                    [ 5, 4],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '2slag'\n",
    "# 3s time lag\n",
    "elif timelagtype == 3:\n",
    "    edges_target_names = [['3slag_pull2_pull1','3slag_pull1_pull2'],\n",
    "                          ['3slag_gaze1_pull1','3slag_gaze2_pull2'],\n",
    "                          ['3slag_pull2_gaze1','3slag_pull1_gaze2'],]\n",
    "    fromNodesIDs = [[ 1, 0],\n",
    "                    [ 2, 3],\n",
    "                    [ 1, 0],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '2slag'\n",
    "# 1s and 2s time lag merged\n",
    "elif timelagtype == 12:\n",
    "    edges_target_names = [['12slag_pull2_pull1','12slag_pull1_pull2'],\n",
    "                          ['12slag_gaze1_pull1','12slag_gaze2_pull2'],\n",
    "                          ['12slag_pull2_gaze1','12slag_pull1_gaze2'],]\n",
    "    fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "    toNodesIDs = [[ 0, 0],[ 1, 1],\n",
    "                  [ 0, 0],[ 1, 1],\n",
    "                  [ 2, 2],[ 3, 3],]\n",
    "    timelagname = '1and2smerged'\n",
    "# 1s and 2s and 3s time lag merged\n",
    "elif timelagtype == 0:\n",
    "    edges_target_names = [['123slag_pull2_pull1','123slag_pull1_pull2'],\n",
    "                          ['123slag_gaze1_pull1','123slag_gaze2_pull2'],\n",
    "                          ['123slag_pull2_gaze1','123slag_pull1_gaze2'],]\n",
    "    fromNodesIDs = [[ 9, 5, 1],[ 8, 4, 0],\n",
    "                    [10, 6, 2],[11, 7, 3],\n",
    "                    [ 9, 5, 1],[ 8, 4, 0],]\n",
    "    toNodesIDs = [[ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 2, 2, 2],[ 3, 3, 3],]\n",
    "    timelagname = 'merged'\n",
    "    \n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meanpullnum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "# initiate the final data set\n",
    "edges_measure_slopes_all = np.zeros((nanimalpairs,n_edges))\n",
    "edges_measure_corrR_all = np.zeros((nanimalpairs,n_edges))\n",
    "edges_measure_regR_all = np.zeros((nanimalpairs,n_edges))\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1.5)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==-1)]\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    \n",
    "    # calculate the linear regression and correlation metrics for tgt edges\n",
    "    for i_edge in np.arange(0,n_edges,1):\n",
    "        #\n",
    "        edgeweight_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "        edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "        edgeweight_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "        edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "\n",
    "        edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "        #\n",
    "        if (timelagtype == 12) | (timelagtype == 0):\n",
    "            fromNodesID = np.array(fromNodesIDs)[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs)[i_edge]\n",
    "        else:\n",
    "            fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            \n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "            sig_edges_tgt[sig_edges_tgt==0]=np.nan\n",
    "            \n",
    "            # weighted_graphs_tgt = weighted_graphs_tgt*sig_edges_tgt\n",
    "            \n",
    "            edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "            edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "            edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "            edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "\n",
    "        # calculate correlation and linear regression\n",
    "        if xplottype == 'succrate':\n",
    "            xxx = succ_rate_all_dates[sorting_tgt_df.index]\n",
    "        elif xplottype == 'meangazenum':   \n",
    "            xxx = gazemean_num_all_dates[sorting_tgt_df.index]\n",
    "        elif xplottype == 'meanpullnum':   \n",
    "            xxx = pullmean_num_all_dates[sorting_tgt_df.index]\n",
    "        #     \n",
    "        yyy = edgeweight_mean_forplot_all_dates\n",
    "        #\n",
    "        rr_spe,pp_spe = scipy.stats.spearmanr(xxx, yyy)\n",
    "        slope, intercept, rr_reg, pp_reg, std_err = st.linregress(xxx.astype(float).T[0], yyy.astype(float).T[0])\n",
    "    \n",
    "        #\n",
    "        edges_measure_slopes_all[ianimalpair,i_edge] = slope\n",
    "        edges_measure_corrR_all[ianimalpair,i_edge] = rr_spe\n",
    "        edges_measure_regR_all[ianimalpair,i_edge] = rr_reg\n",
    "\n",
    "# plot\n",
    "edges_measure_slopes_all=np.vstack([edges_measure_slopes_all[:,np.arange(0,n_edges,2)],edges_measure_slopes_all[:,np.arange(1,n_edges,2)]])\n",
    "edges_measure_corrR_all=np.vstack([edges_measure_corrR_all[:,np.arange(0,n_edges,2)],edges_measure_corrR_all[:,np.arange(1,n_edges,2)]])\n",
    "edges_measure_regR_all=np.vstack([edges_measure_regR_all[:,np.arange(0,n_edges,2)],edges_measure_regR_all[:,np.arange(1,n_edges,2)]])\n",
    "\n",
    "# \n",
    "\n",
    "dependencytargets = ['pull-pull','within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency\n",
    "# edge_measure_tgt_all = edges_measure_regR_all # regression slope or correlation R or regression R\n",
    "# measure_tgt_name = 'regression_R' # 'regression_slopes' or 'correlation_R' or 'regression_R'\n",
    "edge_measure_tgt_all = edges_measure_corrR_all # regression slope or correlation R or regression R\n",
    "measure_tgt_name = 'correlation_R' # 'regression_slopes' or 'correlation_R' or 'regression_R'\n",
    "# \n",
    "edge_measure_tgt_all_df = pd.DataFrame(edge_measure_tgt_all)\n",
    "edge_measure_tgt_all_df.columns = dependencytargets\n",
    "edge_measure_tgt_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([edge_measure_tgt_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].set_ylabel('edge weight v.s. '+xplotlabel,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals; '+measure_tgt_name ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[0].set_ylim([-1,1])\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "edge_measure_tgt_male_df = pd.DataFrame(edge_measure_tgt_all[[0,1,2,9],:])\n",
    "edge_measure_tgt_male_df.columns = dependencytargets\n",
    "edge_measure_tgt_male_df['type'] = 'male'\n",
    "#\n",
    "edge_measure_tgt_female_df = pd.DataFrame(edge_measure_tgt_all[[3,4,5,6,7,8],:])\n",
    "edge_measure_tgt_female_df.columns = dependencytargets\n",
    "edge_measure_tgt_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([edge_measure_tgt_male_df,edge_measure_tgt_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].set_ylabel('edge weight v.s. '+xplotlabel,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female; '+measure_tgt_name ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[1].set_ylim([-1,1])\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "edge_measure_tgt_sub_df = pd.DataFrame(edge_measure_tgt_all[[0,1,2,3,4],:])\n",
    "edge_measure_tgt_sub_df.columns = dependencytargets\n",
    "edge_measure_tgt_sub_df['type'] = 'subordinate'\n",
    "#\n",
    "edge_measure_tgt_dom_df = pd.DataFrame(edge_measure_tgt_all[[5,6,7,8,9],:])\n",
    "edge_measure_tgt_dom_df.columns = dependencytargets\n",
    "edge_measure_tgt_dom_df['type'] = 'dominant'\n",
    "#\n",
    "df_long=pd.concat([edge_measure_tgt_sub_df,edge_measure_tgt_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].set_ylabel('edge weight v.s. '+xplotlabel,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom; '+measure_tgt_name ,fontsize=24)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[2].set_ylim([-1,1])\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+measure_tgt_name+'_'+timelagname+'.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b236436",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(edge_measure_tgt_all_df['pull-pull'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8720304",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(edge_measure_tgt_all_df['across_pullgaze'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54703e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(edge_measure_tgt_female_df['pull-pull'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(edge_measure_tgt_male_df['pull-pull'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f503348",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(edge_measure_tgt_male_df['pull-pull'],edge_measure_tgt_female_df['pull-pull'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(edge_measure_tgt_female_df['across_pullgaze'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd921eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(edge_measure_tgt_male_df['across_pullgaze'],edge_measure_tgt_female_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(edge_measure_tgt_dom_df['across_pullgaze'],edge_measure_tgt_sub_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_measure_tgt_sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e094a",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "###  mean edge weights of selected edges v.s. other behavioral measures\n",
    "### pool the weight and bhv measures together, then run the correlation\n",
    "#### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffff2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "# DBN analysis types\n",
    "# \n",
    "# 1s time lag\n",
    "timelagtype = 0 # 1, 2, 3, 12(12lagmerged), 0(all merged)\n",
    "if timelagtype == 1:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 8],\n",
    "                    [10,11],\n",
    "                    [ 9, 8],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '1slag'\n",
    "# 2s time lag\n",
    "elif timelagtype == 2:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 5, 4],\n",
    "                    [ 6, 7],\n",
    "                    [ 5, 4],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '2slag'\n",
    "# 3s time lag\n",
    "elif timelagtype == 3:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 1, 0],\n",
    "                    [ 2, 3],\n",
    "                    [ 1, 0],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '3slag'\n",
    "# 1s and 2s time lag merged\n",
    "elif timelagtype == 12:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "    toNodesIDs = [[ 0, 0],[ 1, 1],\n",
    "                  [ 0, 0],[ 1, 1],\n",
    "                  [ 2, 2],[ 3, 3],]\n",
    "    timelagname = '1and2smerged'\n",
    "# 1s and 2s and 3s time lag merged\n",
    "elif timelagtype == 0:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5, 1],[ 8, 4, 0],\n",
    "                    [10, 6, 2],[11, 7, 3],\n",
    "                    [ 9, 5, 1],[ 8, 4, 0],]\n",
    "    toNodesIDs = [[ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 2, 2, 2],[ 3, 3, 3],]\n",
    "    timelagname = 'merged'\n",
    "    \n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "#\n",
    "xplottype = 'succ_rates' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meanpullnum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'gaze_numbers' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'pull_numbers' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "\n",
    "\n",
    "# initiate the final data set\n",
    "edgeweights_bhvmeasures_all = pd.DataFrame(columns=['dates','act_animal','edge_name','edge_weight',\n",
    "                                                    'succ_rates','gaze_numbers','pull_numbers'])\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "        animal2_filename = 'kanga'\n",
    "    else:\n",
    "        animal2_filename = animal2_fixedorder\n",
    "    \n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        sess_videotimes_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1.5)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==-1)]\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    \n",
    "    # calculate the linear regression and correlation metrics for tgt edges\n",
    "    for i_edge in np.arange(0,n_edges,1):\n",
    "\n",
    "        edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "        \n",
    "        actanimal_id = edge_tgt_name.split(' ')[0]\n",
    "        if actanimal_id == 'animal1':\n",
    "            actanimal = animal1_fixedorder\n",
    "        elif actanimal_id == 'animal2':\n",
    "            actanimal = animal2_fixedorder\n",
    "            \n",
    "        edgename = edge_tgt_name.split(' ')[1]\n",
    "        \n",
    "        #\n",
    "        if (timelagtype == 12) | (timelagtype == 0):\n",
    "            fromNodesID = np.array(fromNodesIDs)[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs)[i_edge]\n",
    "        else:\n",
    "            fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            succrate = succ_rate_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "            \n",
    "            if actanimal_id == 'animal1':\n",
    "                pullnumber = pull1_num_all_dates[sorting_tgt_df.index[idate]][0]/sess_videotimes_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "                gazenumber = gaze1_num_all_dates[sorting_tgt_df.index[idate]][0]/sess_videotimes_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "            elif actanimal_id == 'animal2':\n",
    "                pullnumber = pull2_num_all_dates[sorting_tgt_df.index[idate]][0]/sess_videotimes_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "                gazenumber = gaze2_num_all_dates[sorting_tgt_df.index[idate]][0]/sess_videotimes_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "            \n",
    "            \n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            \n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "            sig_edges_tgt[sig_edges_tgt==0]=np.nan\n",
    "            \n",
    "            weighted_graphs_tgt = weighted_graphs_tgt*sig_edges_tgt\n",
    "            \n",
    "            # \n",
    "            edgeweights_bhvmeasures_all = edgeweights_bhvmeasures_all.append(\n",
    "                                            {'dates':idate_name,\n",
    "                                             'act_animal':actanimal,\n",
    "                                             'edge_name':edgename,\n",
    "                                             'edge_weight':np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID]),\n",
    "                                             'succ_rates':succrate,\n",
    "                                             'gaze_numbers':pullnumber,\n",
    "                                             'pull_numbers':gazenumber,\n",
    "                                            },ignore_index=True)\n",
    "\n",
    "\n",
    "# for plot\n",
    "dependencytargets = np.unique(edgeweights_bhvmeasures_all['edge_name'])    \n",
    "nedges_forplot = np.shape(dependencytargets)[0]\n",
    "#\n",
    "fig, axs = plt.subplots(1,nedges_forplot)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5*nedges_forplot)\n",
    "\n",
    "for iedge in np.arange(0,nedges_forplot,1):\n",
    "    dependencyname_forplot = dependencytargets[iedge]\n",
    "    \n",
    "    edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_all[edgeweights_bhvmeasures_all['edge_name']==dependencyname_forplot]\n",
    "    \n",
    "    # xxx = edgeweights_bhvmeasures_forplot['succ_rates']\n",
    "    xxx = edgeweights_bhvmeasures_forplot[xplottype]\n",
    "    yyy = edgeweights_bhvmeasures_forplot['edge_weight']\n",
    "    ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "    xxx = xxx[~ind_nan]\n",
    "    yyy = yyy[~ind_nan]\n",
    "    \n",
    "    p_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').pvalue\n",
    "    r_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').rvalue\n",
    "    \n",
    "    seaborn.regplot(ax=axs[iedge], data = edgeweights_bhvmeasures_forplot, \n",
    "                    x=xplottype, y='edge_weight')\n",
    "    \n",
    "    axs[iedge].set_title('all animals' ,fontsize=17)\n",
    "    axs[iedge].set_xlabel(xplotlabel,fontsize=15)\n",
    "    # axs[iedge].set_xlim([0.2,0.9])\n",
    "    axs[iedge].set_ylabel(dependencyname_forplot+\" dependency weight\",fontsize=15)\n",
    "    axs[iedge].set_ylim([-0.05,1.05])\n",
    "    axs[iedge].legend()\n",
    "    axs[iedge].text(0.25,0.00,'regression r='+\"{:.2f}\".format(r_reg),fontsize=10)\n",
    "    axs[iedge].text(0.25,0.05,'regression p='+\"{:.2f}\".format(p_reg),fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+measure_tgt_name+'_'+timelagname+'correlationAfterpooling.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ede40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull1_num_all_dates[sorting_tgt_df.index[idate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4493bc6e",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "###  mean MI compared with Self reward v.s. other behavioral measures - sueccess rates\n",
    "### pool the weight and bhv measures together across animals, then run the correlation\n",
    "### normalize the success rate\n",
    "#### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "malenames = ['eddie','dodson','dannon','vermelho']\n",
    "subnames = ['eddie','dodson','dannon','ginger','koala']\n",
    "\n",
    "#\n",
    "# DBN analysis types\n",
    "# \n",
    "# 1s time lag\n",
    "timelagtype = 12 # 1, 2, 3, 12(12lagmerged), 0(all merged)\n",
    "if timelagtype == 1:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 8],\n",
    "                    [10,11],\n",
    "                    [ 9, 8],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '1slag'\n",
    "# 2s time lag\n",
    "elif timelagtype == 2:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 5, 4],\n",
    "                    [ 6, 7],\n",
    "                    [ 5, 4],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '2slag'\n",
    "# 3s time lag\n",
    "elif timelagtype == 3:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 1, 0],\n",
    "                    [ 2, 3],\n",
    "                    [ 1, 0],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '3slag'\n",
    "# 1s and 2s time lag merged\n",
    "elif timelagtype == 12:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "    toNodesIDs = [[ 0, 0],[ 1, 1],\n",
    "                  [ 0, 0],[ 1, 1],\n",
    "                  [ 2, 2],[ 3, 3],]\n",
    "    timelagname = '1and2smerged'\n",
    "# 1s and 2s and 3s time lag merged\n",
    "elif timelagtype == 0:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5, 1],[ 8, 4, 0],\n",
    "                    [10, 6, 2],[11, 7, 3],\n",
    "                    [ 9, 5, 1],[ 8, 4, 0],]\n",
    "    toNodesIDs = [[ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 2, 2, 2],[ 3, 3, 3],]\n",
    "    timelagname = 'merged'\n",
    "    \n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meanpullnum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "\n",
    "#\n",
    "doxplot_norm = 1\n",
    "\n",
    "# initiate the final data set\n",
    "edgeweights_bhvmeasures_all = pd.DataFrame(columns=['dates','act_animal','edge_name','edge_weight','succ_rates','sex','hierarchy'])\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "        animal2_filename = 'kanga'\n",
    "    else:\n",
    "        animal2_filename = animal2_fixedorder\n",
    "    \n",
    "    \n",
    "    # load the DBN - combined session and self reward for the baseline to calculate MI\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    #\n",
    "    # load edge weight data for the self \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = get_significant_edges(weighted_graphs_self,weighted_graphs_sf_self)\n",
    "    \n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data for DBN run for each days\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1.5)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==-1)]\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    \n",
    "    # calculate the linear regression and correlation metrics for tgt edges\n",
    "    for i_edge in np.arange(0,n_edges,1):\n",
    "\n",
    "        edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "        \n",
    "        actanimal_id = edge_tgt_name.split(' ')[0]\n",
    "        if actanimal_id == 'animal1':\n",
    "            actanimal = animal1_fixedorder\n",
    "        elif actanimal_id == 'animal2':\n",
    "            actanimal = animal2_fixedorder\n",
    "            \n",
    "        edgename = edge_tgt_name.split(' ')[1]\n",
    "        \n",
    "        #\n",
    "        if np.isin(actanimal,malenames):\n",
    "            actanimal_sex = 'male'\n",
    "        else:\n",
    "            actanimal_sex = 'female'\n",
    "        \n",
    "        #\n",
    "        if np.isin(actanimal,subnames):\n",
    "            actanimal_hie = 'sub'\n",
    "        else:\n",
    "            actanimal_hie = 'dom'\n",
    "        \n",
    "        \n",
    "        #\n",
    "        if (timelagtype == 12) | (timelagtype == 0):\n",
    "            fromNodesID = np.array(fromNodesIDs)[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs)[i_edge]\n",
    "        else:\n",
    "            fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            succrate = succ_rate_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "            \n",
    "            if doxplot_norm:\n",
    "                # succrate = zscore(succ_rate_all_dates[sorting_tgt_df.index])[idate][0]\n",
    "                min_val = np.nanmin(succ_rate_all_dates[sorting_tgt_df.index])\n",
    "                max_val = np.nanmax(succ_rate_all_dates[sorting_tgt_df.index])\n",
    "                succrate = [(x - min_val) / (max_val - min_val) for x in succ_rate_all_dates[sorting_tgt_df.index].flatten()][idate]\n",
    "\n",
    "            \n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "            \n",
    "            MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_tgt,\n",
    "                                          sig_edges_self, sig_edges_tgt, 150)\n",
    "            \n",
    "            sig_edges_coop_self = sig_edges_coop_self.astype('float')\n",
    "            sig_edges_coop_self[sig_edges_coop_self==0]=np.nan\n",
    "            \n",
    "            MI_coop_self_all = MI_coop_self_all*sig_edges_coop_self\n",
    "            \n",
    "            # \n",
    "            edgeweights_bhvmeasures_all = edgeweights_bhvmeasures_all.append(\n",
    "                                            {'dates':idate_name,\n",
    "                                             'act_animal':actanimal,\n",
    "                                             'edge_name':edgename,\n",
    "                                             'edge_weight':np.nanmean(MI_coop_self_all[:,fromNodesID,toNodesID]),\n",
    "                                             'succ_rates':succrate,\n",
    "                                             'sex':actanimal_sex,\n",
    "                                             'hierarchy':actanimal_hie,},ignore_index=True)\n",
    "\n",
    "\n",
    "# for plot\n",
    "dependencytargets = np.unique(edgeweights_bhvmeasures_all['edge_name'])    \n",
    "nedges_forplot = np.shape(dependencytargets)[0]\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(3,nedges_forplot)\n",
    "fig.set_figheight(5*3)\n",
    "fig.set_figwidth(5*nedges_forplot)\n",
    "\n",
    "for iedge in np.arange(0,nedges_forplot,1):\n",
    "    dependencyname_forplot = dependencytargets[iedge]\n",
    "    \n",
    "    # all animals\n",
    "    edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_all[edgeweights_bhvmeasures_all['edge_name']==dependencyname_forplot]\n",
    "    \n",
    "    # separating high and low success rate\n",
    "    # ind_tgt = edgeweights_bhvmeasures_forplot['succ_rates']<0.55\n",
    "    # edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_forplot[ind_tgt]\n",
    "    \n",
    "    xxx = edgeweights_bhvmeasures_forplot['succ_rates']\n",
    "    yyy = edgeweights_bhvmeasures_forplot['edge_weight']\n",
    "    ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "    xxx = xxx[~ind_nan]\n",
    "    yyy = yyy[~ind_nan]\n",
    "    \n",
    "    p_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').pvalue\n",
    "    r_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').rvalue\n",
    "    \n",
    "    seaborn.regplot(ax=axs[0,iedge], data = edgeweights_bhvmeasures_forplot, \n",
    "                    x='succ_rates', y='edge_weight')\n",
    "    \n",
    "    axs[0,iedge].set_title('all animals' ,fontsize=17)\n",
    "    axs[0,iedge].set_xlabel('success rate',fontsize=15)\n",
    "    axs[0,iedge].set_xlim([-0.1,1.1])\n",
    "    axs[0,iedge].set_ylabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "    axs[0,iedge].set_ylim([-1.05,1.05])\n",
    "    axs[0,iedge].legend()\n",
    "    axs[0,iedge].text(0.25,-0.6,'regression r='+\"{:.2f}\".format(r_reg),fontsize=10)\n",
    "    axs[0,iedge].text(0.25,-0.75,'regression p='+\"{:.2f}\".format(p_reg),fontsize=10)\n",
    "\n",
    "    # male female\n",
    "    # male\n",
    "    xxx_m = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['sex']=='male']['succ_rates']\n",
    "    yyy_m = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['sex']=='male']['edge_weight']\n",
    "    ind_nan = np.isnan(xxx_m) | np.isnan(yyy_m)\n",
    "    xxx_m = xxx_m[~ind_nan]\n",
    "    yyy_m = yyy_m[~ind_nan]\n",
    "    p_reg_m = scipy.stats.linregress(xxx_m, yyy_m, alternative='two-sided').pvalue\n",
    "    r_reg_m = scipy.stats.linregress(xxx_m, yyy_m, alternative='two-sided').rvalue\n",
    "    #\n",
    "    seaborn.regplot(ax=axs[1,iedge], \n",
    "                   data = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['sex']=='male'], \n",
    "                    x='succ_rates', y='edge_weight',label = 'male')\n",
    "    \n",
    "    # female\n",
    "    xxx_f = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['sex']=='female']['succ_rates']\n",
    "    yyy_f = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['sex']=='female']['edge_weight']\n",
    "    ind_nan = np.isnan(xxx_f) | np.isnan(yyy_f)\n",
    "    xxx_f = xxx_f[~ind_nan]\n",
    "    yyy_f = yyy_f[~ind_nan]\n",
    "    p_reg_f = scipy.stats.linregress(xxx_f, yyy_f, alternative='two-sided').pvalue\n",
    "    r_reg_f = scipy.stats.linregress(xxx_f, yyy_f, alternative='two-sided').rvalue\n",
    "    #\n",
    "    seaborn.regplot(ax=axs[1,iedge], \n",
    "                   data = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['sex']=='female'], \n",
    "                    x='succ_rates', y='edge_weight',label = 'female')\n",
    "    \n",
    "    \n",
    "    axs[1,iedge].set_title('male and female' ,fontsize=17)\n",
    "    axs[1,iedge].set_xlabel('success rate',fontsize=15)\n",
    "    axs[1,iedge].set_xlim([-0.1,1.1])\n",
    "    axs[1,iedge].set_ylabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "    axs[1,iedge].set_ylim([-1.05,1.05])\n",
    "    axs[1,iedge].legend()\n",
    "    axs[1,iedge].text(0.25,-0.6,'male regression r='+\"{:.2f}\".format(r_reg_m),fontsize=10)\n",
    "    axs[1,iedge].text(0.25,-0.7,'male regression p='+\"{:.2f}\".format(p_reg_m),fontsize=10)\n",
    "    axs[1,iedge].text(0.25,-0.8,'female regression r='+\"{:.2f}\".format(r_reg_f),fontsize=10)\n",
    "    axs[1,iedge].text(0.25,-0.9,'female regression p='+\"{:.2f}\".format(p_reg_f),fontsize=10)\n",
    "    \n",
    "    # sub dom\n",
    "    # sub\n",
    "    xxx_s = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['hierarchy']=='sub']['succ_rates']\n",
    "    yyy_s = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['hierarchy']=='sub']['edge_weight']\n",
    "    ind_nan = np.isnan(xxx_s) | np.isnan(yyy_s)\n",
    "    xxx_s = xxx_s[~ind_nan]\n",
    "    yyy_s = yyy_s[~ind_nan]\n",
    "    p_reg_s = scipy.stats.linregress(xxx_s, yyy_s, alternative='two-sided').pvalue\n",
    "    r_reg_s = scipy.stats.linregress(xxx_s, yyy_s, alternative='two-sided').rvalue\n",
    "    #\n",
    "    seaborn.regplot(ax=axs[2,iedge], \n",
    "                   data = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['hierarchy']=='sub'], \n",
    "                    x='succ_rates', y='edge_weight',label = 'sub')\n",
    "    \n",
    "    # dom\n",
    "    xxx_d = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['hierarchy']=='dom']['succ_rates']\n",
    "    yyy_d = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['hierarchy']=='dom']['edge_weight']\n",
    "    ind_nan = np.isnan(xxx_d) | np.isnan(yyy_d)\n",
    "    xxx_d = xxx_d[~ind_nan]\n",
    "    yyy_d = yyy_d[~ind_nan]\n",
    "    p_reg_d = scipy.stats.linregress(xxx_d, yyy_d, alternative='two-sided').pvalue\n",
    "    r_reg_d = scipy.stats.linregress(xxx_d, yyy_d, alternative='two-sided').rvalue\n",
    "    #\n",
    "    seaborn.regplot(ax=axs[2,iedge], \n",
    "                   data = edgeweights_bhvmeasures_forplot[edgeweights_bhvmeasures_forplot['hierarchy']=='dom'], \n",
    "                    x='succ_rates', y='edge_weight',label = 'dom')\n",
    "    \n",
    "    \n",
    "    axs[2,iedge].set_title('sub and dom' ,fontsize=17)\n",
    "    axs[2,iedge].set_xlabel('success rate',fontsize=15)\n",
    "    axs[2,iedge].set_xlim([-0.1,1.1])\n",
    "    axs[2,iedge].set_ylabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "    axs[2,iedge].set_ylim([-1.05,1.05])\n",
    "    axs[2,iedge].legend()\n",
    "    axs[2,iedge].text(0.25,-0.6,'sub regression r='+\"{:.2f}\".format(r_reg_s),fontsize=10)\n",
    "    axs[2,iedge].text(0.25,-0.7,'sub regression p='+\"{:.2f}\".format(p_reg_s),fontsize=10)\n",
    "    axs[2,iedge].text(0.25,-0.8,'dom regression r='+\"{:.2f}\".format(r_reg_d),fontsize=10)\n",
    "    axs[2,iedge].text(0.25,-0.9,'dom regression p='+\"{:.2f}\".format(p_reg_d),fontsize=10)\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if not doxplot_norm:\n",
    "        plt.savefig(figsavefolder+\"edgeMI_vs_\"+xplottype+\"_\"+measure_tgt_name+'_'+timelagname+'correlationAfterpooling.pdf')\n",
    "    elif doxplot_norm:\n",
    "        plt.savefig(figsavefolder+\"edgeMI_vs_normalized\"+xplottype+\"_\"+measure_tgt_name+'_'+timelagname+'correlationAfterpooling.pdf')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e9cc4",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "###  mean MI compared with Self reward v.s. other behavioral measures\n",
    "### pool the weight and bhv measures together, then run the correlation\n",
    "#### only the cooperation days and plot each pair seprately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "# DBN analysis types\n",
    "# \n",
    "# 1s time lag\n",
    "timelagtype = 0 # 1, 2, 3, 12(12lagmerged), 0(all merged)\n",
    "if timelagtype == 1:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 8],\n",
    "                    [10,11],\n",
    "                    [ 9, 8],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '1slag'\n",
    "# 2s time lag\n",
    "elif timelagtype == 2:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 5, 4],\n",
    "                    [ 6, 7],\n",
    "                    [ 5, 4],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '2slag'\n",
    "# 3s time lag\n",
    "elif timelagtype == 3:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 1, 0],\n",
    "                    [ 2, 3],\n",
    "                    [ 1, 0],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '3slag'\n",
    "# 1s and 2s time lag merged\n",
    "elif timelagtype == 12:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "    toNodesIDs = [[ 0, 0],[ 1, 1],\n",
    "                  [ 0, 0],[ 1, 1],\n",
    "                  [ 2, 2],[ 3, 3],]\n",
    "    timelagname = '1and2smerged'\n",
    "# 1s and 2s and 3s time lag merged\n",
    "elif timelagtype == 0:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5, 1],[ 8, 4, 0],\n",
    "                    [10, 6, 2],[11, 7, 3],\n",
    "                    [ 9, 5, 1],[ 8, 4, 0],]\n",
    "    toNodesIDs = [[ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 2, 2, 2],[ 3, 3, 3],]\n",
    "    timelagname = 'merged'\n",
    "    \n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meanpullnum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "\n",
    "\n",
    "# initiate the final data set\n",
    "edgeweights_bhvmeasures_all = pd.DataFrame(columns=['dates','act_animal','edge_name','edge_weight','succ_rates'])\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "        animal2_filename = 'kanga'\n",
    "    else:\n",
    "        animal2_filename = animal2_fixedorder\n",
    "    \n",
    "    \n",
    "    # load the DBN - combined session and self reward for the baseline to calculate MI\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    #\n",
    "    # load edge weight data for the self \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = get_significant_edges(weighted_graphs_self,weighted_graphs_sf_self)\n",
    "    \n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data for DBN run for each days\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1.5)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==-1)]\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    \n",
    "    # calculate the linear regression and correlation metrics for tgt edges\n",
    "    for i_edge in np.arange(0,n_edges,1):\n",
    "\n",
    "        edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "        \n",
    "        actanimal_id = edge_tgt_name.split(' ')[0]\n",
    "        if actanimal_id == 'animal1':\n",
    "            actanimal = animal1_fixedorder\n",
    "        elif actanimal_id == 'animal2':\n",
    "            actanimal = animal2_fixedorder\n",
    "            \n",
    "        edgename = edge_tgt_name.split(' ')[1]\n",
    "        \n",
    "        #\n",
    "        if (timelagtype == 12) | (timelagtype == 0):\n",
    "            fromNodesID = np.array(fromNodesIDs)[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs)[i_edge]\n",
    "        else:\n",
    "            fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            succrate = succ_rate_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "            \n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "            \n",
    "            MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_tgt,\n",
    "                                          sig_edges_self, sig_edges_tgt, 150)\n",
    "            \n",
    "            sig_edges_coop_self = sig_edges_coop_self.astype('float')\n",
    "            sig_edges_coop_self[sig_edges_coop_self==0]=np.nan\n",
    "            \n",
    "            MI_coop_self_all = MI_coop_self_all*sig_edges_coop_self\n",
    "            \n",
    "            # \n",
    "            edgeweights_bhvmeasures_all = edgeweights_bhvmeasures_all.append(\n",
    "                                            {'dates':idate_name,\n",
    "                                             'act_animal':actanimal,\n",
    "                                             'edge_name':edgename,\n",
    "                                             'edge_weight':np.nanmean(MI_coop_self_all[:,fromNodesID,toNodesID]),\n",
    "                                             'succ_rates':succrate},ignore_index=True)\n",
    "\n",
    "\n",
    "# for plot   \n",
    "dependencytargets = np.unique(edgeweights_bhvmeasures_all['edge_name'])    \n",
    "nedges_forplot = np.shape(dependencytargets)[0]\n",
    "#\n",
    "fig, axs = plt.subplots(1,nedges_forplot)\n",
    "fig.set_figheight(5*1)\n",
    "fig.set_figwidth(5*nedges_forplot)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    ind_pair = (edgeweights_bhvmeasures_all['act_animal'] == animal1_fixedorder)|(edgeweights_bhvmeasures_all['act_animal'] == animal2_fixedorder)\n",
    "    \n",
    "    edgeweights_bhvmeasures_ipair = edgeweights_bhvmeasures_all[ind_pair]\n",
    "    \n",
    "    for iedge in np.arange(0,nedges_forplot,1):\n",
    "        dependencyname_forplot = dependencytargets[iedge]\n",
    "\n",
    "        edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_ipair[edgeweights_bhvmeasures_ipair['edge_name']==dependencyname_forplot]\n",
    "\n",
    "        # separating high and low success rate\n",
    "        # ind_tgt = edgeweights_bhvmeasures_forplot['succ_rates']>=0.55\n",
    "        # edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_forplot[ind_tgt]\n",
    "        \n",
    "        xxx = edgeweights_bhvmeasures_forplot['succ_rates']\n",
    "        yyy = edgeweights_bhvmeasures_forplot['edge_weight']\n",
    "        ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "        xxx = xxx[~ind_nan]\n",
    "        yyy = yyy[~ind_nan]\n",
    "\n",
    "        p_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').pvalue\n",
    "        r_reg = scipy.stats.linregress(xxx, yyy, alternative='two-sided').rvalue\n",
    "\n",
    "        # seaborn.regplot(ax=axs[iedge], data = edgeweights_bhvmeasures_forplot, \n",
    "        #                 x='succ_rates', y='edge_weight')\n",
    "        seaborn.scatterplot(ax=axs[iedge], data = edgeweights_bhvmeasures_forplot,s=100, \n",
    "                         x='succ_rates', y='edge_weight',label = animal1_fixedorder+'+'+animal2_fixedorder)\n",
    "\n",
    "        axs[iedge].set_title('all animals' ,fontsize=17)\n",
    "        axs[iedge].set_xlabel('success rate',fontsize=15)\n",
    "        axs[iedge].set_xlim([0.2,1.2])\n",
    "        axs[iedge].set_ylabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "        axs[iedge].set_ylim([-1.05,1.05])\n",
    "        axs[iedge].legend()\n",
    "        # axs[iedge].text(0.25,-0.6,'regression r='+\"{:.2f}\".format(r_reg),fontsize=10)\n",
    "        # axs[iedge].text(0.25,-0.75,'regression p='+\"{:.2f}\".format(p_reg),fontsize=10)\n",
    "        \n",
    "        \n",
    "            \n",
    "plt.tight_layout()\n",
    "\n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeMI_vs_\"+xplottype+\"_\"+measure_tgt_name+'_'+timelagname+'correlationAfterpooling_separatePairs.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a52351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_d','kanga_g','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "malenames = ['eddie','dodson','dannon','vermelho']\n",
    "subnames = ['eddie','dodson','dannon','ginger','vermelho']\n",
    "\n",
    "#\n",
    "# DBN analysis types\n",
    "# \n",
    "# 1s time lag\n",
    "timelagtype = 0 # 1, 2, 3, 12(12lagmerged), 0(all merged)\n",
    "if timelagtype == 1:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 8],\n",
    "                    [10,11],\n",
    "                    [ 9, 8],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '1slag'\n",
    "# 2s time lag\n",
    "elif timelagtype == 2:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 5, 4],\n",
    "                    [ 6, 7],\n",
    "                    [ 5, 4],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '2slag'\n",
    "# 3s time lag\n",
    "elif timelagtype == 3:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 1, 0],\n",
    "                    [ 2, 3],\n",
    "                    [ 1, 0],]\n",
    "    toNodesIDs = [[0,1],\n",
    "                  [0,1],\n",
    "                  [2,3]]\n",
    "    timelagname = '3slag'\n",
    "# 1s and 2s time lag merged\n",
    "elif timelagtype == 12:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "    toNodesIDs = [[ 0, 0],[ 1, 1],\n",
    "                  [ 0, 0],[ 1, 1],\n",
    "                  [ 2, 2],[ 3, 3],]\n",
    "    timelagname = '1and2smerged'\n",
    "# 1s and 2s and 3s time lag merged\n",
    "elif timelagtype == 0:\n",
    "    edges_target_names = [['animal1 across_pull_pull','animal2 across_pull_pull'],\n",
    "                          ['animal1 within_gaze_pull','animal2 within_gaze_pull'],\n",
    "                          ['animal1 across_pull_gaze','animal2 across_pull_gaze'],]\n",
    "    fromNodesIDs = [[ 9, 5, 1],[ 8, 4, 0],\n",
    "                    [10, 6, 2],[11, 7, 3],\n",
    "                    [ 9, 5, 1],[ 8, 4, 0],]\n",
    "    toNodesIDs = [[ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 0, 0, 0],[ 1, 1, 1],\n",
    "                  [ 2, 2, 2],[ 3, 3, 3],]\n",
    "    timelagname = 'merged'\n",
    "    \n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "# xplottype = 'meanpullnum' # 'succrate', 'meangazenum', 'meanpullnum'\n",
    "# xplotlabel = 'mean pull number' # 'successful rate', 'mean gaze number', 'mean pull number'\n",
    "\n",
    "\n",
    "# initiate the final data set\n",
    "edgeweights_bhvmeasures_all = pd.DataFrame(columns=['dates','act_animal','edge_name','edge_weight','succ_rates'])\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    if (animal2_fixedorder == 'kanga_d') | (animal2_fixedorder == 'kanga_g'):\n",
    "        animal2_filename = 'kanga'\n",
    "    else:\n",
    "        animal2_filename = animal2_fixedorder\n",
    "    \n",
    "    \n",
    "    # load the DBN - combined session and self reward for the baseline to calculate MI\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    #\n",
    "    # load edge weight data for the self \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = get_significant_edges(weighted_graphs_self,weighted_graphs_sf_self)\n",
    "    \n",
    "    # as a comparision load the edge weight data for the cooperation for combined data\n",
    "    weighted_graphs_comcoop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_comcoop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_comcoop = get_significant_edges(weighted_graphs_comcoop,weighted_graphs_sf_comcoop)\n",
    "    \n",
    "    MI_comcoop_self_all,sig_edges_comcoop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_comcoop,\n",
    "                                          sig_edges_self, sig_edges_comcoop, 150)\n",
    "    sig_edges_comcoop_self = sig_edges_comcoop_self.astype('float')\n",
    "    sig_edges_comcoop_self[sig_edges_comcoop_self==0]=np.nan\n",
    "    # MI_comcoop_self_all = MI_comcoop_self_all*sig_edges_comcoop_self\n",
    "    \n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data for DBN run for each days\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_filename+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1.5)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==-1)]\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    \n",
    "    # calculate the linear regression and correlation metrics for tgt edges\n",
    "    for i_edge in np.arange(0,n_edges,1):\n",
    "\n",
    "        edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "        \n",
    "        actanimal_id = edge_tgt_name.split(' ')[0]\n",
    "        if actanimal_id == 'animal1':\n",
    "            actanimal = animal1_fixedorder\n",
    "        elif actanimal_id == 'animal2':\n",
    "            actanimal = animal2_fixedorder\n",
    "            \n",
    "        edgename = edge_tgt_name.split(' ')[1]\n",
    "        \n",
    "        #\n",
    "        if (timelagtype == 12) | (timelagtype == 0):\n",
    "            fromNodesID = np.array(fromNodesIDs)[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs)[i_edge]\n",
    "        else:\n",
    "            fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "            toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            succrate = succ_rate_all_dates[sorting_tgt_df.index[idate]][0]\n",
    "            \n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "            \n",
    "            MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_tgt,\n",
    "                                          sig_edges_self, sig_edges_tgt, 150)\n",
    "            \n",
    "            sig_edges_coop_self = sig_edges_coop_self.astype('float')\n",
    "            sig_edges_coop_self[sig_edges_coop_self==0]=np.nan\n",
    "            \n",
    "            # MI_coop_self_all = MI_coop_self_all*sig_edges_coop_self\n",
    "            \n",
    "            # \n",
    "            edgeweights_bhvmeasures_all = edgeweights_bhvmeasures_all.append(\n",
    "                                            {'dates':idate_name,\n",
    "                                             'act_animal':actanimal,\n",
    "                                             'edge_name':edgename,\n",
    "                                             # 'edge_weight':np.nanmean(MI_coop_self_all[:,fromNodesID,toNodesID]),\n",
    "                                             'edge_weight':np.nanmean(MI_comcoop_self_all[:,fromNodesID,toNodesID]),\n",
    "                                             'succ_rates':succrate},ignore_index=True)\n",
    "\n",
    "\n",
    "# for plot\n",
    "dependencytargets = np.unique(edgeweights_bhvmeasures_all['edge_name'])    \n",
    "nedges_forplot = np.shape(dependencytargets)[0]\n",
    "#\n",
    "fig, axs = plt.subplots(3,nedges_forplot)\n",
    "fig.set_figheight(5*3)\n",
    "fig.set_figwidth(5*nedges_forplot)\n",
    "\n",
    "    \n",
    "for iedge in np.arange(0,nedges_forplot,1):\n",
    "    dependencyname_forplot = dependencytargets[iedge]\n",
    "\n",
    "    edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_all[edgeweights_bhvmeasures_all['edge_name']==dependencyname_forplot]\n",
    "\n",
    "    # separating high and low success rate\n",
    "    # ind_tgt = edgeweights_bhvmeasures_forplot['succ_rates']>=0.55\n",
    "    # edgeweights_bhvmeasures_forplot = edgeweights_bhvmeasures_forplot[ind_tgt]\n",
    "\n",
    "\n",
    "    # Calculate mean and standard error for each animal\n",
    "    grouped = edgeweights_bhvmeasures_forplot.groupby('act_animal').agg(\n",
    "        edge_weight_mean=('edge_weight', 'mean'),\n",
    "        edge_weight_std=('edge_weight', 'std'),\n",
    "        edge_weight_count=('edge_weight', 'count'),\n",
    "        succ_rates_mean=('succ_rates', 'mean'),\n",
    "        succ_rates_std=('succ_rates', 'std'),\n",
    "        succ_rates_count=('succ_rates', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate 95% CI for edge_weight and succ_rates\n",
    "    confidence_level = 0.95\n",
    "    degrees_freedom_edge_weight = grouped['edge_weight_count'] - 1\n",
    "    degrees_freedom_succ_rates = grouped['succ_rates_count'] - 1\n",
    "\n",
    "    # t-critical value for 95% confidence\n",
    "    t_critical_edge_weight = st.t.ppf((1 + confidence_level) / 2, degrees_freedom_edge_weight)\n",
    "    t_critical_succ_rates = st.t.ppf((1 + confidence_level) / 2, degrees_freedom_succ_rates)\n",
    "\n",
    "    # Calculate the margin of error for each mean\n",
    "    grouped['edge_weight_margin_error'] = t_critical_edge_weight * (grouped['edge_weight_std'] / np.sqrt(grouped['edge_weight_count']))\n",
    "    grouped['succ_rates_margin_error'] = t_critical_succ_rates * (grouped['succ_rates_std'] / np.sqrt(grouped['succ_rates_count']))\n",
    "\n",
    "\n",
    "    # Calculate Pearson correlation coefficient and p-value\n",
    "    # r_value, p_value = st.pearsonr(grouped['succ_rates_mean'],grouped['edge_weight_mean'])\n",
    "    # r_value, p_value = st.spearmanr(grouped['succ_rates_mean'],grouped['edge_weight_mean'])\n",
    "    # r_value, p_value = st.kendalltau(grouped['succ_rates_mean'],grouped['edge_weight_mean'])\n",
    "    _, _, r_value, p_value, std_err = st.linregress(grouped['edge_weight_mean'],grouped['succ_rates_mean'])\n",
    "\n",
    "    # Plot each animal in different color\n",
    "    for i, row in grouped.iterrows():\n",
    "        axs[0,iedge].errorbar(\n",
    "            x = row['edge_weight_mean'], \n",
    "            y = row['succ_rates_mean'],\n",
    "            xerr=row['edge_weight_margin_error'], \n",
    "            yerr=row['succ_rates_margin_error'], \n",
    "            fmt='o', label=row['act_animal'], \n",
    "            ecolor='lightgray', elinewidth=2, capsize=3\n",
    "        )\n",
    "\n",
    "    # Add regression line based on mean values\n",
    "    seaborn.regplot(ax = axs[0,iedge],\n",
    "        x='edge_weight_mean', \n",
    "        y='succ_rates_mean', \n",
    "        data=grouped, \n",
    "        ci=None, scatter=False, color=\"black\", line_kws={\"linestyle\":\"--\"}\n",
    "    )\n",
    "\n",
    "    axs[0,iedge].set_title('all animals' ,fontsize=17)\n",
    "    axs[0,iedge].set_ylabel('success rate',fontsize=15)\n",
    "    axs[0,iedge].set_ylim([0.2,1.2])\n",
    "    axs[0,iedge].set_xlabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "    axs[0,iedge].set_xlim([-1.05,1.05])\n",
    "    axs[0,iedge].legend()\n",
    "    \n",
    "    # Display correlation coefficient (r) and p-value\n",
    "    axs[0,iedge].text(-0.9,0.25, f\"regression: r = {r_value:.2f}, p = {p_value:.3f}\")\n",
    "\n",
    "    \n",
    "    # plot male female\n",
    "    # male\n",
    "    grouped_male = grouped[np.isin(grouped['act_animal'],malenames)]\n",
    "    _, _, r_value_m, p_value_m, std_err_m = st.linregress(grouped_male['edge_weight_mean'],grouped_male['succ_rates_mean'])\n",
    "    # Plot regression line based on mean values\n",
    "    seaborn.regplot(ax=axs[1,iedge],x='edge_weight_mean', y='succ_rates_mean', \n",
    "                    data=grouped_male, ci=None, color=\"blue\",label='male')\n",
    "    # Add error bars for each point\n",
    "    axs[1,iedge].errorbar(\n",
    "        grouped_male['edge_weight_mean'], \n",
    "        grouped_male['succ_rates_mean'],\n",
    "        xerr=grouped_male['edge_weight_std'], \n",
    "        yerr=grouped_male['succ_rates_std'], \n",
    "        fmt='o', color=\"blue\", ecolor='lightgray', elinewidth=2, capsize=3\n",
    "    )\n",
    "    # female\n",
    "    grouped_female = grouped[~np.isin(grouped['act_animal'],malenames)]\n",
    "    _, _, r_value_f, p_value_f, std_err_f = st.linregress(grouped_female['edge_weight_mean'],grouped_female['succ_rates_mean'])\n",
    "    # Plot regression line based on mean values\n",
    "    seaborn.regplot(ax=axs[1,iedge],x='edge_weight_mean', y='succ_rates_mean', \n",
    "                    data=grouped_female, ci=None, color=\"red\",label='female')\n",
    "    # Add error bars for each point\n",
    "    axs[1,iedge].errorbar(\n",
    "        grouped_female['edge_weight_mean'], \n",
    "        grouped_female['succ_rates_mean'],\n",
    "        xerr=grouped_female['edge_weight_std'], \n",
    "        yerr=grouped_female['succ_rates_std'], \n",
    "        fmt='o', color=\"red\", ecolor='lightgray', elinewidth=2, capsize=3\n",
    "    )\n",
    "    \n",
    "    axs[1,iedge].set_title('male female' ,fontsize=17)\n",
    "    axs[1,iedge].set_ylabel('success rate',fontsize=15)\n",
    "    axs[1,iedge].set_ylim([0.2,1.2])\n",
    "    axs[1,iedge].set_xlabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "    axs[1,iedge].set_xlim([-1.05,1.05])\n",
    "    axs[1,iedge].legend()\n",
    "    # Display correlation coefficient (r) and p-value\n",
    "    axs[1,iedge].text(-0.9,0.25, f\"male regression: r = {r_value_m:.2f}, p = {p_value_m:.3f}\")\n",
    "    axs[1,iedge].text(-0.9,0.3, f\"female regression: r = {r_value_f:.2f}, p = {p_value_f:.3f}\")\n",
    "    \n",
    "    \n",
    "    # plot sub dom\n",
    "    # sub\n",
    "    grouped_sub = grouped[np.isin(grouped['act_animal'],subnames)]\n",
    "    _, _, r_value_m, p_value_m, std_err_m = st.linregress(grouped_sub['edge_weight_mean'],grouped_sub['succ_rates_mean'])\n",
    "    # Plot regression line based on mean values\n",
    "    seaborn.regplot(ax=axs[2,iedge],x='edge_weight_mean', y='succ_rates_mean', \n",
    "                    data=grouped_sub, ci=None, color=\"blue\",label='sub')\n",
    "    # Add error bars for each point\n",
    "    axs[2,iedge].errorbar(\n",
    "        grouped_sub['edge_weight_mean'], \n",
    "        grouped_sub['succ_rates_mean'],\n",
    "        xerr=grouped_sub['edge_weight_std'], \n",
    "        yerr=grouped_sub['succ_rates_std'], \n",
    "        fmt='o', color=\"blue\", ecolor='lightgray', elinewidth=2, capsize=3\n",
    "    )\n",
    "    # dom\n",
    "    grouped_dom = grouped[~np.isin(grouped['act_animal'],subnames)]\n",
    "    _, _, r_value_f, p_value_f, std_err_f = st.linregress(grouped_dom['edge_weight_mean'],grouped_dom['succ_rates_mean'])\n",
    "    # Plot regression line based on mean values\n",
    "    seaborn.regplot(ax=axs[2,iedge],x='edge_weight_mean', y='succ_rates_mean', \n",
    "                    data=grouped_dom, ci=None, color=\"red\",label='dom')\n",
    "    # Add error bars for each point\n",
    "    axs[2,iedge].errorbar(\n",
    "        grouped_dom['edge_weight_mean'], \n",
    "        grouped_dom['succ_rates_mean'],\n",
    "        xerr=grouped_dom['edge_weight_std'], \n",
    "        yerr=grouped_dom['succ_rates_std'], \n",
    "        fmt='o', color=\"red\", ecolor='lightgray', elinewidth=2, capsize=3\n",
    "    )\n",
    "    \n",
    "    axs[2,iedge].set_title('sub dom' ,fontsize=17)\n",
    "    axs[2,iedge].set_ylabel('success rate',fontsize=15)\n",
    "    axs[2,iedge].set_ylim([0.2,1.2])\n",
    "    axs[2,iedge].set_xlabel(dependencyname_forplot+\" Modulation index vs. SR\",fontsize=15)\n",
    "    axs[2,iedge].set_xlim([-1.05,1.05])\n",
    "    axs[2,iedge].legend()\n",
    "    # Display correlation coefficient (r) and p-value\n",
    "    axs[2,iedge].text(-0.9,0.25, f\"sub regression: r = {r_value_m:.2f}, p = {p_value_m:.3f}\")\n",
    "    axs[2,iedge].text(-0.9,0.3, f\"dom regression: r = {r_value_f:.2f}, p = {p_value_f:.3f}\")\n",
    "            \n",
    "plt.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeMI_vs_\"+xplottype+\"_\"+measure_tgt_name+'_'+timelagname+'correlationAfterpooling_separateIndividuals.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fa56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c034114a",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "####  plot the coorelation between pull time, and social gaze time\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "# initiate the final data set\n",
    "pull_gaze_time_corr_mean_all = np.zeros((nanimalpairs*2,2))\n",
    "\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    \n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    # \n",
    "    pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "    #\n",
    "    gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "    gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "    gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    if not mergetempRos:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "            \n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    elif minmaxfullSampSize:\n",
    "        samplingsizes_name = ['full_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "    #\n",
    "    # only load one set of analysis parameter\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]  \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)].keys())\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "    \n",
    "    #\n",
    "    # initiate the final data set\n",
    "    within_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "    across_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "    within_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "    across_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "    \n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "\n",
    "        DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "        #\n",
    "        if 0:\n",
    "            # single behavioral events  \n",
    "            # pull1_t0 and gaze1_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "            rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "            # pull2_t0 and gaze1_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "            rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "            # pull2_t0 and gaze2_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "            rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "            # pull1_t0 and gaze2_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "            rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "        #\n",
    "        if 0:\n",
    "            # single behavioral events with synced pull\n",
    "            xxx1_1 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['pull1_t1'])==1))*1\n",
    "            xxx1_2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['pull2_t1'])==1))*1 \n",
    "            # pull1_t0 and gaze1_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "            rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "            # pull2_t0 and gaze1_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "            rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "            # pull2_t0 and gaze2_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "            rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "            # pull1_t0 and gaze2_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "            rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "        #\n",
    "        if 1:\n",
    "            # paired behavioral events\n",
    "            xxx1_1 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['pull1_t1'])==1))*1\n",
    "            xxx1_2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['pull2_t1'])==1))*1            \n",
    "            # pull1_t1 and gaze1_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "            #\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr1_spe = np.nan\n",
    "                pp1_spe = np.nan            \n",
    "            # pull2_t0 and gaze1_t1\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr2_spe = np.nan\n",
    "                pp2_spe = np.nan        \n",
    "            # pull2_t1 and gaze2_t0\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr3_spe = np.nan\n",
    "                pp3_spe = np.nan           \n",
    "            # pull1_t0 and gaze2_t1\n",
    "            xxx1 = xxx1_1 + xxx1_2\n",
    "            xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "            try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            except:\n",
    "                rr4_spe = np.nan\n",
    "                pp4_spe = np.nan\n",
    "  \n",
    "            \n",
    "        #    \n",
    "        within_pull_gaze_time_corr_all_ipair[idate_name] = [rr1_spe,rr3_spe]\n",
    "        across_pull_gaze_time_corr_all_ipair[idate_name] = [rr2_spe,rr4_spe]\n",
    "        within_pull_gaze_time_corP_all_ipair[idate_name] = [pp1_spe,pp3_spe]\n",
    "        across_pull_gaze_time_corP_all_ipair[idate_name] = [pp2_spe,pp4_spe]\n",
    "    \n",
    "    # organize the data to the summarizing mean variables\n",
    "    pull_gaze_time_corr_mean_all[[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "    pull_gaze_time_corr_mean_all[[ianimalpair*2,ianimalpair*2+1],1]=np.nanmean(pd.DataFrame(across_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "\n",
    "    \n",
    "    # plot each animal pair first\n",
    "    # figure initiate\n",
    "    fig, axs = plt.subplots(2,2)\n",
    "    fig.set_figheight(5*2)\n",
    "    fig.set_figwidth(10*2)\n",
    "    #\n",
    "    plottype_names = ['within animal gaze to pull, '+animal1_fixedorder,\n",
    "                      'across animal pull to gaze, '+animal1_fixedorder,\n",
    "                      'within animal gaze to pull, '+animal2_fixedorder,\n",
    "                      'across animal pull to gaze, '+animal2_fixedorder]\n",
    "    plotCorrs_pooled = [\n",
    "                        np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                        np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                        np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                        np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                       ]\n",
    "    #\n",
    "    for iplot in np.arange(0,4,1):\n",
    "        #\n",
    "        plottype_name = plottype_names[iplot]\n",
    "        plotCorrs = plotCorrs_pooled[iplot]\n",
    "        \n",
    "        # plot \n",
    "        axs.flatten()[iplot].plot(np.arange(0,ndates_tgt,1),plotCorrs,'ko',markersize=10)\n",
    "        #\n",
    "        axs.flatten()[iplot].set_title(plottype_name,fontsize=16)\n",
    "        axs.flatten()[iplot].set_ylabel('time coorelation with pull <-> pull',fontsize=13)\n",
    "        axs.flatten()[iplot].set_ylim([-1.1,1.1])\n",
    "        axs.flatten()[iplot].set_xlim([-0.5,ndates_tgt-0.5])\n",
    "        #\n",
    "        if iplot > 1:\n",
    "            axs.flatten()[iplot].set_xticks(np.arange(0,ndates_tgt,1))\n",
    "            axs.flatten()[iplot].set_xticklabels(dates_list_tgt, rotation=90,fontsize=10)\n",
    "        else:\n",
    "            axs.flatten()[iplot].set_xticklabels('')\n",
    "        #\n",
    "        # tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "        tasktypes = ['coop(3s)','coop(2s)','coop(1.5s)','coop(1s)']\n",
    "        taskswitches = np.where(np.array(sorting_tgt_df['coopthres'])[1:]-np.array(sorting_tgt_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs.flatten()[iplot].plot([taskswitch,taskswitch],[-1.1,1.1],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs.flatten()[iplot].text(taskswitch+0.25,-0.9,tasktypes[itaskswitch],fontsize=10)\n",
    "        axs.flatten()[iplot].plot([0,ndates_tgt],[0,0],'k--')\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'       \n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'syncedpulltime_pullgazetime_correlation_'+animal1_fixedorder+animal2_fixedorder+'.pdf')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the summarizing figure\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "dependencytargets = ['within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency\n",
    "pull_gaze_time_corr_tgt_all = pull_gaze_time_corr_mean_all \n",
    "measure_tgt_name = 'time point correlation' \n",
    "# \n",
    "pull_gaze_time_corr_tgt_all_df = pd.DataFrame(pull_gaze_time_corr_tgt_all)\n",
    "pull_gaze_time_corr_tgt_all_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals' ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[0].set_ylim([-1,1])\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "pull_gaze_time_corr_tgt_male_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_df['type'] = 'male'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_df['type'] = 'female'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_male_df,pull_gaze_time_corr_tgt_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female' ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[1].set_ylim([-1,1])\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "pull_gaze_time_corr_tgt_sub_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_df['type'] = 'subordinate'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_df = pd.DataFrame(pull_gaze_time_corr_tgt_all[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_df['type'] = 'dominant'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_sub_df,pull_gaze_time_corr_tgt_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom' ,fontsize=24)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[2].set_ylim([-1,1])\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"syncedpulltime_pullgazetime_correlation_summaryplot.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fdb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22cbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299ce69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55324239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659e669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0321a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
