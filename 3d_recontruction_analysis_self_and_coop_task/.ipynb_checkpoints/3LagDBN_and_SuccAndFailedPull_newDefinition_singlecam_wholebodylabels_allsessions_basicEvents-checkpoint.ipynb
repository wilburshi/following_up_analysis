{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the all sessions separately\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, DBN is run with sucessful and failed pulls seperately\n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db8a5c",
   "metadata": {},
   "source": [
    "### function - cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c32db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.xcorr import xcorr\n",
    "from ana_functions.xcorr import correlagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 5*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 0 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "            \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "            \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          # \"20220912\",\n",
    "                          \"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                          \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                          \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                          \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                          \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                          \"20230117\",\"20230118\",\"20230124\",\n",
    "                          # \"20230126\",\n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    # 18.10, \n",
    "                                     0.00, 33.03,  6.50,  0.00, \n",
    "                                     2.80, 27.80, 27.90, 27.00,  \n",
    "                                    51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                    26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                     0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                     0.00,  0.00,  0.00, \n",
    "                                     # 0.00,  \n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20220915\",\"20220920\",\"20221010\",\"20230208\", # SR\n",
    "                          \n",
    "                          \"20230321\",\"20230322\",\"20230323\",\"20230324\",\"20230412\",\"20230413\", # trained MC\n",
    "                          \n",
    "                          \"20230117\",\"20230118\",\"20230124\", # NV \n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                     0.00, 33.03,  6.50,  0.00, \n",
    "                                     \n",
    "                                     20.5,  21.4,  21.0,  24.5,  20.5,  26.6,\n",
    "                    \n",
    "                                     0.00,  0.00,  0.00,  \n",
    "                                  ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "    \n",
    "# eddie sparkle\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20221122\",  \"20221125\",  \n",
    "                          \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                          \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                          \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                          \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                          \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                      8.00,  38.00, \n",
    "                                      9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                      9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                      8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                      7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                      4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20221122\",  \"20221125\",  # sr\n",
    "                \n",
    "                          \"20230410\",  \"20230411\",  \"20230412\",  \"20230413\",  \"20230616\", # trained MC\n",
    "                \n",
    "                          \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\", # nv\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                      8.00, 38.00, \n",
    "                \n",
    "                                      23.2,  23.0,  21.2,  25.0,  23.0,   \n",
    "                \n",
    "                                      4.50,  9.30, 25.50, 20.40, 21.30,\n",
    "                \n",
    "                                  ] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "    \n",
    "# ginger kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second \n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          #\"20230213\",\n",
    "                          \"20230214\",\"20230216\",\n",
    "                          \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                          \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                          \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                          \"20230323\",\"20230412\",\"20230413\",\"20230517\",\n",
    "                          \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                    # 0.00, \n",
    "                                     0.00, 48.00, \n",
    "                                    23.00, 28.50, 34.00, 25.50, \n",
    "                                    25.50, 31.50, 28.00, 30.50,\n",
    "                                     0.00,  0.00,  0.00,  0.00, \n",
    "                                     0.00,  0.00,  0.00,  0.00, \n",
    "                                     0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                  ] # in second \n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20230214\",   \"20230216\",  # SR\n",
    "                          \n",
    "                          \"20230614\",   \"20230615\",  \"20230711\",\"20230712\", # trained MC\n",
    "                \n",
    "                          \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\", # nv  \n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                     0.00, 48.00, \n",
    "                                    \n",
    "                                     0.00,  0.00,  54.5,  24.7,\n",
    "                \n",
    "                                     0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                  ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                              \n",
    "                              ] # in second \n",
    "    elif do_bestsession: \n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20230718\",\"20230720\",\"20230914\",\"20230726\",\"20230727\",\"20230809\",\n",
    "                          \"20230810\",\"20230811\",\"20230814\",\"20230816\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                          \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                          \"20231013\",\"20231020\",\"20231024\",\"20231025\",\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                        0,    0,    0, 32.2, 27.2, 37.5,\n",
    "                                     21.0, 21.5, 19.8, 32.0,    0,    0,   0, \n",
    "                                        0,    0,    0,    0,    0,    0,\n",
    "                                        0,    0,    0,    0, \n",
    "                                  ] # in second \n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20230718\",\"20230720\",\"20230914\", # sr\n",
    "                \n",
    "                          \"20231030\",\"20231031\",\"20231101\",\"20231102\",\"20240304\",\"20240305\", # trained MC\n",
    "                \n",
    "                          \"20231011\",\"20231013\",\"20231020\",\"20231024\",\"20231025\", # nv\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                       0,    0,    0,\n",
    "                \n",
    "                                    18.2, 14.0, 15.8, 15.2, 16.3, 37.9,\n",
    "                \n",
    "                                       0,    0,    0,    0,    0, \n",
    "                                  ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "# Koala Vermelho\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                     \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                               \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20231222\",\"20231226\",\"20231227\",  \"20231229\",\"20231230\",\n",
    "                          \"20231231\",\"20240102\",\"20240104-2\",\"20240105\",\"20240108\",\n",
    "                          \"20240109\",\"20240115\",\"20240116\",  \"20240117\",\"20240118\",\"20240119\",\n",
    "                          \"20240207\",\"20240208\",\"20240209\",  \"20240212\",\"20240213\",\n",
    "                          \"20240214\",\"20240215\",\"20240216\",  \n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    21.5,  0.00,  0.00,  0.00,  0.00, \n",
    "                                    0.00,  12.2,  0.00,  18.8,  31.2,  \n",
    "                                    32.5,  0.00,  50.0,  0.00,  37.5,  29.5,\n",
    "                                    58.5,  72.0,  0.00,  71.5,  70.5,\n",
    "                                    86.8,  94.0,  65.0, \n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20231222\",\"20231226\",\"20231227\", # SR\n",
    "                          \n",
    "                          \"20240220\",\"20240222\",\"20240223\",\"20240226\", # trained MC\n",
    "                 \n",
    "                          \"20240214\",\"20240215\",\"20240216\",  # NV\n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    21.5,  0.00,  0.00, \n",
    "                                    \n",
    "                                    68.8,  43.8,  13.2,  47.5,\n",
    "                \n",
    "                                    86.8,  94.0,  65.0, \n",
    "                                  ] # in second\n",
    "\n",
    "    animal1_fixedorder = ['koala']\n",
    "    animal2_fixedorder = ['vermelho']\n",
    "\n",
    "    animal1_filename = \"Koala\"\n",
    "    animal2_filename = \"Vermelho\"\n",
    "       \n",
    "#    \n",
    "#dates_list = [\"20221128\"]\n",
    "#session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "        try: \n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "            video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "            video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "        pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "        owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "        mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "        mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "\n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 1:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                              lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                              animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                              animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                              video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aada694",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "\n",
    "pulltypes = ['succpull','failedpull']\n",
    "npulltypes = np.shape(pulltypes)[0]\n",
    "\n",
    "prepare_input_data = 1\n",
    "\n",
    "DBN_input_data_alltypes = dict.fromkeys(pulltypes, [])\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    for ipulltype in np.arange(0,npulltypes,1):\n",
    "        pulltype = pulltypes[ipulltype]\n",
    "\n",
    "        DBN_input_data_alltypes[pulltype] = dict.fromkeys(dates_list, [])\n",
    "            \n",
    "        for idate in np.arange(0,ndates,1):\n",
    "            date_tgt = dates_list[idate]\n",
    "            session_start_time = session_start_times[idate]\n",
    "                 \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1    \n",
    "            \n",
    "\n",
    "            # load behavioral event results\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            #\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # redefine the totalsess_time for the length of each recording (NOT! remove the session_start_time)\n",
    "            totalsess_time = int(np.ceil(np.shape(look_at_other_or_not_merge['dodson'])[0]/fps))\n",
    "        \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            # time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            # time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']   \n",
    "\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            if pulltype == 'succpull':\n",
    "                time_point_pull1 = np.round(successful_pull1,1)\n",
    "                time_point_pull2 = np.round(successful_pull2,1)\n",
    "            elif pulltype == 'failedpull':\n",
    "                time_point_pull1 = np.round(failed_pull1,1)\n",
    "                time_point_pull2 = np.round(failed_pull2,1)\n",
    "            \n",
    "            if mergetempRos:\n",
    "                temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "                # use bhv event to decide temporal resolution\n",
    "                #\n",
    "                #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "\n",
    "            ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "            # try different temporal resolutions\n",
    "            for temp_resolu in temp_resolus:\n",
    "                bhv_df = []\n",
    "\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                else:\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "                if len(bhv_df)==0:\n",
    "                    bhv_df = bhv_df_itr\n",
    "                else:\n",
    "                    bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                    bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "                DBN_input_data_alltypes[pulltype][date_tgt] = bhv_df\n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d9698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9463585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c9eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25754dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c00ff539",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "###  plot the coorelation between pull time, and social gaze time\n",
    "#### separate the successful and failed pulls\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "\n",
    "do_onlyLearningsess = 0 # only consider pairs from the learning analysis\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "if do_onlyLearningsess:\n",
    "    animal1_fixedorders = ['eddie','dodson','ginger',]\n",
    "    animal2_fixedorders = ['sparkle','scorch','kanga_2',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "succorfailpulls = ['succpull','failedpull']\n",
    "nsuccorfail = np.shape(succorfailpulls)[0]\n",
    "\n",
    "do_synedpulltime_pullgazetime = 0 # correlation between pairs of bhv events\n",
    "do_pulltime_gazetime = 1 # correlation between single bhv events\n",
    "\n",
    "# initiate the final data set\n",
    "pull_gaze_time_corr_mean_all = dict.fromkeys(succorfailpulls,[])\n",
    "\n",
    "\n",
    "for isuccorfail in np.arange(0,nsuccorfail,1):\n",
    "    \n",
    "    succorfailpull = succorfailpulls[isuccorfail]\n",
    "    \n",
    "    pull_gaze_time_corr_mean_all[succorfailpull] = np.zeros((nanimalpairs*2,2))\n",
    "\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "        \n",
    "        if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "            animal2_filename = 'kanga'\n",
    "        else:\n",
    "            animal2_filename = animal2_fixedorder\n",
    "\n",
    "        # load the basic behavioral measures\n",
    "        # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            owgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            owgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            mtgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            mtgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            pull1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            pull2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            bhv_intv_all_dates = pickle.load(f)\n",
    "        # \n",
    "        pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "        #\n",
    "        gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "        gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "        gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "        # load the DBN related analysis\n",
    "        # load data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "        #\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        #\n",
    "        # load data for successful and failed pulls\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "        #\n",
    "        # re-organize the target dates\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "        #\n",
    "        # sort the data based on task type and dates\n",
    "        dates_list = list(DBN_input_data_alltypes.keys())\n",
    "        sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "        sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "        #\n",
    "        # only select the targeted dates\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]        \n",
    "        sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "        # sorting_tgt_df = sorting_df\n",
    "        dates_list_tgt = sorting_tgt_df['dates']\n",
    "        dates_list_tgt = np.array(dates_list_tgt)\n",
    "        #\n",
    "        ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "        #FigX_DBN_moreConditions.pdf\n",
    "        # initiate the final data set\n",
    "        within_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        across_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        within_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        across_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            DBN_input_data_idate_succfail = DBN_input_data_alltypes_succfail[succorfailpull][idate_name]\n",
    "            DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "            #\n",
    "            if 0:\n",
    "                # single behavioral events  \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            #\n",
    "            if do_pulltime_gazetime: # this is the best!\n",
    "                kernel_size = 3\n",
    "                # single behavioral events  \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                # try:\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                xxx1 = np.where(xxx1==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx1 = np.exp(log_dens)\n",
    "                #\n",
    "                xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                xxx2 = np.where(xxx2==1)[0]\n",
    "                kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                xxx2 = np.exp(log_dens)\n",
    "                #\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "               #  except:\n",
    "                #     rr1_spe = np.nan\n",
    "               #      pp1_spe = np.nan          \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr2_spe = np.nan\n",
    "                    pp2_spe = np.nan\n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr3_spe = np.nan\n",
    "                    pp3_spe = np.nan\n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr4_spe = np.nan\n",
    "                    pp4_spe = np.nan\n",
    "            #\n",
    "            if 0:\n",
    "                # single behavioral events with synced pull\n",
    "                xxx1_1 = ((np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)&(np.array(DBN_input_data_idate_succfail['pull1_t1'])==1))*1\n",
    "                xxx1_2 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)&(np.array(DBN_input_data_idate_succfail['pull2_t1'])==1))*1 \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            #\n",
    "            if do_synedpulltime_pullgazetime:\n",
    "                # paired behavioral events\n",
    "                # pull1_t1 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "                #\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "                # pull2_t0 and gaze1_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                # pull2_t1 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)     \n",
    "                # pull1_t0 and gaze2_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            #\n",
    "            if 0:\n",
    "                kernel_size = 3\n",
    "                # paired behavioral events\n",
    "                # pull1_t1 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "                #\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr1_spe = np.nan\n",
    "                    pp1_spe = np.nan          \n",
    "                # pull2_t0 and gaze1_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr2_spe = np.nan\n",
    "                    pp2_spe = np.nan\n",
    "                # pull2_t1 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr3_spe = np.nan\n",
    "                    pp3_spe = np.nan\n",
    "                # pull1_t0 and gaze2_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr4_spe = np.nan\n",
    "                    pp4_spe = np.nan\n",
    "\n",
    "\n",
    "            #    \n",
    "            within_pull_gaze_time_corr_all_ipair[idate_name] = [rr1_spe,rr3_spe]\n",
    "            across_pull_gaze_time_corr_all_ipair[idate_name] = [rr2_spe,rr4_spe]\n",
    "            within_pull_gaze_time_corP_all_ipair[idate_name] = [pp1_spe,pp3_spe]\n",
    "            across_pull_gaze_time_corP_all_ipair[idate_name] = [pp2_spe,pp4_spe]\n",
    "\n",
    "        # organize the data to the summarizing mean variables\n",
    "        pull_gaze_time_corr_mean_all[succorfailpull][[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "        pull_gaze_time_corr_mean_all[succorfailpull][[ianimalpair*2,ianimalpair*2+1],1]=np.nanmean(pd.DataFrame(across_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "\n",
    "\n",
    "        # plot each animal pair first\n",
    "        # figure initiate\n",
    "        fig, axs = plt.subplots(2,2)\n",
    "        fig.set_figheight(5*2)\n",
    "        fig.set_figwidth(10*2)\n",
    "        #\n",
    "        plottype_names = ['within animal gaze to pull, '+animal1_fixedorder,\n",
    "                          'across animal pull to gaze, '+animal1_fixedorder,\n",
    "                          'within animal gaze to pull, '+animal2_fixedorder,\n",
    "                          'across animal pull to gaze, '+animal2_fixedorder]\n",
    "        plotCorrs_pooled = [\n",
    "                            np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                            np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                            np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                            np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                           ]\n",
    "        #\n",
    "        for iplot in np.arange(0,4,1):\n",
    "            #\n",
    "            plottype_name = plottype_names[iplot]\n",
    "            plotCorrs = plotCorrs_pooled[iplot]\n",
    "\n",
    "            # plot \n",
    "            axs.flatten()[iplot].plot(np.arange(0,ndates_tgt,1),plotCorrs,'ko',markersize=10)\n",
    "            #\n",
    "            axs.flatten()[iplot].set_title(plottype_name,fontsize=16)\n",
    "            axs.flatten()[iplot].set_ylabel('time coorelation with pull <-> pull',fontsize=13)\n",
    "            axs.flatten()[iplot].set_ylim([-1.1,1.1])\n",
    "            axs.flatten()[iplot].set_xlim([-0.5,ndates_tgt-0.5])\n",
    "            #\n",
    "            if iplot > 1:\n",
    "                axs.flatten()[iplot].set_xticks(np.arange(0,ndates_tgt,1))\n",
    "                axs.flatten()[iplot].set_xticklabels(dates_list_tgt, rotation=90,fontsize=10)\n",
    "            else:\n",
    "                axs.flatten()[iplot].set_xticklabels('')\n",
    "            #\n",
    "            # tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "            tasktypes = ['coop(3s)','coop(2s)','coop(1.5s)','coop(1s)']\n",
    "            taskswitches = np.where(np.array(sorting_tgt_df['coopthres'])[1:]-np.array(sorting_tgt_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "            for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "                taskswitch = taskswitches[itaskswitch]\n",
    "                axs.flatten()[iplot].plot([taskswitch,taskswitch],[-1.1,1.1],'k--')\n",
    "            taskswitches = np.concatenate(([0],taskswitches))\n",
    "            for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "                taskswitch = taskswitches[itaskswitch]\n",
    "                axs.flatten()[iplot].text(taskswitch+0.25,-0.9,tasktypes[itaskswitch],fontsize=10)\n",
    "            axs.flatten()[iplot].plot([0,ndates_tgt],[0,0],'k--')\n",
    "\n",
    "        savefigs = 1\n",
    "        if savefigs:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'       \n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            plt.savefig(figsavefolder+succorfailpull+'_pulltime_gazetime_correlation_'+animal1_fixedorder+animal2_filename+'.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the summarizing figure\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "dependencytargets = ['within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency, separate successful and failed pull\n",
    "pull_gaze_time_corr_tgt_succpull = pull_gaze_time_corr_mean_all['succpull']\n",
    "pull_gaze_time_corr_tgt_failpull = pull_gaze_time_corr_mean_all['failedpull']\n",
    "measure_tgt_name = 'time point correlation' \n",
    "# \n",
    "pull_gaze_time_corr_tgt_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull)\n",
    "pull_gaze_time_corr_tgt_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_succpull_df['type'] = 'succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull)\n",
    "pull_gaze_time_corr_tgt_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_failpull_df['type'] = 'failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_succpull_df,pull_gaze_time_corr_tgt_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "# seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.violinplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "# seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[0].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals' ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[0].set_ylim([-0.5,0.8])\n",
    "axs.ravel()[0].legend(fontsize=18)\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "pull_gaze_time_corr_tgt_male_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_succpull_df['type'] = 'male succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_male_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_failpull_df['type'] = 'male failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_succpull_df['type'] = 'female succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_failpull_df['type'] = 'female failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_male_succpull_df,pull_gaze_time_corr_tgt_male_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_female_succpull_df,pull_gaze_time_corr_tgt_female_failpull_df,])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "# seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.violinplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "# seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[1].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female' ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[1].set_ylim([-0.5,0.8])\n",
    "axs.ravel()[1].legend(fontsize=18)\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df['type'] = 'subordinate succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df['type'] = 'subordinate failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df['type'] = 'dominant succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df['type'] = 'dominant failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_sub_succpull_df,pull_gaze_time_corr_tgt_sub_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_dom_succpull_df,pull_gaze_time_corr_tgt_dom_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "# seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.violinplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "# seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[2].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom' ,fontsize=34)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[2].set_ylim([-0.5,0.8])\n",
    "axs.ravel()[2].legend(fontsize=18)\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if  do_synedpulltime_pullgazetime:\n",
    "        plt.savefig(figsavefolder+\"succorfailpulls_syncedpulltime_pullgazetime_correlation_summaryplot.pdf\")\n",
    "    elif do_pulltime_gazetime:       \n",
    "        plt.savefig(figsavefolder+\"succorfailpulls_pulltime_gazetime_correlation_summaryplot.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94605760",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_1samp(pull_gaze_time_corr_tgt_succpull_df['across_pullgaze'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.wilcoxon(pull_gaze_time_corr_tgt_failpull_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(pull_gaze_time_corr_tgt_failpull_df['across_pullgaze'],pull_gaze_time_corr_tgt_succpull_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(pull_gaze_time_corr_tgt_female_failpull_df['within_gazepull'],pull_gaze_time_corr_tgt_female_succpull_df['within_gazepull'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(pull_gaze_time_corr_tgt_female_failpull_df['within_gazepull'],pull_gaze_time_corr_tgt_male_failpull_df['within_gazepull'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5829538",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.wilcoxon(pull_gaze_time_corr_tgt_failpull_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca63c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanstd(pull_gaze_time_corr_tgt_failpull_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anova and post hoc for male and female comparison\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# separating male and female\n",
    "pull_gaze_time_corr_tgt_male_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_succpull_df['type'] = 'male succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_male_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_failpull_df['type'] = 'male failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_succpull_df['type'] = 'female succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_failpull_df['type'] = 'female failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_male_succpull_df,pull_gaze_time_corr_tgt_male_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_female_succpull_df,pull_gaze_time_corr_tgt_female_failpull_df,])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "df_long2 = df_long2[~np.isnan(df_long2['value'])]\n",
    "# df_long2 = df_long2[df_long2['condition']=='within_gazepull']\n",
    "df_long2 = df_long2[df_long2['condition']=='across_pullgaze']\n",
    "df_long2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42166ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anova\n",
    "cw_lm=ols('value ~ type', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['type'], alpha=0.05)\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2141f1",
   "metadata": {},
   "source": [
    "### distribution of gaze before and after pulls\n",
    "#### similar concept of the previous plots\n",
    "#### similar as the previous plots, separate the successful and failed pulls\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d842bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga_1','kanga_2','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "dist_twin_range = 5\n",
    "\n",
    "succorfailpulls = ['succpull','failedpull']\n",
    "nsuccorfail = np.shape(succorfailpulls)[0]\n",
    "\n",
    "# initiate the final data set\n",
    "SameAnimal_gazeDist_mean_all = dict.fromkeys(succorfailpulls,[])\n",
    "AcroAnimal_gazeDist_mean_all = dict.fromkeys(succorfailpulls,[])\n",
    "#\n",
    "SameAnimal_gazeDist_shuffle_all = dict.fromkeys(succorfailpulls,[])\n",
    "AcroAnimal_gazeDist_shuffle_all = dict.fromkeys(succorfailpulls,[])\n",
    "#\n",
    "SameAnimal_gazeDist_mean_averAcxDays = dict.fromkeys(succorfailpulls,[])\n",
    "AcroAnimal_gazeDist_mean_averAcxDays = dict.fromkeys(succorfailpulls,[])\n",
    "\n",
    "malenames = ['eddie','dodson','dannon','vermelho']\n",
    "femalenames = ['sparkle','scorch','kanga_1','kanga_2','ginger','koala']\n",
    "\n",
    "for isuccorfail in np.arange(0,nsuccorfail,1):\n",
    "    \n",
    "    succorfailpull = succorfailpulls[isuccorfail]\n",
    "    \n",
    "    SameAnimal_gazeDist_mean_all[succorfailpull] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    AcroAnimal_gazeDist_mean_all[succorfailpull] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    #\n",
    "    SameAnimal_gazeDist_shuffle_all[succorfailpull] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    AcroAnimal_gazeDist_shuffle_all[succorfailpull] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    #\n",
    "    SameAnimal_gazeDist_mean_averAcxDays[succorfailpull] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    AcroAnimal_gazeDist_mean_averAcxDays[succorfailpull] = dict.fromkeys(animal1_fixedorders+animal2_fixedorders,[])\n",
    "    \n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "        if (animal2_fixedorder == 'kanga_1') | (animal2_fixedorder == 'kanga_2'):\n",
    "            animal2_filename = 'kanga'\n",
    "        else:\n",
    "            animal2_filename = animal2_fixedorder\n",
    "            \n",
    "        # load the basic behavioral measures\n",
    "        # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_filename+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "        #     \n",
    "        # load the DBN related analysis\n",
    "        # load data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "        #\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        #\n",
    "        # load data for successful and failed pulls\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_filename+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "        #\n",
    "        # re-organize the target dates\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "        #\n",
    "        # sort the data based on task type and dates\n",
    "        dates_list = list(DBN_input_data_alltypes.keys())\n",
    "        sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "        sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "        #\n",
    "        # only select the targeted dates\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]        \n",
    "        sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "        # sorting_tgt_df = sorting_df\n",
    "        dates_list_tgt = sorting_tgt_df['dates']\n",
    "        dates_list_tgt = np.array(dates_list_tgt)\n",
    "        #\n",
    "        ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "        #\n",
    "        # initiate the final data set\n",
    "        SameAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        SameAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        AcroAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        AcroAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        #\n",
    "        SameAnimal_gazeDist_shuffle_all[succorfailpull][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        SameAnimal_gazeDist_shuffle_all[succorfailpull][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        AcroAnimal_gazeDist_shuffle_all[succorfailpull][animal1_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "        AcroAnimal_gazeDist_shuffle_all[succorfailpull][animal2_fixedorder] = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "        # \n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            DBN_input_data_idate_succfail = DBN_input_data_alltypes_succfail[succorfailpull][idate_name]\n",
    "            DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "            \n",
    "            # pull1_t0 and gaze1_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate_succfail['owgaze1_t0'])==1)*1\n",
    "            xxx1_shuffle = xxx1.copy()\n",
    "            np.random.shuffle(xxx1_shuffle)\n",
    "            xxx2_shuffle = xxx2.copy()\n",
    "            np.random.shuffle(xxx2_shuffle)\n",
    "            # pad the two sides\n",
    "            xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            # \n",
    "            npulls = int(np.nansum(xxx1))\n",
    "            pullIDs = np.where(xxx1 == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            SameAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                SameAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "            # shuffle\n",
    "            npulls = int(np.nansum(xxx1_shuffle))\n",
    "            pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            SameAnimal_gazeDist_shuffle_all[succorfailpull][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                SameAnimal_gazeDist_shuffle_all[succorfailpull][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "            \n",
    "            # pull2_t0 and gaze2_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate_succfail['owgaze2_t0'])==1)*1\n",
    "            xxx1_shuffle = xxx1.copy()\n",
    "            np.random.shuffle(xxx1_shuffle)\n",
    "            xxx2_shuffle = xxx2.copy()\n",
    "            np.random.shuffle(xxx2_shuffle)\n",
    "            # pad the two sides\n",
    "            xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            # \n",
    "            npulls = int(np.nansum(xxx1))\n",
    "            pullIDs = np.where(xxx1 == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            SameAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                SameAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "            # shuffle\n",
    "            npulls = int(np.nansum(xxx1_shuffle))\n",
    "            pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            SameAnimal_gazeDist_shuffle_all[succorfailpull][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                SameAnimal_gazeDist_shuffle_all[succorfailpull][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "                \n",
    "            # pull1_t0 and gaze2_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate_succfail['owgaze2_t0'])==1)*1\n",
    "            xxx1_shuffle = xxx1.copy()\n",
    "            np.random.shuffle(xxx1_shuffle)\n",
    "            xxx2_shuffle = xxx2.copy()\n",
    "            np.random.shuffle(xxx2_shuffle)\n",
    "            # pad the two sides\n",
    "            xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            # \n",
    "            npulls = int(np.nansum(xxx1))\n",
    "            pullIDs = np.where(xxx1 == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            AcroAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                AcroAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "            # shuffle\n",
    "            npulls = int(np.nansum(xxx1_shuffle))\n",
    "            pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            AcroAnimal_gazeDist_shuffle_all[succorfailpull][animal2_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                AcroAnimal_gazeDist_shuffle_all[succorfailpull][animal2_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "            \n",
    "            # pull2_t0 and gaze1_t0\n",
    "            xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "            xxx2 = (np.array(DBN_input_data_idate_succfail['owgaze1_t0'])==1)*1\n",
    "            xxx1_shuffle = xxx1.copy()\n",
    "            np.random.shuffle(xxx1_shuffle)\n",
    "            xxx2_shuffle = xxx2.copy()\n",
    "            np.random.shuffle(xxx2_shuffle)\n",
    "            # pad the two sides\n",
    "            xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "            # \n",
    "            npulls = int(np.nansum(xxx1))\n",
    "            pullIDs = np.where(xxx1 == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "            AcroAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                AcroAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan \n",
    "            # shuffle\n",
    "            npulls = int(np.nansum(xxx1_shuffle))\n",
    "            pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "            gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "            #\n",
    "            for ipull in np.arange(0,npulls,1):\n",
    "                pullID = pullIDs[ipull]\n",
    "                gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]           \n",
    "            AcroAnimal_gazeDist_shuffle_all[succorfailpull][animal1_fixedorder][idate_name]=np.nanmean(gazenum_dist_temp,axis=0)/(np.nansum(gazenum_dist_temp)/np.nansum(xxx1))#/(np.sum(xxx2)/np.sum(xxx1))\n",
    "            if npulls == 0:\n",
    "                AcroAnimal_gazeDist_shuffle_all[succorfailpull][animal1_fixedorder][idate_name]=np.ones((1,2*dist_twin_range+1))[0]*np.nan        \n",
    "            \n",
    "    \n",
    "        #\n",
    "        SameAnimal_gazeDist_mean_averAcxDays[succorfailpull][animal1_fixedorder] = np.nanmean(list(SameAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder].values()),axis=0)\n",
    "        SameAnimal_gazeDist_mean_averAcxDays[succorfailpull][animal2_fixedorder] = np.nanmean(list(SameAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder].values()),axis=0)\n",
    "        AcroAnimal_gazeDist_mean_averAcxDays[succorfailpull][animal1_fixedorder] = np.nanmean(list(AcroAnimal_gazeDist_mean_all[succorfailpull][animal1_fixedorder].values()),axis=0)\n",
    "        AcroAnimal_gazeDist_mean_averAcxDays[succorfailpull][animal2_fixedorder] = np.nanmean(list(AcroAnimal_gazeDist_mean_all[succorfailpull][animal2_fixedorder].values()),axis=0)\n",
    "\n",
    "\n",
    "#\n",
    "# plot the summarizing figure\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_figheight(5*2)\n",
    "fig.set_figwidth(5*2)            \n",
    "\n",
    "xxx = np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "# plot the same animal condition\n",
    "df = pd.DataFrame([SameAnimal_gazeDist_mean_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_succ = np.vstack(df.stack().values)\n",
    "yyy_succ_mean = np.nanmean(yyy_succ,axis=0)\n",
    "yyy_succ_std = np.nanstd(yyy_succ,axis=0)/np.sqrt(np.shape(yyy_succ)[0])\n",
    "#  \n",
    "df = pd.DataFrame([SameAnimal_gazeDist_mean_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_fail = np.vstack(df.stack().values)\n",
    "yyy_fail_mean = np.nanmean(yyy_fail,axis=0)\n",
    "yyy_fail_std = np.nanstd(yyy_fail,axis=0)/np.sqrt(np.shape(yyy_fail)[0])\n",
    "#\n",
    "df = pd.DataFrame([SameAnimal_gazeDist_shuffle_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_succ_sf = np.vstack(df.stack().values)\n",
    "yyy_succ_sf_mean = np.nanmean(yyy_succ_sf,axis=0)\n",
    "yyy_succ_sf_std = np.nanstd(yyy_succ_sf,axis=0)/np.sqrt(np.shape(yyy_succ_sf)[0])\n",
    "#  \n",
    "df = pd.DataFrame([SameAnimal_gazeDist_shuffle_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_fail_sf = np.vstack(df.stack().values)\n",
    "yyy_fail_sf_mean = np.nanmean(yyy_fail_sf,axis=0)\n",
    "yyy_fail_sf_std = np.nanstd(yyy_fail_sf,axis=0)/np.sqrt(np.shape(yyy_fail_sf)[0])\n",
    "#\n",
    "axs[0,0].errorbar(xxx,yyy_succ_mean,yyy_succ_std,label='succ pull')\n",
    "axs[0,0].errorbar(xxx,yyy_fail_mean,yyy_fail_std,label='failed pull')\n",
    "axs[0,0].errorbar(xxx,yyy_succ_sf_mean,yyy_succ_sf_std,label='shuffled succ pull')\n",
    "axs[0,0].errorbar(xxx,yyy_fail_sf_mean,yyy_fail_sf_std,label='shuffled failed pull')\n",
    "axs[0,0].plot([0,0],[0,1],'--',color='0.5')\n",
    "axs[0,0].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "axs[0,0].set_ylim(0,0.3)\n",
    "axs[0,0].set_xlabel('time (s)',fontsize=15)\n",
    "axs[0,0].set_ylabel('social gaze probability',fontsize=15)\n",
    "axs[0,0].legend()   \n",
    "axs[0,0].set_title('within animal',fontsize=16) \n",
    "\n",
    "# plot the across animal condition\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_succ = np.vstack(df.stack().values)\n",
    "yyy_succ_mean = np.nanmean(yyy_succ,axis=0)\n",
    "yyy_succ_std = np.nanstd(yyy_succ,axis=0)/np.sqrt(np.shape(yyy_succ)[0])\n",
    "# \n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_fail = np.vstack(df.stack().values)\n",
    "yyy_fail_mean = np.nanmean(yyy_fail,axis=0)\n",
    "yyy_fail_std = np.nanstd(yyy_fail,axis=0)/np.sqrt(np.shape(yyy_fail)[0])\n",
    "#\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_shuffle_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_succ_sf = np.vstack(df.stack().values)\n",
    "yyy_succ_sf_mean = np.nanmean(yyy_succ_sf,axis=0)\n",
    "yyy_succ_sf_std = np.nanstd(yyy_succ_sf,axis=0)/np.sqrt(np.shape(yyy_succ_sf)[0])\n",
    "#  \n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_shuffle_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "yyy_fail_sf = np.vstack(df.stack().values)\n",
    "yyy_fail_sf_mean = np.nanmean(yyy_fail_sf,axis=0)\n",
    "yyy_fail_sf_std = np.nanstd(yyy_fail_sf,axis=0)/np.sqrt(np.shape(yyy_fail_sf)[0])\n",
    "#\n",
    "axs[0,1].errorbar(xxx,yyy_succ_mean,yyy_succ_std,label='succ pull')\n",
    "axs[0,1].errorbar(xxx,yyy_fail_mean,yyy_fail_std,label='failed pull')\n",
    "axs[0,1].errorbar(xxx,yyy_succ_sf_mean,yyy_succ_sf_std,label='shuffled succ pull')\n",
    "axs[0,1].errorbar(xxx,yyy_fail_sf_mean,yyy_fail_sf_std,label='shuffled failed pull')\n",
    "axs[0,1].plot([0,0],[0,1],'--',color='0.5')\n",
    "axs[0,1].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "axs[0,1].set_ylim(0,0.3)\n",
    "axs[0,1].set_xlabel('time (s)',fontsize=15)\n",
    "axs[0,1].set_ylabel('social gaze probability',fontsize=15)\n",
    "axs[0,1].legend()   \n",
    "axs[0,1].set_title('across animal',fontsize=16)\n",
    "\n",
    "# plot the same animal condition - male and female\n",
    "df = pd.DataFrame([SameAnimal_gazeDist_mean_all['succpull'][name] for name in malenames])\n",
    "yyy_succ_m = np.vstack(df.stack().values)\n",
    "yyy_succ_m_mean = np.nanmean(yyy_succ_m,axis=0)\n",
    "yyy_succ_m_std = np.nanstd(yyy_succ_m,axis=0)/np.sqrt(np.shape(yyy_succ_m)[0])\n",
    "# \n",
    "df = pd.DataFrame([SameAnimal_gazeDist_mean_all['failedpull'][name] for name in malenames])\n",
    "yyy_fail_m = np.vstack(df.stack().values)\n",
    "yyy_fail_m_mean = np.nanmean(yyy_fail_m,axis=0)\n",
    "yyy_fail_m_std = np.nanstd(yyy_fail_m,axis=0)/np.sqrt(np.shape(yyy_fail_m)[0])\n",
    "#\n",
    "df = pd.DataFrame([SameAnimal_gazeDist_mean_all['succpull'][name] for name in femalenames])\n",
    "yyy_succ_f = np.vstack(df.stack().values)\n",
    "yyy_succ_f_mean = np.nanmean(yyy_succ_f,axis=0)\n",
    "yyy_succ_f_std = np.nanstd(yyy_succ_f,axis=0)/np.sqrt(np.shape(yyy_succ_f)[0])\n",
    "# \n",
    "df = pd.DataFrame([SameAnimal_gazeDist_mean_all['failedpull'][name] for name in femalenames])\n",
    "yyy_fail_f = np.vstack(df.stack().values)\n",
    "yyy_fail_f_mean = np.nanmean(yyy_fail_f,axis=0)\n",
    "yyy_fail_f_std = np.nanstd(yyy_fail_f,axis=0)/np.sqrt(np.shape(yyy_fail_f)[0])\n",
    "#\n",
    "axs[1,0].errorbar(xxx,yyy_succ_m_mean,yyy_succ_m_std,label='male succ pull')\n",
    "axs[1,0].errorbar(xxx,yyy_fail_m_mean,yyy_fail_m_std,label='male failed pull')\n",
    "axs[1,0].errorbar(xxx,yyy_succ_f_mean,yyy_succ_f_std,label='female succ pull')\n",
    "axs[1,0].errorbar(xxx,yyy_fail_f_mean,yyy_fail_f_std,label='female failed pull')\n",
    "axs[1,0].plot([0,0],[0,1],'--',color='0.5')\n",
    "axs[1,0].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "axs[1,0].set_ylim(0,0.3)\n",
    "axs[1,0].set_xlabel('time (s)',fontsize=15)\n",
    "axs[1,0].set_ylabel('social gaze probability',fontsize=15)\n",
    "axs[1,0].legend()   \n",
    "axs[1,0].set_title('within animal',fontsize=16) \n",
    "\n",
    "# plot the across animal condition - male and female\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['succpull'][name] for name in malenames])\n",
    "yyy_succ_m = np.vstack(df.stack().values)\n",
    "yyy_succ_m_mean = np.nanmean(yyy_succ_m,axis=0)\n",
    "yyy_succ_m_std = np.nanstd(yyy_succ_m,axis=0)/np.sqrt(np.shape(yyy_succ_m)[0])\n",
    "# \n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['failedpull'][name] for name in malenames])\n",
    "yyy_fail_m = np.vstack(df.stack().values)\n",
    "yyy_fail_m_mean = np.nanmean(yyy_fail_m,axis=0)\n",
    "yyy_fail_m_std = np.nanstd(yyy_fail_m,axis=0)/np.sqrt(np.shape(yyy_fail_m)[0])\n",
    "#\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['succpull'][name] for name in femalenames])\n",
    "yyy_succ_f = np.vstack(df.stack().values)\n",
    "yyy_succ_f_mean = np.nanmean(yyy_succ_f,axis=0)\n",
    "yyy_succ_f_std = np.nanstd(yyy_succ_f,axis=0)/np.sqrt(np.shape(yyy_succ_f)[0])\n",
    "# \n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['failedpull'][name] for name in femalenames])\n",
    "yyy_fail_f = np.vstack(df.stack().values)\n",
    "yyy_fail_f_mean = np.nanmean(yyy_fail_f,axis=0)\n",
    "yyy_fail_f_std = np.nanstd(yyy_fail_f,axis=0)/np.sqrt(np.shape(yyy_fail_f)[0])\n",
    "#\n",
    "axs[1,1].errorbar(xxx,yyy_succ_m_mean,yyy_succ_m_std,label='male succ pull')\n",
    "axs[1,1].errorbar(xxx,yyy_fail_m_mean,yyy_fail_m_std,label='male failed pull')\n",
    "axs[1,1].errorbar(xxx,yyy_succ_f_mean,yyy_succ_f_std,label='female succ pull')\n",
    "axs[1,1].errorbar(xxx,yyy_fail_f_mean,yyy_fail_f_std,label='female failed pull')\n",
    "axs[1,1].plot([0,0],[0,1],'--',color='0.5')\n",
    "axs[1,1].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "axs[1,1].set_ylim(0,0.3)\n",
    "axs[1,1].set_xlabel('time (s)',fontsize=15)\n",
    "axs[1,1].set_ylabel('social gaze probability',fontsize=15)\n",
    "axs[1,1].legend()   \n",
    "axs[1,1].set_title('across animal',fontsize=16);\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    \n",
    "    plt.savefig(figsavefolder+\"succorfailpulls_socialgaze_distribution_summaryplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e776c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "/np.nansum(gazenum_dist_temp)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test for each time point\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "xxx1 = np.vstack(df.stack().values)\n",
    "xxx1 = xxx1[~np.isnan(np.sum(xxx1,axis=1))]\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "xxx2 = np.vstack(df.stack().values)\n",
    "xxx2 = xxx2[~np.isnan(np.sum(xxx2,axis=1))]\n",
    "#\n",
    "ntimepoints = np.shape(xxx1)[1]\n",
    "pvalues_all = np.ones((1,ntimepoints))[0]\n",
    "#\n",
    "for itimepoint in np.arange(0,ntimepoints,1):\n",
    "    st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint])\n",
    "    pvalues_all[itimepoint] = st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint]).pvalue\n",
    "    pvalues_all[itimepoint] = round(pvalues_all[itimepoint]*1000)/1000\n",
    "print(pvalues_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test for each time point\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "xxx1 = np.vstack(df.stack().values)\n",
    "xxx1 = xxx1[~np.isnan(np.sum(xxx1,axis=1))]\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_shuffle_all['succpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "xxx2 = np.vstack(df.stack().values)\n",
    "xxx2 = xxx2[~np.isnan(np.sum(xxx2,axis=1))]\n",
    "#\n",
    "ntimepoints = np.shape(xxx1)[1]\n",
    "pvalues_all = np.ones((1,ntimepoints))[0]\n",
    "#\n",
    "for itimepoint in np.arange(0,ntimepoints,1):\n",
    "    st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint])\n",
    "    pvalues_all[itimepoint] = st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint]).pvalue\n",
    "    pvalues_all[itimepoint] = round(pvalues_all[itimepoint]*1000)/1000\n",
    "print(pvalues_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38759b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test for each time point\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_mean_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "xxx1 = np.vstack(df.stack().values)\n",
    "xxx1 = xxx1[~np.isnan(np.sum(xxx1,axis=1))]\n",
    "df = pd.DataFrame([AcroAnimal_gazeDist_shuffle_all['failedpull'][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "xxx2 = np.vstack(df.stack().values)\n",
    "xxx2 = xxx2[~np.isnan(np.sum(xxx2,axis=1))]\n",
    "#\n",
    "ntimepoints = np.shape(xxx1)[1]\n",
    "pvalues_all = np.ones((1,ntimepoints))[0]\n",
    "#\n",
    "for itimepoint in np.arange(0,ntimepoints,1):\n",
    "    st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint])\n",
    "    pvalues_all[itimepoint] = st.ttest_ind(xxx1[:,itimepoint],xxx2[:,itimepoint]).pvalue\n",
    "    pvalues_all[itimepoint] = round(pvalues_all[itimepoint]*1000)/1000\n",
    "print(pvalues_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e024d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(xxx1[:,[3,4]].ravel(),xxx1[:,[6,7]].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ec659",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(xxx2[:,[3,4]].ravel(),xxx2[:,[6,7]].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffd3f8",
   "metadata": {},
   "source": [
    "#### get the half (max - min) width for selected conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splrep, sproot, splev\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.optimize import curve_fit \n",
    "\n",
    "class MultiplePeaks(Exception): pass\n",
    "class NoPeaksFound(Exception): pass\n",
    "\n",
    "def fwhm(x, y, k=10):\n",
    "    \"\"\"\n",
    "    Determine full-with-half-maximum of a peaked set of points, x and y.\n",
    "\n",
    "    Assumes that there is only one peak present in the datasset.  The function\n",
    "    uses a spline interpolation of order k.\n",
    "    \"\"\"\n",
    "\n",
    "    half_max = max(y)/2.0\n",
    "    # half_max = y[round(np.shape(y)[0]/2)-1]\n",
    "    s = splrep(x, y - half_max, k=k)\n",
    "    roots = sproot(s)\n",
    "\n",
    "    if len(roots) > 2:\n",
    "    #     raise MultiplePeaks(\"The dataset appears to have multiple peaks, and \"\n",
    "    #             \"thus the FWHM can't be determined.\")\n",
    "        # return np.nan\n",
    "        return abs(roots[1] - roots[0])\n",
    "    elif len(roots) < 2:\n",
    "    #     raise NoPeaksFound(\"No proper peaks were found in the data set; likely \"\n",
    "    #             \"the dataset is flat (e.g. all zeros).\")\n",
    "        # return np.max(x)-np.min(x)\n",
    "        return np.nan\n",
    "    else:\n",
    "        return abs(roots[1] - roots[0])\n",
    "        \n",
    "        \n",
    "#\n",
    "# Define the Gaussian function \n",
    "def Gauss(x, A, B): \n",
    "    y = A*np.exp(-1*B*x**2) \n",
    "    return y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "conditions = list(AcroAnimal_gazeDist_mean_all.keys())\n",
    "nconds = np.shape(conditions)[0]\n",
    "\n",
    "halfwidth_all = dict.fromkeys(conditions)\n",
    "\n",
    "for icond in np.arange(0,nconds,1):\n",
    "\n",
    "    condname = conditions[icond]\n",
    "    \n",
    "    df = pd.DataFrame([AcroAnimal_gazeDist_mean_all[condname][name] for name in animal1_fixedorders+animal2_fixedorders])\n",
    "    y_allsess = np.vstack(df.stack().values)\n",
    "\n",
    "    nsess = np.shape(y_allsess)[0]\n",
    "    \n",
    "    halfwidth_all[condname] = np.ones((1,nsess))[0]*np.nan\n",
    "\n",
    "    for isess in np.arange(0,nsess,1):\n",
    "    \n",
    "        try:\n",
    "            y =  y_allsess[isess]\n",
    "            y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))      \n",
    "\n",
    "            parameters, covariance = curve_fit(Gauss, x, y) \n",
    "            #\n",
    "            fit_A = parameters[0] \n",
    "            fit_B = parameters[1] \n",
    "            #\n",
    "            fit_y = Gauss(x, fit_A, fit_B) \n",
    "            y = (fit_y-np.nanmin(fit_y))/(np.nanmax(fit_y)-np.nanmin(fit_y)) \n",
    "\n",
    "            halfwidth_all[condname][isess] = fwhm(x, y, k=3)\n",
    "            \n",
    "        except:\n",
    "            halfwidth_all[condname][isess] = np.nan\n",
    "        \n",
    "# box plot \n",
    "fig, axs = plt.subplots(1,1)\n",
    "fig.set_figheight(3)\n",
    "fig.set_figwidth(3)\n",
    "\n",
    "# subplot 1 - all animals\n",
    "halfwidth_all_df = pd.DataFrame.from_dict(halfwidth_all,orient='index')\n",
    "halfwidth_all_df = halfwidth_all_df.transpose()\n",
    "halfwidth_all_df['type'] = 'all'\n",
    "#\n",
    "df_long=pd.concat([halfwidth_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=conditions,var_name='condition', value_name='value')\n",
    "# \n",
    "# barplot ans swarmplot\n",
    "# seaborn.boxplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "seaborn.violinplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "# seaborn.swarmplot(ax=axs[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.set_xlabel('')\n",
    "axs.set_xticklabels(conditions)\n",
    "axs.xaxis.set_tick_params(labelsize=15,rotation=45)\n",
    "axs.set_ylabel(\"half max width\",fontsize=15)\n",
    "axs.set_title('all animals' ,fontsize=24)\n",
    "axs.set_ylim([0,10])\n",
    "axs.legend(fontsize=18)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    \n",
    "    plt.savefig(figsavefolder+\"succorfailpulls_socialgaze_distribution_summaryplot_halfmaxWitdh.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86335978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(halfwidth_all_df['failedpull']))\n",
    "print(np.nanstd(halfwidth_all_df['failedpull']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1c311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long2 = df_long2[~np.isnan(df_long2.value)]\n",
    "# anova\n",
    "cw_lm=ols('value ~ condition', data=df_long2).fit() #Specify C for Categorical\n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "# post hoc test \n",
    "tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition'], alpha=0.05)\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ba015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04f776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6ed5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e92b5531",
   "metadata": {},
   "source": [
    "###  plot the coorelation between pull time, and social gaze time, pooled across animals\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "succorfailpulls = ['succpull','failedpull']\n",
    "nsuccorfail = np.shape(succorfailpulls)[0]\n",
    "\n",
    "do_synedpulltime_pullgazetime = 0 # correlation between pairs of bhv events\n",
    "do_pulltime_gazetime = 1 # correlation between single bhv events\n",
    "do_kernel = 1 # include a gaussian kernel to smooth the data\n",
    "\n",
    "# initiate the final data set\n",
    "pull_gaze_time_corr_mean_all = dict.fromkeys(succorfailpulls,[])\n",
    "\n",
    "\n",
    "\n",
    "for isuccorfail in np.arange(0,nsuccorfail,1):\n",
    "    \n",
    "    succorfailpull = succorfailpulls[isuccorfail]\n",
    "    \n",
    "    pull_gaze_time_corr_mean_all[succorfailpull] = np.zeros((nanimalpairs*2,2))\n",
    "\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "        # load the basic behavioral measures\n",
    "        # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            owgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            owgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            mtgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            mtgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            pull1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            pull2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            bhv_intv_all_dates = pickle.load(f)\n",
    "        # \n",
    "        pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "        #\n",
    "        gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "        gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "        gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "        # load the DBN related analysis\n",
    "        # load data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        #\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        #\n",
    "        # load data for successful and failed pulls\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "        #\n",
    "        # re-organize the target dates\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "        #\n",
    "        # sort the data based on task type and dates\n",
    "        dates_list = list(DBN_input_data_alltypes.keys())\n",
    "        sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "        sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "        #\n",
    "        # only select the targeted dates\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]        \n",
    "        sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "        # sorting_tgt_df = sorting_df\n",
    "        dates_list_tgt = sorting_tgt_df['dates']\n",
    "        dates_list_tgt = np.array(dates_list_tgt)\n",
    "        #\n",
    "        ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "        #\n",
    "        # initiate the final data set\n",
    "        within_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        across_pull_gaze_time_corr_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        within_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        across_pull_gaze_time_corP_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            DBN_input_data_idate_succfail = DBN_input_data_alltypes_succfail[succorfailpull][idate_name]\n",
    "            DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "            #\n",
    "            if do_pulltime_gazetime & ~do_kernel:\n",
    "                # single behavioral events  \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            #\n",
    "            if do_pulltime_gazetime & do_kernel: # this is the best!\n",
    "                kernel_size = 3 # 3\n",
    "                # single behavioral events  \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr1_spe = np.nan\n",
    "                    pp1_spe = np.nan          \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr2_spe = np.nan\n",
    "                    pp2_spe = np.nan\n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr3_spe = np.nan\n",
    "                    pp3_spe = np.nan\n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                # xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr4_spe = np.nan\n",
    "                    pp4_spe = np.nan\n",
    "            #\n",
    "            if 0:\n",
    "                # single behavioral events with synced pull\n",
    "                xxx1_1 = ((np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)&(np.array(DBN_input_data_idate_succfail['pull1_t1'])==1))*1\n",
    "                xxx1_2 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)&(np.array(DBN_input_data_idate_succfail['pull2_t1'])==1))*1 \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)                 \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t1'])==1)*1\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)            \n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = xxx1_1 + xxx1_2\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t1'])==1)*1\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            #\n",
    "            if do_synedpulltime_pullgazetime:\n",
    "                # paired behavioral events\n",
    "                # pull1_t1 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "                #\n",
    "                rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)           \n",
    "                # pull2_t0 and gaze1_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "                rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                # pull2_t1 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "                rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)     \n",
    "                # pull1_t0 and gaze2_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "                rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "            #\n",
    "            if 0:\n",
    "                kernel_size = 3\n",
    "                # paired behavioral events\n",
    "                # pull1_t1 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "                #\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr1_spe,pp1_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr1_spe = np.nan\n",
    "                    pp1_spe = np.nan          \n",
    "                # pull2_t0 and gaze1_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr2_spe,pp2_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr2_spe = np.nan\n",
    "                    pp2_spe = np.nan\n",
    "                # pull2_t1 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr3_spe,pp3_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr3_spe = np.nan\n",
    "                    pp3_spe = np.nan\n",
    "                # pull1_t0 and gaze2_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    rr4_spe,pp4_spe = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "                except:\n",
    "                    rr4_spe = np.nan\n",
    "                    pp4_spe = np.nan\n",
    "\n",
    "\n",
    "            #    \n",
    "            within_pull_gaze_time_corr_all_ipair[idate_name] = [rr1_spe,rr3_spe]\n",
    "            across_pull_gaze_time_corr_all_ipair[idate_name] = [rr2_spe,rr4_spe]\n",
    "            within_pull_gaze_time_corP_all_ipair[idate_name] = [pp1_spe,pp3_spe]\n",
    "            across_pull_gaze_time_corP_all_ipair[idate_name] = [pp2_spe,pp4_spe]\n",
    "\n",
    "        # organize the data to the summarizing mean variables\n",
    "        pull_gaze_time_corr_mean_all[succorfailpull][[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "        pull_gaze_time_corr_mean_all[succorfailpull][[ianimalpair*2,ianimalpair*2+1],1]=np.nanmean(pd.DataFrame(across_pull_gaze_time_corr_all_ipair),axis=1)\n",
    "\n",
    "\n",
    "        # plot each animal pair first\n",
    "        # figure initiate\n",
    "        fig, axs = plt.subplots(2,2)\n",
    "        fig.set_figheight(5*2)\n",
    "        fig.set_figwidth(10*2)\n",
    "        #\n",
    "        plottype_names = ['within animal gaze to pull, '+animal1_fixedorder,\n",
    "                          'across animal pull to gaze, '+animal1_fixedorder,\n",
    "                          'within animal gaze to pull, '+animal2_fixedorder,\n",
    "                          'across animal pull to gaze, '+animal2_fixedorder]\n",
    "        plotCorrs_pooled = [\n",
    "                            np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                            np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,0],\n",
    "                            np.array(pd.DataFrame(within_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                            np.array(pd.DataFrame(across_pull_gaze_time_corr_all_ipair).T)[:,1],\n",
    "                           ]\n",
    "        #\n",
    "        for iplot in np.arange(0,4,1):\n",
    "            #\n",
    "            plottype_name = plottype_names[iplot]\n",
    "            plotCorrs = plotCorrs_pooled[iplot]\n",
    "\n",
    "            # plot \n",
    "            axs.flatten()[iplot].plot(np.arange(0,ndates_tgt,1),plotCorrs,'ko',markersize=10)\n",
    "            #\n",
    "            axs.flatten()[iplot].set_title(plottype_name,fontsize=16)\n",
    "            axs.flatten()[iplot].set_ylabel('time coorelation with pull <-> pull',fontsize=13)\n",
    "            axs.flatten()[iplot].set_ylim([-1.1,1.1])\n",
    "            axs.flatten()[iplot].set_xlim([-0.5,ndates_tgt-0.5])\n",
    "            #\n",
    "            if iplot > 1:\n",
    "                axs.flatten()[iplot].set_xticks(np.arange(0,ndates_tgt,1))\n",
    "                axs.flatten()[iplot].set_xticklabels(dates_list_tgt, rotation=90,fontsize=10)\n",
    "            else:\n",
    "                axs.flatten()[iplot].set_xticklabels('')\n",
    "            #\n",
    "            # tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "            tasktypes = ['coop(3s)','coop(2s)','coop(1.5s)','coop(1s)']\n",
    "            taskswitches = np.where(np.array(sorting_tgt_df['coopthres'])[1:]-np.array(sorting_tgt_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "            for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "                taskswitch = taskswitches[itaskswitch]\n",
    "                axs.flatten()[iplot].plot([taskswitch,taskswitch],[-1.1,1.1],'k--')\n",
    "            taskswitches = np.concatenate(([0],taskswitches))\n",
    "            for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "                taskswitch = taskswitches[itaskswitch]\n",
    "                axs.flatten()[iplot].text(taskswitch+0.25,-0.9,tasktypes[itaskswitch],fontsize=10)\n",
    "            axs.flatten()[iplot].plot([0,ndates_tgt],[0,0],'k--')\n",
    "\n",
    "        savefigs = 1\n",
    "        if savefigs:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'       \n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            plt.savefig(figsavefolder+succorfailpull+'_pulltime_gazetime_correlation_'+animal1_fixedorder+animal2_fixedorder+'.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the summarizing figure\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "dependencytargets = ['within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency, separate successful and failed pull\n",
    "pull_gaze_time_corr_tgt_succpull = pull_gaze_time_corr_mean_all['succpull']\n",
    "pull_gaze_time_corr_tgt_failpull = pull_gaze_time_corr_mean_all['failedpull']\n",
    "measure_tgt_name = 'time point correlation' \n",
    "# \n",
    "pull_gaze_time_corr_tgt_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull)\n",
    "pull_gaze_time_corr_tgt_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_succpull_df['type'] = 'succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull)\n",
    "pull_gaze_time_corr_tgt_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_failpull_df['type'] = 'failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_succpull_df,pull_gaze_time_corr_tgt_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[0].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals' ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[0].set_ylim([-1,1])\n",
    "axs.ravel()[0].legend(fontsize=18)\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "pull_gaze_time_corr_tgt_male_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_succpull_df['type'] = 'male succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_male_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_failpull_df['type'] = 'male failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_succpull_df['type'] = 'female succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_failpull_df['type'] = 'female failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_male_succpull_df,pull_gaze_time_corr_tgt_male_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_female_succpull_df,pull_gaze_time_corr_tgt_female_failpull_df,])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[1].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female' ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[1].set_ylim([-1,1])\n",
    "axs.ravel()[1].legend(fontsize=18)\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df['type'] = 'subordinate succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df['type'] = 'subordinate failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df['type'] = 'dominant succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df['type'] = 'dominant failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_sub_succpull_df,pull_gaze_time_corr_tgt_sub_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_dom_succpull_df,pull_gaze_time_corr_tgt_dom_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[2].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom' ,fontsize=34)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "axs.ravel()[2].set_ylim([-1,1])\n",
    "axs.ravel()[2].legend(fontsize=18)\n",
    "\n",
    "\n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if  do_synedpulltime_pullgazetime:\n",
    "        plt.savefig(figsavefolder+\"succorfailpulls_syncedpulltime_pullgazetime_correlation_summaryplot.pdf\")\n",
    "    elif do_pulltime_gazetime:       \n",
    "        plt.savefig(figsavefolder+\"succorfailpulls_pulltime_gazetime_correlation_summaryplot.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5ed96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1fd0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35923d9e",
   "metadata": {},
   "source": [
    "### same as the previous plot, but try to find the best shifting lag in the time series data\n",
    "####  plot the coorelation between pull time, and social gaze time\n",
    "#### pull <-> pull; within animal gaze -> pull; across animal pull -> gaze; within animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "succorfailpulls = ['succpull','failedpull']\n",
    "nsuccorfail = np.shape(succorfailpulls)[0]\n",
    "\n",
    "do_synedpulltime_pullgazetime = 0 # correlation between pairs of bhv events\n",
    "do_pulltime_gazetime = 1 # plot the summarizing figure\n",
    "do_kernel = 0\n",
    "#\n",
    "\n",
    "# initiate the final data set\n",
    "pull_gaze_time_bestlag_mean_all = dict.fromkeys(succorfailpulls,[])\n",
    "\n",
    "across_pull_gaze_bestlag_allsessions_all = dict.fromkeys(succorfailpulls,[])\n",
    "within_gaze_pull_bestlag_allsessions_all = dict.fromkeys(succorfailpulls,[])\n",
    "\n",
    "\n",
    "for isuccorfail in np.arange(0,nsuccorfail,1):\n",
    "    \n",
    "    succorfailpull = succorfailpulls[isuccorfail]\n",
    "    \n",
    "    pull_gaze_time_bestlag_mean_all[succorfailpull] = np.zeros((nanimalpairs*2,2))\n",
    "    \n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "        # load the basic behavioral measures\n",
    "        # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        #\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            owgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            owgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            mtgaze1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            mtgaze2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            pull1_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            pull2_num_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            tasktypes_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            coopthres_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            succ_rate_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            interpullintv_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            trialnum_all_dates = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            bhv_intv_all_dates = pickle.load(f)\n",
    "        # \n",
    "        pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "        #\n",
    "        gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "        gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "        gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "        # load the DBN related analysis\n",
    "        # load data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        #\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        #\n",
    "        # load data for successful and failed pulls\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "        #\n",
    "        # re-organize the target dates\n",
    "        # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "        tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "        coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "        coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "        #\n",
    "        # sort the data based on task type and dates\n",
    "        dates_list = list(DBN_input_data_alltypes.keys())\n",
    "        sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "        sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "        #\n",
    "        # only select the targeted dates\n",
    "        sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "        # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "        # sorting_tgt_df = sorting_df\n",
    "        dates_list_tgt = sorting_tgt_df['dates']\n",
    "        dates_list_tgt = np.array(dates_list_tgt)\n",
    "        #\n",
    "        ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "        #\n",
    "        # initiate the final data set\n",
    "        within_pull_gaze_time_bestlag_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "        across_pull_gaze_time_bestlag_all_ipair = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "\n",
    "        for idate in np.arange(0,ndates_tgt,1):\n",
    "            idate_name = dates_list_tgt[idate]\n",
    "\n",
    "            DBN_input_data_idate_succfail = DBN_input_data_alltypes_succfail[succorfailpull][idate_name]\n",
    "            DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "            #\n",
    "            if do_pulltime_gazetime & ~do_kernel: \n",
    "                bestlag_limit = 10 # a limit to find the best lag, e.g. 60 second\n",
    "                # single behavioral events  \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    _,_,bestlag1 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag1 = np.nan          \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    _,_,bestlag2 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag2 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag2 = np.nan\n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    _,_,bestlag3 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag3 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag3 = np.nan\n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    _,_,bestlag4 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag4 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag4 = np.nan\n",
    "            #\n",
    "            if do_pulltime_gazetime & do_kernel: # this is the best!\n",
    "                kernel_size = 3\n",
    "                bestlag_limit = 10 # a limit to find the best lag, e.g. 60 second\n",
    "                # single behavioral events  \n",
    "                # pull1_t0 and gaze1_t0\n",
    "                # xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag1 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag1 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                    # lags,cvalue,_,_ = plt.xcorr(xxx2,xxx1,maxlags=10)\n",
    "                    # plt.close()\n",
    "                    # bestlag1 = lags[cvalue==np.max(cvalue)][0]\n",
    "                except:\n",
    "                    bestlag1 = np.nan          \n",
    "                # pull2_t0 and gaze1_t0\n",
    "                # xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag2 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag2 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                    # lags,cvalue,_,_ = plt.xcorr(xxx2,xxx1,maxlags=10)\n",
    "                    # plt.close()\n",
    "                    # bestlag2 = lags[cvalue==np.max(cvalue)][0]\n",
    "                except:\n",
    "                    bestlag2 = np.nan\n",
    "                # pull2_t0 and gaze2_t0\n",
    "                # xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag3 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag3 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                    # lags,cvalue,_,_ = plt.xcorr(xxx2,xxx1,maxlags=10)\n",
    "                    # plt.close()\n",
    "                    # bestlag3 = lags[cvalue==np.max(cvalue)][0]\n",
    "                except:\n",
    "                    bestlag3 = np.nan\n",
    "                # pull1_t0 and gaze2_t0\n",
    "                # xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx1 = ((np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)|(np.array(DBN_input_data_idate_succfail['pull2_t0'])==1))*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag4 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                    # _,_,bestlag4 = correlagram(xxx2,xxx1,bestlag_limit)\n",
    "                    # lags,cvalue,_,_ = plt.xcorr(xxx2,xxx1,maxlags=10)\n",
    "                    # plt.close()\n",
    "                    # bestlag4 = lags[cvalue==np.max(cvalue)][0]\n",
    "                except:\n",
    "                    bestlag4 = np.nan\n",
    "            #\n",
    "            if do_synedpulltime_pullgazetime:\n",
    "                kernel_size = 10\n",
    "                bestlag_limit = 60 # a limit to find the best lag, e.g. 60 second\n",
    "                # paired behavioral events\n",
    "                # pull1_t1 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t1'])==1)&(np.array(DBN_input_data_idate['owgaze1_t0'])==1))*1\n",
    "                #\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag1 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag1 = np.nan          \n",
    "                # pull2_t0 and gaze1_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t0'])==1)&(np.array(DBN_input_data_idate['owgaze1_t1'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag2 = xcorr(xxx2,xxx1,bestlag_limit1)\n",
    "                except:\n",
    "                    bestlag2 = np.nan\n",
    "                # pull2_t1 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull2_t1'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull2_t1'])==1)&(np.array(DBN_input_data_idate['owgaze2_t0'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag3 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag3 = np.nan\n",
    "                # pull1_t0 and gaze2_t1\n",
    "                xxx1 = (np.array(DBN_input_data_idate_succfail['pull1_t0'])==1)*1\n",
    "                xxx2 = ((np.array(DBN_input_data_idate['pull1_t0'])==1)&(np.array(DBN_input_data_idate['owgaze2_t1'])==1))*1\n",
    "                try:\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "                    xxx1 = np.where(xxx1==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx1.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx1 = np.exp(log_dens)\n",
    "                    #\n",
    "                    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "                    xxx2 = np.where(xxx2==1)[0]\n",
    "                    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_size).fit(xxx2.reshape(-1, 1))\n",
    "                    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "                    xxx2 = np.exp(log_dens)\n",
    "                    #\n",
    "                    _,_,bestlag4 = xcorr(xxx2,xxx1,bestlag_limit)\n",
    "                except:\n",
    "                    bestlag4 = np.nan\n",
    "\n",
    "\n",
    "            #    \n",
    "            within_pull_gaze_time_bestlag_all_ipair[idate_name] = [bestlag1,bestlag3]\n",
    "            across_pull_gaze_time_bestlag_all_ipair[idate_name] = [bestlag2,bestlag4]\n",
    "\n",
    "\n",
    "        # organize the data to the summarizing mean variables\n",
    "        pull_gaze_time_bestlag_mean_all[succorfailpull][[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_bestlag_all_ipair),axis=1)\n",
    "        pull_gaze_time_bestlag_mean_all[succorfailpull][[ianimalpair*2,ianimalpair*2+1],1]=np.nanmean(pd.DataFrame(across_pull_gaze_time_bestlag_all_ipair),axis=1)\n",
    "\n",
    "        # organize the data to the summarizing all session variables \n",
    "        df = pd.DataFrame(within_pull_gaze_time_bestlag_all_ipair).T\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        if ianimalpair == 0:\n",
    "            within_gaze_pull_bestlag_allsessions_all[succorfailpull]=df\n",
    "        else:\n",
    "            within_gaze_pull_bestlag_allsessions_all[succorfailpull] = pd.concat([within_gaze_pull_bestlag_allsessions_all[succorfailpull],df],axis = 1)\n",
    "        #    \n",
    "        df = pd.DataFrame(across_pull_gaze_time_bestlag_all_ipair).T\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        if ianimalpair == 0:\n",
    "            across_pull_gaze_bestlag_allsessions_all[succorfailpull]=df\n",
    "        else:\n",
    "            across_pull_gaze_bestlag_allsessions_all[succorfailpull] = pd.concat([across_pull_gaze_bestlag_allsessions_all[succorfailpull],df],axis = 1)\n",
    "        \n",
    "\n",
    "        # plot each animal pair first\n",
    "        # figure initiate\n",
    "        fig, axs = plt.subplots(2,2)\n",
    "        fig.set_figheight(5*2)\n",
    "        fig.set_figwidth(10*2)\n",
    "        #\n",
    "        plottype_names = ['within animal gaze to pull, '+animal1_fixedorder,\n",
    "                          'across animal pull to gaze, '+animal1_fixedorder,\n",
    "                          'within animal gaze to pull, '+animal2_fixedorder,\n",
    "                          'across animal pull to gaze, '+animal2_fixedorder]\n",
    "        plotbestlags_pooled = [\n",
    "                            np.array(pd.DataFrame(within_pull_gaze_time_bestlag_all_ipair).T)[:,0],\n",
    "                            np.array(pd.DataFrame(across_pull_gaze_time_bestlag_all_ipair).T)[:,0],\n",
    "                            np.array(pd.DataFrame(within_pull_gaze_time_bestlag_all_ipair).T)[:,1],\n",
    "                            np.array(pd.DataFrame(across_pull_gaze_time_bestlag_all_ipair).T)[:,1],\n",
    "                           ]\n",
    "        #\n",
    "        for iplot in np.arange(0,4,1):\n",
    "            #\n",
    "            plottype_name = plottype_names[iplot]\n",
    "            plotbestlags = plotbestlags_pooled[iplot]\n",
    "\n",
    "            # plot \n",
    "            axs.flatten()[iplot].plot(np.arange(0,ndates_tgt,1),plotbestlags,'ko',markersize=10)\n",
    "            #\n",
    "            axs.flatten()[iplot].set_title(plottype_name,fontsize=16)\n",
    "            axs.flatten()[iplot].set_ylabel('best coorelation lag with pull <-> pull',fontsize=13)\n",
    "            #axs.flatten()[iplot].set_ylim([-1.1,1.1])\n",
    "            axs.flatten()[iplot].set_xlim([-0.5,ndates_tgt-0.5])\n",
    "            #\n",
    "            if iplot > 1:\n",
    "                axs.flatten()[iplot].set_xticks(np.arange(0,ndates_tgt,1))\n",
    "                axs.flatten()[iplot].set_xticklabels(dates_list_tgt, rotation=90,fontsize=10)\n",
    "            else:\n",
    "                axs.flatten()[iplot].set_xticklabels('')\n",
    "            #\n",
    "            # tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "            tasktypes = ['coop(3s)','coop(2s)','coop(1.5s)','coop(1s)']\n",
    "            taskswitches = np.where(np.array(sorting_tgt_df['coopthres'])[1:]-np.array(sorting_tgt_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "            for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "                taskswitch = taskswitches[itaskswitch]\n",
    "                axs.flatten()[iplot].plot([taskswitch,taskswitch],[-1.1,1.1],'k--')\n",
    "            taskswitches = np.concatenate(([0],taskswitches))\n",
    "            for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "                taskswitch = taskswitches[itaskswitch]\n",
    "                axs.flatten()[iplot].text(taskswitch+0.25,-0.9,tasktypes[itaskswitch],fontsize=10)\n",
    "            axs.flatten()[iplot].plot([0,ndates_tgt],[0,0],'k--')\n",
    "\n",
    "        savefigs = 0\n",
    "        if savefigs:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'       \n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            plt.savefig(figsavefolder+succorfailpull+'_pulltime_gazetime_bestCorrlag_'+animal1_fixedorder+animal2_fixedorder+'.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the summarizing figure\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "dependencytargets = ['within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency, separate successful and failed pull\n",
    "# pull_gaze_time_bestlag_tgt_succpull = pull_gaze_time_bestlag_mean_all['succpull']\n",
    "# pull_gaze_time_bestlag_tgt_failpull = pull_gaze_time_bestlag_mean_all['failedpull']\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['failedpull'].values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['failedpull'].values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_failpull = np.array([ay1,ay2]).T\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['succpull'].values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['succpull'].values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_succpull = np.array([ay1,ay2]).T\n",
    "measure_tgt_name = 'best correlation lags' \n",
    "# \n",
    "pull_gaze_time_bestlag_tgt_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_succpull)\n",
    "pull_gaze_time_bestlag_tgt_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_succpull_df['type'] = 'succpull'\n",
    "#\n",
    "pull_gaze_time_bestlag_tgt_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_failpull)\n",
    "pull_gaze_time_bestlag_tgt_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_failpull_df['type'] = 'failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_bestlag_tgt_succpull_df,pull_gaze_time_bestlag_tgt_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[0].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals' ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "# axs.ravel()[0].set_ylim([-1,1])\n",
    "axs.ravel()[0].legend(fontsize=18)\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['succpull'].T.iloc[[0,2,4,9]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['succpull'].T.iloc[[0,2,4,9]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_male_succpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_male_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_male_succpull)\n",
    "#pull_gaze_time_bestlag_tgt_male_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_succpull[[0,2,4],:])\n",
    "pull_gaze_time_bestlag_tgt_male_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_male_succpull_df['type'] = 'male succpull'\n",
    "#\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['failedpull'].T.iloc[[0,2,4,9]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['failedpull'].T.iloc[[0,2,4,9]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_male_failpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_male_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_male_failpull)\n",
    "#pull_gaze_time_bestlag_tgt_male_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_failpull[[0,2,4],:])\n",
    "pull_gaze_time_bestlag_tgt_male_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_male_failpull_df['type'] = 'male failpull'\n",
    "#\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['succpull'].T.iloc[[1,3,5,6,7,8]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['succpull'].T.iloc[[1,3,5,6,7,8]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_female_succpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_female_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_female_succpull)\n",
    "#pull_gaze_time_bestlag_tgt_female_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_succpull[[1,3,5,6,7],:])\n",
    "pull_gaze_time_bestlag_tgt_female_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_female_succpull_df['type'] = 'female succpull'\n",
    "#\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['failedpull'].T.iloc[[1,3,5,6,7,8]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['failedpull'].T.iloc[[1,3,5,6,7,8]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_female_failpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_female_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_female_failpull)\n",
    "#pull_gaze_time_bestlag_tgt_female_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_failpull[[1,3,5,6,7],:])\n",
    "pull_gaze_time_bestlag_tgt_female_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_female_failpull_df['type'] = 'female failpull'\n",
    "\n",
    "    \n",
    "    \n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_bestlag_tgt_male_succpull_df,pull_gaze_time_bestlag_tgt_male_failpull_df,\n",
    "                   pull_gaze_time_bestlag_tgt_female_succpull_df,pull_gaze_time_bestlag_tgt_female_failpull_df,])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[1].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female' ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "# axs.ravel()[1].set_ylim([-1,1])\n",
    "axs.ravel()[1].legend(fontsize=18)\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['succpull'].T.iloc[[0,2,4,6,8]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['succpull'].T.iloc[[0,2,4,6,8]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_sub_succpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_sub_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_sub_succpull)\n",
    "#pull_gaze_time_bestlag_tgt_sub_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_succpull[[0,2,4,6],:])\n",
    "pull_gaze_time_bestlag_tgt_sub_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_sub_succpull_df['type'] = 'subordinate succpull'\n",
    "#\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['failedpull'].T.iloc[[0,2,4,6,8]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['failedpull'].T.iloc[[0,2,4,6,8]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_sub_failpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_sub_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_sub_failpull)\n",
    "#pull_gaze_time_bestlag_tgt_sub_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_failpull[[0,2,4,6],:])\n",
    "pull_gaze_time_bestlag_tgt_sub_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_sub_failpull_df['type'] = 'subordinate failpull'\n",
    "#\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['succpull'].T.iloc[[1,3,5,7,9]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['succpull'].T.iloc[[1,3,5,7,9]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_dom_succpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_dom_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_dom_succpull)\n",
    "#pull_gaze_time_bestlag_tgt_dom_succpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_succpull[[1,3,5,7],:])\n",
    "pull_gaze_time_bestlag_tgt_dom_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_dom_succpull_df['type'] = 'dominant succpull'\n",
    "#\n",
    "ay1 = within_gaze_pull_bestlag_allsessions_all['failedpull'].T.iloc[[1,3,5,7,9]].T.values.flatten()\n",
    "ay2 = across_pull_gaze_bestlag_allsessions_all['failedpull'].T.iloc[[1,3,5,7,9]].T.values.flatten()\n",
    "pull_gaze_time_bestlag_tgt_dom_failpull = np.array([ay1,ay2]).T\n",
    "pull_gaze_time_bestlag_tgt_dom_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_dom_failpull)\n",
    "# pull_gaze_time_bestlag_tgt_dom_failpull_df = pd.DataFrame(pull_gaze_time_bestlag_tgt_failpull[[1,3,5,7],:])\n",
    "pull_gaze_time_bestlag_tgt_dom_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_bestlag_tgt_dom_failpull_df['type'] = 'dominant failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_bestlag_tgt_sub_succpull_df,pull_gaze_time_bestlag_tgt_sub_failpull_df,\n",
    "                   pull_gaze_time_bestlag_tgt_dom_succpull_df,pull_gaze_time_bestlag_tgt_dom_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[2].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom' ,fontsize=34)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "# axs.ravel()[2].set_ylim([-1,1])\n",
    "axs.ravel()[2].legend(fontsize=18)\n",
    "\n",
    "\n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if  do_synedpulltime_pullgazetime:\n",
    "        plt.savefig(figsavefolder+\"succorfailpulls_syncedpulltime_pullgazetime_bestcorrlag_summaryplot.pdf\")\n",
    "    elif do_pulltime_gazetime:       \n",
    "        plt.savefig(figsavefolder+\"succorfailpulls_pulltime_gazetime_bestcorrlag_summaryplot.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54554195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144d23d4",
   "metadata": {},
   "source": [
    "### basic social gaze count per sec, separate the successful and failed duration\n",
    "#### similar kind of analysis as the social gaze time v.s. pull time correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "#\n",
    "animal1_filenames = ['Eddie','Dodson','Dannon','Ginger','Koala']\n",
    "animal2_filenames = ['Sparkle','Scorch','Kanga','Kanga','Vermelho']\n",
    "# animal1_filenames = ['Eddie',]\n",
    "# animal2_filenames = ['Sparkle',]\n",
    "#\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "temp_resolu = 1\n",
    "\n",
    "succorfailpulls = ['succpull','failedpull']\n",
    "nsuccorfail = np.shape(succorfailpulls)[0]\n",
    "\n",
    "# initiate the final data set\n",
    "pull_gaze_time_corr_mean_all = dict.fromkeys(succorfailpulls,[])\n",
    "within_pull_gaze_time_corr_all_ipair = dict.fromkeys(succorfailpulls,[])\n",
    "for isuccorfail in np.arange(0,nsuccorfail,1):\n",
    "    succorfailpull = succorfailpulls[isuccorfail]   \n",
    "    pull_gaze_time_corr_mean_all[succorfailpull] = np.zeros((nanimalpairs*2,1))\n",
    "    \n",
    "#\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "    animal1_filename = animal1_filenames[ianimalpair]\n",
    "    animal2_filename = animal2_filenames[ianimalpair]\n",
    "\n",
    "    print('organize data for '+animal1_fixedorder+' '+animal2_fixedorder)\n",
    "\n",
    "    # load the basic behavioral measures\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/sessstart_time_all_dates_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "        sessstart_time_all_dates = pickle.load(f)\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if not mergetempRos:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "    #\n",
    "    # load data for successful and failed pulls\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    if not mergetempRos:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    # re-organize the target dates\n",
    "    # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "    tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "    coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "    coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "    #\n",
    "    # sort the data based on task type and dates\n",
    "    dates_list = list(DBN_input_data_alltypes.keys())\n",
    "    sorting_df = pd.DataFrame({'dates':dates_list,'coopthres':coopthres_forsort.ravel(),'sessstarttime':sessstart_time_all_dates}, columns=['dates', 'coopthres','sessstarttime'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "    #\n",
    "    # only select the targeted dates\n",
    "    sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]        \n",
    "    # sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "    # sorting_tgt_df = sorting_df\n",
    "    dates_list_tgt = sorting_tgt_df['dates']\n",
    "    dates_list_tgt = np.array(dates_list_tgt)\n",
    "    session_start_times = np.array(sorting_tgt_df['sessstarttime'])\n",
    "    #\n",
    "    ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "    #\n",
    "    # initiate the final data set\n",
    "    within_pull_gaze_time_corr_all_ipair['failedpull'] = dict.fromkeys(dates_list_tgt,[])\n",
    "    within_pull_gaze_time_corr_all_ipair['succpull'] = dict.fromkeys(dates_list_tgt,[])\n",
    "\n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "\n",
    "        print('organize data for '+idate_name)\n",
    "\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + idate_name+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        # find the successful and failed trials, and get the start and end time\n",
    "        # fail trials\n",
    "        fail_trialnum = np.array(trial_record_clean[trial_record_clean['rewarded']==0]['trial_number'])\n",
    "        nfailtrials = np.shape(fail_trialnum)[0]\n",
    "        fail_startendtimes = np.ones((nfailtrials,2))*np.nan\n",
    "        for itrial in np.arange(0,nfailtrials,1):\n",
    "            itrialnum = fail_trialnum[itrial]\n",
    "            fail_startendtimes[itrial,0] = np.nanmin(bhv_data[(np.isin(bhv_data['trial_number'],itrialnum))&(bhv_data['behavior_events']==0)]['time_points'])\n",
    "            fail_startendtimes[itrial,1] = np.nanmax(bhv_data[(np.isin(bhv_data['trial_number'],itrialnum))&(bhv_data['behavior_events']==9)]['time_points'])\n",
    "        # remove the very long one - resting state, not pulling state\n",
    "        badperiod = (fail_startendtimes[:,1]-fail_startendtimes[:,0])>400 # 15 is arbitrary, need to change\n",
    "        fail_startendtimes = fail_startendtimes[~badperiod,:]\n",
    "        nfailtrials = np.shape(fail_startendtimes)[0]\n",
    "        \n",
    "        # successful trials\n",
    "        succ_trialnum = np.array(trial_record_clean[trial_record_clean['rewarded']>0]['trial_number'])\n",
    "        nsucctrials = np.shape(succ_trialnum)[0]\n",
    "        succ_startendtimes = np.ones((nsucctrials,2))*np.nan\n",
    "        for itrial in np.arange(0,nsucctrials,1):\n",
    "            itrialnum = succ_trialnum[itrial]\n",
    "            succ_startendtimes[itrial,0] = np.nanmin(bhv_data[(np.isin(bhv_data['trial_number'],itrialnum))&(bhv_data['behavior_events']==0)]['time_points'])\n",
    "            succ_startendtimes[itrial,1] = np.nanmax(bhv_data[(np.isin(bhv_data['trial_number'],itrialnum))&(bhv_data['behavior_events']==9)]['time_points'])\n",
    "        # remove the very long one - resting state, not pulling state\n",
    "        badperiod = (succ_startendtimes[:,1]-succ_startendtimes[:,0])>400 # 15 is arbitrary, need to change\n",
    "        sycc_startendtimes = succ_startendtimes[~badperiod,:]\n",
    "        nsucctrials = np.shape(succ_startendtimes)[0]\n",
    "\n",
    "\n",
    "        # load the raw gaze time \n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder+animal2_fixedorder+\"/\"+cameraID+'/'+idate_name+'/output_look_ornot.pkl', 'rb') as f:\n",
    "            output_look_ornot = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder+animal2_fixedorder+\"/\"+cameraID+'/'+idate_name+'/output_allvectors.pkl', 'rb') as f:\n",
    "            output_allvectors = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder+animal2_fixedorder+\"/\"+cameraID+'/'+idate_name+'/output_allangles.pkl', 'rb') as f:\n",
    "            output_allangles = pickle.load(f)  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        if animal1 == animal1_fixedorder:\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        elif animal1 == animal2_fixedorder:\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        #\n",
    "        time_point_pull1 = np.array(time_point_pull1)\n",
    "        time_point_pull2 = np.array(time_point_pull2)\n",
    "        time_point_gaze1 = np.sort(np.hstack((oneway_gaze1,mutual_gaze1)))\n",
    "        time_point_gaze2 = np.sort(np.hstack((oneway_gaze2,mutual_gaze2)))\n",
    "        # \n",
    "        # change social gaze as the start of a period of social gaze\n",
    "        if 0:\n",
    "            ind_gazestart = np.hstack(([1],(time_point_gaze1[1:]-time_point_gaze1[:-1])>=0.5))\n",
    "            ind_gazestart = ind_gazestart.astype(bool)\n",
    "            time_point_gaze1 = time_point_gaze1[ind_gazestart]\n",
    "            ind_gazestart = np.hstack(([1],(time_point_gaze2[1:]-time_point_gaze2[:-1])>=0.5))\n",
    "            ind_gazestart = ind_gazestart.astype(bool)\n",
    "            time_point_gaze2 = time_point_gaze2[ind_gazestart]\n",
    "            \n",
    "        # socialgaze number and pull numbers during fail trials    \n",
    "        socialgaze1_num_failedpull = 0 \n",
    "        socialgaze2_num_failedpull = 0       \n",
    "        pull1_num_failedpull = 0\n",
    "        pull2_num_failedpull = 0\n",
    "        for ipull in np.arange(0,nfailtrials,1):\n",
    "            starttime = fail_startendtimes[ipull,0]\n",
    "            endtime = fail_startendtimes[ipull,1]\n",
    "            socialgaze1_num_failedpull = socialgaze1_num_failedpull+np.nansum((time_point_gaze1<endtime)&(time_point_gaze1>starttime))\n",
    "            socialgaze2_num_failedpull = socialgaze2_num_failedpull+np.nansum((time_point_gaze2<endtime)&(time_point_gaze2>starttime))\n",
    "            pull1_num_failedpull = pull1_num_failedpull+np.nansum((time_point_pull1<endtime)&(time_point_pull1>starttime))\n",
    "            pull2_num_failedpull = pull2_num_failedpull+np.nansum((time_point_pull2<endtime)&(time_point_pull2>starttime))\n",
    "        #\n",
    "        socialgaze1_num_failedpull = socialgaze1_num_failedpull/np.nansum(fail_startendtimes[:,1]-fail_startendtimes[:,0])\n",
    "        socialgaze2_num_failedpull = socialgaze2_num_failedpull/np.nansum(fail_startendtimes[:,1]-fail_startendtimes[:,0])\n",
    "        pull1_num_failedpull = pull1_num_failedpull/np.nansum(fail_startendtimes[:,1]-fail_startendtimes[:,0])\n",
    "        pull2_num_failedpull = pull2_num_failedpull/np.nansum(fail_startendtimes[:,1]-fail_startendtimes[:,0])\n",
    "        #\n",
    "        within_pull_gaze_time_corr_all_ipair['failedpull'][idate_name] = np.array([socialgaze1_num_failedpull,socialgaze2_num_failedpull])/(pull1_num_failedpull+pull2_num_failedpull)\n",
    "\n",
    "        # socialgaze number during successful trials    \n",
    "        socialgaze1_num_succpull = 0 \n",
    "        socialgaze2_num_succpull = 0   \n",
    "        pull1_num_succpull = 0\n",
    "        pull2_num_succpull = 0\n",
    "        for ipull in np.arange(0,nsucctrials,1):\n",
    "            starttime = succ_startendtimes[ipull,0]\n",
    "            endtime = succ_startendtimes[ipull,1]\n",
    "            socialgaze1_num_succpull = socialgaze1_num_succpull+np.nansum((time_point_gaze1<endtime)&(time_point_gaze1>starttime))\n",
    "            socialgaze2_num_succpull = socialgaze2_num_succpull+np.nansum((time_point_gaze2<endtime)&(time_point_gaze2>starttime))\n",
    "            pull1_num_succpull = pull1_num_succpull+np.nansum((time_point_pull1<endtime)&(time_point_pull1>starttime))\n",
    "            pull2_num_succpull = pull2_num_succpull+np.nansum((time_point_pull2<endtime)&(time_point_pull2>starttime))\n",
    "        #\n",
    "        socialgaze1_num_succpull = socialgaze1_num_succpull/np.nansum(succ_startendtimes[:,1]-succ_startendtimes[:,0])\n",
    "        socialgaze2_num_succpull = socialgaze2_num_succpull/np.nansum(succ_startendtimes[:,1]-succ_startendtimes[:,0])\n",
    "        pull1_num_succpull = pull1_num_succpull/np.nansum(succ_startendtimes[:,1]-succ_startendtimes[:,0])\n",
    "        pull2_num_succpull = pull2_num_succpull/np.nansum(succ_startendtimes[:,1]-succ_startendtimes[:,0])\n",
    "        #\n",
    "        within_pull_gaze_time_corr_all_ipair['succpull'][idate_name] = np.array([socialgaze1_num_succpull,socialgaze2_num_succpull])/(pull1_num_succpull+pull2_num_succpull)\n",
    "\n",
    "    # organize the data to the summarizing mean variables\n",
    "    pull_gaze_time_corr_mean_all['failedpull'][[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_corr_all_ipair['failedpull']),axis=1)\n",
    "    pull_gaze_time_corr_mean_all['succpull'][[ianimalpair*2,ianimalpair*2+1],0]=np.nanmean(pd.DataFrame(within_pull_gaze_time_corr_all_ipair['succpull']),axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "# plot the summarizing figure\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "dependencytargets = ['gazenumber_persec']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency, separate successful and failed pull\n",
    "pull_gaze_time_corr_tgt_succpull = pull_gaze_time_corr_mean_all['succpull']\n",
    "pull_gaze_time_corr_tgt_failpull = pull_gaze_time_corr_mean_all['failedpull']\n",
    "measure_tgt_name = 'time point correlation' \n",
    "# \n",
    "pull_gaze_time_corr_tgt_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull)\n",
    "pull_gaze_time_corr_tgt_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_succpull_df['type'] = 'succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull)\n",
    "pull_gaze_time_corr_tgt_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_failpull_df['type'] = 'failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_succpull_df,pull_gaze_time_corr_tgt_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[0].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals' ,fontsize=24)\n",
    "# axs.ravel()[0].set_ylim([-2.35,2.35])\n",
    "# axs.ravel()[0].set_ylim([0,1])\n",
    "axs.ravel()[0].legend(fontsize=18)\n",
    "\n",
    "# plot 2\n",
    "# separating male and female\n",
    "pull_gaze_time_corr_tgt_male_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_succpull_df['type'] = 'male succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_male_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,9],:])\n",
    "pull_gaze_time_corr_tgt_male_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_male_failpull_df['type'] = 'male failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_succpull_df['type'] = 'female succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_female_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,6,7,8],:])\n",
    "pull_gaze_time_corr_tgt_female_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_female_failpull_df['type'] = 'female failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_male_succpull_df,pull_gaze_time_corr_tgt_male_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_female_succpull_df,pull_gaze_time_corr_tgt_female_failpull_df,])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[1].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female' ,fontsize=24)\n",
    "# axs.ravel()[1].set_ylim([-2.35,2.35])\n",
    "# axs.ravel()[1].set_ylim([0,1])\n",
    "axs.ravel()[1].legend(fontsize=18)\n",
    "\n",
    "# plot 3\n",
    "# separating subordinate and dominant\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_succpull_df['type'] = 'subordinate succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[0,2,4,6,8],:])\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_sub_failpull_df['type'] = 'subordinate failpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_succpull[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_succpull_df['type'] = 'dominant succpull'\n",
    "#\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df = pd.DataFrame(pull_gaze_time_corr_tgt_failpull[[1,3,5,7,9],:])\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df.columns = dependencytargets\n",
    "pull_gaze_time_corr_tgt_dom_failpull_df['type'] = 'dominant failpull'\n",
    "#\n",
    "df_long=pd.concat([pull_gaze_time_corr_tgt_sub_succpull_df,pull_gaze_time_corr_tgt_sub_failpull_df,\n",
    "                   pull_gaze_time_corr_tgt_dom_succpull_df,pull_gaze_time_corr_tgt_dom_failpull_df])\n",
    "df_long2 = df_long.melt(id_vars=['type'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "# barplot ans swarmplot\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='type',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].xaxis.set_tick_params(labelsize=20)\n",
    "axs.ravel()[2].set_ylabel(measure_tgt_name,fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom' ,fontsize=34)\n",
    "# axs.ravel()[2].set_ylim([-2.35,2.35])\n",
    "# axs.ravel()[2].set_ylim([0,1])\n",
    "axs.ravel()[2].legend(fontsize=18)\n",
    "\n",
    "\n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    #      \n",
    "    plt.savefig(figsavefolder+\"succorfailpulls_gazenumber_per_second.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29051e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115ca7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "409fec98",
   "metadata": {},
   "source": [
    "##### one example session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal1_fixedorder = 'eddie'\n",
    "animal2_fixedorder = 'sparkle'\n",
    "idate_name = '20230327'\n",
    "\n",
    "kernel_width = 1\n",
    "do_kernel = 0\n",
    "\n",
    "# load the DBN related analysis\n",
    "# load data\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "#\n",
    "if not mergetempRos:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes = pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes = pickle.load(f)\n",
    "#\n",
    "# load data for successful and failed pulls\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "if not mergetempRos:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes_succfail = pickle.load(f)\n",
    "\n",
    "DBN_input_data_idate_succ = DBN_input_data_alltypes_succfail['succpull'][idate_name]\n",
    "DBN_input_data_idate_fail = DBN_input_data_alltypes_succfail['failedpull'][idate_name]\n",
    "DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(2,1)\n",
    "fig.set_figheight(2*5)\n",
    "fig.set_figwidth(1*10)\n",
    "\n",
    "#####\n",
    "# animal 1 gaze\n",
    "\n",
    "xxx1 = (np.array(DBN_input_data_idate_succ['pull1_t0'])==1)*1\n",
    "#\n",
    "if do_kernel:\n",
    "    xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "    xxx1 = np.where(xxx1==1)[0]\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx1.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "    xxx1 = np.exp(log_dens)\n",
    "                \n",
    "xxx2 = (np.array(DBN_input_data_idate_fail['pull1_t0'])==1)*1\n",
    "#\n",
    "if do_kernel:\n",
    "    xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "    xxx2 = np.where(xxx2==1)[0]\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx2.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "    xxx2 = np.exp(log_dens)\n",
    "\n",
    "xxx3 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "#\n",
    "if do_kernel:\n",
    "    xxx_plot = np.linspace(0, np.shape(xxx3)[0], np.shape(xxx3)[0])\n",
    "    xxx3 = np.where(xxx3==1)[0]\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx3.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "    xxx3 = np.exp(log_dens)\n",
    "\n",
    "#\n",
    "axs[0].plot(xxx1)\n",
    "axs[0].plot(xxx2)\n",
    "axs[0].plot(xxx3)\n",
    "axs[0].set_title(animal1_fixedorder + ' in '+idate_name)\n",
    "axs[0].legend(['succ pulls','failed pulls','social gaze'])\n",
    "axs[0].set_ylabel('probability density')\n",
    "axs[0].set_ylim(-0.0001,0.0081)\n",
    "\n",
    "\n",
    "rr,pp = scipy.stats.spearmanr(xxx1, xxx2)\n",
    "print('corr between animal 1 succ and failed pulls: '+str(rr))\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx1, xxx3)\n",
    "print('corr between animal 1 succ pulls and animal 1 social gaze: '+str(rr))\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx2, xxx3)\n",
    "print('corr between animal 1 failed pulls and animal 1 socal gaze: '+str(rr))\n",
    "\n",
    "\n",
    "#####\n",
    "# animal 2 gaze\n",
    "\n",
    "xxx21 = (np.array(DBN_input_data_idate_succ['pull2_t0'])==1)*1\n",
    "#\n",
    "if do_kernel:\n",
    "    xxx_plot = np.linspace(0, np.shape(xxx21)[0], np.shape(xxx21)[0])\n",
    "    xxx21 = np.where(xxx21==1)[0]\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx21.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "    xxx21 = np.exp(log_dens)\n",
    "                \n",
    "xxx22 = (np.array(DBN_input_data_idate_fail['pull2_t0'])==1)*1\n",
    "#\n",
    "if do_kernel:\n",
    "    xxx_plot = np.linspace(0, np.shape(xxx22)[0], np.shape(xxx22)[0])\n",
    "    xxx22 = np.where(xxx22==1)[0]\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx22.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "    xxx22 = np.exp(log_dens)\n",
    "\n",
    "xxx23 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "#\n",
    "if do_kernel:\n",
    "    xxx_plot = np.linspace(0, np.shape(xxx23)[0], np.shape(xxx23)[0])\n",
    "    xxx23 = np.where(xxx23==1)[0]\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx23.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "    xxx23 = np.exp(log_dens)\n",
    "\n",
    "axs[1].plot(xxx21)\n",
    "axs[1].plot(xxx22)\n",
    "axs[1].plot(xxx23)\n",
    "axs[1].set_title(animal2_fixedorder + ' in '+idate_name)\n",
    "axs[1].legend(['succ pulls','failed pulls','social gaze'])\n",
    "axs[1].set_ylabel('probability density')\n",
    "axs[1].set_xlabel('time (s)')\n",
    "axs[1].set_ylim(-0.0001,0.0081)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"example_session_gaze_succ_failed_pulls.pdf\")\n",
    "\n",
    "\n",
    "rr,pp = scipy.stats.spearmanr(xxx21, xxx22)\n",
    "print('corr between animal 2 succ and failed pulls: '+str(rr))\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx21, xxx23)\n",
    "print('corr between animal 2 succ pulls and animal 2 social gaze: '+str(rr))\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx22, xxx23)\n",
    "print('corr between animal 2 failed pulls and animal 2 socal gaze: '+str(rr))\n",
    "\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx1, xxx23)\n",
    "print('corr between animal 1 succ pulls and animal 2 social gaze: '+str(rr))\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx2, xxx23)\n",
    "print('corr between animal 1 failed pulls and animal 2 socal gaze: '+str(rr))\n",
    "\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx21, xxx3)\n",
    "print('corr between animal 2 succ pulls and animal 1 social gaze: '+str(rr))\n",
    "#\n",
    "rr,pp = scipy.stats.spearmanr(xxx22, xxx3)\n",
    "print('corr between animal 2 failed pulls and animal 1 socal gaze: '+str(rr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e946637",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx1 = (np.array(DBN_input_data_idate_succ['pull1_t0'])==1)*1\n",
    "#\n",
    "xxx_plot = np.linspace(0, np.shape(xxx1)[0], np.shape(xxx1)[0])\n",
    "xxx1 = np.where(xxx1==1)[0]\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx1.reshape(-1, 1))\n",
    "log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "xxx1 = np.exp(log_dens)\n",
    "                \n",
    "xxx2 = (np.array(DBN_input_data_idate_succ['pull2_t0'])==1)*1\n",
    "#\n",
    "xxx_plot = np.linspace(0, np.shape(xxx2)[0], np.shape(xxx2)[0])\n",
    "xxx2 = np.where(xxx2==1)[0]\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_width).fit(xxx2.reshape(-1, 1))\n",
    "log_dens = kde.score_samples(xxx_plot.reshape(-1, 1))\n",
    "xxx2 = np.exp(log_dens)\n",
    "\n",
    "scipy.stats.spearmanr(xxx1, xxx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c7860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xcorr(x,y,bestlag_limit):\n",
    "    \n",
    "    import scipy.signal as signal\n",
    "    import scipy.stats as st\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"\n",
    "    Perform Cross-Correlation on x and y\n",
    "    x    : 1st signal\n",
    "    y    : 2nd signal\n",
    "    bestlag_limit: a limit for finding the bestlag_limit, in the unit of second\n",
    "\n",
    "    returns\n",
    "    lags : lags of correlation\n",
    "    corr : coefficients of correlation\n",
    "    \"\"\"\n",
    "    corr_signal = signal.correlate(x, y, mode=\"full\")\n",
    "\n",
    "    lags = signal.correlation_lags(len(x), len(y), mode=\"full\")\n",
    "   \n",
    "    corr_signal = abs(corr_signal)\n",
    " \n",
    "    ind_bestlag = np.where(corr_signal==np.max(corr_signal[(lags<bestlag_limit)&(lags>-bestlag_limit)]))[0][0]\n",
    "    bestlag = lags[ind_bestlag]\n",
    "    \n",
    "    return lags, corr_signal, bestlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlagram(x,y,bestlag_limit):\n",
    "    \n",
    "    import scipy.stats as st\n",
    "    import numpy as np\n",
    "    \n",
    "    lags = np.arange(-bestlag_limit,bestlag_limit,1)\n",
    "    \n",
    "    nlags = np.shape(lags)[0]\n",
    "    \n",
    "    corrs = np.nan*np.ones(np.shape(lags))\n",
    "    \n",
    "    ndatasize = np.shape(x)[0]\n",
    "    \n",
    "    x_rightpad = np.concatenate([np.array(x),np.zeros((1,bestlag_limit))[0]])\n",
    "    x_leftpad  = np.concatenate([np.zeros((1,bestlag_limit))[0],np.array(x)])\n",
    "    y_rightpad = np.concatenate([np.array(y),np.zeros((1,bestlag_limit))[0]])\n",
    "    y_leftpad  = np.concatenate([np.zeros((1,bestlag_limit))[0],np.array(y)])\n",
    "    \n",
    "    x_bothpad = np.concatenate([np.zeros((1,bestlag_limit))[0],np.array(x),np.zeros((1,bestlag_limit))[0]])\n",
    "    y_bothpad = np.concatenate([np.zeros((1,bestlag_limit))[0],np.array(y),np.zeros((1,bestlag_limit))[0]])\n",
    "    \n",
    "    \n",
    "    for ilag in np.arange(0,nlags,1):\n",
    "        if lags[ilag] < 0:\n",
    "            try:\n",
    "                corrs[ilag],_ = st.spearmanr(x[0:lags[ilag]],y[-lags[ilag]:])\n",
    "                corrs[ilag] = corrs[ilag]*np.shape(x[0:lags[ilag]])[0]\n",
    "                # corrs[ilag],_ = st.spearmanr(x_rightpad[-lags[ilag]:],y_leftpad[-lags[ilag]:])\n",
    "            except:\n",
    "                corrs[ilag] = np.nan\n",
    "        elif lags[ilag] == 0:\n",
    "            try:\n",
    "                corrs[ilag],_ = st.spearmanr(x,y)\n",
    "                corrs[ilag] = corrs[ilag]*np.shape(x)[0]\n",
    "                # corrs[ilag],_ = st.spearmanr(x_bothpad,y_bothpad)\n",
    "            except:\n",
    "                corrs[ilag] = np.nan\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                corrs[ilag],_ = st.spearmanr(y[0:-lags[ilag]],x[lags[ilag]:])\n",
    "                corrs[ilag] = corrs[ilag]*np.shape(x[lags[ilag]:])[0]\n",
    "                # corrs[ilag],_ = st.spearmanr(x_leftpad[lags[ilag]:],y_rightpad[lags[ilag]:])\n",
    "            except:\n",
    "                corrs[ilag] = np.nan\n",
    "        \n",
    "    ind_bestlag = np.where(abs(corrs)==np.nanmax(abs(corrs)))[0][0]\n",
    "    bestlag = lags[ind_bestlag]\n",
    "    \n",
    "    return lags, corrs, bestlag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05426c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.spearmanr(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.spearmanr([1,2,0,0,0,0,0,0,0,0,],[1,3,0,0,0,0,0,0,0,0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fdfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63ec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaaf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ae4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = np.array([i for i in range(0,15)])\n",
    "# x = 0.84**n\n",
    "# y = np.roll(x,5);\n",
    "# x=xxx2\n",
    "# y=xxx3\n",
    "x = [1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0]\n",
    "y = [0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9]\n",
    "# y = [2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1]\n",
    "# y=x\n",
    "lags,corrs_signal,bestlag = xcorr(x,y,16);\n",
    "plt.figure()\n",
    "plt.stem(lags,corrs_signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags, corrs,bestlag2 = correlagram(x,y,100)\n",
    "plt.figure()\n",
    "plt.stem(lags,abs(corrs))\n",
    "plt.show()\n",
    "bestlag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr,_=st.spearmanr(x[0:-10],y[10:])\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f25e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.correlate(x, y, mode=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.spearmanr(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837293ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc065cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa91f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
