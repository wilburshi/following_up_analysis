{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the all the sessions\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) \n",
    "### only focus on the strategy switching session (among Ginger/Kanga/Dodson/Dannon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions_StraSwitch'\n",
    "else:\n",
    "    savefile_sufix = '_StraSwitch'\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "            \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "            \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20240924\",\"20240926\",\"20241001\",\"20241003\",\"20241007\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                             0.00, 43.0, 20.0, 0.00, 0.00,\n",
    "                              ] # in second\n",
    "            \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['ginger']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Ginger\"\n",
    "     \n",
    "# ginger kanga\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second \n",
    "    elif do_bestsession:       \n",
    "        dates_list = [\n",
    "                      \"20240923\",\"20240925\",\"20240930\",\"20241002\",\"20241004\",\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 19.0, 0.00, 26.8, 35.0, 15.4,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                              \n",
    "                              ] # in second \n",
    "    elif do_bestsession: \n",
    "        dates_list = [\n",
    "                      \"20240926\", \"20241001\", \"20241003\", \"20241007\",\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                   0.00,  37.0, 0.00, 0.00,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "\n",
    "#    \n",
    "# dates_list = [\"20221128\"]\n",
    "# session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "sess_videotimes_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# save the session start time\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "if not os.path.exists(data_saved_subfolder):\n",
    "    os.makedirs(data_saved_subfolder)\n",
    "#\n",
    "with open(data_saved_subfolder+'sessstart_time_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "    pickle.dump(session_start_times, f)\n",
    "with open(data_saved_subfolder+'dates_list_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "    pickle.dump(dates_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        sess_videotimes_all_dates = pickle.load(f)    \n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        try:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        min_length = np.min(list(bodyparts_locs_camI.values())[0].shape[0])\n",
    "        \n",
    "        sess_videotimes_all_dates[idate] = min_length/fps\n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]#/(min_length/fps)\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]#/(min_length/fps)\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]#/(min_length/fps)\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]#/(min_length/fps)\n",
    "        else:\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]#/(min_length/fps)\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]#/(min_length/fps)\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]#/(min_length/fps)\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]#/(min_length/fps)\n",
    "\n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 1:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                              lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                              animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                              animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                              video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sess_videotimes_all_dates, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasktypes_all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aada694",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900d890",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b179dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['MC']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-0,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "\n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b01345",
   "metadata": {},
   "source": [
    "### plot some other basis behavioral measures\n",
    "#### successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e545fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),succ_rate_all_dates[sorting_df.index],'o',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"successful rate\",fontsize=13)\n",
    "ax1.set_ylim([-0.1,1.1])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['MC']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"successfulrate_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b5e6d",
   "metadata": {},
   "source": [
    "#### animal pull numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e99c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull1_num_all_dates[sorting_df.index],'bv',markersize=5,label='animal1 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull2_num_all_dates[sorting_df.index],'rv',markersize=5,label='animal2 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pullmean_num_all_dates[sorting_df.index],'kv',markersize=8,label='mean pull #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"pull numbers\",fontsize=13)\n",
    "ax1.set_ylim([-20,240])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['MC',]\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,240],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pullnumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba624af5",
   "metadata": {},
   "source": [
    "#### gaze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20149789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "print(np.nanmax(gaze1_num_all_dates))\n",
    "print(np.nanmax(gaze2_num_all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze1_num_all_dates[sorting_df.index],'b^',markersize=5,label='animal1 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze2_num_all_dates[sorting_df.index],'r^',markersize=5,label='animal2 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazemean_num_all_dates[sorting_df.index],'k^',markersize=8,label='mean gaze #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"social gaze number\",fontsize=13)\n",
    "ax1.set_ylim([-20,1500])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['MC']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,1500],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_numbers = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/30\n",
    "gaze_pull_ratios = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/(pull1_num_all_dates+pull2_num_all_dates)/30\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['MC']\n",
    "\n",
    "gaze_numbers_groups = [\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                       ]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(gaze_numbers_groups)\n",
    "\n",
    "plt.xticks(np.arange(1, len(grouptypes)+1, 1), grouptypes, fontsize = 12);\n",
    "ax1.set_ylim([-30/30,5400/30])\n",
    "ax1.set_ylabel(\"average social gaze numbers\",fontsize=13)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240c38b",
   "metadata": {},
   "source": [
    "## plot the gaze numbers for all individuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56538078",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "\n",
    "    animal1_fixedorders = ['dodson',       'ginger_withK', 'dannon']\n",
    "    animal2_fixedorders = ['ginger_withD', 'kanga_withG',  'kanga_withD']\n",
    "    \n",
    "    animal1_filenames = ['dodson', 'ginger', 'dannon']\n",
    "    animal2_filenames = ['ginger', 'kanga',  'kanga']\n",
    "    \n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    grouptypes = ['MC',]\n",
    "    coopthres_IDs = [ 1, ]\n",
    "    \n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    gazenum_foreachgroup_foreachAni = pd.DataFrame(columns=['dates','condition','act_animal','gazenumber','pullnumber'])\n",
    "    #\n",
    "\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        #\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1 = animal1_fixedorders[ianimalpair]\n",
    "            animal2 = animal2_fixedorders[ianimalpair]\n",
    "\n",
    "            animal1_filename = animal1_filenames[ianimalpair]\n",
    "            animal2_filename = animal2_filenames[ianimalpair]\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_filename+animal2_filename+'/'\n",
    "            with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                owgaze1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                owgaze2_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                mtgaze1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                mtgaze2_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                pull1_num_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                succ_rate_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                interpullintv_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                trialnum_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "            with open(data_saved_subfolder+'/sess_videotimes_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                sess_videotimes_all_dates = pickle.load(f)\n",
    "            \n",
    "            with open(data_saved_subfolder+'/dates_list_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                dates_list_all_dates = pickle.load(f)\n",
    "            dates_list_all_dates = np.array(dates_list_all_dates)\n",
    "        \n",
    "            \n",
    "            # combine owgaze and mtgaze\n",
    "            gaze1_num_all_dates = (owgaze1_num_all_dates + mtgaze1_num_all_dates)/sess_videotimes_all_dates\n",
    "            gaze2_num_all_dates = (owgaze2_num_all_dates + mtgaze2_num_all_dates)/sess_videotimes_all_dates\n",
    "\n",
    "            #\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "            dates_list_tgt = dates_list_all_dates[np.transpose(coopthres_forsort==coopthres_ID)[0]]\n",
    "            gaze1_nums_tgt = gaze1_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            gaze2_nums_tgt = gaze2_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            pull1_nums_tgt = pull1_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            pull2_nums_tgt = pull2_num_all_dates[coopthres_forsort==coopthres_ID]\n",
    "            ndates = np.shape(dates_list_tgt)[0]\n",
    "            \n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list_tgt[idate]\n",
    "                gaze1_num = gaze1_nums_tgt[idate]\n",
    "                gaze2_num = gaze2_nums_tgt[idate]\n",
    "                pull1_num = pull1_nums_tgt[idate]\n",
    "                pull2_num = pull2_nums_tgt[idate]\n",
    "                \n",
    "                gazenum_foreachgroup_foreachAni = gazenum_foreachgroup_foreachAni.append({'dates': date_tgt, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'gazenumber':gaze1_num,\n",
    "                                                                                    'pullnumber':pull1_num,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                \n",
    "                gazenum_foreachgroup_foreachAni = gazenum_foreachgroup_foreachAni.append({'dates': date_tgt, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'gazenumber':gaze2_num,\n",
    "                                                                                    'pullnumber':pull2_num,      \n",
    "                                                                                   }, ignore_index=True)\n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "    # for plot\n",
    "    fig, axs = plt.subplots(2,ngrouptypes)\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(7*ngrouptypes)\n",
    "    \n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "\n",
    "        gazenum_foreachgroup_foreachAni_toplot = gazenum_foreachgroup_foreachAni[gazenum_foreachgroup_foreachAni['condition']==grouptype]\n",
    "\n",
    "        # seaborn.boxplot(ax=axs[0],data=gazenum_foreachgroup_foreachAni_toplot,\n",
    "        #                 x='act_animal',y='pullnumber')  \n",
    "        seaborn.violinplot(ax=axs[0],data=gazenum_foreachgroup_foreachAni_toplot,\n",
    "                        x='act_animal',y='pullnumber')  \n",
    "        axs[0].set_title('pull number')\n",
    "        \n",
    "        # seaborn.boxplot(ax=axs[1],data=gazenum_foreachgroup_foreachAni_toplot,\n",
    "        #                 x='act_animal',y='gazenumber')  \n",
    "        seaborn.violinplot(ax=axs[1],data=gazenum_foreachgroup_foreachAni_toplot,\n",
    "                        x='act_animal',y='gazenumber')  \n",
    "        axs[1].set_title('gaze number')\n",
    "        \n",
    "        # perform the anova on all animals\n",
    "        if 0:\n",
    "            import statsmodels.api as sm\n",
    "            from statsmodels.formula.api import ols\n",
    "            from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "            # anova\n",
    "            cw_lm=ols('pullnumber ~ act_animal', data=gazenum_foreachgroup_foreachAni_toplot).fit() #Specify C for Categorical\n",
    "            print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "            # post hoc test \n",
    "            tukey = pairwise_tukeyhsd(endog=gazenum_foreachgroup_foreachAni_toplot['pullnumber'], \n",
    "                                      groups=gazenum_foreachgroup_foreachAni_toplot['act_animal'], alpha=0.05)\n",
    "            print(tukey)\n",
    "\n",
    "\n",
    "            cw_lm=ols('gazenumber ~ act_animal', data=gazenum_foreachgroup_foreachAni_toplot).fit() #Specify C for Categorical\n",
    "            print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "            # post hoc test \n",
    "            tukey = pairwise_tukeyhsd(endog=gazenum_foreachgroup_foreachAni_toplot['gazenumber'], \n",
    "                                      groups=gazenum_foreachgroup_foreachAni_toplot['act_animal'], alpha=0.05)\n",
    "            print(tukey)\n",
    "    \n",
    "        # perform t test\n",
    "        if 1:\n",
    "            data1 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='ginger_withD']['gazenumber']\n",
    "            data2 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='ginger_withK']['gazenumber']\n",
    "            t_stat, p_value = st.ttest_ind(data1, data2)\n",
    "            print('Ginger with D or K gazenumber '+'ttest p value = '+str(p_value))\n",
    "    \n",
    "            data1 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='ginger_withD']['pullnumber']\n",
    "            data2 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='ginger_withK']['pullnumber']\n",
    "            t_stat, p_value = st.ttest_ind(data1, data2)\n",
    "            print('Ginger with D or K pullnumber '+'ttest p value = '+str(p_value))\n",
    "            \n",
    "            data1 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='kanga_withD']['gazenumber']\n",
    "            data2 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='kanga_withG']['gazenumber']\n",
    "            t_stat, p_value = st.ttest_ind(data1, data2)\n",
    "            print('Kanga with D or G gazenumber '+'ttest p value = '+str(p_value))\n",
    "    \n",
    "            data1 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='kanga_withD']['pullnumber']\n",
    "            data2 = gazenum_foreachgroup_foreachAni_toplot[gazenum_foreachgroup_foreachAni_toplot['act_animal']=='kanga_withG']['pullnumber']\n",
    "            t_stat, p_value = st.ttest_ind(data1, data2)\n",
    "            print('Kanga with D or G pullnumber '+'ttest p value = '+str(p_value))\n",
    "            \n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        plt.savefig(figsavefolder+\"socialgazenumber_pullnumber_summary_forallAnimals.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_figheight(4*1)\n",
    "fig.set_figwidth(8*2)\n",
    "\n",
    "gazenum_toplot = gazenum_foreachgroup_foreachAni[gazenum_foreachgroup_foreachAni['condition']=='MC']\n",
    "    \n",
    "# for Ginger\n",
    "ind_G = (gazenum_toplot['act_animal']=='ginger_withD') | (gazenum_toplot['act_animal']=='ginger_withK')\n",
    "gazenum_toplot_G = gazenum_toplot[ind_G]\n",
    "gazenum_toplot_sorted = gazenum_toplot_G.sort_values(by=['dates'])\n",
    "\n",
    "seaborn.lineplot(ax=axs[0],data=gazenum_toplot_sorted,\n",
    "                 x='dates',y='gazenumber',color='darkgray') \n",
    "seaborn.scatterplot(ax=axs[0],data=gazenum_toplot_sorted,\n",
    "                 x='dates',y='gazenumber',hue='act_animal',s=150) \n",
    "axs[0].set_ylabel('gaze number per second')\n",
    "\n",
    "\n",
    "# for Kanga\n",
    "ind_K = (gazenum_toplot['act_animal']=='kanga_withD') | (gazenum_toplot['act_animal']=='kanga_withG')\n",
    "gazenum_toplot_K = gazenum_toplot[ind_K]\n",
    "gazenum_toplot_sorted = gazenum_toplot_K.sort_values(by=['dates'])\n",
    "\n",
    "seaborn.lineplot(ax=axs[1],data=gazenum_toplot_sorted,\n",
    "                 x='dates',y='gazenumber',color='darkgray') \n",
    "seaborn.scatterplot(ax=axs[1],data=gazenum_toplot_sorted,\n",
    "                 x='dates',y='gazenumber',hue='act_animal',s=150) \n",
    "axs[1].set_ylabel('gaze number per second')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig.savefig(figsavefolder+\"socialgazenumber_ChangeOverDays_GingerAndKanga.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazenum_foreachgroup_foreachAni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_input_data_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "doBhvitv_timebin = 0 # 1: if use the mean bhv event interval for time bin\n",
    "\n",
    "prepare_input_data = 0\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "        except:    \n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            \n",
    "        # get animal info\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "        \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "            \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "\n",
    "        # load behavioral event results\n",
    "        print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "            output_look_ornot = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "            output_allvectors = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "            output_allangles = pickle.load(f)  \n",
    "        #\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # redefine the totalsess_time for the length of each recording (NOT! remove the session_start_time)\n",
    "        totalsess_time = int(np.ceil(np.shape(look_at_other_or_not_merge['dodson'])[0]/fps))\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']   \n",
    "\n",
    "        \n",
    "\n",
    "        if mergetempRos:\n",
    "            temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            # use bhv event to decide temporal resolution\n",
    "            #\n",
    "            #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "        #\n",
    "        if doBhvitv_timebin:\n",
    "            pull_other_intv_ii = pd.Series(bhv_intv_all_dates[date_tgt]['pull_other_pooled'])\n",
    "            # remove the interval that is too large\n",
    "            pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "            # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "            temp_resolus = [np.nanmean(pull_other_intv_ii)]          \n",
    "        #\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "        \n",
    "        # try different temporal resolutions\n",
    "        for temp_resolu in temp_resolus:\n",
    "            bhv_df = []\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            else:\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "            if len(bhv_df)==0:\n",
    "                bhv_df = bhv_df_itr\n",
    "            else:\n",
    "                bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "            DBN_input_data_alltypes[date_tgt] = bhv_df\n",
    "            \n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b853146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(np.ceil(np.shape(look_at_other_or_not_merge['dodson'])[0]/fps-session_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb30f8",
   "metadata": {},
   "source": [
    "#### plot the gaze distribution around pulls, analysis is based on the DBN_input_data all session format\n",
    "#### similar plot was in \"3LagDBN_and_SuccAndFailedPull_singlecam_wholebodylabels_allsessions_basicEvents\" looking at the difference between successful and failed pulls\n",
    "#### pool across all animals, compared self reward, 3s to 1s cooperation and no vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39116ea6",
   "metadata": {},
   "source": [
    "#### get the half (max - min) width for selected conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splrep, sproot, splev\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.optimize import curve_fit \n",
    "\n",
    "class MultiplePeaks(Exception): pass\n",
    "class NoPeaksFound(Exception): pass\n",
    "\n",
    "def fwhm(x, y, k=10):\n",
    "    \"\"\"\n",
    "    Determine full-with-half-maximum of a peaked set of points, x and y.\n",
    "\n",
    "    Assumes that there is only one peak present in the datasset.  The function\n",
    "    uses a spline interpolation of order k.\n",
    "    \"\"\"\n",
    "\n",
    "    half_max = max(y)/2.0\n",
    "    # half_max = y[round(np.shape(y)[0]/2)-1]\n",
    "    s = splrep(x, y - half_max, k=k)\n",
    "    roots = sproot(s)\n",
    "\n",
    "    if len(roots) > 2:\n",
    "    #     raise MultiplePeaks(\"The dataset appears to have multiple peaks, and \"\n",
    "    #             \"thus the FWHM can't be determined.\")\n",
    "        # return np.nan\n",
    "        return abs(roots[1] - roots[0])\n",
    "    elif len(roots) < 2:\n",
    "    #     raise NoPeaksFound(\"No proper peaks were found in the data set; likely \"\n",
    "    #             \"the dataset is flat (e.g. all zeros).\")\n",
    "        # return np.max(x)-np.min(x)\n",
    "        return np.nan\n",
    "    else:\n",
    "        return abs(roots[1] - roots[0])\n",
    "        \n",
    "        \n",
    "#\n",
    "# Define the Gaussian function \n",
    "def Gauss(x, A, B): \n",
    "    y = A*np.exp(-1*B*x**2) \n",
    "    return y \n",
    "\n",
    "# Define the Gaussian function\n",
    "def gaussian(x, A, B, C):\n",
    "    y = A*np.exp(-1*B*(x-C)**2) \n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac75899",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # PLOT multiple pairs in one plot, so need to load data seperately\n",
    "    mergetempRos = 0 # 1: merge different time bins\n",
    "    minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "    moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "    \n",
    "    temp_resolu = 1\n",
    "    dist_twin_range = 5\n",
    "    \n",
    "    #\n",
    "    animal1_fixedorders = ['dodson',       'ginger_withK', 'dannon']\n",
    "    animal2_fixedorders = ['ginger_withD', 'kanga_withG',  'kanga_withD']\n",
    "    \n",
    "    animal1_filenames = ['dodson', 'ginger', 'dannon']\n",
    "    animal2_filenames = ['ginger', 'kanga',  'kanga']\n",
    "    \n",
    "    nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "    grouptypes = ['MC',]\n",
    "    coopthres_IDs = [ 1, ]\n",
    "    ngrouptypes = np.shape(grouptypes)[0]\n",
    "\n",
    "    # initiate the final data set\n",
    "    SameAnimal_gazeDist_mean_forEachAni = pd.DataFrame(columns=['dates','condition','act_animal','trig_average'])\n",
    "    AcroAnimal_gazeDist_mean_forEachAni = pd.DataFrame(columns=['dates','condition','act_animal','trig_average'])\n",
    "    # shuffle both the pull and gaze time stamp\n",
    "    SameAnimal_gazeDist_shuffle_forEachAni = pd.DataFrame(columns=['dates','condition','act_animal','trig_average'])\n",
    "    AcroAnimal_gazeDist_shuffle_forEachAni = pd.DataFrame(columns=['dates','condition','act_animal','trig_average'])\n",
    "    \n",
    "    \n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "        grouptype = grouptypes[igrouptype]\n",
    "        coopthres_ID = coopthres_IDs[igrouptype]\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "            animal1 = animal1_fixedorders[ianimalpair]\n",
    "            animal2 = animal2_fixedorders[ianimalpair]\n",
    "            #\n",
    "            animal1_filename = animal1_filenames[ianimalpair]\n",
    "            animal2_filename = animal2_filenames[ianimalpair]\n",
    "\n",
    "            # load the basic behavioral measures\n",
    "            # load saved data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_filename+animal2_filename+'/'\n",
    "            #\n",
    "            with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                tasktypes_all_dates = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_filename+animal2_filename+'.pkl', 'rb') as f:\n",
    "                coopthres_all_dates = pickle.load(f)\n",
    "\n",
    "            #     \n",
    "            # load the DBN related analysis\n",
    "            # load data\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_filename+animal2_filename+'/'\n",
    "            #\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename+animal2_filename+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_filename+animal2_filename+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            #\n",
    "            # re-organize the target dates\n",
    "            # 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "            tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "            coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "            coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "            #\n",
    "            # sort the data based on task type and dates\n",
    "            dates_list = list(DBN_input_data_alltypes.keys())\n",
    "            sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "            sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "            #\n",
    "            # only select the targeted dates\n",
    "            sorting_tgt_df = sorting_df[(sorting_df['coopthres']==coopthres_ID)]\n",
    "            dates_list_tgt = sorting_tgt_df['dates']\n",
    "            dates_list_tgt = np.array(dates_list_tgt)\n",
    "            #\n",
    "            ndates_tgt = np.shape(dates_list_tgt)[0]\n",
    "\n",
    "            # \n",
    "            for idate in np.arange(0,ndates_tgt,1):\n",
    "                idate_name = dates_list_tgt[idate]\n",
    "\n",
    "                DBN_input_data_idate = DBN_input_data_alltypes[idate_name]\n",
    "\n",
    "                # pull1_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_mean_forEachAni = SameAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2)/np.sum(xxx1)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_mean_forEachAni = SameAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_shuffle_forEachAni = SameAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2_shuffle)/np.sum(xxx1_shuffle)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_shuffle_forEachAni = SameAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # pull2_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_mean_forEachAni = SameAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2)/np.sum(xxx1)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_mean_forEachAni = SameAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                SameAnimal_gazeDist_shuffle_forEachAni = SameAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2_shuffle)/np.sum(xxx1_shuffle)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    SameAnimal_gazeDist_shuffle_forEachAni = SameAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "                # pull1_t0 and gaze2_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull1_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze2_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_mean_forEachAni = AcroAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2)/np.sum(xxx1)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_mean_forEachAni = AcroAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_shuffle_forEachAni = AcroAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2_shuffle)/np.sum(xxx1_shuffle)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_shuffle_forEachAni = AcroAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal2,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # pull2_t0 and gaze1_t0\n",
    "                xxx1 = (np.array(DBN_input_data_idate['pull2_t0'])==1)*1\n",
    "                xxx2 = (np.array(DBN_input_data_idate['owgaze1_t0'])==1)*1\n",
    "                xxx1_shuffle = xxx1.copy()\n",
    "                np.random.shuffle(xxx1_shuffle)\n",
    "                xxx2_shuffle = xxx2.copy()\n",
    "                np.random.shuffle(xxx2_shuffle)\n",
    "                # pad the two sides\n",
    "                xxx1 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2 = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx1_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx1_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                xxx2_shuffle = np.hstack([np.zeros((1,dist_twin_range))[0],xxx2_shuffle,np.zeros((1,dist_twin_range))[0]])\n",
    "                # \n",
    "                npulls = int(np.nansum(xxx1))\n",
    "                pullIDs = np.where(xxx1 == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_mean_forEachAni = AcroAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2)/np.sum(xxx1)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_mean_forEachAni = AcroAnimal_gazeDist_mean_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                # shuffle\n",
    "                npulls = int(np.nansum(xxx1_shuffle))\n",
    "                pullIDs = np.where(xxx1_shuffle == 1)[0]\n",
    "                gazenum_dist_temp = np.zeros((npulls,2*dist_twin_range+1))\n",
    "                #\n",
    "                for ipull in np.arange(0,npulls,1):\n",
    "                    pullID = pullIDs[ipull]\n",
    "                    gazenum_dist_temp[ipull,:] = xxx2_shuffle[np.arange(pullID-dist_twin_range,pullID+dist_twin_range+1,1)]\n",
    "                AcroAnimal_gazeDist_shuffle_forEachAni = AcroAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.nanmean(gazenum_dist_temp,axis=0)/(np.sum(xxx2_shuffle)/np.sum(xxx1_shuffle)),\n",
    "                                                                                   }, ignore_index=True)\n",
    "                if npulls == 0:\n",
    "                    AcroAnimal_gazeDist_shuffle_forEachAni = AcroAnimal_gazeDist_shuffle_forEachAni.append({'dates': idate_name, \n",
    "                                                                                    'condition':grouptype,\n",
    "                                                                                    'act_animal':animal1,\n",
    "                                                                                    'trig_average':np.ones((1,2*dist_twin_range+1))[0]*np.nan,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "    #\n",
    "    if 1:\n",
    "\n",
    "        xxx = np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "        # for plot\n",
    "        fig, axs = plt.subplots(ngrouptypes,2)\n",
    "        fig.set_figheight(5*ngrouptypes)\n",
    "        fig.set_figwidth(5*2)\n",
    "\n",
    "        for iplottype in np.arange(0,2,1):\n",
    "            \n",
    "            for igrouptype in np.arange(0,ngrouptypes,1):\n",
    "\n",
    "                grouptype = grouptypes[igrouptype]\n",
    "\n",
    "                SameAnimal_mean_toplot = SameAnimal_gazeDist_mean_forEachAni[SameAnimal_gazeDist_mean_forEachAni['condition']==grouptype]\n",
    "                SameAnimal_shuffle_toplot = SameAnimal_gazeDist_shuffle_forEachAni[SameAnimal_gazeDist_shuffle_forEachAni['condition']==grouptype]\n",
    "\n",
    "                AcroAnimal_mean_toplot = AcroAnimal_gazeDist_mean_forEachAni[AcroAnimal_gazeDist_mean_forEachAni['condition']==grouptype]\n",
    "                AcroAnimal_shuffle_toplot = AcroAnimal_gazeDist_shuffle_forEachAni[AcroAnimal_gazeDist_shuffle_forEachAni['condition']==grouptype]\n",
    "\n",
    "                # plot, all animals in one figure\n",
    "                conds_forplot = ['ginger_withK','ginger_withD','kanga_withG','kanga_withD','dodson','dannon']\n",
    "                # conds_forplot = ['ginger_withK','ginger_withD','kanga_withG','kanga_withD',]\n",
    "\n",
    "                gazeDist_average_forplot = dict.fromkeys(conds_forplot,[])\n",
    "                gazeDist_std_forplot = dict.fromkeys(conds_forplot,[])\n",
    "                gazeDist_average_shf_forplot = dict.fromkeys(conds_forplot,[])\n",
    "                gazeDist_std_shf_forplot = dict.fromkeys(conds_forplot,[])\n",
    "                for cond_forplot in conds_forplot:\n",
    "                    if iplottype == 0:\n",
    "                        gazeDist_average_forplot[cond_forplot] = np.nanmean(np.vstack(list(SameAnimal_mean_toplot[SameAnimal_mean_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)\n",
    "                        gazeDist_std_forplot[cond_forplot] = np.nanstd(np.vstack(list(SameAnimal_mean_toplot[SameAnimal_mean_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)/np.sqrt(np.shape(np.vstack(list(SameAnimal_mean_toplot[SameAnimal_mean_toplot['act_animal']==cond_forplot]['trig_average'])))[0])\n",
    "                        #\n",
    "                        gazeDist_average_shf_forplot[cond_forplot] = np.nanmean(np.vstack(list(SameAnimal_shuffle_toplot[SameAnimal_shuffle_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)\n",
    "                        gazeDist_std_shf_forplot[cond_forplot] = np.nanstd(np.vstack(list(SameAnimal_shuffle_toplot[SameAnimal_shuffle_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)/np.sqrt(np.shape(np.vstack(list(SameAnimal_shuffle_toplot[SameAnimal_shuffle_toplot['act_animal']==cond_forplot]['trig_average'])))[0])\n",
    "                    elif iplottype == 1:\n",
    "                        gazeDist_average_forplot[cond_forplot] = np.nanmean(np.vstack(list(AcroAnimal_mean_toplot[AcroAnimal_mean_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)\n",
    "                        gazeDist_std_forplot[cond_forplot] = np.nanstd(np.vstack(list(AcroAnimal_mean_toplot[AcroAnimal_mean_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)/np.sqrt(np.shape(np.vstack(list(AcroAnimal_mean_toplot[AcroAnimal_mean_toplot['act_animal']==cond_forplot]['trig_average'])))[0])\n",
    "                        #\n",
    "                        gazeDist_average_shf_forplot[cond_forplot] = np.nanmean(np.vstack(list(AcroAnimal_shuffle_toplot[AcroAnimal_shuffle_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)\n",
    "                        gazeDist_std_shf_forplot[cond_forplot] = np.nanstd(np.vstack(list(AcroAnimal_shuffle_toplot[AcroAnimal_shuffle_toplot['act_animal']==cond_forplot]['trig_average'])),axis=0)/np.sqrt(np.shape(np.vstack(list(AcroAnimal_shuffle_toplot[AcroAnimal_shuffle_toplot['act_animal']==cond_forplot]['trig_average'])))[0])\n",
    "\n",
    "                    if ngrouptypes > 1:\n",
    "                        axs[igrouptype,iplottype].errorbar(xxx,gazeDist_average_forplot[cond_forplot],\n",
    "                                        gazeDist_std_forplot[cond_forplot],label=cond_forplot)\n",
    "                        # axs[igrouptype,iplottype].errorbar(xxx,gazeDist_average_shf_forplot[cond_forplot],\n",
    "                        #                 gazeDist_std_shf_forplot[cond_forplot],label=\"shuffled \"+cond_forplot)\n",
    "                    elif ngrouptypes == 1:\n",
    "                        axs[iplottype].errorbar(xxx,gazeDist_average_forplot[cond_forplot],\n",
    "                                        gazeDist_std_forplot[cond_forplot],label=cond_forplot)\n",
    "                        # axs[iplottype].errorbar(xxx,gazeDist_average_shf_forplot[cond_forplot],\n",
    "                        #                gazeDist_std_shf_forplot[cond_forplot],label=\"shuffled \"+cond_forplot)\n",
    "               \n",
    "                if ngrouptypes > 1:        \n",
    "                    axs[igrouptype,iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "                    axs[igrouptype,iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "                    axs[igrouptype,iplottype].set_ylim(0,0.3)\n",
    "                    # axs[igrouptype,iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "                    axs[igrouptype,iplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "                    axs[igrouptype,iplottype].legend()   \n",
    "                    if iplottype == 0:\n",
    "                        axs[igrouptype,iplottype].set_title('within animal: all animals',fontsize=16)   \n",
    "                    elif iplottype == 1:\n",
    "                        axs[igrouptype,iplottype].set_title('across animal: all animals',fontsize=16)\n",
    "\n",
    "                elif ngrouptypes == 1:\n",
    "                    axs[iplottype].plot([0,0],[0,1],'--',color='0.5')\n",
    "                    axs[iplottype].set_xlim(-dist_twin_range-0.75,dist_twin_range+0.75)\n",
    "                    axs[iplottype].set_ylim(0,0.3)\n",
    "                    # axs[iplottype].set_xlabel('time (s)',fontsize=15)\n",
    "                    axs[iplottype].set_ylabel('social gaze probability',fontsize=15)\n",
    "                    axs[iplottype].legend()   \n",
    "                    if iplottype == 0:\n",
    "                        axs[iplottype].set_title('within animal: all animals',fontsize=16)   \n",
    "                    elif iplottype == 1:\n",
    "                        axs[iplottype].set_title('across animal: all animals',fontsize=16)\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "        savefigs = 1\n",
    "        if savefigs:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            fig.savefig(figsavefolder+\"socialgaze_distribution_summaryplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    x =  np.arange(-dist_twin_range,dist_twin_range+1,1)\n",
    "\n",
    "    conditions = ['ginger_withK','ginger_withD','kanga_withG','kanga_withD','dodson','dannon']\n",
    "    # conditions = ['ginger_withK','ginger_withD','kanga_withG','kanga_withD',]\n",
    "    nconds = np.shape(conditions)[0]\n",
    "\n",
    "    halfwidth_all = dict.fromkeys(conditions)\n",
    "\n",
    "    for icond in np.arange(0,nconds,1):\n",
    "\n",
    "        condname = conditions[icond]\n",
    "\n",
    "        y_allsess = np.array(AcroAnimal_gazeDist_mean_forEachAni[AcroAnimal_gazeDist_mean_forEachAni['act_animal']==condname]['trig_average'])\n",
    "        nsess = np.shape(y_allsess)[0]\n",
    "\n",
    "        halfwidth_all[condname] = np.ones((1,nsess))[0]*np.nan\n",
    "\n",
    "        for isess in np.arange(0,nsess,1):\n",
    "\n",
    "            try:\n",
    "                y =  y_allsess[isess]\n",
    "                y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))      \n",
    "\n",
    "                # parameters, covariance = curve_fit(Gauss, x, y) \n",
    "                parameters, covariance = curve_fit(gaussian, x, y) \n",
    "                #\n",
    "                fit_A = parameters[0] \n",
    "                fit_B = parameters[1] \n",
    "                fit_C = parameters[2] \n",
    "                #\n",
    "                # fit_y = Gauss(x, fit_A, fit_B, fit_C) \n",
    "                fit_y = gaussian(x,fit_A,fit_B,fit_C)\n",
    "                y = (fit_y-np.nanmin(fit_y))/(np.nanmax(fit_y)-np.nanmin(fit_y)) \n",
    "\n",
    "                halfwidth_all[condname][isess] = fwhm(x, y, k=3)\n",
    "\n",
    "            except:\n",
    "                halfwidth_all[condname][isess] = np.nan\n",
    "\n",
    "    # box plot \n",
    "    fig, axs = plt.subplots(1,1)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "\n",
    "    # subplot 1 - all animals\n",
    "    halfwidth_all_df = pd.DataFrame.from_dict(halfwidth_all,orient='index')\n",
    "    halfwidth_all_df = halfwidth_all_df.transpose()\n",
    "    halfwidth_all_df['type'] = 'all'\n",
    "    #\n",
    "    df_long=pd.concat([halfwidth_all_df])\n",
    "    df_long2 = df_long.melt(id_vars=['type'], value_vars=conditions,var_name='condition', value_name='value')\n",
    "    # \n",
    "    # barplot ans swarmplot\n",
    "    seaborn.boxplot(ax=axs,data=df_long2,x='condition',y='value',hue='type')\n",
    "    # seaborn.swarmplot(ax=axs,data=df_long2,x='condition',y='value',hue='type',\n",
    "    #                   alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs.set_xlabel('')\n",
    "    axs.set_xticklabels(conditions)\n",
    "    axs.xaxis.set_tick_params(labelsize=15,rotation=45)\n",
    "    axs.set_ylabel(\"half max width\",fontsize=15)\n",
    "    axs.set_title('all animals' ,fontsize=24)\n",
    "    axs.set_ylim([0,10])\n",
    "    axs.legend(fontsize=18)\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        plt.savefig(figsavefolder+\"socialgaze_distribution_summaryplot_halfmaxWitdh.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    df_long2 = df_long2[~np.isnan(df_long2.value)]\n",
    "    # anova\n",
    "    cw_lm=ols('value ~ condition', data=df_long2).fit() #Specify C for Categorical\n",
    "    print(sm.stats.anova_lm(cw_lm, typ=2))\n",
    "\n",
    "    # post hoc test \n",
    "    tukey = pairwise_tukeyhsd(endog=df_long2['value'], groups=df_long2['condition'], alpha=0.05)\n",
    "    print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d25d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a743731",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7d323",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d13d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "        if minmaxfullSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            #samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            #samplingsizes_name = ['min_row_number','max_row_number','full_row_number']\n",
    "            samplingsizes = [np.nan]\n",
    "            samplingsizes_name = ['full_row_number']\n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        # for jj in np.arange(0,nsamplings,1):\n",
    "        for jj in np.arange(0,1,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                if samplingsizes_name[jj]=='full_row_number':\n",
    "                    isamplingsize = np.shape(DBN_input_data_alltypes[date_tgt])[0]\n",
    "\n",
    "                try:\n",
    "                    bhv_df_all = DBN_input_data_alltypes[date_tgt]\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                    colnames = list(bhv_df_all.columns)\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = np.shape(from_pops)[0]\n",
    "                    nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[date_tgt] = sig_edges\n",
    "                    \n",
    "                except:\n",
    "                    DAGs_alltypes[date_tgt] = [] \n",
    "                    DAGs_shuffle_alltypes[date_tgt] = []\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt] = []\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt] = []\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                    sig_edges_alltypes[date_tgt] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "    print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647783a",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alls = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "        if minmaxfullSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            # samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            # samplingsizes_name = ['min_row_number','max_row_number','full_row_number']   \n",
    "            samplingsizes = [np.nan]\n",
    "            samplingsizes_name = ['full_row_number']\n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                if samplingsizes_name[jj]=='full_row_number':\n",
    "                    isamplingsize = np.shape(DBN_input_data_allsessions[date_tgt])[0]\n",
    "\n",
    "                # try:\n",
    "                bhv_df_all = DBN_input_data_alltypes[date_tgt]\n",
    "\n",
    "\n",
    "                # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                colnames = list(bhv_df_all.columns)\n",
    "                eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                nevents = np.size(eventnames)\n",
    "\n",
    "                all_pops = list(bhv_df_all.columns)\n",
    "                from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                nFromNodes = np.shape(from_pops)[0]\n",
    "                nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                score_randstart = np.zeros((num_starting_points))\n",
    "                score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                # step 1: randomize the starting point for num_starting_points times\n",
    "                for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                    # try different down/re-sampling size\n",
    "                    bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                    aic = AicScore(bhv_df)\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                    aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                    np.random.seed(istarting_points)\n",
    "                    random.seed(istarting_points)\n",
    "                    starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                    starting_graph = DAG()\n",
    "                    starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                    starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                    best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                    DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                    DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                    score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                    # step 2: add the shffled data results\n",
    "                    # shuffled bhv_df\n",
    "                    best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                    DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                    DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                    score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                sig_edges_alltypes[date_tgt] = sig_edges\n",
    "                    \n",
    "                # except:\n",
    "                #     DAGs_alltypes[date_tgt] = [] \n",
    "                #     DAGs_shuffle_alltypes[date_tgt] = []\n",
    "                # \n",
    "                #     DAGs_scores_alltypes[date_tgt] = []\n",
    "                #     DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "                # \n",
    "                #     weighted_graphs_alltypes[date_tgt] = []\n",
    "                #     weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                #     sig_edges_alltypes[date_tgt] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    savedata = 0\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if moreSampSize:  \n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "        elif minmaxfullSampSize:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)        \n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb83aa",
   "metadata": {},
   "source": [
    "### plot the edges over time (session)\n",
    "#### mean edge weights of selected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30602b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(10*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        idate_name = dates_list_sorted[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_mean_forplot_all_dates,'ko',markersize=10)\n",
    "    #axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_shuffled_mean_forplot_all_dates,'bo',markersize=10)\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    axs.flatten()[i_edge].set_xlim([-0.5,ndates_sorted-0.5])\n",
    "    #\n",
    "    if i_edge > int(n_edges-1):\n",
    "        axs.flatten()[i_edge].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs.flatten()[i_edge].set_xticklabels(dates_list_sorted, rotation=90,fontsize=10)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweight_acrossAllSessions_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56642be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_graphs_diffTempRo_diffSampSize[('1','full_row_number')].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5ffd",
   "metadata": {},
   "source": [
    "#### mean edge weights of selected edges v.s. other behavioral measures\n",
    "##### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the targeted dates\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)]\n",
    "sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==1.5)|(sorting_df['coopthres']==2)]\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "dates_list_tgt = sorting_tgt_df['dates']\n",
    "dates_list_tgt = np.array(dates_list_tgt)\n",
    "#\n",
    "ndates_tgt = np.shape(dates_list_tgt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 1s time lag\n",
    "edges_target_names = [['1slag_pull2_pull1','1slag_pull1_pull2'],\n",
    "                      ['1slag_gaze1_pull1','1slag_gaze2_pull2'],\n",
    "                      ['1slag_pull2_gaze1','1slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[ 9, 8],\n",
    "                [10,11],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number'\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(5*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    if xplottype == 'succrate':\n",
    "        xxx = succ_rate_all_dates[sorting_tgt_df.index]\n",
    "    elif xplottype == 'meangazenum':   \n",
    "        xxx = gazemean_num_all_dates[sorting_tgt_df.index]\n",
    "    #     \n",
    "    yyy = edgeweight_mean_forplot_all_dates\n",
    "    #\n",
    "    rr_spe,pp_spe = scipy.stats.spearmanr(xxx, yyy)\n",
    "    slope, intercept, rr_reg, pp_reg, std_err = st.linregress(xxx.astype(float).T[0], yyy.astype(float).T[0])\n",
    "    #\n",
    "    axs.flatten()[i_edge].plot(xxx,yyy,'bo',markersize=8)\n",
    "    axs.flatten()[i_edge].plot(np.array([xxx.min(),xxx.max()]),np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    #\n",
    "    if i_edge > int(n_edges-3):\n",
    "        axs.flatten()[i_edge].set_xlabel(xplotlabel,fontsize=13)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    axs.flatten()[i_edge].text(xxx.min(),1.0,'spearman r='+\"{:.2f}\".format(rr_spe),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.9,'spearman p='+\"{:.2f}\".format(pp_spe),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.8,'regression r='+\"{:.2f}\".format(rr_reg),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.7,'regression p='+\"{:.2f}\".format(pp_reg),fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec6a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fromNodesIDs = [[ 9, 5],[ 8, 4],\n",
    "                    [10, 6],[11, 7],\n",
    "                    [ 9, 5],[ 8, 4],]\n",
    "np.array(fromNodesIDs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034114a",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "### Modulation index for Ginger between with kanga minus with dodson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "temp_resolu = 1\n",
    "j_sampsize_name = 'full_row_number'\n",
    "if moreSampSize:\n",
    "    j_sampsize_name = '3400'\n",
    "condname = 'coop(1s)'\n",
    "    \n",
    "timelag = 0 # 1 or 2 or 3 or 0(merged - merge all three lags) or 12 (merged lag 1 and 2)\n",
    "# timelagname = '1second' # '1/2/3second' or 'merged' or '12merged'\n",
    "timelagname = 'merged' # together with timelag = 0\n",
    "# timelagname = '12merged' # together with timelag = 12\n",
    "\n",
    "#\n",
    "if timelag == 1:\n",
    "    pull_pull_fromNodes_all = [9,8]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [11,10]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [8,9]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [9,8]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [10,11]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [11,10]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 2:\n",
    "    pull_pull_fromNodes_all = [5,4]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [7,6]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [4,5]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [5,4]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [6,7]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [7,6]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 3:\n",
    "    pull_pull_fromNodes_all = [1,0]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [3,2]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [0,1]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [1,0]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [2,3]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [3,2]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 0:\n",
    "    pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "    pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "    gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "    across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "    across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "elif timelag == 12:\n",
    "    pull_pull_fromNodes_all = [[5,9],[4,8]]\n",
    "    pull_pull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[7,11],[6,10]]\n",
    "    gaze_gaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[4,8],[5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[5,9],[4,8]]\n",
    "    across_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[6,10],[7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[7,11],[6,10]]\n",
    "    across_gazepull_toNodes_all = [[0,0],[1,1]]  \n",
    "    \n",
    "#    \n",
    "weighted_all_df = pd.DataFrame(columns=['dependency','dates','act_animal','shuffleID','DepWeights'])\n",
    "weighted_mean_df = pd.DataFrame(columns=['dependency','dates','act_animal','DepWeights'])\n",
    "\n",
    "animal1_all = ['dodson','ginger','dannon']\n",
    "animal2_all = ['ginger', 'kanga', 'kanga']\n",
    "\n",
    "act_animal1_all = ['dodson',      'ginger_withK',      'dannon']\n",
    "act_animal2_all = ['ginger_withD', 'kanga_withG', 'kanga_withD']\n",
    "\n",
    "nanimalpairs = np.shape(animal1_all)[0]\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "    animal1 = animal1_all[ianimalpair]\n",
    "    animal2 = animal2_all[ianimalpair]\n",
    "\n",
    "    act_animal1 = act_animal1_all[ianimalpair]\n",
    "    act_animal2 = act_animal2_all[ianimalpair]\n",
    "\n",
    "    # load the DBN related analysis\n",
    "    # load data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1+animal2+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1+animal2+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1+animal2+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1+animal2+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1+animal2+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1+animal2+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1+animal2+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    #\n",
    "    if not mergetempRos:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1+animal2+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1+animal2+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "            DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "    weighted_graphs_MC1 = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]\n",
    "    weighted_graphs_sf_MC1 = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]\n",
    "    sig_edges_MC1 = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]\n",
    "\n",
    "    dateslist = list(weighted_graphs_MC1.keys())\n",
    "    ndates = np.shape(dateslist)[0]\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dateslist[idate]\n",
    "        weighted_graphs_tgt = weighted_graphs_MC1[date_tgt]\n",
    "        weighted_graphs_sf_tgt = weighted_graphs_sf_MC1[date_tgt]\n",
    "        sig_edges_tgt = sig_edges_MC1[date_tgt]\n",
    "\n",
    "        #\n",
    "        sig_edges_tgt = sig_edges_tgt.astype('float')\n",
    "        sig_edges_tgt[sig_edges_tgt==0] = np.nan\n",
    "        # \n",
    "        # weighted_graphs_tgt = weighted_graphs_tgt * sig_edges_tgt\n",
    "\n",
    "        #\n",
    "        for ianimal in np.arange(0,2,1):\n",
    "\n",
    "            if ianimal == 0:\n",
    "                act_animal = act_animal1\n",
    "            elif ianimal == 1:\n",
    "                act_animal = act_animal2\n",
    "\n",
    "            #                \n",
    "            # pull-pull\n",
    "            a1 = (weighted_graphs_tgt[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]]).flatten()\n",
    "            xxx1 = np.nanmean(a1)\n",
    "            # gaze-gaze\n",
    "            a2 = (weighted_graphs_tgt[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "            xxx2 = np.nanmean(a2)\n",
    "            # within animal gazepull\n",
    "            a3 = (weighted_graphs_tgt[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "            xxx3 = np.nanmean(a3)\n",
    "            # across animal gazepull\n",
    "            a4 = (weighted_graphs_tgt[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "            xxx4 = np.nanmean(a4)\n",
    "            # within animal pullgaze\n",
    "            a5 = (weighted_graphs_tgt[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "            xxx5 = np.nanmean(a5)\n",
    "            # across animal pullgaze\n",
    "            a6 = (weighted_graphs_tgt[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "            xxx6 = np.nanmean(a6)\n",
    "\n",
    "            # fill up the weighted_all_df\n",
    "            nshuffles = np.shape(a1)[0]\n",
    "            for ishuffle in np.arange(0,nshuffles,1):\n",
    "                # pull-pull\n",
    "                weighted_all_df = weighted_all_df.append({'dependency':'pull-pull',\n",
    "                                                          'act_animal': act_animal,\n",
    "                                                          'dates':date_tgt,\n",
    "                                                          'shuffleID':ishuffle,\n",
    "                                                          'DepWeights':a1[ishuffle]},ignore_index=True)\n",
    "                # gaze-gaze\n",
    "                weighted_all_df = weighted_all_df.append({'dependency':'gaze-gaze',\n",
    "                                                          'act_animal': act_animal,\n",
    "                                                          'dates':date_tgt,\n",
    "                                                          'shuffleID':ishuffle,\n",
    "                                                          'DepWeights':a2[ishuffle]},ignore_index=True)\n",
    "                # within animal gazepull\n",
    "                weighted_all_df = weighted_all_df.append({'dependency':'within animal gazepull',\n",
    "                                                          'act_animal': act_animal,\n",
    "                                                          'dates':date_tgt,\n",
    "                                                          'shuffleID':ishuffle,\n",
    "                                                          'DepWeights':a3[ishuffle]},ignore_index=True)\n",
    "                # across animal gazepull\n",
    "                weighted_all_df = weighted_all_df.append({'dependency':'across animal gazepull',\n",
    "                                                          'act_animal': act_animal,\n",
    "                                                          'dates':date_tgt,\n",
    "                                                          'shuffleID':ishuffle,\n",
    "                                                          'DepWeights':a4[ishuffle]},ignore_index=True)\n",
    "                # within animal pullgaze\n",
    "                weighted_all_df = weighted_all_df.append({'dependency':'within animal pullgaze',\n",
    "                                                          'act_animal': act_animal,\n",
    "                                                          'dates':date_tgt,\n",
    "                                                          'shuffleID':ishuffle,\n",
    "                                                          'DepWeights':a5[ishuffle]},ignore_index=True)\n",
    "                # across animal pullgaze\n",
    "                weighted_all_df = weighted_all_df.append({'dependency':'across animal pullgaze',\n",
    "                                                          'act_animal': act_animal,\n",
    "                                                          'dates':date_tgt,\n",
    "                                                          'shuffleID':ishuffle,\n",
    "                                                          'DepWeights':a6[ishuffle]},ignore_index=True)\n",
    "\n",
    "            # fill up the weighted_mean_df\n",
    "            # pull-pull\n",
    "            weighted_mean_df = weighted_mean_df.append({'dependency':'pull-pull',\n",
    "                                                      'act_animal': act_animal,\n",
    "                                                      'dates':date_tgt,\n",
    "                                                      'DepWeights':xxx1},ignore_index=True)\n",
    "            # gaze-gaze\n",
    "            weighted_mean_df = weighted_mean_df.append({'dependency':'gaze-gaze',\n",
    "                                                      'act_animal': act_animal,\n",
    "                                                      'dates':date_tgt,\n",
    "                                                      'DepWeights':xxx2},ignore_index=True)\n",
    "            # within animal gazepull\n",
    "            weighted_mean_df = weighted_mean_df.append({'dependency':'within animal gazepull',\n",
    "                                                      'act_animal': act_animal,\n",
    "                                                      'dates':date_tgt,\n",
    "                                                      'DepWeights':xxx3},ignore_index=True)\n",
    "            # across animal gazepull\n",
    "            weighted_mean_df = weighted_mean_df.append({'dependency':'across animal gazepull',\n",
    "                                                      'act_animal': act_animal,\n",
    "                                                      'dates':date_tgt,\n",
    "                                                      'DepWeights':xxx4},ignore_index=True)\n",
    "            # within animal pullgaze\n",
    "            weighted_mean_df = weighted_mean_df.append({'dependency':'within animal pullgaze',\n",
    "                                                      'act_animal': act_animal,\n",
    "                                                      'dates':date_tgt,\n",
    "                                                      'DepWeights':xxx5},ignore_index=True)\n",
    "            # across animal pullgaze\n",
    "            weighted_mean_df = weighted_mean_df.append({'dependency':'across animal pullgaze',\n",
    "                                                      'act_animal': act_animal,\n",
    "                                                      'dates':date_tgt,\n",
    "                                                      'DepWeights':xxx6},ignore_index=True)\n",
    "\n",
    "            \n",
    "# for plot     \n",
    "fig, axs = plt.subplots(1,1)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "#\n",
    "toplotorder = ['ginger_withD','ginger_withK','kanga_withD','kanga_withG']\n",
    "weighted_mean_toplot = weighted_mean_df[np.isin(weighted_mean_df['act_animal'],toplotorder)]\n",
    "weighted_mean_toplot['act_animal'] = pd.Categorical(weighted_mean_toplot['act_animal'], \n",
    "                                                    categories=toplotorder, ordered=True)\n",
    "weighted_mean_toplot_sorted = weighted_mean_toplot.sort_values(by='act_animal')\n",
    "\n",
    "#\n",
    "seaborn.boxplot(ax=axs,data=weighted_mean_toplot_sorted,x='dependency',y='DepWeights',hue='act_animal') \n",
    "# seaborn.violinplot(ax=axs,data=weighted_mean_toplot_sorted,x='dependency',y='DepWeights',hue='act_animal') \n",
    "axs.set_title('time lag: '+timelagname)\n",
    "axs.set_xlabel('dependencies')\n",
    "axs.set_xticklabels(axs.get_xticklabels(),rotation=45)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig.savefig(figsavefolder+\"DependencyWeights_GingerAndKanga_\"+timelagname+'timelag.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind1=(weighted_mean_toplot['dependency']=='across animal pullgaze')&(weighted_mean_toplot['act_animal']=='ginger_withD')\n",
    "# ind2=(weighted_mean_toplot['dependency']=='across animal pullgaze')&(weighted_mean_toplot['act_animal']=='ginger_withK')\n",
    "# ind1=(weighted_mean_toplot['dependency']=='across animal pullgaze')&(weighted_mean_toplot['act_animal']=='kanga_withD')\n",
    "# ind2=(weighted_mean_toplot['dependency']=='across animal pullgaze')&(weighted_mean_toplot['act_animal']=='kanga_withG')\n",
    "\n",
    "# ind1=(weighted_mean_toplot['dependency']=='within animal gazepull')&(weighted_mean_toplot['act_animal']=='ginger_withD')\n",
    "# ind2=(weighted_mean_toplot['dependency']=='within animal gazepull')&(weighted_mean_toplot['act_animal']=='ginger_withK')\n",
    "ind1=(weighted_mean_toplot['dependency']=='within animal gazepull')&(weighted_mean_toplot['act_animal']=='kanga_withD')\n",
    "ind2=(weighted_mean_toplot['dependency']=='within animal gazepull')&(weighted_mean_toplot['act_animal']=='kanga_withG')\n",
    "\n",
    "xx1 = np.array(weighted_mean_toplot[ind1]['DepWeights'])\n",
    "xx2 = np.array(weighted_mean_toplot[ind2]['DepWeights'])\n",
    "\n",
    "st.ttest_ind(xx1[xx1!=np.nan],xx2[xx2!=np.nan])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55324239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plot   \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "dependnames = np.unique(weighted_mean_df['dependency'])\n",
    "ndependnames = np.shape(dependnames)[0]\n",
    "\n",
    "fig, axs = plt.subplots(ndependnames,2)\n",
    "fig.set_figheight(4*ndependnames)\n",
    "fig.set_figwidth(8*2)\n",
    "\n",
    "for idependname in np.arange(0,ndependnames,1):\n",
    "    \n",
    "    dependname = dependnames[idependname]\n",
    "    \n",
    "    weighted_mean_toplot = weighted_mean_df[weighted_mean_df['dependency']==dependname]\n",
    "    \n",
    "    # for Ginger\n",
    "    ind_G = (weighted_mean_toplot['act_animal']=='ginger_withD') | (weighted_mean_toplot['act_animal']=='ginger_withK')\n",
    "    weighted_mean_toplot_G = weighted_mean_toplot[ind_G]\n",
    "    weighted_mean_toplot_sorted = weighted_mean_toplot_G.sort_values(by=['dates'])\n",
    "    \n",
    "    seaborn.lineplot(ax=axs[idependname,0],data=weighted_mean_toplot_sorted,\n",
    "                     x='dates',y='DepWeights',color='darkgray') \n",
    "    seaborn.scatterplot(ax=axs[idependname,0],data=weighted_mean_toplot_sorted,\n",
    "                     x='dates',y='DepWeights',hue='act_animal',s=150) \n",
    "    axs[idependname,0].set_title('time lag: '+timelagname)\n",
    "    axs[idependname,0].set_ylabel('dependency weight:'+dependname)\n",
    "    \n",
    "    \n",
    "    # for Kanga\n",
    "    ind_K = (weighted_mean_toplot['act_animal']=='kanga_withD') | (weighted_mean_toplot['act_animal']=='kanga_withG')\n",
    "    weighted_mean_toplot_K = weighted_mean_toplot[ind_K]\n",
    "    weighted_mean_toplot_sorted = weighted_mean_toplot_K.sort_values(by=['dates'])\n",
    "    \n",
    "    seaborn.lineplot(ax=axs[idependname,1],data=weighted_mean_toplot_sorted,\n",
    "                     x='dates',y='DepWeights',color='darkgray') \n",
    "    seaborn.scatterplot(ax=axs[idependname,1],data=weighted_mean_toplot_sorted,\n",
    "                     x='dates',y='DepWeights',hue='act_animal',s=150) \n",
    "    axs[idependname,1].set_title('time lag: '+timelagname)\n",
    "    axs[idependname,1].set_ylabel('dependency weight:'+dependname)\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig.savefig(figsavefolder+\"DependencyWeights_ChangeOverDays_GingerAndKanga_\"+timelagname+'timelag.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[idependname,0].get_xticklabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659e669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0321a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
