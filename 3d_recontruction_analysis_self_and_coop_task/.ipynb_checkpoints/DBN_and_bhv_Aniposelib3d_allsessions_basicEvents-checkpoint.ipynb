{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea1560b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on each session, regardless of the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac4ea0",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037084bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN import train_DBN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - Alec's methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_alec import train_DBN_alec\n",
    "from ana_functions.train_DBN_alec import train_DBN_alec_create_df_only\n",
    "from ana_functions.train_DBN_alec import train_DBN_alec_training_only\n",
    "from ana_functions.train_DBN_alec import graph_to_matrix\n",
    "from ana_functions.train_DBN_alec import get_weighted_dags\n",
    "from ana_functions.train_DBN_alec import get_significant_edges\n",
    "from ana_functions.train_DBN_alec import threshold_edges\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20220909\",\"20220912\",\"20220915\",\"20220920\",\"20220922\",\"20220923\",\"20221010\",\n",
    "                      \"20221011\",\"20221013\",\"20221014\",\"20221015\",\"20221017\",\"20230215\",     \n",
    "                      \"20221018\",\"20221019\",\"20221020\",\"20221021\",\"20221022\",\"20221026\",\"20221028\",\"20221030\",\n",
    "                      \"20221107\",\"20221108\",\"20221109\",\"20221110\",\"20221111\",\"20221114\",\"20221115\",\"20221116\",\n",
    "                      \"20221117\",\"20221118\",\"20221121\",\"20221122\",\"20221123\",\"20221125\",\"20221128\",\"20221129\",              \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221212\",\"20221214\",\"20221216\",\"20221219\",\"20221220\",\n",
    "                      \"20221221\",\"20230208\",\"20230209\",\"20230213\",\"20230214\",\"20230111\",\"20230112\",\"20230201\",\n",
    "\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 6.50, 18.10, 0,      33.03, 549.0, 116.80, 6.50,\n",
    "                                 2.80, 27.80, 272.50, 27.90, 27.00,  33.00,\n",
    "                                28.70, 45.30, 21.10,  27.10, 51.90,  21.00, 30.80, 17.50,                      \n",
    "                                15.70,  2.65, 27.30,   0.00,  0.00,  71.80,  0.00,  0.00, \n",
    "                                75.50, 20.20,  0.00,  24.20, 36.70,  26.40, 22.50, 28.50,                       \n",
    "                                 0.00,  0.00, 21.70,  84.70, 17.00,  19.80, 23.50, 25.20,  \n",
    "                                 0.00,  0.00,  0.00,   0.00,  0.00, 130.00, 14.20, 24.20, \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20220912\",\"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                      \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                      \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                      \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                18.10,  0.00, 33.03,  6.50,  0.00, \n",
    "                                 2.80, 27.80, 27.90, 27.00,  \n",
    "                                51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                 0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# eddie sparkle\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20221122\",\"20221125\",\"20221128\",\"20221129\",\"20221130\",\"20221202\",\"20221206\",\n",
    "                      \"20221207\",\"20221208\",\"20221209\",\"20230126\",\"20230127\",\"20230130\",\"20230201\",\"20230203-1\",\n",
    "                      \"20230206\",\"20230207\",\"20230208-1\",\"20230209\",\"20230222\",\"20230223-1\",\"20230227-1\",\n",
    "                      \"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230321\",\"20230322\",\"20230324\",\"20230327\",\"20230328\",\n",
    "                      \"20230330\",\"20230331\",\"20230403\",\"20230404\",\"20230405\",\"20230406\",\"20230407\",\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 8.00,38.00,1.00,3.00,5.00,9.50,1.00,\n",
    "                                 4.50,4.50,5.00,38.00,166.00,4.20,3.80,3.60,\n",
    "                                 7.50,9.00,7.50,8.50,14.50,7.80,8.00,7.50,\n",
    "                                 8.00,8.00,4.00,123.00,14.00,8.80,\n",
    "                                 7.00,7.50,5.50,11.00,9.00,\n",
    "                                 17.00,4.50,9.30,25.50,20.40,21.30,24.80,\n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20221122\",  \"20221125\",  \n",
    "                      \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                      \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                      \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                      \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                      \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                  8.00,  38.00, \n",
    "                                  9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                  9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                  8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                  7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                  4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "# ginger kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230209\",\"20230213\",\"20230214\",\"20230216\",\"20230222\",\"20230223\",\"20230228\",\"20230302\",\n",
    "                      \"20230303\",\"20230307\",\"20230314\",\"20230315\",\"20230316\",\"20230317\"         \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00,  0.00, 48.00, 26.20, 18.00, 23.00, 28.50,\n",
    "                                34.00, 25.50, 25.50, 31.50, 28.00, 30.50\n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20230213\",\"20230214\",\"20230216\",\n",
    "                      \"20230228\",\"20230302\",\"20230303\",\n",
    "                      \"20230307\",          \n",
    "                      \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                      \"20230323\",\"20230412\",\"20230413\",\"20230517\",\"20230614\",\"20230615\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00, 48.00, \n",
    "                                23.00, 28.50, 34.00, \n",
    "                                25.50, \n",
    "                                25.50, 31.50, 28.00, 30.50,\n",
    "                                33.50, 22.20, 50.00,  0.00, \n",
    "                                33.00, 18.20, 22.80, 31.00, 24.00, 21.00\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "#    \n",
    "#dates_list = [\"20221128\"]\n",
    "#session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# where to save the demo video\n",
    "withboxCorner = 1\n",
    "video_file_dir = data_saved_folder+'/example_videos_Anipose_bhv_demo/'+animal1_filename+'_'+animal2_filename\n",
    "if not os.path.exists(video_file_dir):\n",
    "    os.makedirs(video_file_dir)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a453879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "# NOTE: THIS STEP will save the data to the combinedsession_Anipose folder, since they are the same\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "        \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "except:\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        for imergepair in np.arange(0,np.shape(merge_campairs)[0],1):\n",
    "            \n",
    "            # should be only one merge type - \"Anipose\"\n",
    "            merge_campair = merge_campairs[imergepair]\n",
    "\n",
    "            # load camera tracking results\n",
    "            try:\n",
    "                # dummy\n",
    "                if reanalyze_video:\n",
    "                    print(\"re-analyze the data \",date_tgt)\n",
    "                    dummy\n",
    "                ## read\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                    body_part_locs_Anipose = pickle.load(f)                 \n",
    "            except:\n",
    "                print(\"did not save data for Anipose - body part tracking \"+date_tgt)\n",
    "                # analyze and save\n",
    "                Anipose_h5_file = Anipose_analyzed_path +date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "                Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "                body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                    pickle.dump(body_part_locs_Anipose, f)            \n",
    "            \n",
    "            min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "                    \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "            tasktypes_all_dates[idate] = tasktype\n",
    "            coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # analyze behavior results\n",
    "            # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "            succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "\n",
    "            trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "            #\n",
    "            pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<10]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "            # load behavioral event results\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                    output_key_locations = pickle.load(f)\n",
    "            except:\n",
    "                print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "                output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "               \n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_key_locations, f)\n",
    "                \n",
    "             \n",
    "            look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "            look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "            look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "            look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "            look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "            look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                               \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                               \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "            timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "            timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "            timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "                \n",
    "            #\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]            \n",
    "            \n",
    "            # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "            # could be used for define time bin for DBN\n",
    "            if 1:\n",
    "                _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                             oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #\n",
    "                pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "                bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                                'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158391c",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9be41",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0aaea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-5,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "    \n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35cadc",
   "metadata": {},
   "source": [
    "### plot some other basis behavioral measures\n",
    "#### successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29625a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),succ_rate_all_dates[sorting_df.index],'o',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"successful rate\",fontsize=13)\n",
    "ax1.set_ylim([-0.1,1.1])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"successfulrate_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ad0c9",
   "metadata": {},
   "source": [
    "#### animal pull numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull1_num_all_dates[sorting_df.index],'bv',markersize=5,label='animal1 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull2_num_all_dates[sorting_df.index],'rv',markersize=5,label='animal2 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pullmean_num_all_dates[sorting_df.index],'kv',markersize=8,label='mean pull #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"pull numbers\",fontsize=13)\n",
    "ax1.set_ylim([-20,240])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,240],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pullnumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e48e54",
   "metadata": {},
   "source": [
    "#### gaze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "print(np.nanmax(gaze1_num_all_dates))\n",
    "print(np.nanmax(gaze2_num_all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze1_num_all_dates[sorting_df.index],'b^',markersize=5,label='animal1 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze2_num_all_dates[sorting_df.index],'r^',markersize=5,label='animal2 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazemean_num_all_dates[sorting_df.index],'k^',markersize=8,label='mean gaze #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"social gaze number\",fontsize=13)\n",
    "ax1.set_ylim([-20,1500])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,1500],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_numbers = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)\n",
    "gaze_pull_ratios = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/(pull1_num_all_dates+pull2_num_all_dates)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "\n",
    "gaze_numbers_groups = [np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==3)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==2)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1.5)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(gaze_numbers_groups)\n",
    "\n",
    "plt.xticks(np.arange(1, len(grouptypes)+1, 1), grouptypes, fontsize = 12);\n",
    "ax1.set_ylim([-20,3000])\n",
    "ax1.set_ylabel(\"average social gaze numbers\",fontsize=13)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b000d",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN\n",
    "#### for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd258c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_input_data_allsessions = dict.fromkeys(dates_list, [])\n",
    "\n",
    "doBhvitv_timebin = 0 # 1: if use the mean bhv event interval for time bin\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "## use the same time bins for all the sessions; Alternative, use the mean bhv event interval \n",
    "if not doBhvitv_timebin:    \n",
    "    # temp_resolus = [0.5,1,1.5,2.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "#\n",
    "mergetempRos = 0 # 1: if merge different time bin\n",
    "\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if 0:\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        # get animal info\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "        \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "\n",
    "        # load behavioral event results\n",
    "        print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "            output_look_ornot = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "            output_allvectors = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "            output_allangles = pickle.load(f)  \n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "            output_key_locations = pickle.load(f)     \n",
    "        look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "        look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "        look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "        look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "        look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "        # \n",
    "        look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                           \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                           \"othertube\":look_at_othertube_or_not_Anipose}      \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "        timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "        timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "        timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "\n",
    "\n",
    "        if mergetempRos:\n",
    "            temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            # use bhv event to decide temporal resolution\n",
    "            #\n",
    "            #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "\n",
    "        if doBhvitv_timebin:\n",
    "            pull_other_intv_ii = pd.Series(bhv_intv_all_dates[date_tgt]['pull_other_pooled'])\n",
    "            # remove the interval that is too large\n",
    "            pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "            # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "            temp_resolus = [np.nanmean(pull_other_intv_ii)]\n",
    "            \n",
    "        ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "        # try different temporal resolutions\n",
    "        for temp_resolu in temp_resolus:\n",
    "            bhv_df = []\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                bhv_df_itr = train_DBN_alec_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            else:\n",
    "                bhv_df_itr = train_DBN_alec_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "            if len(bhv_df)==0:\n",
    "                bhv_df = bhv_df_itr\n",
    "            else:\n",
    "                bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                #bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "            DBN_input_data_allsessions[date_tgt] = bhv_df\n",
    "            \n",
    "            \n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_allsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_allsessions_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_allsessions, f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_allsessions_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_allsessions, f)\n",
    "        if mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_allsessions_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_allsessions, f)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c8d47",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: if use more fixed sample size\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    #dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_allsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(100,3100,300)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]\n",
    "        samplingsizes = np.arange(300,1100,100)\n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    \n",
    "    if not doBhvitv_timebin:\n",
    "        # temp_resolus = [0.5,1,1.5,2,2.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    else:\n",
    "        temp_resolus = [0] # use the bhv interval for each session, so they are different for each session\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_allsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_allsessions_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_allsessions_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "        if mergetempRos:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_allsessions_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_allsessions = pickle.load(f)     \n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "        # only try three sample sizes\n",
    "        #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "        #- full row number of each session\n",
    "       \n",
    "        if minmaxfullSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_allsessions.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "            samplingsizes_name = ['min_row_number','max_row_number','full_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "        \n",
    "        \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "            # different individual sessions\n",
    "            ndates = np.shape(dates_list)[0]\n",
    "            for idate in np.arange(0,ndates,1):\n",
    "                date_tgt = dates_list[idate]\n",
    "                \n",
    "                if samplingsizes_name[jj]=='full_row_number':\n",
    "                    isamplingsize = np.shape(DBN_input_data_allsessions[date_tgt])[0]\n",
    "\n",
    "                try:\n",
    "                    bhv_df_all = DBN_input_data_allsessions[date_tgt]\n",
    "                    # bhv_df = bhv_df_all.sample(30*100,replace = True, random_state = round(time())) # take the subset for DBN training\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    # bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_alec\n",
    "                    colnames = [\"pull1_t0\",\"pull2_t0\",\"owgaze1_t0\",\"owgaze2_t0\",\"pull1_t1\",\"pull2_t1\",\"owgaze1_t1\",\"owgaze2_t1\"]\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t1')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t1')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = nevents\n",
    "                    nToNodes = nevents\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = num_starting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs,eventnames,from_pops,to_pops = train_DBN_alec_training_only(bhv_df,starting_graph)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs,eventnames,from_pops,to_pops = train_DBN_alec_training_only(bhv_df_shuffle,starting_graph)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[date_tgt] = sig_edges\n",
    "                    \n",
    "                except:\n",
    "                    DAGs_alltypes[date_tgt] = [] \n",
    "                    DAGs_shuffle_alltypes[date_tgt] = []\n",
    "\n",
    "                    DAGs_scores_alltypes[date_tgt] = []\n",
    "                    DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "\n",
    "                    weighted_graphs_alltypes[date_tgt] = []\n",
    "                    weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                    sig_edges_alltypes[date_tgt] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_allsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:  \n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5ca47",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge with arrows; show the best time bin and row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f47ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "\n",
    "if not doBhvitv_timebin:\n",
    "    temp_resolus = [1] # best temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "else:\n",
    "    temp_resolus = [0]\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "# best sampling size\n",
    "if moreSampSize:\n",
    "    samplingsizes_name = ['600']\n",
    "if minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "#eventnames = [\"M1pull\",\"M2pull\",\"M1gazeM2\",\"M2gazeM1\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "eventnodes_color = ['b','r','y','g']\n",
    "nFromNodes = nevents\n",
    "nToNodes = nevents\n",
    "    \n",
    "savefigs = 1\n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "DBN_group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "DBN_group_coopthres = [0,3,2,1.5,1,0]\n",
    "DBN_group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "fig, axs = plt.subplots(2,nDBN_groups)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(30)\n",
    "    \n",
    "# prepare the averaged graph across days/sessions in each condition\n",
    "\n",
    "sig_avg_dags_allsessions = dict.fromkeys(dates_list, [])\n",
    "\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    \n",
    "    weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolus[0]), samplingsizes_name[0])][date_tgt]\n",
    "    sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolus[0]), samplingsizes_name[0])][date_tgt]\n",
    "    # sig_avg_dags_allsessions[date_tgt] = weighted_graphs_tgt.mean(axis = 0) * sig_edges_tgt\n",
    "    sig_avg_dags_allsessions[date_tgt] = weighted_graphs_tgt.mean(axis = 0)\n",
    "    \n",
    "    \n",
    "for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "    try: \n",
    "        iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "        iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "        iDBN_group_cothres = DBN_group_coopthres[iDBN_group]\n",
    "        iDBN_group_sorting_df_coopthres = DBN_group_sorting_df_coopthres[iDBN_group]\n",
    "\n",
    "        iDBN_group_date_tgt = sorting_df['dates'][sorting_df['coopthres']==iDBN_group_sorting_df_coopthres]\n",
    "\n",
    "        sig_avg_dags = np.nanmean([sig_avg_dags_allsessions[x] for x in iDBN_group_date_tgt],axis=0)\n",
    "        \n",
    "        if np.shape(sig_avg_dags)[0]<=1:\n",
    "            dummy\n",
    "    \n",
    "        # plot\n",
    "        axs[0,iDBN_group].set_title(iDBN_group_typename,fontsize=15)\n",
    "        axs[0,iDBN_group].set_xlim([-0.5,1.5])\n",
    "        axs[0,iDBN_group].set_ylim([-0.5,1.5])\n",
    "        axs[0,iDBN_group].set_xticks([])\n",
    "        axs[0,iDBN_group].set_xticklabels([])\n",
    "        axs[0,iDBN_group].set_yticks([])\n",
    "        axs[0,iDBN_group].set_yticklabels([])\n",
    "        axs[0,iDBN_group].spines['top'].set_visible(False)\n",
    "        axs[0,iDBN_group].spines['right'].set_visible(False)\n",
    "        axs[0,iDBN_group].spines['bottom'].set_visible(False)\n",
    "        axs[0,iDBN_group].spines['left'].set_visible(False)\n",
    "        # axs[0,iDBN_group].axis('equal')\n",
    "\n",
    "        for ieventnode in np.arange(0,nevents,1):\n",
    "            # plot the event nodes\n",
    "            axs[0,iDBN_group].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                   '.',markersize=60,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                   markeredgecolor='none')              \n",
    "            axs[0,iDBN_group].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                                   eventnames[ieventnode],fontsize=10)\n",
    "\n",
    "            # plot the event edges\n",
    "            for ifromNode in np.arange(0,nFromNodes,1):\n",
    "                for itoNode in np.arange(0,nToNodes,1):\n",
    "                    edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                    if edge_weight_tgt>0:\n",
    "                        if not ifromNode == itoNode:\n",
    "                            #axs[0,iDBN_group].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                            axs[0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                    nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                    nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                    nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                    head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt)\n",
    "                        if ifromNode == itoNode:\n",
    "                            ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                  .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                  nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                  0.04*edge_weight_tgt)\n",
    "                            p = PatchCollection(\n",
    "                                [ring], \n",
    "                                facecolor='#2693de', \n",
    "                                edgecolor='#000000'\n",
    "                            )\n",
    "                            axs[0,iDBN_group].add_collection(p)\n",
    "                            # add arrow head\n",
    "                            if ifromNode < 2:\n",
    "                                axs[0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        0,-0.05,fc='#2693de',\n",
    "                                                        head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt)\n",
    "                            else:\n",
    "                                axs[0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        0,0.02,fc='#2693de',\n",
    "                                                        head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt)\n",
    "\n",
    "        # heatmap for the weights\n",
    "        sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "        sig_avg_dags_df.columns = eventnames\n",
    "        sig_avg_dags_df.index = eventnames\n",
    "        im = axs[1,iDBN_group].pcolormesh(sig_avg_dags_df,cmap=\"Blues\")\n",
    "        #\n",
    "        if iDBN_group == nDBN_groups-1:\n",
    "            cax = axs[1,iDBN_group].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "            fig.colorbar(im, ax=axs[1,iDBN_group], cax=cax,label='transition probability')\n",
    "\n",
    "        axs[1,iDBN_group].axis('equal')\n",
    "        axs[1,iDBN_group].set_xlabel('to Node',fontsize=14)\n",
    "        axs[1,iDBN_group].set_xticks(np.arange(0.5,4.5,1))\n",
    "        axs[1,iDBN_group].set_xticklabels(eventnames)\n",
    "        if iDBN_group == 0:\n",
    "            axs[1,iDBN_group].set_ylabel('from Node',fontsize=14)\n",
    "            axs[1,iDBN_group].set_yticks(np.arange(0.5,4.5,1))\n",
    "            axs[1,iDBN_group].set_yticklabels(eventnames)\n",
    "        else:\n",
    "            axs[1,iDBN_group].set_yticks([])\n",
    "            axs[1,iDBN_group].set_yticklabels([])\n",
    "                                     \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"oneTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+str(samplingsizes_name[0])+'_rows_ConditionMerged.jpg')\n",
    "    if minmaxfullSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doBhvitv_timebin:\n",
    "            plt.savefig(figsavefolder+\"oneTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_BhvIntBin_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "        else:\n",
    "            plt.savefig(figsavefolder+\"oneTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "                      \n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5f121",
   "metadata": {},
   "source": [
    "## plot some key edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5155e45",
   "metadata": {},
   "source": [
    "### plot the transition probability from social gaze to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd250146",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAGs_all_dates = np.array(list(sig_avg_dags_allsessions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6261d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type, cooperation threshold and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "#\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,2,0],'o-',label = \"socialgaze1->pull1\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,2,1],'o-',label = \"socialgaze1->pull2\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,3,0],'o-',label = \"socialgaze2->pull1\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,3,1],'o-',label = \"socialgaze2->pull2\")\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_ylim(-0.1,1.1)\n",
    "ax1.set_ylabel(\"transition probability\",fontsize=13)\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates,1),np.array(dates_list)[sorting_df.index], rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"transition probability from social gaze to pull\", fontsize = 14)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','novision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.15,1.15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.075,tasktypes[itaskswitch],fontsize=10)\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bcd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','no-vision']\n",
    "\n",
    "gaze1_pull1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],2,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],2,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],2,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],2,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],2,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],2,0]]\n",
    "gaze1_pull2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],2,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],2,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],2,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],2,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],2,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],2,1]]\n",
    "gaze2_pull1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],3,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],3,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],3,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],3,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],3,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],3,0]]\n",
    "gaze2_pull2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],3,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],3,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],3,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],3,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],3,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],3,1]\n",
    "               ]\n",
    "\n",
    "doboxplot = 0\n",
    "domeanplot = 1\n",
    "savefigs=1\n",
    "\n",
    "if doboxplot:\n",
    "    gaze1_pull1_plot = plt.boxplot(gaze1_pull1,positions=np.array(np.arange(len(gaze1_pull1)))*4.0-1.05,widths=0.6)\n",
    "    gaze1_pull2_plot = plt.boxplot(gaze1_pull2,positions=np.array(np.arange(len(gaze1_pull2)))*4.0-0.35,widths=0.6)\n",
    "    gaze2_pull1_plot = plt.boxplot(gaze2_pull1,positions=np.array(np.arange(len(gaze2_pull1)))*4.0+0.35,widths=0.6)\n",
    "    gaze2_pull2_plot = plt.boxplot(gaze2_pull2,positions=np.array(np.arange(len(gaze2_pull2)))*4.0+1.05,widths=0.6)\n",
    "    #\n",
    "    def define_box_properties(plot_name, color_code, label):\n",
    "        for k, v in plot_name.items():\n",
    "            plt.setp(plot_name.get(k), color=color_code)\n",
    "        plt.plot([], c=color_code, label=label)\n",
    "        plt.legend()\n",
    "    #\n",
    "    define_box_properties(gaze1_pull1_plot, '#0343DF', 'social_gaze1->pull1')\n",
    "    define_box_properties(gaze1_pull2_plot, '#FFA500', 'social_gaze1->pull2')\n",
    "    define_box_properties(gaze2_pull1_plot, '#008000', 'social_gaze2->pull1')\n",
    "    define_box_properties(gaze2_pull2_plot, '#8C000F', 'social_gaze2->pull2')\n",
    "    #\n",
    "    # set the x label values\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes);\n",
    "    \n",
    "if domeanplot:\n",
    "    gaze1_pull1_plot = plt.plot(np.array(np.arange(len(gaze1_pull1)))*4.0,np.nanmean(pd.DataFrame(gaze1_pull1),axis=1),'-o',label='M1gazeM2->pull1')\n",
    "    gaze1_pull2_plot = plt.plot(np.array(np.arange(len(gaze1_pull2)))*4.0,np.nanmean(pd.DataFrame(gaze1_pull2),axis=1),'-o',label='M1gazeM2->pull2')\n",
    "    gaze2_pull1_plot = plt.plot(np.array(np.arange(len(gaze2_pull1)))*4.0,np.nanmean(pd.DataFrame(gaze2_pull1),axis=1),'-o',label='M2gazeM1->pull1')\n",
    "    gaze2_pull2_plot = plt.plot(np.array(np.arange(len(gaze2_pull2)))*4.0,np.nanmean(pd.DataFrame(gaze2_pull2),axis=1),'-o',label='M2gazeM1->pull2')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes,fontsize=15,rotation=45);\n",
    "    plt.ylabel('mean transition probability',fontsize=15)\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"gaze_pull_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+str(samplingsizes_name[0])+'_rows_ConditionMerged.jpg')\n",
    "    if minmaxfullSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doBhvitv_timebin:\n",
    "            plt.savefig(figsavefolder+\"gaze_pull_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_BhvIntBin_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "        else:\n",
    "            plt.savefig(figsavefolder+\"gaze_pull_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3feddd6",
   "metadata": {},
   "source": [
    "### plot the transition probability from pull to social gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "#\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,0,2],'o-',label = \"pull1->socialgaze1\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,1,2],'o-',label = \"pull2->socialgaze1\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,0,3],'o-',label = \"pull1->socialgaze2\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,1,3],'o-',label = \"pull2->socialgaze2\")\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_ylim(-0.1,1.1)\n",
    "ax1.set_ylabel(\"transition probability\",fontsize=13)\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates,1),np.array(dates_list)[sorting_df.index], rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"transition probability from pull to social gaze\", fontsize = 14)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.15,1.15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.075,tasktypes[itaskswitch],fontsize=10)\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','no-vision']\n",
    "\n",
    "pull1_gaze1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],0,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],0,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],0,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],0,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],0,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],0,2]]\n",
    "pull2_gaze1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],1,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],1,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],1,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],1,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],1,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],1,2]]\n",
    "pull1_gaze2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],0,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],0,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],0,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],0,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],0,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],0,3]]\n",
    "pull2_gaze2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],1,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],1,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],1,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],1,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],1,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],1,3]]\n",
    "\n",
    "doboxplot = 0\n",
    "domeanplot = 1\n",
    "savefigs=1\n",
    "\n",
    "if doboxplot:\n",
    "    pull1_gaze1_plot = plt.boxplot(pull1_gaze1,positions=np.array(np.arange(len(pull1_gaze1)))*4.0-1.05,widths=0.6)\n",
    "    pull2_gaze1_plot = plt.boxplot(pull2_gaze1,positions=np.array(np.arange(len(pull2_gaze1)))*4.0-0.35,widths=0.6)\n",
    "    pull1_gaze2_plot = plt.boxplot(pull1_gaze2,positions=np.array(np.arange(len(pull1_gaze2)))*4.0+0.35,widths=0.6)\n",
    "    pull2_gaze2_plot = plt.boxplot(pull2_gaze2,positions=np.array(np.arange(len(pull2_gaze2)))*4.0+1.05,widths=0.6)\n",
    "    #\n",
    "    def define_box_properties(plot_name, color_code, label):\n",
    "        for k, v in plot_name.items():\n",
    "            plt.setp(plot_name.get(k), color=color_code)\n",
    "        plt.plot([], c=color_code, label=label)\n",
    "        plt.legend()\n",
    "    #\n",
    "    define_box_properties(pull1_gaze1_plot, '#0343DF', 'pull1->gaze1')\n",
    "    define_box_properties(pull2_gaze1_plot, '#FFA500', 'pull2->gaze1')\n",
    "    define_box_properties(pull1_gaze2_plot, '#008000', 'pull1->gaze2')\n",
    "    define_box_properties(pull2_gaze2_plot, '#8C000F', 'pull2->gaze2')\n",
    "    #\n",
    "    # set the x label values\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes);\n",
    "    \n",
    "if domeanplot:\n",
    "    pull1_gaze1_plot = plt.plot(np.array(np.arange(len(pull1_gaze1)))*4.0,np.nanmean(pd.DataFrame(pull1_gaze1),axis=1),'-o',label='pull1->M1gazeM2')\n",
    "    pull1_gaze2_plot = plt.plot(np.array(np.arange(len(pull1_gaze2)))*4.0,np.nanmean(pd.DataFrame(pull1_gaze2),axis=1),'-o',label='pull1->M2gazeM1')\n",
    "    pull2_gaze1_plot = plt.plot(np.array(np.arange(len(pull2_gaze1)))*4.0,np.nanmean(pd.DataFrame(pull2_gaze1),axis=1),'-o',label='pull2->M1gazeM2')\n",
    "    pull2_gaze2_plot = plt.plot(np.array(np.arange(len(pull2_gaze2)))*4.0,np.nanmean(pd.DataFrame(pull2_gaze2),axis=1),'-o',label='pull2->M2gazeM1')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes,fontsize=15,rotation=45);\n",
    "    plt.ylabel('mean transition probability',fontsize=15)\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"pull_gaze_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+str(samplingsizes_name[0])+'_rows_ConditionMerged.jpg')\n",
    "    if minmaxfullSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doBhvitv_timebin:\n",
    "            plt.savefig(figsavefolder+\"pull_gaze_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_BhvIntBin_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "        else:\n",
    "            plt.savefig(figsavefolder+\"pull_gaze_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f16ef",
   "metadata": {},
   "source": [
    "### plot the transition probability from pull to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ff69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "#\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,0,0],'o-',label = \"pull1->pull1\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,1,1],'o-',label = \"pull2->pull2\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,0,1],'o-',label = \"pull1->pull2\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,1,0],'o-',label = \"pull2->pull1\")\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_ylim(-0.1,1.1)\n",
    "ax1.set_ylabel(\"transition probability\",fontsize=13)\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates,1),np.array(dates_list)[sorting_df.index], rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"transition probability from pull to pull\", fontsize = 14)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','novision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.15,1.15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.075,tasktypes[itaskswitch],fontsize=10)\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "\n",
    "pull1_pull1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],0,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],0,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],0,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],0,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],0,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],0,0]]\n",
    "pull2_pull2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],1,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],1,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],1,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],1,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],1,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],1,1]]\n",
    "pull1_pull2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],0,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],0,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],0,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],0,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],0,1],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],0,1]]\n",
    "pull2_pull1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],1,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],1,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],1,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],1,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],1,0],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],1,0]]\n",
    "\n",
    "doboxplot = 0\n",
    "domeanplot = 1\n",
    "savefigs=1\n",
    "\n",
    "if doboxplot:\n",
    "    pull1_pull1_plot = plt.boxplot(pull1_pull1,positions=np.array(np.arange(len(pull1_pull1)))*4.0-1.05,widths=0.6)\n",
    "    pull2_pull2_plot = plt.boxplot(pull2_pull2,positions=np.array(np.arange(len(pull2_pull2)))*4.0-0.35,widths=0.6)\n",
    "    pull1_pull2_plot = plt.boxplot(pull1_pull2,positions=np.array(np.arange(len(pull1_pull2)))*4.0+0.35,widths=0.6)\n",
    "    pull2_pull1_plot = plt.boxplot(pull2_pull1,positions=np.array(np.arange(len(pull2_pull1)))*4.0+1.05,widths=0.6)\n",
    "    #\n",
    "    def define_box_properties(plot_name, color_code, label):\n",
    "        for k, v in plot_name.items():\n",
    "            plt.setp(plot_name.get(k), color=color_code)\n",
    "        plt.plot([], c=color_code, label=label)\n",
    "        plt.legend()\n",
    "    #\n",
    "    define_box_properties(pull1_pull1_plot, '#0343DF', 'pull1->pull1')\n",
    "    define_box_properties(pull2_pull2_plot, '#FFA500', 'pull2->pull2')\n",
    "    define_box_properties(pull1_pull2_plot, '#008000', 'pull1->pull2')\n",
    "    define_box_properties(pull2_pull1_plot, '#8C000F', 'pull2->pull1')\n",
    "    #\n",
    "    # set the x label values\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes);\n",
    "    \n",
    "if domeanplot:\n",
    "    pull1_pull1_plot = plt.plot(np.array(np.arange(len(pull1_pull1)))*4.0,np.nanmean(pd.DataFrame(pull1_pull1),axis=1),'-o',label='pull1->pull1')\n",
    "    pull1_pull2_plot = plt.plot(np.array(np.arange(len(pull1_pull2)))*4.0,np.nanmean(pd.DataFrame(pull1_pull2),axis=1),'-o',label='pull1->pull2')\n",
    "    pull2_pull1_plot = plt.plot(np.array(np.arange(len(pull2_pull1)))*4.0,np.nanmean(pd.DataFrame(pull2_pull1),axis=1),'-o',label='pull2->pull1')\n",
    "    pull2_pull2_plot = plt.plot(np.array(np.arange(len(pull2_pull2)))*4.0,np.nanmean(pd.DataFrame(pull2_pull2),axis=1),'-o',label='pull2->pull2')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes,fontsize=15,rotation=45);\n",
    "    plt.ylabel('mean transition probability',fontsize=15)\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"pull_pull_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+str(samplingsizes_name[0])+'_rows_ConditionMerged.jpg')\n",
    "    if minmaxfullSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doBhvitv_timebin:\n",
    "            plt.savefig(figsavefolder+\"pull_pull_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_BhvIntBin_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "        else:    \n",
    "            plt.savefig(figsavefolder+\"pull_pull_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb48836",
   "metadata": {},
   "source": [
    "### plot the transition probability from social gaze to social gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23027c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "#\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,2,2],'o-',label = \"socialgaze1->socialgaze1\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,3,3],'o-',label = \"socialgaze2->socialgaze2\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,2,3],'o-',label = \"socialgaze1->socialgaze2\")\n",
    "ax1.plot(DAGs_all_dates[sorting_df.index,3,2],'o-',label = \"socialgaze2->socialgaze1\")\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_ylim(-0.1,1.1)\n",
    "ax1.set_ylabel(\"transition probability\",fontsize=13)\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates,1),np.array(dates_list)[sorting_df.index], rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"transition probability from social gaze to social gaze\", fontsize = 14)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','novision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.15,1.15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.075,tasktypes[itaskswitch],fontsize=10)\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60747f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "\n",
    "gaze1_gaze1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],2,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],2,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],2,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],2,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],2,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],2,2]]\n",
    "gaze2_gaze2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],3,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],3,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],3,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],3,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],3,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],3,3]]\n",
    "gaze1_gaze2 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],2,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],2,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],2,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],2,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],2,3],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],2,3]]\n",
    "gaze2_gaze1 = [DAGs_all_dates[np.transpose(coopthres_forsort==100)[0],3,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==3)[0],3,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==2)[0],3,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1.5)[0],3,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==1)[0],3,2],\n",
    "               DAGs_all_dates[np.transpose(coopthres_forsort==-1)[0],3,2]]\n",
    "\n",
    "doboxplot = 0\n",
    "domeanplot = 1\n",
    "savefigs=1\n",
    "\n",
    "if doboxplot:\n",
    "    \n",
    "    gaze1_gaze1_plot = plt.boxplot(gaze1_gaze1,positions=np.array(np.arange(len(gaze1_gaze1)))*4.0-1.05,widths=0.6)\n",
    "    gaze2_gaze2_plot = plt.boxplot(gaze2_gaze2,positions=np.array(np.arange(len(gaze2_gaze2)))*4.0-0.35,widths=0.6)\n",
    "    gaze1_gaze2_plot = plt.boxplot(gaze1_gaze2,positions=np.array(np.arange(len(gaze1_gaze2)))*4.0+0.35,widths=0.6)\n",
    "    gaze2_gaze1_plot = plt.boxplot(gaze2_gaze1,positions=np.array(np.arange(len(gaze2_gaze1)))*4.0+1.05,widths=0.6)\n",
    "    #\n",
    "    def define_box_properties(plot_name, color_code, label):\n",
    "        for k, v in plot_name.items():\n",
    "            plt.setp(plot_name.get(k), color=color_code)\n",
    "        plt.plot([], c=color_code, label=label)\n",
    "        plt.legend()\n",
    "    #\n",
    "    define_box_properties(gaze1_gaze1_plot, '#0343DF', 'gaze1->gaze1')\n",
    "    define_box_properties(gaze2_gaze2_plot, '#FFA500', 'gaze2->gaze2')\n",
    "    define_box_properties(gaze1_gaze2_plot, '#008000', 'gaze1->gaze2')\n",
    "    define_box_properties(gaze2_gaze1_plot, '#8C000F', 'gaze2->gaze1')\n",
    "    #\n",
    "    # set the x label values\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes);\n",
    "    \n",
    "if domeanplot:\n",
    "    gaze1_gaze1_plot = plt.plot(np.array(np.arange(len(gaze1_gaze1)))*4.0,np.nanmean(pd.DataFrame(gaze1_gaze1),axis=1),'-o',label='M1gazeM2->M1gazeM2')\n",
    "    gaze1_gaze2_plot = plt.plot(np.array(np.arange(len(gaze1_gaze2)))*4.0,np.nanmean(pd.DataFrame(gaze1_gaze2),axis=1),'-o',label='M1gazeM2->M2gazeM1')\n",
    "    gaze2_gaze1_plot = plt.plot(np.array(np.arange(len(gaze2_gaze1)))*4.0,np.nanmean(pd.DataFrame(gaze2_gaze1),axis=1),'-o',label='M2gazeM1->M1gazeM2')\n",
    "    gaze2_gaze2_plot = plt.plot(np.array(np.arange(len(gaze2_gaze2)))*4.0,np.nanmean(pd.DataFrame(gaze2_gaze2),axis=1),'-o',label='M2gazeM1->M2gazeM1')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xticks(np.arange(0, len(grouptypes)*4, 4), grouptypes,fontsize=15,rotation=45);\n",
    "    plt.ylabel('mean transition probability',fontsize=15)\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"gaze_gaze_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+str(samplingsizes_name[0])+'_rows_ConditionMerged.jpg')\n",
    "    if minmaxfullSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_DBN_and_bhv_Aniposelib3d_allsessions_morevars_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doBhvitv_timebin:\n",
    "            plt.savefig(figsavefolder+\"gaze_gaze_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_BhvIntBin_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "        else:\n",
    "            plt.savefig(figsavefolder+\"gaze_gaze_prob_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolus[0])+'_'+samplingsizes_name[0]+'_ConditionMerged.jpg')\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394fa61",
   "metadata": {},
   "source": [
    "### sanity check: plot the pull transition probability vs behavioral measures - successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37316256",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3,figsize=(15, 10))\n",
    "\n",
    "# ind = (tasktypes_all_dates != 1) & (succ_rate_all_dates*trialnum_all_dates > 20)\n",
    "# ind = (tasktypes_all_dates != 1) & (interpullintv_all_dates < 10)\n",
    "# ind = (tasktypes_all_dates != 1) & (coopthres_all_dates == 1)\n",
    "ind = (tasktypes_all_dates != 1)\n",
    "#\n",
    "xxx  = succ_rate_all_dates[ind]\n",
    "yyy1 = DAGs_all_dates[np.transpose(ind)[0],0,0]\n",
    "y1label = \"pull1->pull1\"\n",
    "yyy2 = DAGs_all_dates[np.transpose(ind)[0],1,1]\n",
    "y2label = \"pull2->pull2\"\n",
    "yyy3 = DAGs_all_dates[np.transpose(ind)[0],0,1]\n",
    "y3label = \"pull1->pull2\"\n",
    "yyy4 = DAGs_all_dates[np.transpose(ind)[0],1,0]\n",
    "y4label = \"pull2->pull1\"\n",
    "#\n",
    "axs[0,0].plot(xxx,yyy1,'o')\n",
    "axs[0,0].set_xlabel('successful rate')\n",
    "axs[0,0].set_ylabel('transition probability '+y1label)\n",
    "axs[0,0].set_xlim([-0.1,1.1])\n",
    "axs[0,0].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy1)\n",
    "axs[0,0].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[0,1].plot(xxx,yyy2,'o')\n",
    "axs[0,1].set_xlabel('successful rate')\n",
    "axs[0,1].set_ylabel('transition probability '+y2label)\n",
    "axs[0,1].set_xlim([-0.1,1.1])\n",
    "axs[0,1].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy2)\n",
    "axs[0,1].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[1,0].plot(xxx,yyy3,'o')\n",
    "axs[1,0].set_xlabel('successful rate')\n",
    "axs[1,0].set_ylabel('transition probability '+y3label)\n",
    "axs[1,0].set_xlim([-0.1,1.1])\n",
    "axs[1,0].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy3)\n",
    "axs[1,0].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[1,1].plot(xxx,yyy4,'o')\n",
    "axs[1,1].set_xlabel('successful rate')\n",
    "axs[1,1].set_ylabel('transition probability '+y4label)\n",
    "axs[1,1].set_xlim([-0.1,1.1])\n",
    "axs[1,1].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy4)\n",
    "axs[1,1].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[0,2].plot(np.concatenate((xxx, xxx)),np.concatenate((yyy1, yyy2)),'o')\n",
    "axs[0,2].set_xlabel('successful rate')\n",
    "axs[0,2].set_ylabel('transition probability pull1(2)->1(2)')\n",
    "axs[0,2].set_xlim([-0.1,1.1])\n",
    "axs[0,2].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(np.concatenate((xxx, xxx)),np.concatenate((yyy1, yyy2)))\n",
    "axs[0,2].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[1,2].plot(np.concatenate((xxx, xxx)),np.concatenate((yyy3, yyy4)),'o')\n",
    "axs[1,2].set_xlabel('successful rate')\n",
    "axs[1,2].set_ylabel('transition probability pull1(2)->2(1)')\n",
    "axs[1,2].set_xlim([-0.1,1.1])\n",
    "axs[1,2].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(np.concatenate((xxx, xxx)),np.concatenate((yyy3, yyy4)))\n",
    "axs[1,2].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d3e55",
   "metadata": {},
   "source": [
    "### plot the gaze->pull transition probability vs behavioral measures - successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe62b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3,figsize=(15, 10))\n",
    "\n",
    "# ind = (tasktypes_all_dates != 1) & (succ_rate_all_dates*trialnum_all_dates > 20)\n",
    "# ind = (tasktypes_all_dates != 1) & (interpullintv_all_dates < 10)\n",
    "# ind = (tasktypes_all_dates != 1) & (coopthres_all_dates == 1)\n",
    "ind = (tasktypes_all_dates != 1)\n",
    "\n",
    "xxx  = succ_rate_all_dates[ind]\n",
    "yyy1 = DAGs_all_dates[np.transpose(ind)[0],2,0]\n",
    "y1label = \"socialgaze1->pull1\"\n",
    "yyy2 = DAGs_all_dates[np.transpose(ind)[0],3,1]\n",
    "y2label = \"socialgaze2->pull2\"\n",
    "yyy3 = DAGs_all_dates[np.transpose(ind)[0],2,1]\n",
    "y3label = \"socialgaze1->pull2\"\n",
    "yyy4 = DAGs_all_dates[np.transpose(ind)[0],3,0]\n",
    "y4label = \"socialgaze2->pull1\"\n",
    "\n",
    "#\n",
    "axs[0,0].plot(xxx,yyy1,'o')\n",
    "axs[0,0].set_xlabel('successful rate')\n",
    "axs[0,0].set_ylabel('transition probability '+y1label)\n",
    "axs[0,0].set_xlim([-0.1,1.1])\n",
    "axs[0,0].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy1)\n",
    "axs[0,0].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[0,1].plot(xxx,yyy2,'o')\n",
    "axs[0,1].set_xlabel('successful rate')\n",
    "axs[0,1].set_ylabel('transition probability '+y2label)\n",
    "axs[0,1].set_xlim([-0.1,1.1])\n",
    "axs[0,1].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy2)\n",
    "axs[0,1].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[1,0].plot(xxx,yyy3,'o')\n",
    "axs[1,0].set_xlabel('successful rate')\n",
    "axs[1,0].set_ylabel('transition probability '+y3label)\n",
    "axs[1,0].set_xlim([-0.1,1.1])\n",
    "axs[1,0].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy3)\n",
    "axs[1,0].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[1,1].plot(xxx,yyy4,'o')\n",
    "axs[1,1].set_xlabel('successful rate')\n",
    "axs[1,1].set_ylabel('transition probability '+y4label)\n",
    "axs[1,1].set_xlim([-0.1,1.1])\n",
    "axs[1,1].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(xxx,yyy4)\n",
    "axs[1,1].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[0,2].plot(np.concatenate((xxx, xxx)),np.concatenate((yyy1, yyy2)),'o')\n",
    "axs[0,2].set_xlabel('successful rate')\n",
    "axs[0,2].set_ylabel('transition probability socialgaze1(2)->pull1(2)')\n",
    "axs[0,2].set_xlim([-0.1,1.1])\n",
    "axs[0,2].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(np.concatenate((xxx, xxx)),np.concatenate((yyy1, yyy2)))\n",
    "axs[0,2].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))\n",
    "#\n",
    "axs[1,2].plot(np.concatenate((xxx, xxx)),np.concatenate((yyy3, yyy4)),'o')\n",
    "axs[1,2].set_xlabel('successful rate')\n",
    "axs[1,2].set_ylabel('transition probability socialgaze1(2)->pull2(1)')\n",
    "axs[1,2].set_xlim([-0.1,1.1])\n",
    "axs[1,2].set_ylim([-0.1,1.1])\n",
    "r,p = scipy.stats.pearsonr(np.concatenate((xxx, xxx)),np.concatenate((yyy3, yyy4)))\n",
    "axs[1,2].text(0, 1, 'corr r = '+str(np.round(r,2))+'; '+'corr p = '+str(np.round(p,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907ac74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331cb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e304e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891ec9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e4695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad47aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5d371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879afe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3904c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09680d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
