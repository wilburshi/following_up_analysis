{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN has run and this script is used to make predictions\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval\n",
    "from ana_functions.bhv_events_interval import bhv_events_interval_certainEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 2*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467f2e8",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### For each condition, only use the hypothetical dependencies\n",
    "### For each dyad align animal1 as the subordinate animal and animal2 as the donimant animal\n",
    "### for each dyad, for each iternation, train on 80% of data but test on 20% of other dyad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53984386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "redoFitting = 0\n",
    "\n",
    "do_succfull = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "# donimant animal name; since animal1 and 2 are already aligned, no need to check it; but keep it here for reference\n",
    "dom_animal_names = ['sparkle','scorch','kanga_withG','kanga_withD','koala']\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "# DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "# DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "# DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "DBN_group_typenames = ['coop(2s)']\n",
    "DBN_group_typeIDs  =  [3,]\n",
    "DBN_group_coopthres = [2,]\n",
    "if do_trainedMCs:\n",
    "    DBN_group_typenames = ['coop(1s)']\n",
    "    DBN_group_typeIDs  =  [3]\n",
    "    DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN input data - to and from Nodes\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = '' # '' means 1secondlag, otherwise will be specificed\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "    \n",
    "# hypothetical graph structure that reflect the strategies\n",
    "# hypothetical graph structure that reflect the strategies\n",
    "# strategynames = ['threeMains','sync_pulls','gaze_lead_pull','social_attention','other_dependencies','other_noself_dependcies']\n",
    "strategynames = ['threeMains',]\n",
    "# strategynames = ['sync_pulls'] # ['all_threes','sync_pulls','gaze_lead_pull','social_attention']\n",
    "bina_graphs_specific_strategy = {\n",
    "    'threeMains': np.array([[0,1,0,1],[1,0,1,0],[1,0,0,0],[0,1,0,0]]),\n",
    "    'sync_pulls': np.array([[0,1,0,0],[1,0,0,0],[0,0,0,0],[0,0,0,0]]),\n",
    "    'gaze_lead_pull':np.array([[0,0,0,0],[0,0,0,0],[1,0,0,0],[0,1,0,0]]),\n",
    "    'social_attention':np.array([[0,0,0,1],[0,0,1,0],[0,0,0,0],[0,0,0,0]]),\n",
    "    'other_dependencies': np.array([[1,0,1,0],[0,1,0,1],[0,1,1,1],[1,0,1,1]]),\n",
    "    'other_noself_dependcies': np.array([[0,0,1,0],[0,0,0,1],[0,1,0,1],[1,0,1,0]]),\n",
    "}\n",
    "nstrategies_forplot = np.shape(strategynames)[0]\n",
    "\n",
    "\n",
    "for istrg in np.arange(0,nstrategies_forplot,1):\n",
    "    \n",
    "    strategyname = strategynames[istrg]\n",
    "\n",
    "    #\n",
    "    bina_graph_mean_strg = bina_graphs_specific_strategy[strategyname]\n",
    "    \n",
    "    # translate the binary DAGs to edge\n",
    "    nrows,ncols = np.shape(bina_graph_mean_strg)\n",
    "    edgenames = []\n",
    "    for irow in np.arange(0,nrows,1):\n",
    "        for icol in np.arange(0,ncols,1):\n",
    "            if bina_graph_mean_strg[irow,icol] > 0:\n",
    "                edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "    # define the DBN predicting model\n",
    "    bn = BayesianNetwork()\n",
    "    bn.add_nodes_from(fromNodes)\n",
    "    bn.add_nodes_from(toNodes)\n",
    "    bn.add_edges_from(edgenames)\n",
    "    \n",
    "    effect_slice = toNodes\n",
    "    \n",
    "    # load ROC_summary_all data\n",
    "    try:\n",
    "        if redoFitting:\n",
    "            dumpy\n",
    "        \n",
    "        print('load all ROC data for hypothetical dependencies, and only plot the summary figure')\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions_cross_dyad_validation/'+\\\n",
    "                               savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_'+strategyname+timelagtype+'.pkl', 'rb') as f:\n",
    "            ROC_summary_all = pickle.load(f)\n",
    "   \n",
    "    except:  \n",
    "    \n",
    "        # initialize a summary dataframe for plotting the summary figure across animals \n",
    "        ROC_summary_all = pd.DataFrame(columns=['train_animal','test_animal','action','testCondition','predROC'])\n",
    "\n",
    "        #\n",
    "        # session type to analyze\n",
    "        for igroup in np.arange(0,nDBN_groups,1):\n",
    "            DBN_group_typename = DBN_group_typenames[igroup]\n",
    "               \n",
    "            #    \n",
    "            # load the dyad for training\n",
    "            for ianimalpair_train in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "                #\n",
    "                # load the DBN input data\n",
    "                animal1_train = animal1_fixedorders[ianimalpair_train]\n",
    "                animal2_train = animal2_fixedorders[ianimalpair_train]\n",
    "                #\n",
    "                # only for kanga\n",
    "                if animal2_train == 'kanga':\n",
    "                    if animal1_train == 'ginger':\n",
    "                        animal2_train_nooverlap = 'kanga_withG'\n",
    "                    elif animal1_train == 'dannon':\n",
    "                        animal2_train_nooverlap = 'kanga_withD'\n",
    "                    else:\n",
    "                        animal2_train_nooverlap = animal2_train\n",
    "                else:\n",
    "                    animal2_train_nooverlap = animal2_train\n",
    "                #\n",
    "                if not do_succfull:\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_train+animal2_train+'/'\n",
    "                    if not mergetempRos:\n",
    "                        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_train+animal2_train+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                            DBN_input_data_alltypes = pickle.load(f)\n",
    "                    else:\n",
    "                        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_train+animal2_train+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                            DBN_input_data_alltypes = pickle.load(f)\n",
    "                    #\n",
    "                    DBN_input_data_train = DBN_input_data_alltypes[DBN_group_typename]\n",
    "                #    \n",
    "                elif do_succfull:\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_train+animal2_train+'/'\n",
    "                    if not mergetempRos:\n",
    "                        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_train+animal2_train+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                            DBN_input_data_alltypes = pickle.load(f)\n",
    "                    else:\n",
    "                        with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_train+animal2_train+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                            DBN_input_data_alltypes = pickle.load(f)\n",
    "                    #\n",
    "                    DBN_input_data_train = DBN_input_data_alltypes['succpull'][DBN_group_typename]\n",
    "            \n",
    "                #    \n",
    "                # load the dyad for testing\n",
    "                for ianimalpair_test in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "                    # load the DBN input data\n",
    "                    animal1_test = animal1_fixedorders[ianimalpair_test]\n",
    "                    animal2_test = animal2_fixedorders[ianimalpair_test]\n",
    "                    #\n",
    "                    # only for kanga\n",
    "                    if animal2_test == 'kanga':\n",
    "                        if animal1_test == 'ginger':\n",
    "                            animal2_test_nooverlap = 'kanga_withG'\n",
    "                        elif animal1_test == 'dannon':\n",
    "                            animal2_test_nooverlap = 'kanga_withD'\n",
    "                        else:\n",
    "                            animal2_test_nooverlap = animal2_test\n",
    "                    else:\n",
    "                        animal2_test_nooverlap = animal2_test\n",
    "                    #\n",
    "                    if not do_succfull:\n",
    "                        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_test+animal2_test+'/'\n",
    "                        if not mergetempRos:\n",
    "                            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_test+animal2_test+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                                DBN_input_data_alltypes = pickle.load(f)\n",
    "                        else:\n",
    "                            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_test+animal2_test+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                                DBN_input_data_alltypes = pickle.load(f)\n",
    "                        #\n",
    "                        DBN_input_data_test = DBN_input_data_alltypes[DBN_group_typename]\n",
    "                    #    \n",
    "                    elif do_succfull:\n",
    "                        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_test+animal2_test+'/'\n",
    "                        if not mergetempRos:\n",
    "                            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_test+animal2_test+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                                DBN_input_data_alltypes = pickle.load(f)\n",
    "                        else:\n",
    "                            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_test+animal2_test+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                                DBN_input_data_alltypes = pickle.load(f)\n",
    "                        #\n",
    "                        DBN_input_data_test = DBN_input_data_alltypes['succpull'][DBN_group_typename]\n",
    "           \n",
    "        \n",
    "                    #\n",
    "                    # run niters iterations for each condition\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                        # Split data into training and testing sets\n",
    "                        train_data, _ = train_test_split(DBN_input_data_train, test_size=0.2)\n",
    "                        _,  test_data = train_test_split(DBN_input_data_test, test_size=0.5)\n",
    "\n",
    "                        # Perform parameter learning for each time slice\n",
    "                        bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                        # Perform inference\n",
    "                        infer = VariableElimination(bn)\n",
    "\n",
    "                        # Prediction for each behavioral events\n",
    "                        # With aligned animals across dyad - animal1:sub, animal2:dom\n",
    "                        for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                            var = effect_slice[ievent]\n",
    "                            Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                            for index, row in test_data.iterrows():\n",
    "                                evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                            fromNodes[1]: row[fromNodes[1]], \n",
    "                                            fromNodes[2]: row[fromNodes[2]], \n",
    "                                            fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                                # Query the probability distribution for Pulls given evidence\n",
    "                                aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                                # Extract the probability of outcome = 1\n",
    "                                prob = aucPpredBehavior.values[1]\n",
    "                                Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                            # Calculate the AUC score\n",
    "                            trueBeh = test_data[var].values\n",
    "                            try:\n",
    "                                auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                            except:\n",
    "                                auc = np.nan\n",
    "                            print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                            # put data in the summarizing data frame\n",
    "                            if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                                ROC_summary_all = ROC_summary_all.append({'train_animal':animal1_train,\n",
    "                                                                          'test_animal':animal1_test,\n",
    "                                                                          'train_dyadID':ianimalpair_train,\n",
    "                                                                          'test_dyadID':ianimalpair_test,\n",
    "                                                                          'action':eventnames[ievent][2:],\n",
    "                                                                          'testCondition':DBN_group_typename,\n",
    "                                                                          'predROC':auc,\n",
    "                                                                          'iters': iiter,\n",
    "                                                                         }, ignore_index=True)\n",
    "                            else:\n",
    "                                ROC_summary_all = ROC_summary_all.append({'train_animal':animal2_train_nooverlap,\n",
    "                                                                          'test_animal':animal2_test_nooverlap,\n",
    "                                                                          'train_dyadID':ianimalpair_train,\n",
    "                                                                          'test_dyadID':ianimalpair_test,\n",
    "                                                                          'action':eventnames[ievent][2:],\n",
    "                                                                          'testCondition':DBN_group_typename,\n",
    "                                                                          'predROC':auc,\n",
    "                                                                          'iters': iiter,\n",
    "                                                                     }, ignore_index=True)\n",
    "                                \n",
    "                        #         \n",
    "                        # Prediction for each behavioral events with swapped animal1 and animal2\n",
    "                        # With training set and testing set has swapped animal type:\n",
    "                        # training set: animal1 - sub; animal2 - dom\n",
    "                        # testing set: animal1 - dom; animal2 - sub\n",
    "                        test_data_swap = test_data.copy()\n",
    "                        # Create a column mapping\n",
    "                        new_columns = {}\n",
    "                        for col in test_data_swap.columns:\n",
    "                            if 'pull1' in col:\n",
    "                                new_columns[col] = col.replace('pull1', 'pull2')\n",
    "                            elif 'pull2' in col:\n",
    "                                new_columns[col] = col.replace('pull2', 'pull1')\n",
    "                            elif 'owgaze1' in col:\n",
    "                                new_columns[col] = col.replace('owgaze1', 'owgaze2')\n",
    "                            elif 'owgaze2' in col:\n",
    "                                new_columns[col] = col.replace('owgaze2', 'owgaze1')\n",
    "                        # Rename the columns using the mapping\n",
    "                        test_data_swap = test_data_swap.rename(columns=new_columns)\n",
    "                        \n",
    "                        for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                            var = effect_slice[ievent]\n",
    "                            Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                            for index, row in test_data_swap.iterrows():\n",
    "                                evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                            fromNodes[1]: row[fromNodes[1]], \n",
    "                                            fromNodes[2]: row[fromNodes[2]], \n",
    "                                            fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                                # Query the probability distribution for Pulls given evidence\n",
    "                                aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                                # Extract the probability of outcome = 1\n",
    "                                prob = aucPpredBehavior.values[1]\n",
    "                                Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                            # Calculate the AUC score\n",
    "                            trueBeh = test_data_swap[var].values\n",
    "                            try:\n",
    "                                auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                            except:\n",
    "                                auc = np.nan\n",
    "                            print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                            # put data in the summarizing data frame\n",
    "                            if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                                ROC_summary_all = ROC_summary_all.append({'train_animal':animal1_train,\n",
    "                                                                          'test_animal':animal2_test_nooverlap,\n",
    "                                                                          'train_dyadID':ianimalpair_train,\n",
    "                                                                          'test_dyadID':ianimalpair_test,\n",
    "                                                                          'action':eventnames[ievent][2:],\n",
    "                                                                          'testCondition':DBN_group_typename,\n",
    "                                                                          'predROC':auc,\n",
    "                                                                          'iters': iiter,\n",
    "                                                                         }, ignore_index=True)\n",
    "                            else:\n",
    "                                ROC_summary_all = ROC_summary_all.append({'train_animal':animal2_train_nooverlap,\n",
    "                                                                          'test_animal':animal1_test,\n",
    "                                                                          'train_dyadID':ianimalpair_train,\n",
    "                                                                          'test_dyadID':ianimalpair_test,\n",
    "                                                                          'action':eventnames[ievent][2:],\n",
    "                                                                          'testCondition':DBN_group_typename,\n",
    "                                                                          'predROC':auc,\n",
    "                                                                          'iters': iiter,\n",
    "                                                                     }, ignore_index=True)\n",
    "\n",
    "                            \n",
    "                    \n",
    "        \n",
    "        \n",
    "        # save the summarizing data ROC_summary_all\n",
    "        savedata = 0\n",
    "        if savedata:\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions_cross_dyad_validation/'+\\\n",
    "                                   savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "\n",
    "            with open(data_saved_subfolder+'/ROC_summary_all_dependencies_'+strategyname+timelagtype+'.pkl', 'wb') as f:\n",
    "                pickle.dump(ROC_summary_all, f)\n",
    "\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    ind = (ROC_summary_all['action']=='pull') & (ROC_summary_all['testCondition']=='coop(1s)')\n",
    "    ROC_summary_all_tgt = ROC_summary_all[ind]\n",
    "\n",
    "    print(ROC_summary_all_tgt.keys())\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Pivot the DataFrame to have train_animal as rows and test_animal as columns\n",
    "    heatmap_data = ROC_summary_all_tgt.pivot(index='train_animal', columns='test_animal', values='predROC')\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='viridis', vmin=0.5, vmax=1.0)\n",
    "    plt.title('ROC AUC Heatmap')\n",
    "    plt.xlabel('Test Animal')\n",
    "    plt.ylabel('Train Animal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further analysis\n",
    "###\n",
    "# step 1: add dom/sub information\n",
    "###\n",
    "\n",
    "# Function to assign rank\n",
    "def get_rank(name):\n",
    "    return 'dom' if name in dom_animal_names else 'sub'\n",
    "\n",
    "# Apply to create new columns\n",
    "ROC_summary_all['train_rank'] = ROC_summary_all['train_animal'].apply(get_rank)\n",
    "ROC_summary_all['test_rank'] = ROC_summary_all['test_animal'].apply(get_rank)\n",
    "\n",
    "###\n",
    "# Step 2: Group by unique combination (excluding iteration) and get mean/std of predROC\n",
    "###\n",
    "# Group and compute mean and std\n",
    "ROC_summary_grouped = (\n",
    "    ROC_summary_all\n",
    "    .groupby(['train_animal', 'test_animal', 'action', 'testCondition'])\n",
    "    .agg(predROC_mean=('predROC', 'mean'),\n",
    "         predROC_std=('predROC', 'std'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Drop duplicates to preserve unique train/test animal rank combinations\n",
    "rank_info = ROC_summary_all[['train_animal', 'test_animal', 'train_rank', 'test_rank']].drop_duplicates()\n",
    "\n",
    "# Merge into grouped summary\n",
    "ROC_summary_grouped = ROC_summary_grouped.merge(rank_info, on=['train_animal', 'test_animal'], how='left')\n",
    "\n",
    "###\n",
    "# add statistic column\n",
    "###\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Step 1: collect all t-tests\n",
    "p_values = []\n",
    "group_keys = []\n",
    "for name, group in ROC_summary_all.groupby(['train_animal', 'test_animal', 'action', 'testCondition']):\n",
    "    t_stat, p = ttest_1samp(group['predROC'], 0.5)\n",
    "    p_values.append(p)\n",
    "    group_keys.append(name)\n",
    "\n",
    "# Step 2: correct p-values\n",
    "rejected, pvals_corrected, _, _ = multipletests(p_values, method='fdr_bh')  # or method='bonferroni'\n",
    "\n",
    "# Step 3: build a DataFrame\n",
    "significance_df = pd.DataFrame(group_keys, columns=['train_animal', 'test_animal', 'action', 'testCondition'])\n",
    "significance_df['p_value'] = p_values\n",
    "significance_df['p_value_corrected'] = pvals_corrected\n",
    "significance_df['significant_vs_0.5'] = rejected\n",
    "\n",
    "# Step 4: merge into ROC_summary_grouped\n",
    "ROC_summary_grouped = ROC_summary_grouped.merge(significance_df, \n",
    "                        on=['train_animal', 'test_animal', 'action', 'testCondition'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4bb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Step 3: plot - heatmap plot\n",
    "###\n",
    "if 1:\n",
    "    # Combine train and test animals with their dyad and rank info\n",
    "    # We'll use the original ROC_summary_all (before grouping) or ROC_summary_all_tgt if it has this info\n",
    "    # Here I assume 'train_animal', 'train_dyadID', 'train_rank' are available\n",
    "\n",
    "    # Extract unique animals with their dyad and rank info\n",
    "    train_info = ROC_summary_all[['train_animal', 'train_dyadID', 'train_rank']].drop_duplicates()\n",
    "    test_info = ROC_summary_all[['test_animal', 'test_dyadID', 'test_rank']].drop_duplicates()\n",
    "\n",
    "    # Rename columns to unify\n",
    "    train_info = train_info.rename(columns={'train_animal':'animal', 'train_dyadID':'dyadID', 'train_rank':'rank'})\n",
    "    test_info = test_info.rename(columns={'test_animal':'animal', 'test_dyadID':'dyadID', 'test_rank':'rank'})\n",
    "\n",
    "    # Combine and drop duplicates (some animals appear in both)\n",
    "    animal_info = pd.concat([train_info, test_info]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Sort by dyadID ascending, then rank (put 'dom' before 'sub')\n",
    "    animal_info['rank_order'] = animal_info['rank'].map({'dom': 0, 'sub': 1})\n",
    "    animal_info = animal_info.sort_values(by=['dyadID', 'rank_order'])\n",
    "\n",
    "    # Create the ordered list\n",
    "    ordered_animals = animal_info['animal'].tolist()\n",
    "\n",
    "    # choose the target entries\n",
    "    ind = (ROC_summary_grouped['action']=='pull') & (ROC_summary_grouped['testCondition']=='coop(1s)')\n",
    "    ROC_summary_all_tgt = ROC_summary_grouped[ind]\n",
    "\n",
    "    print(ROC_summary_all_tgt.keys())\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Pivot the DataFrame to have train_animal as rows and test_animal as columns\n",
    "    heatmap_data = ROC_summary_all_tgt.pivot(index='train_animal', columns='test_animal', values='predROC_mean')\n",
    "\n",
    "    heatmap_data = heatmap_data.reindex(index=ordered_animals, columns=ordered_animals)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='viridis', vmin=0.5, vmax=1.0)\n",
    "    plt.title('ROC AUC Heatmap')\n",
    "    plt.xlabel('Test Animal')\n",
    "    plt.ylabel('Train Animal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Step 3: plot - violin/bar plot\n",
    "###\n",
    "if 1:\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "    # --- Step 1: Define group column ---\n",
    "    ROC_summary_all_tgt['train_test_rank'] = (\n",
    "        'train_' + ROC_summary_all_tgt['train_rank'] + '_test_' + ROC_summary_all_tgt['test_rank']\n",
    "    )\n",
    "\n",
    "    group_order = ['train_dom_test_dom', 'train_dom_test_sub', 'train_sub_test_dom', 'train_sub_test_sub']\n",
    "\n",
    "    # --- Step 2: Run Tukey HSD post hoc test ---\n",
    "    tukey = pairwise_tukeyhsd(\n",
    "        endog=ROC_summary_all_tgt['predROC_mean'],\n",
    "        groups=ROC_summary_all_tgt['train_test_rank'],\n",
    "        alpha=0.05\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    tukey_results = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
    "\n",
    "    # Keep only significant results (p < 0.05)\n",
    "    sig_results = tukey_results[tukey_results['p-adj'].astype(float) < 0.05]\n",
    "\n",
    "    # --- Step 3: Create violin plot ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=ROC_summary_all_tgt, x='train_test_rank', y='predROC_mean',\n",
    "                   order=group_order, palette='muted')\n",
    "\n",
    "    sns.stripplot(data=ROC_summary_all_tgt, x='train_test_rank', y='predROC_mean',\n",
    "                  order=group_order, color='black', size=2, alpha=0.5, jitter=True)\n",
    "\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.ylabel('Mean ROC AUC')\n",
    "    plt.xlabel('Train-Test Rank Group')\n",
    "    plt.title('Distribution of ROC AUC by Train/Test Rank Combination')\n",
    "\n",
    "    # --- Step 4: Annotate significant pairs ---\n",
    "    group_pos = {name: i for i, name in enumerate(group_order)}\n",
    "    ymax = ROC_summary_all_tgt['predROC_mean'].max()\n",
    "    line_offset = 0.01\n",
    "    line_height = 0.02\n",
    "    current_offset = 0\n",
    "\n",
    "    for _, row in sig_results.iterrows():\n",
    "        g1, g2 = row['group1'], row['group2']\n",
    "        p_val = float(row['p-adj'])\n",
    "\n",
    "        if g1 not in group_pos or g2 not in group_pos:\n",
    "            continue\n",
    "\n",
    "        x1, x2 = group_pos[g1], group_pos[g2]\n",
    "        y = ymax + line_offset + current_offset\n",
    "        plt.plot([x1, x1, x2, x2], [y, y + line_height, y + line_height, y], color='black')\n",
    "        plt.text((x1 + x2) / 2, y + line_height + 0.005, f'p = {p_val:.3f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "        current_offset += 0.04  # Stack annotations\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Plot 2: 2-group violin with t-test ===\n",
    "    def classify_same_vs_cross(row):\n",
    "        return 'same_rank' if row['train_rank'] == row['test_rank'] else 'cross_rank'\n",
    "\n",
    "    ROC_summary_all_tgt['rank_pair_type'] = ROC_summary_all_tgt.apply(classify_same_vs_cross, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.violinplot(data=ROC_summary_all_tgt, x='rank_pair_type', y='predROC_mean', palette='pastel')\n",
    "    sns.stripplot(data=ROC_summary_all_tgt, x='rank_pair_type', y='predROC_mean',\n",
    "                  color='black', size=2, alpha=0.5, jitter=True)\n",
    "\n",
    "    # Stats: t-test\n",
    "    same_vals = ROC_summary_all_tgt[ROC_summary_all_tgt['rank_pair_type'] == 'same_rank']['predROC_mean']\n",
    "    cross_vals = ROC_summary_all_tgt[ROC_summary_all_tgt['rank_pair_type'] == 'cross_rank']['predROC_mean']\n",
    "    t_stat, p_ttest = ttest_ind(same_vals, cross_vals)\n",
    "\n",
    "    # Annotate t-test result\n",
    "    if p_ttest < 0.5:\n",
    "        stars = '***' if p_ttest < 0.001 else '**' if p_ttest < 0.01 else '*'\n",
    "        ymax = ROC_summary_all_tgt['predROC_mean'].max()\n",
    "        plt.text(0.5, ymax + 0.01, f'p = {p_ttest:.3e} {stars}', ha='center', fontsize=12)\n",
    "\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.ylabel('Mean ROC AUC')\n",
    "    plt.xlabel('Train-Test Rank Type')\n",
    "    plt.title('ROC AUC: Same vs Cross Rank')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ROC_summary_all_tgt['test_rank']=='sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ROC_summary_all_tgt['train_test_rank']=='train_sub_test_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ROC_summary_grouped['significant_vs_0.5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279a191",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### training set and testing set are from the same conditions\n",
    "### use the DBN learned structure, only the 0 1 DAG, do not consider the weights\n",
    "### do not consider the self dependencies (dependencies to variables themselves, no diagonal dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "redoFitting = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "# donimant animal name; since animal1 and 2 are already aligned, no need to check it; but keep it here for reference\n",
    "# dom_animal_names = ['sparkle','scorch','kanga_withG','kanga_withD','vermelho']\n",
    "dom_animal_names = ['sparkle','scorch','kanga_withG','kanga_withD','koala']\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "if do_trainedMCs:\n",
    "    DBN_group_typenames = ['coop(1s)']\n",
    "    DBN_group_typeIDs  =  [3]\n",
    "    DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN model\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = 'allthreelags'\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3], [4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1and2secondlag'\n",
    "# time_lags = ['t_-2','t_-1']\n",
    "# fromRowIDs =[[4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1secondlag'\n",
    "# time_lags = ['t_-1']\n",
    "# fromRowIDs =[[8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '2secondlag'\n",
    "# time_lags = ['t_-2']\n",
    "# fromRowIDs =[[4,5,6,7]]\n",
    "#\n",
    "# timelagtype = '3secondlag'\n",
    "# time_lags = ['t_-3']\n",
    "# fromRowIDs =[[0,1,2,3]]\n",
    "#\n",
    "nlags = np.shape(fromRowIDs)[0]\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "\n",
    "# load ROC_summary_all data\n",
    "try:\n",
    "    if redoFitting:\n",
    "        dumpy\n",
    "    print('load all ROC data for within task condition (only binary dependencies without self dependencies), and only plot the summary figure')\n",
    "        \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions_cross_dyad_validation/'+\\\n",
    "                               savefile_sufix+'/'+cameraID+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/ROC_summary_all_dependencies_DBNdependenciesAfterMI_binary_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "        ROC_summary_all = pickle.load(f)\n",
    "\n",
    "except:\n",
    "\n",
    "    # initialize a summary dataframe for plotting the summary figure across animals \n",
    "    ROC_summary_all = pd.DataFrame(columns=['train_animal','test_animal','action','testCondition','predROC'])\n",
    "    \n",
    "    for igroup in np.arange(0,nDBN_groups,1):\n",
    "        DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "        \n",
    "        #    \n",
    "        # load the dyad for training\n",
    "        for ianimalpair_train in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "            #\n",
    "            # load the DBN input data\n",
    "            animal1_train = animal1_fixedorders[ianimalpair_train]\n",
    "            animal2_train = animal2_fixedorders[ianimalpair_train]\n",
    "            #\n",
    "            # only for kanga\n",
    "            if animal2_train == 'kanga':\n",
    "                if animal1_train == 'ginger':\n",
    "                    animal2_train_nooverlap = 'kanga_withG'\n",
    "                elif animal1_train == 'dannon':\n",
    "                    animal2_train_nooverlap = 'kanga_withD'\n",
    "                else:\n",
    "                    animal2_train_nooverlap = animal2_train\n",
    "            else:\n",
    "                animal2_train_nooverlap = animal2_train\n",
    "            #\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_train+animal2_train+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_train+animal2_train+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_train+animal2_train+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            # load the DBN training outcome\n",
    "            if moreSampSize:\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_train+animal2_train+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_train+animal2_train+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_train+animal2_train+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_train+animal2_train+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_train+animal2_train+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_train+animal2_train+'.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "            # make sure these variables are the same as in the previous steps\n",
    "            # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            ntemp_reses = np.shape(temp_resolus)[0]\n",
    "            #\n",
    "            if moreSampSize:\n",
    "                # different data (down/re)sampling numbers\n",
    "                # samplingsizes = np.arange(1100,3000,100)\n",
    "                samplingsizes = [1100]\n",
    "                # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "                # samplingsizes = [100,500]\n",
    "                # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "                samplingsizes_name = list(map(str, samplingsizes))\n",
    "            else:\n",
    "                samplingsizes_name = ['min_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "            #\n",
    "            temp_resolu = temp_resolus[0]\n",
    "            j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "            #\n",
    "            DBN_input_data_train = DBN_input_data_alltypes[DBN_group_typename]\n",
    "            \n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "            \n",
    "            # self reward as the baseline to compare with\n",
    "            weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "            weighted_graphs_shuffled_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "            sig_edges_self = get_significant_edges(weighted_graphs_self,weighted_graphs_shuffled_self)\n",
    "\n",
    "            # calculate the modulation index\n",
    "            MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_tgt,\n",
    "                                          sig_edges_self, sig_edges_tgt, 150)\n",
    "            # only consider the edges that has significant MI and enhanced\n",
    "            nfromNodes = np.shape(MI_coop_self_all)[1]\n",
    "            ntoNodes = np.shape(MI_coop_self_all)[2]\n",
    "            sig_edges_MI = np.zeros((np.shape(sig_edges_coop_self)))\n",
    "            #\n",
    "            for ifromNode in np.arange(0,nfromNodes,1):\n",
    "                for itoNode in np.arange(0,ntoNodes,1):\n",
    "                    _,pp = st.ttest_1samp(MI_coop_self_all[:,ifromNode,itoNode],0)\n",
    "                    \n",
    "                    if (pp<0.01) & (np.nanmean(MI_coop_self_all[:,ifromNode,itoNode])>0):\n",
    "                        sig_edges_MI[ifromNode,itoNode] = 1\n",
    "                        \n",
    "            bina_graphs_mean_tgt = sig_edges_MI*sig_edges_coop_self\n",
    "            \n",
    "            #\n",
    "            # consider the time lags\n",
    "            if nlags == 1:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:] \n",
    "            elif nlags == 2:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]\n",
    "            elif nlags == 3:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]+bina_graphs_mean_tgt[fromRowIDs[2],:]\n",
    "\n",
    "            #\n",
    "            # translate the binary DAGs to edge\n",
    "            nrows,ncols = np.shape(bina_graphs_mean_tgt)\n",
    "            edgenames = []\n",
    "            for irow in np.arange(0,nrows,1):\n",
    "                for icol in np.arange(0,ncols,1):\n",
    "                    \n",
    "                    # remove the self dependencies\n",
    "                    if irow == icol:\n",
    "                        bina_graphs_mean_tgt[irow,icol] = 0\n",
    "                    \n",
    "                    if bina_graphs_mean_tgt[irow,icol] > 0:\n",
    "                        edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "            # define the DBN predicting model\n",
    "            bn = BayesianNetwork()\n",
    "            bn.add_nodes_from(fromNodes)\n",
    "            bn.add_nodes_from(toNodes)\n",
    "            bn.add_edges_from(edgenames)\n",
    "\n",
    "            effect_slice = toNodes        \n",
    "            \n",
    "            #    \n",
    "            # load the dyad for testing\n",
    "            for ianimalpair_test in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "                # load the DBN input data\n",
    "                animal1_test = animal1_fixedorders[ianimalpair_test]\n",
    "                animal2_test = animal2_fixedorders[ianimalpair_test]\n",
    "                #\n",
    "                # only for kanga\n",
    "                if animal2_test == 'kanga':\n",
    "                    if animal1_test == 'ginger':\n",
    "                        animal2_test_nooverlap = 'kanga_withG'\n",
    "                    elif animal1_test == 'dannon':\n",
    "                        animal2_test_nooverlap = 'kanga_withD'\n",
    "                    else:\n",
    "                        animal2_test_nooverlap = animal2_test\n",
    "                else:\n",
    "                    animal2_test_nooverlap = animal2_test\n",
    "                #\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_test+animal2_test+'/'\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_test+animal2_test+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_test+animal2_test+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                #\n",
    "                DBN_input_data_test = DBN_input_data_alltypes[DBN_group_typename]\n",
    "               \n",
    "            \n",
    "                #\n",
    "                # run niters iterations for each condition\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                    # Split data into training and testing sets\n",
    "                    train_data, _ = train_test_split(DBN_input_data_train, test_size=0.2)\n",
    "                    _,  test_data = train_test_split(DBN_input_data_test, test_size=0.5)\n",
    "\n",
    "                    # Perform parameter learning for each time slice\n",
    "                    bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                    # Perform inference\n",
    "                    infer = VariableElimination(bn)\n",
    "\n",
    "                    # Prediction for each behavioral events\n",
    "                    # With aligned animals across dyad - animal1:sub, animal2:dom\n",
    "                    for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                        var = effect_slice[ievent]\n",
    "                        Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                        for index, row in test_data.iterrows():\n",
    "                            evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                        fromNodes[1]: row[fromNodes[1]], \n",
    "                                        fromNodes[2]: row[fromNodes[2]], \n",
    "                                        fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                            # Query the probability distribution for Pulls given evidence\n",
    "                            aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                            # Extract the probability of outcome = 1\n",
    "                            prob = aucPpredBehavior.values[1]\n",
    "                            Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                        # Calculate the AUC score\n",
    "                        trueBeh = test_data[var].values\n",
    "                        try:\n",
    "                            auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                        except:\n",
    "                            auc = np.nan\n",
    "                        print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                        # put data in the summarizing data frame\n",
    "                        if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                            ROC_summary_all = ROC_summary_all.append({'train_animal':animal1_train,\n",
    "                                                                      'test_animal':animal1_test,\n",
    "                                                                      'train_dyadID':ianimalpair_train,\n",
    "                                                                      'test_dyadID':ianimalpair_test,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc,\n",
    "                                                                      'iters': iiter,\n",
    "                                                                     }, ignore_index=True)\n",
    "                        else:\n",
    "                            ROC_summary_all = ROC_summary_all.append({'train_animal':animal2_train_nooverlap,\n",
    "                                                                      'test_animal':animal2_test_nooverlap,\n",
    "                                                                      'train_dyadID':ianimalpair_train,\n",
    "                                                                      'test_dyadID':ianimalpair_test,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc,\n",
    "                                                                      'iters': iiter,\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "                    #         \n",
    "                    # Prediction for each behavioral events with swapped animal1 and animal2\n",
    "                    # With training set and testing set has swapped animal type:\n",
    "                    # training set: animal1 - sub; animal2 - dom\n",
    "                    # testing set: animal1 - dom; animal2 - sub\n",
    "                    test_data_swap = test_data.copy()\n",
    "                    # Create a column mapping\n",
    "                    new_columns = {}\n",
    "                    for col in test_data_swap.columns:\n",
    "                        if 'pull1' in col:\n",
    "                            new_columns[col] = col.replace('pull1', 'pull2')\n",
    "                        elif 'pull2' in col:\n",
    "                            new_columns[col] = col.replace('pull2', 'pull1')\n",
    "                        elif 'owgaze1' in col:\n",
    "                            new_columns[col] = col.replace('owgaze1', 'owgaze2')\n",
    "                        elif 'owgaze2' in col:\n",
    "                            new_columns[col] = col.replace('owgaze2', 'owgaze1')\n",
    "                    # Rename the columns using the mapping\n",
    "                    test_data_swap = test_data_swap.rename(columns=new_columns)\n",
    "\n",
    "                    for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                        var = effect_slice[ievent]\n",
    "                        Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                        for index, row in test_data_swap.iterrows():\n",
    "                            evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                        fromNodes[1]: row[fromNodes[1]], \n",
    "                                        fromNodes[2]: row[fromNodes[2]], \n",
    "                                        fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                            # Query the probability distribution for Pulls given evidence\n",
    "                            aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                            # Extract the probability of outcome = 1\n",
    "                            prob = aucPpredBehavior.values[1]\n",
    "                            Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                        # Calculate the AUC score\n",
    "                        trueBeh = test_data_swap[var].values\n",
    "                        try:\n",
    "                            auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                        except:\n",
    "                            auc = np.nan\n",
    "                        print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                        # put data in the summarizing data frame\n",
    "                        if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                            ROC_summary_all = ROC_summary_all.append({'train_animal':animal1_train,\n",
    "                                                                      'test_animal':animal2_test_nooverlap,\n",
    "                                                                      'train_dyadID':ianimalpair_train,\n",
    "                                                                      'test_dyadID':ianimalpair_test,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc,\n",
    "                                                                      'iters': iiter,\n",
    "                                                                     }, ignore_index=True)\n",
    "                        else:\n",
    "                            ROC_summary_all = ROC_summary_all.append({'train_animal':animal2_train_nooverlap,\n",
    "                                                                      'test_animal':animal1_test,\n",
    "                                                                      'train_dyadID':ianimalpair_train,\n",
    "                                                                      'test_dyadID':ianimalpair_test,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc,\n",
    "                                                                      'iters': iiter,\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "                \n",
    "\n",
    "   \n",
    "    # save the summarizing data ROC_summary_all\n",
    "    savedata = 0\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions_cross_dyad_validation/'+\\\n",
    "                                       savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_DBNdependenciesAfterMI_binary_noself_'+timelagtype+'.pkl', 'wb') as f:\n",
    "            pickle.dump(ROC_summary_all, f)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eed961",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    ind = (ROC_summary_all['action']=='pull') & (ROC_summary_all['testCondition']=='coop(1s)')\n",
    "    ROC_summary_all_tgt = ROC_summary_all[ind]\n",
    "\n",
    "    print(ROC_summary_all_tgt.keys())\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Pivot the DataFrame to have train_animal as rows and test_animal as columns\n",
    "    heatmap_data = ROC_summary_all_tgt.pivot(index='train_animal', columns='test_animal', values='predROC')\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='viridis', vmin=0.5, vmax=1.0)\n",
    "    plt.title('ROC AUC Heatmap')\n",
    "    plt.xlabel('Test Animal')\n",
    "    plt.ylabel('Train Animal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdcdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further analysis\n",
    "###\n",
    "# step 1: add dom/sub information\n",
    "###\n",
    "\n",
    "# Function to assign rank\n",
    "def get_rank(name):\n",
    "    return 'dom' if name in dom_animal_names else 'sub'\n",
    "\n",
    "# Apply to create new columns\n",
    "ROC_summary_all['train_rank'] = ROC_summary_all['train_animal'].apply(get_rank)\n",
    "ROC_summary_all['test_rank'] = ROC_summary_all['test_animal'].apply(get_rank)\n",
    "\n",
    "###\n",
    "# Step 2: Group by unique combination (excluding iteration) and get mean/std of predROC\n",
    "###\n",
    "# Group and compute mean and std\n",
    "ROC_summary_grouped = (\n",
    "    ROC_summary_all\n",
    "    .groupby(['train_animal', 'test_animal', 'action', 'testCondition'])\n",
    "    .agg(predROC_mean=('predROC', 'mean'),\n",
    "         predROC_std=('predROC', 'std'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Drop duplicates to preserve unique train/test animal rank combinations\n",
    "rank_info = ROC_summary_all[['train_animal', 'test_animal', 'train_rank', 'test_rank']].drop_duplicates()\n",
    "\n",
    "# Merge into grouped summary\n",
    "ROC_summary_grouped = ROC_summary_grouped.merge(rank_info, on=['train_animal', 'test_animal'], how='left')\n",
    "\n",
    "###\n",
    "# add statistic column\n",
    "###\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Step 1: collect all t-tests\n",
    "p_values = []\n",
    "group_keys = []\n",
    "for name, group in ROC_summary_all.groupby(['train_animal', 'test_animal', 'action', 'testCondition']):\n",
    "    t_stat, p = ttest_1samp(group['predROC'], 0.5)\n",
    "    p_values.append(p)\n",
    "    group_keys.append(name)\n",
    "\n",
    "# Step 2: correct p-values\n",
    "rejected, pvals_corrected, _, _ = multipletests(p_values, method='fdr_bh')  # or method='bonferroni'\n",
    "\n",
    "# Step 3: build a DataFrame\n",
    "significance_df = pd.DataFrame(group_keys, columns=['train_animal', 'test_animal', 'action', 'testCondition'])\n",
    "significance_df['p_value'] = p_values\n",
    "significance_df['p_value_corrected'] = pvals_corrected\n",
    "significance_df['significant_vs_0.5'] = rejected\n",
    "\n",
    "# Step 4: merge into ROC_summary_grouped\n",
    "ROC_summary_grouped = ROC_summary_grouped.merge(significance_df, \n",
    "                        on=['train_animal', 'test_animal', 'action', 'testCondition'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35953af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Step 3: plot - heatmap plot\n",
    "###\n",
    "if 1:\n",
    "    # Combine train and test animals with their dyad and rank info\n",
    "    # We'll use the original ROC_summary_all (before grouping) or ROC_summary_all_tgt if it has this info\n",
    "    # Here I assume 'train_animal', 'train_dyadID', 'train_rank' are available\n",
    "\n",
    "    # Extract unique animals with their dyad and rank info\n",
    "    train_info = ROC_summary_all[['train_animal', 'train_dyadID', 'train_rank']].drop_duplicates()\n",
    "    test_info = ROC_summary_all[['test_animal', 'test_dyadID', 'test_rank']].drop_duplicates()\n",
    "\n",
    "    # Rename columns to unify\n",
    "    train_info = train_info.rename(columns={'train_animal':'animal', 'train_dyadID':'dyadID', 'train_rank':'rank'})\n",
    "    test_info = test_info.rename(columns={'test_animal':'animal', 'test_dyadID':'dyadID', 'test_rank':'rank'})\n",
    "\n",
    "    # Combine and drop duplicates (some animals appear in both)\n",
    "    animal_info = pd.concat([train_info, test_info]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Sort by dyadID ascending, then rank (put 'dom' before 'sub')\n",
    "    animal_info['rank_order'] = animal_info['rank'].map({'dom': 0, 'sub': 1})\n",
    "    animal_info = animal_info.sort_values(by=['dyadID', 'rank_order'])\n",
    "\n",
    "    # Create the ordered list\n",
    "    ordered_animals = animal_info['animal'].tolist()\n",
    "\n",
    "    # choose the target entries\n",
    "    ind = (ROC_summary_grouped['action']=='pull') & (ROC_summary_grouped['testCondition']=='coop(1s)')\n",
    "    ROC_summary_all_tgt = ROC_summary_grouped[ind]\n",
    "\n",
    "    print(ROC_summary_all_tgt.keys())\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Pivot the DataFrame to have train_animal as rows and test_animal as columns\n",
    "    heatmap_data = ROC_summary_all_tgt.pivot(index='train_animal', columns='test_animal', values='predROC_mean')\n",
    "\n",
    "    heatmap_data = heatmap_data.reindex(index=ordered_animals, columns=ordered_animals)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='viridis', vmin=0.5, vmax=1.0)\n",
    "    plt.title('ROC AUC Heatmap')\n",
    "    plt.xlabel('Test Animal')\n",
    "    plt.ylabel('Train Animal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Step 3: plot - violin/bar plot\n",
    "###\n",
    "if 1:\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    from scipy.stats import f_oneway, ttest_ind\n",
    "\n",
    "\n",
    "    # --- Step 1: Define group column ---\n",
    "    ind_same = ROC_summary_all_tgt['train_animal']==ROC_summary_all_tgt['test_animal']\n",
    "    ROC_summary_all_tgt = ROC_summary_all_tgt[~ind_same]\n",
    "    \n",
    "    ROC_summary_all_tgt['train_test_rank'] = (\n",
    "        'train_' + ROC_summary_all_tgt['train_rank'] + '_test_' + ROC_summary_all_tgt['test_rank']\n",
    "    )\n",
    "\n",
    "    group_order = ['train_dom_test_dom', 'train_dom_test_sub', 'train_sub_test_dom', 'train_sub_test_sub']\n",
    "\n",
    "    # --- Step 2: Run Tukey HSD post hoc test ---\n",
    "    tukey = pairwise_tukeyhsd(\n",
    "        endog=ROC_summary_all_tgt['predROC_mean'],\n",
    "        groups=ROC_summary_all_tgt['train_test_rank'],\n",
    "        alpha=0.05\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    tukey_results = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
    "\n",
    "    # Keep only significant results (p < 0.05)\n",
    "    sig_results = tukey_results[tukey_results['p-adj'].astype(float) < 0.05]\n",
    "\n",
    "    # --- Step 3: Create violin plot ---\n",
    "    fig1 = plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=ROC_summary_all_tgt, x='train_test_rank', y='predROC_mean',\n",
    "                   order=group_order, palette='muted')\n",
    "\n",
    "    sns.stripplot(data=ROC_summary_all_tgt, x='train_test_rank', y='predROC_mean',\n",
    "                  order=group_order, color='black', size=2, alpha=0.5, jitter=True)\n",
    "\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.ylabel('Mean ROC AUC')\n",
    "    plt.xlabel('Train-Test Rank Group')\n",
    "    plt.title('Distribution of ROC AUC by Train/Test Rank Combination')\n",
    "\n",
    "    # --- Step 4: Annotate significant pairs ---\n",
    "    group_pos = {name: i for i, name in enumerate(group_order)}\n",
    "    ymax = ROC_summary_all_tgt['predROC_mean'].max()\n",
    "    line_offset = 0.01\n",
    "    line_height = 0.02\n",
    "    current_offset = 0\n",
    "\n",
    "    for _, row in sig_results.iterrows():\n",
    "        g1, g2 = row['group1'], row['group2']\n",
    "        p_val = float(row['p-adj'])\n",
    "\n",
    "        if g1 not in group_pos or g2 not in group_pos:\n",
    "            continue\n",
    "\n",
    "        x1, x2 = group_pos[g1], group_pos[g2]\n",
    "        y = ymax + line_offset + current_offset\n",
    "        plt.plot([x1, x1, x2, x2], [y, y + line_height, y + line_height, y], color='black')\n",
    "        plt.text((x1 + x2) / 2, y + line_height + 0.005, f'p = {p_val:.3f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "        current_offset += 0.04  # Stack annotations\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Plot 2: 2-group violin with t-test ===\n",
    "    def classify_same_vs_cross(row):\n",
    "        return 'same_rank' if row['train_rank'] == row['test_rank'] else 'cross_rank'\n",
    "\n",
    "    ROC_summary_all_tgt['rank_pair_type'] = ROC_summary_all_tgt.apply(classify_same_vs_cross, axis=1)\n",
    "\n",
    "    fig2 = plt.figure(figsize=(6, 6))\n",
    "    sns.violinplot(data=ROC_summary_all_tgt, x='rank_pair_type', y='predROC_mean', palette='pastel')\n",
    "    sns.stripplot(data=ROC_summary_all_tgt, x='rank_pair_type', y='predROC_mean',\n",
    "                  color='black', size=2, alpha=0.5, jitter=True)\n",
    "\n",
    "    # Stats: t-test\n",
    "    same_vals = ROC_summary_all_tgt[ROC_summary_all_tgt['rank_pair_type'] == 'same_rank']['predROC_mean']\n",
    "    cross_vals = ROC_summary_all_tgt[ROC_summary_all_tgt['rank_pair_type'] == 'cross_rank']['predROC_mean']\n",
    "    t_stat, p_ttest = ttest_ind(same_vals, cross_vals)\n",
    "\n",
    "    # Annotate t-test result\n",
    "    if p_ttest < 0.5:\n",
    "        stars = '***' if p_ttest < 0.001 else '**' if p_ttest < 0.01 else '*'\n",
    "        ymax = ROC_summary_all_tgt['predROC_mean'].max()\n",
    "        plt.text(0.5, ymax + 0.01, f'p = {p_ttest:.3e} {stars}', ha='center', fontsize=12)\n",
    "\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.ylabel('Mean ROC AUC')\n",
    "    plt.xlabel('Train-Test Rank Type')\n",
    "    plt.title('ROC AUC: Same vs Cross Rank')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions_cross_dyad_validation/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig2.savefig(figsavefolder+'withinCondition_Acrossdyads_SameOrAcrossRanks_DBNdependenciesAfterMI_binary_noself_summarizingplot.pdf')\n",
    "        \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROC_summary_all_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a82c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4512a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b3302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e43803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d1e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DLC]",
   "language": "python",
   "name": "conda-env-.conda-DLC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
