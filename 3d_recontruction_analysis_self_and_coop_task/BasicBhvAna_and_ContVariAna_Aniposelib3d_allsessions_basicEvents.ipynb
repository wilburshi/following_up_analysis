{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea1560b",
   "metadata": {},
   "source": [
    "### This script runs some basic bhv analysis, and detailed analysis focus on the continuous behavioral variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d0681",
   "metadata": {},
   "source": [
    "#### The output of this script will also be used by the DBN scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 1*30\n",
    "nframes = 1\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 1\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20220909\",\"20220912\",\"20220915\",\"20220920\",\"20220922\",\"20220923\",\"20221010\",\n",
    "                      \"20221011\",\"20221013\",\"20221014\",\"20221015\",\"20221017\",\"20230215\",     \n",
    "                      \"20221018\",\"20221019\",\"20221020\",\"20221021\",\"20221022\",\"20221026\",\"20221028\",\"20221030\",\n",
    "                      \"20221107\",\"20221108\",\"20221109\",\"20221110\",\"20221111\",\"20221114\",\"20221115\",\"20221116\",\n",
    "                      \"20221117\",\"20221118\",\"20221121\",\"20221122\",\"20221123\",\"20221125\",\"20221128\",\"20221129\",              \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221212\",\"20221214\",\"20221216\",\"20221219\",\"20221220\",\n",
    "                      \"20221221\",\"20230208\",\"20230209\",\"20230213\",\"20230214\",\"20230111\",\"20230112\",\"20230201\",\n",
    "\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 6.50, 18.10, 0,      33.03, 549.0, 116.80, 6.50,\n",
    "                                 2.80, 27.80, 272.50, 27.90, 27.00,  33.00,\n",
    "                                28.70, 45.30, 21.10,  27.10, 51.90,  21.00, 30.80, 17.50,                      \n",
    "                                15.70,  2.65, 27.30,   0.00,  0.00,  71.80,  0.00,  0.00, \n",
    "                                75.50, 20.20,  0.00,  24.20, 36.70,  26.40, 22.50, 28.50,                       \n",
    "                                 0.00,  0.00, 21.70,  84.70, 17.00,  19.80, 23.50, 25.20,  \n",
    "                                 0.00,  0.00,  0.00,   0.00,  0.00, 130.00, 14.20, 24.20, \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20220912\",\"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                      \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                      \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                      \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                      \"20230117\",\"20230118\",\"20230124\",\"20230126\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                18.10,  0.00, 33.03,  6.50,  0.00, \n",
    "                                 2.80, 27.80, 27.90, 27.00,  \n",
    "                                51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                 0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                 0.00,  0.00,  0.00,  0.00,  \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# eddie sparkle\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20221122\",\"20221125\",\"20221128\",\"20221129\",\"20221130\",\"20221202\",\"20221206\",\n",
    "                      \"20221207\",\"20221208\",\"20221209\",\"20230126\",\"20230127\",\"20230130\",\"20230201\",\"20230203-1\",\n",
    "                      \"20230206\",\"20230207\",\"20230208-1\",\"20230209\",\"20230222\",\"20230223-1\",\"20230227-1\",\n",
    "                      \"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230321\",\"20230322\",\"20230324\",\"20230327\",\"20230328\",\n",
    "                      \"20230330\",\"20230331\",\"20230403\",\"20230404\",\"20230405\",\"20230406\",\"20230407\",\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 8.00,38.00,1.00,3.00,5.00,9.50,1.00,\n",
    "                                 4.50,4.50,5.00,38.00,166.00,4.20,3.80,3.60,\n",
    "                                 7.50,9.00,7.50,8.50,14.50,7.80,8.00,7.50,\n",
    "                                 8.00,8.00,4.00,123.00,14.00,8.80,\n",
    "                                 7.00,7.50,5.50,11.00,9.00,\n",
    "                                 17.00,4.50,9.30,25.50,20.40,21.30,24.80,\n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20221122\",  \"20221125\",  \n",
    "                      \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                      \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                      \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                      \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                      \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                  8.00,  38.00, \n",
    "                                  9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                  9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                  8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                  7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                  4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "# ginger kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230209\",\"20230213\",\"20230214\",\"20230216\",\"20230222\",\"20230223\",\"20230228\",\"20230302\",\n",
    "                      \"20230303\",\"20230307\",\"20230314\",\"20230315\",\"20230316\",\"20230317\"         \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00,  0.00, 48.00, 26.20, 18.00, 23.00, 28.50,\n",
    "                                34.00, 25.50, 25.50, 31.50, 28.00, 30.50\n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      # \"20230213\", # camera2 is set up wrong\n",
    "                      \"20230214\",\"20230216\",\n",
    "                      \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                      \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                      \"20230323\",\"20230412\",\"20230413\",\"20230517\",\"20230614\",\"20230615\",\n",
    "                      \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                               #  0.00, # camera2 is set up wrong\n",
    "                                 0.00, 48.00, \n",
    "                                23.00, 28.50, 34.00, 25.50, \n",
    "                                25.50, 31.50, 28.00, 30.50,\n",
    "                                33.50, 22.20, 50.00,  0.00, \n",
    "                                33.00, 18.20, 22.80, 31.00, 24.00, 21.00,\n",
    "                                 0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "      \n",
    "# a test case\n",
    "if 0:\n",
    "    dates_list = [\"20221128\"]\n",
    "    session_start_times = [1.00] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_succtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_errtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# where to save the demo video\n",
    "withboxCorner = 1\n",
    "video_file_dir = data_saved_folder+'/example_videos_Anipose_bhv_demo/'+animal1_filename+'_'+animal2_filename\n",
    "if not os.path.exists(video_file_dir):\n",
    "    os.makedirs(video_file_dir)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "# NOTE: THIS STEP will save the data to the combinedsession_Anipose folder, since they are the same\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    #dummy\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_succtrials_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_errtrials_all_dates = pickle.load(f) \n",
    "    \n",
    "\n",
    "        \n",
    "except:\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        for imergepair in np.arange(0,np.shape(merge_campairs)[0],1):\n",
    "            \n",
    "            # should be only one merge type - \"Anipose\"\n",
    "            merge_campair = merge_campairs[imergepair]\n",
    "\n",
    "            # load camera tracking results\n",
    "            try:\n",
    "                # dummy\n",
    "                if reanalyze_video:\n",
    "                    print(\"re-analyze the data \",date_tgt)\n",
    "                    dummy\n",
    "                ## read\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                    body_part_locs_Anipose = pickle.load(f)                 \n",
    "            except:\n",
    "                print(\"did not save data for Anipose - body part tracking \"+date_tgt)\n",
    "                # analyze and save\n",
    "                Anipose_h5_file = Anipose_analyzed_path +date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "                Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "                body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                    pickle.dump(body_part_locs_Anipose, f)            \n",
    "            \n",
    "            min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "                    \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "            tasktypes_all_dates[idate] = tasktype\n",
    "            coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "            # successful trial or not\n",
    "            succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "            succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # analyze behavior results\n",
    "            # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "            succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "\n",
    "            trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "            #\n",
    "            pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<10]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "            # load behavioral event results\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                    output_key_locations = pickle.load(f)\n",
    "            except:\n",
    "                print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "                output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "               \n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_key_locations, f)\n",
    "                \n",
    "             \n",
    "            look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "            look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "            look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "            look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "            look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "            look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                               \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                               \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "            timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "            timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "            timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "                \n",
    "            # # plot behavioral events\n",
    "            if 0:\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    plot_bhv_events_levertube(date_tgt+merge_campair,animal1, animal2, session_start_time, totalsess_time, \n",
    "                                              time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2,\n",
    "                                              timepoint_lever1,timepoint_lever2,timepoint_tube1,timepoint_tube2)\n",
    "                else:\n",
    "                    plot_bhv_events_levertube(date_tgt+merge_campair,animal2, animal1, session_start_time, totalsess_time, \n",
    "                                              time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1,\n",
    "                                              timepoint_lever2,timepoint_lever1,timepoint_tube2,timepoint_tube1)\n",
    "            #\n",
    "            # save behavioral events plot\n",
    "            if 0:\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                plt.savefig(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/'+date_tgt+\"_Anipose.pdf\")\n",
    "  \n",
    "            #\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "\n",
    "            \n",
    "            # plot key continuous behavioral variables\n",
    "            if 1:\n",
    "                filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+date_tgt+'/'\n",
    "                if not os.path.exists(filepath_cont_var):\n",
    "                    os.makedirs(filepath_cont_var)\n",
    "                \n",
    "                savefig = 1\n",
    "                pull_trig_events_summary, pull_trig_events_succtrial_summary, pull_trig_events_errtrial_summary = plot_continuous_bhv_var(filepath_cont_var+date_tgt+merge_campair,savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length,succpulls_ornot,\n",
    "                                        time_point_pull1, time_point_pull2,animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                pull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "                pull_trig_events_succtrials_all_dates[date_tgt] = pull_trig_events_succtrial_summary\n",
    "                pull_trig_events_errtrials_all_dates[date_tgt] = pull_trig_events_errtrial_summary\n",
    "                    \n",
    "                \n",
    "            # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "            # could be used for define time bin for DBN\n",
    "            if 1:\n",
    "                _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                             oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #\n",
    "                pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "                bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                                'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        \n",
    "            \n",
    "            # plot the tracking demo video\n",
    "            if 1:      \n",
    "                video_file = video_file_dir+'/'+date_tgt+'_'+animal1_filename+'_'+animal2_filename+'_anipose_bhv_demo.mp4'\n",
    "                tracking_video_Anipose_events_demo(body_part_locs_Anipose,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                   time_point_pull1,time_point_pull2,animalnames_videotrack,bodypartnames_videotrack,\n",
    "                                                   date_tgt,animal1_filename,animal2_filename,animal1,animal2,\n",
    "                                                   session_start_time,fps,nframes,video_file,withboxCorner)\n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_succtrials_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_errtrials_all_dates, f)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158391c",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9be41",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0aaea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-5,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "    \n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35cadc",
   "metadata": {},
   "source": [
    "### plot some other basis behavioral measures\n",
    "#### successful rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29625a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),succ_rate_all_dates[sorting_df.index],'o',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"successful rate\",fontsize=13)\n",
    "ax1.set_ylim([-0.1,1.1])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"successfulrate_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ad0c9",
   "metadata": {},
   "source": [
    "#### animal pull numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pullmean_num_all_dates = (pull1_num_all_dates+pull2_num_all_dates)/2\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull1_num_all_dates[sorting_df.index],'bv',markersize=5,label='animal1 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pull2_num_all_dates[sorting_df.index],'rv',markersize=5,label='animal2 pull #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),pullmean_num_all_dates[sorting_df.index],'kv',markersize=8,label='mean pull #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"pull numbers\",fontsize=13)\n",
    "ax1.set_ylim([-20,240])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,240],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pullnumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e48e54",
   "metadata": {},
   "source": [
    "#### gaze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze1_num_all_dates = owgaze1_num_all_dates + mtgaze1_num_all_dates\n",
    "gaze2_num_all_dates = owgaze2_num_all_dates + mtgaze2_num_all_dates\n",
    "gazemean_num_all_dates = (gaze1_num_all_dates+gaze2_num_all_dates)/2\n",
    "\n",
    "print(np.nanmax(gaze1_num_all_dates))\n",
    "print(np.nanmax(gaze2_num_all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze1_num_all_dates[sorting_df.index],'b^',markersize=5,label='animal1 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gaze2_num_all_dates[sorting_df.index],'r^',markersize=5,label='animal2 gaze #')\n",
    "ax1.plot(np.arange(0,ndates_sorted,1),gazemean_num_all_dates[sorting_df.index],'k^',markersize=8,label='mean gaze #')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#\n",
    "ax1.set_ylabel(\"social gaze number\",fontsize=13)\n",
    "ax1.set_ylim([-20,1500])\n",
    "ax1.set_xlim([-0.5,ndates_sorted-0.5])\n",
    "\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-20,1500],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-10,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_numbers = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/30\n",
    "gaze_pull_ratios = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/(pull1_num_all_dates+pull2_num_all_dates)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "grouptypes = ['self reward','cooperative','no-vision']\n",
    "\n",
    "gaze_numbers_groups = [np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                      # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==3)[0]])[0],\n",
    "                      # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==2)[0]])[0],\n",
    "                      # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1.5)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(gaze_numbers_groups,whis=1.5, meanline=True)\n",
    "\n",
    "plt.xticks(np.arange(1, len(grouptypes)+1, 1), grouptypes, fontsize = 14);\n",
    "ax1.set_ylim([240/30,2100/30])\n",
    "ax1.set_ylabel(\"average social gaze time (s)\",fontsize=14)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b59374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "grouptypes = ['self reward','cooperative','no-vision']\n",
    "\n",
    "BhvIntv_groups = [pull_other_intv_mean[np.where(coopthres_forsort==100)[0]],\n",
    "                  # pull_other_intv_mean[np.where(coopthres_forsort==3)[0]],\n",
    "                  # pull_other_intv_mean[np.where(coopthres_forsort==2)[0]],\n",
    "                  # pull_other_intv_mean[np.where(coopthres_forsort==1.5)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==1)[0]],\n",
    "                  pull_other_intv_mean[np.where(coopthres_forsort==-1)[0]]\n",
    "                 ]\n",
    "\n",
    "#BhvIntv_groups = [np.array(pd.DataFrame.stack(pull_other_intv_forplots[np.where(np.isin(\n",
    "#                  dates_list_sorted,dates_list_sorted[np.where(coopthres_forsort==100)[0]]))[0]])),\n",
    "#                  np.array(pd.DataFrame.stack(pull_other_intv_forplots[np.where(np.isin(\n",
    "#                  dates_list_sorted,dates_list_sorted[np.where(coopthres_forsort==1)[0]]))[0]])),\n",
    "#                  np.array(pd.DataFrame.stack(pull_other_intv_forplots[np.where(np.isin(\n",
    "#                  dates_list_sorted,dates_list_sorted[np.where(coopthres_forsort==-1)[0]]))[0]])),\n",
    "#                 ]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(BhvIntv_groups,whis=3, meanline=True)\n",
    "\n",
    "plt.xticks(np.arange(1, len(grouptypes)+1, 1), grouptypes, fontsize = 14);\n",
    "#ax1.set_ylim([250,2000])\n",
    "ax1.set_ylabel(\"average behavioral event intervals (s)\",fontsize=14)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_bhvIntv_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f202d5",
   "metadata": {},
   "source": [
    "### plot pull triggered event related plot\n",
    "#### plot averaged trace for all pulls, seprating into different training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "varis_ylims = [[0,np.pi],[0,np.pi],[0,20],[0,1]]\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,ngroups)\n",
    "fig.set_figheight(10*nvaris*2)\n",
    "fig.set_figwidth(10*ngroups)\n",
    "\n",
    "# set the x axis info, make sure they are the same as in plot_continuous_bhv_var.py\n",
    "trig_twin = [-6,6] #s\n",
    "xxx_trigevent = np.arange(trig_twin[0],trig_twin[1],1/fps) \n",
    "\n",
    "for igroup in np.arange(0,ngroups,1):\n",
    "    \n",
    "    igroup_typename = group_typenames[igroup] \n",
    "    igroup_typeID =  group_typeIDs[igroup] \n",
    "    igroup_cothres = group_coopthres[igroup]\n",
    "    igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "    igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "       \n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "        tracecolor = varis_tracecolor[ivari]\n",
    "\n",
    "        pull1_trig_events_toplot = []\n",
    "        pull2_trig_events_toplot = []\n",
    "\n",
    "        for date_tgt in igroup_dates_tgt:\n",
    "\n",
    "            # get the mean trigger events for all pulls - animal 1\n",
    "            pull1_trig_events_toplot = np.nanmean(pull_trig_events_all_dates[date_tgt][(animal1_toplot,vari_toplot)],axis=0)\n",
    "            axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "            #\n",
    "            axs[ivari,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari,igroup].set_xticks([])\n",
    "            axs[ivari,igroup].set_xticklabels([])\n",
    "            axs[ivari,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari,igroup].set_title(animal1_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari,igroup].set_yticks([])\n",
    "                axs[ivari,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "                \n",
    "        \n",
    "            # get the mean trigger events for all pulls - animal 2\n",
    "            pull2_trig_events_toplot = np.nanmean(pull_trig_events_all_dates[date_tgt][(animal2_toplot,vari_toplot)],axis=0)\n",
    "            axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "            #\n",
    "            axs[ivari+nvaris,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari+nvaris,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari+nvaris,igroup].set_xticks([])\n",
    "            axs[ivari+nvaris,igroup].set_xticklabels([])\n",
    "            axs[ivari+nvaris,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari+nvaris,igroup].set_title(animal2_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari+nvaris,igroup].set_yticks([])\n",
    "                axs[ivari+nvaris,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari+nvaris,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "            \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averagePullTrigEventTraces_summary_allpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5730f",
   "metadata": {},
   "source": [
    "#### plot averaged trace for SUCCESSFUL pulls, seprating into different training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "varis_ylims = [[0,np.pi],[0,np.pi],[0,20],[0,1]]\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,ngroups)\n",
    "fig.set_figheight(10*nvaris*2)\n",
    "fig.set_figwidth(10*ngroups)\n",
    "\n",
    "# set the x axis info, make sure they are the same as in plot_continuous_bhv_var.py\n",
    "trig_twin = [-6,6] #s\n",
    "xxx_trigevent = np.arange(trig_twin[0],trig_twin[1],1/fps) \n",
    "\n",
    "for igroup in np.arange(0,ngroups,1):\n",
    "    \n",
    "    igroup_typename = group_typenames[igroup] \n",
    "    igroup_typeID =  group_typeIDs[igroup] \n",
    "    igroup_cothres = group_coopthres[igroup]\n",
    "    igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "    igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "       \n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "        tracecolor = varis_tracecolor[ivari]\n",
    "\n",
    "        pull1_trig_events_toplot = []\n",
    "        pull2_trig_events_toplot = []\n",
    "\n",
    "        for date_tgt in igroup_dates_tgt:\n",
    "\n",
    "            # get the mean trigger events for all pulls - animal 1\n",
    "            try:\n",
    "                pull1_trig_events_toplot = np.nanmean(pull_trig_events_succtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull1_trig_events_toplot = pull_trig_events_succtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "                    axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "        \n",
    "            axs[ivari,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari,igroup].set_xticks([])\n",
    "            axs[ivari,igroup].set_xticklabels([])\n",
    "            axs[ivari,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari,igroup].set_title(animal1_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari,igroup].set_yticks([])\n",
    "                axs[ivari,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "                \n",
    "        \n",
    "            # get the mean trigger events for all pulls - animal 2\n",
    "            try:\n",
    "                pull2_trig_events_toplot = np.nanmean(pull_trig_events_succtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull2_trig_events_toplot = pull_trig_events_succtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "                    axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "            axs[ivari+nvaris,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari+nvaris,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari+nvaris,igroup].set_xticks([])\n",
    "            axs[ivari+nvaris,igroup].set_xticklabels([])\n",
    "            axs[ivari+nvaris,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari+nvaris,igroup].set_title(animal2_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari+nvaris,igroup].set_yticks([])\n",
    "                axs[ivari+nvaris,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari+nvaris,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "            \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averagePullTrigEventTraces_summary_succpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367a0c1",
   "metadata": {},
   "source": [
    "#### plot averaged trace for FAILED pulls, seprating into different training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "varis_ylims = [[0,np.pi],[0,np.pi],[0,20],[0,1]]\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,ngroups)\n",
    "fig.set_figheight(10*nvaris*2)\n",
    "fig.set_figwidth(10*ngroups)\n",
    "\n",
    "# set the x axis info, make sure they are the same as in plot_continuous_bhv_var.py\n",
    "trig_twin = [-6,6] #s\n",
    "xxx_trigevent = np.arange(trig_twin[0],trig_twin[1],1/fps) \n",
    "\n",
    "for igroup in np.arange(0,ngroups,1):\n",
    "    \n",
    "    igroup_typename = group_typenames[igroup] \n",
    "    igroup_typeID =  group_typeIDs[igroup] \n",
    "    igroup_cothres = group_coopthres[igroup]\n",
    "    igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "    igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "       \n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        vari_toplot = varis_toplot[ivari]\n",
    "        vari_ylabel = varis_ylabel[ivari]\n",
    "        tracecolor = varis_tracecolor[ivari]\n",
    "\n",
    "        pull1_trig_events_toplot = []\n",
    "        pull2_trig_events_toplot = []\n",
    "\n",
    "        for date_tgt in igroup_dates_tgt:\n",
    "\n",
    "            # get the mean trigger events for all pulls - animal 1\n",
    "            try:\n",
    "                pull1_trig_events_toplot = np.nanmean(pull_trig_events_errtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull1_trig_events_toplot = pull_trig_events_errtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "                    axs[ivari,igroup].plot(xxx_trigevent,pull1_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "        \n",
    "            axs[ivari,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari,igroup].set_xticks([])\n",
    "            axs[ivari,igroup].set_xticklabels([])\n",
    "            axs[ivari,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari,igroup].set_title(animal1_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari,igroup].set_yticks([])\n",
    "                axs[ivari,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "                \n",
    "        \n",
    "            # get the mean trigger events for all pulls - animal 2\n",
    "            try:\n",
    "                pull2_trig_events_toplot = np.nanmean(pull_trig_events_errtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)],axis=0)\n",
    "                axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "            except:\n",
    "                try:\n",
    "                    pull2_trig_events_toplot = pull_trig_events_errtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "                    axs[ivari+nvaris,igroup].plot(xxx_trigevent,pull2_trig_events_toplot,color=tracecolor)\n",
    "                except:\n",
    "                    pass\n",
    "            #\n",
    "            axs[ivari+nvaris,igroup].set_xlim(trig_twin)\n",
    "            axs[ivari+nvaris,igroup].set_ylim(varis_ylims[ivari])\n",
    "            axs[ivari+nvaris,igroup].set_xticks([])\n",
    "            axs[ivari+nvaris,igroup].set_xticklabels([])\n",
    "            axs[ivari+nvaris,igroup].plot([0,0],varis_ylims[ivari],'k--')\n",
    "            axs[ivari+nvaris,igroup].set_title(animal2_toplot+' '+igroup_typename,fontsize = 20)\n",
    "            if igroup != 0:\n",
    "                axs[ivari+nvaris,igroup].set_yticks([])\n",
    "                axs[ivari+nvaris,igroup].set_yticklabels([])\n",
    "            elif igroup == 0:\n",
    "                axs[ivari+nvaris,igroup].set_ylabel('average '+ vari_toplot+' ('+vari_ylabel+')',fontsize=18)\n",
    "            \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averagePullTrigEventTraces_summary_failedpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70ddf0",
   "metadata": {},
   "source": [
    "#### plot the correlation among pull trigger traces - all pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,1)\n",
    "fig.set_figheight(5*nvaris*2)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "\n",
    "# \n",
    "tracecorr1_allpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "tracecorr2_allpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "\n",
    "for ivari in np.arange(0,nvaris,1):\n",
    "    \n",
    "    vari_toplot = varis_toplot[ivari]\n",
    "    vari_ylabel = varis_ylabel[ivari]\n",
    "\n",
    "    # step 1 - prepare the data\n",
    "    \n",
    "    tracecorr1_all_dates = dict.fromkeys(dates_list,[])\n",
    "    tracecorr2_all_dates = dict.fromkeys(dates_list,[])\n",
    "    tracecorr1_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "    tracecorr2_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "\n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        \n",
    "        date_tgt = dates_list_sorted[idate]\n",
    "        \n",
    "        # \n",
    "        # animal 1\n",
    "        pull1_trig_events_idate = pull_trig_events_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "        ntrigs = np.shape(pull1_trig_events_idate)[0]\n",
    "        #\n",
    "        pull1_trig_events_corr_idates = []\n",
    "        #\n",
    "        for itrig in np.arange(0,ntrigs-1,1):\n",
    "            for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                # corr_tracepair = np.corrcoef(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig])[0,1]\n",
    "                corr_tracepair,pp = scipy.stats.spearmanr(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                if pp>0.05:\n",
    "                    corr_tracepair = np.nan\n",
    "                corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                # \n",
    "                if (itrig==0) & (jtrig==0):\n",
    "                    pull1_trig_events_corr_idates = corr_tracepair\n",
    "                else:\n",
    "                    pull1_trig_events_corr_idates = np.append(pull1_trig_events_corr_idates,corr_tracepair)\n",
    "        #\n",
    "        tracecorr1_all_dates[date_tgt]=pd.Series(pull1_trig_events_corr_idates)\n",
    "        tracecorr1_mean_all_dates[idate] = np.nanmean(pull1_trig_events_corr_idates)\n",
    "        \n",
    "        # animal 2\n",
    "        pull2_trig_events_idate = pull_trig_events_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "        ntrigs = np.shape(pull2_trig_events_idate)[0]\n",
    "        #\n",
    "        pull2_trig_events_corr_idates = []\n",
    "        #\n",
    "        for itrig in np.arange(0,ntrigs-1,1):\n",
    "            for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                # corr_tracepair = np.corrcoef(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig])[0,1]\n",
    "                corr_tracepair,pp = scipy.stats.spearmanr(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                if pp>0.05:\n",
    "                    corr_tracepair = np.nan\n",
    "                corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                # \n",
    "                if (itrig==0) & (jtrig==0):\n",
    "                    pull2_trig_events_corr_idates = corr_tracepair\n",
    "                else:\n",
    "                    pull2_trig_events_corr_idates = np.append(pull2_trig_events_corr_idates,corr_tracepair)\n",
    "        #\n",
    "        tracecorr2_all_dates[date_tgt]=pd.Series(pull2_trig_events_corr_idates)\n",
    "        tracecorr2_mean_all_dates[idate] = np.nanmean(pull2_trig_events_corr_idates)     \n",
    "    # \n",
    "    tracecorr1_allpull_trigevent_sum[vari_toplot] = tracecorr1_all_dates\n",
    "    tracecorr2_allpull_trigevent_sum[vari_toplot] = tracecorr2_all_dates\n",
    "\n",
    "    # step 2 - plot\n",
    "    # animal 1\n",
    "    #\n",
    "    tracecorr1_all_dates = pd.DataFrame(tracecorr1_all_dates)\n",
    "    #\n",
    "    tracecorr1_all_dates.plot(kind = 'box',ax=axs[ivari*2], positions=np.arange(0,ndates_sorted,1))\n",
    "    axs[ivari*2].plot(np.arange(0,ndates_sorted,1),tracecorr1_mean_all_dates,'r*',markersize=10)\n",
    "    #\n",
    "    axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2].set_ylim([-0.2,1.2])\n",
    "    #\n",
    "    axs[ivari*2].set_xticklabels([])\n",
    "    #\n",
    "    axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "    #\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "        \n",
    "    # animal 2\n",
    "    #\n",
    "    tracecorr2_all_dates = pd.DataFrame(tracecorr2_all_dates)\n",
    "    #\n",
    "    tracecorr2_all_dates.plot(kind = 'box',ax=axs[ivari*2+1], positions=np.arange(0,ndates_sorted,1))\n",
    "    axs[ivari*2+1].plot(np.arange(0,ndates_sorted,1),tracecorr2_mean_all_dates,'r*',markersize=10)\n",
    "    #\n",
    "    axs[ivari*2+1].set_ylabel(\"paiwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "    #\n",
    "    if ivari == nvaris-1:\n",
    "        axs[ivari*2+1].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2+1].set_xticklabels(dates_list_sorted,rotation=90, fontsize=10)\n",
    "    else:\n",
    "        axs[ivari*2+1].set_xticklabels([])\n",
    "    #\n",
    "    axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "    #\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2+1].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2+1].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_allpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c803b4e",
   "metadata": {},
   "source": [
    "#### plot the correlation among pull trigger traces - successful pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,1)\n",
    "fig.set_figheight(5*nvaris*2)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "\n",
    "# \n",
    "tracecorr1_succpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "tracecorr2_succpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "\n",
    "for ivari in np.arange(0,nvaris,1):\n",
    "    \n",
    "    vari_toplot = varis_toplot[ivari]\n",
    "    vari_ylabel = varis_ylabel[ivari]\n",
    "\n",
    "    # step 1 - prepare the data\n",
    "    \n",
    "    tracecorr1_all_dates = dict.fromkeys(dates_list,[])\n",
    "    tracecorr2_all_dates = dict.fromkeys(dates_list,[])\n",
    "    tracecorr1_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "    tracecorr2_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "\n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        \n",
    "        date_tgt = dates_list_sorted[idate]\n",
    "        \n",
    "        # \n",
    "        # animal 1\n",
    "        pull1_trig_events_idate = pull_trig_events_succtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "        ntrigs = np.shape(pull1_trig_events_idate)[0]\n",
    "        #\n",
    "        pull1_trig_events_corr_idates = []\n",
    "        #\n",
    "        for itrig in np.arange(0,ntrigs-1,1):\n",
    "            for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                # corr_tracepair = np.corrcoef(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig])[0,1]\n",
    "                corr_tracepair,pp = scipy.stats.spearmanr(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                if pp>0.05:\n",
    "                    corr_tracepair = np.nan\n",
    "                corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                # \n",
    "                if (itrig==0) & (jtrig==0):\n",
    "                    pull1_trig_events_corr_idates = corr_tracepair\n",
    "                else:\n",
    "                    pull1_trig_events_corr_idates = np.append(pull1_trig_events_corr_idates,corr_tracepair)\n",
    "        #\n",
    "        tracecorr1_all_dates[date_tgt]=pd.Series(pull1_trig_events_corr_idates)\n",
    "        tracecorr1_mean_all_dates[idate] = np.nanmean(pull1_trig_events_corr_idates)\n",
    "        \n",
    "        # animal 2\n",
    "        pull2_trig_events_idate = pull_trig_events_succtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "        ntrigs = np.shape(pull2_trig_events_idate)[0]\n",
    "        #\n",
    "        pull2_trig_events_corr_idates = []\n",
    "        #\n",
    "        for itrig in np.arange(0,ntrigs-1,1):\n",
    "            for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                # corr_tracepair = np.corrcoef(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig])[0,1]\n",
    "                corr_tracepair,pp = scipy.stats.spearmanr(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                if pp>0.05:\n",
    "                    corr_tracepair = np.nan\n",
    "                corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                # \n",
    "                if (itrig==0) & (jtrig==0):\n",
    "                    pull2_trig_events_corr_idates = corr_tracepair\n",
    "                else:\n",
    "                    pull2_trig_events_corr_idates = np.append(pull2_trig_events_corr_idates,corr_tracepair)\n",
    "        #\n",
    "        tracecorr2_all_dates[date_tgt]=pd.Series(pull2_trig_events_corr_idates)\n",
    "        tracecorr2_mean_all_dates[idate] = np.nanmean(pull2_trig_events_corr_idates)\n",
    "    # \n",
    "    tracecorr1_succpull_trigevent_sum[vari_toplot] = tracecorr1_all_dates\n",
    "    tracecorr2_succpull_trigevent_sum[vari_toplot] = tracecorr2_all_dates              \n",
    "    \n",
    "    # step 2 - plot\n",
    "    # animal 1\n",
    "    #\n",
    "    tracecorr1_all_dates = pd.DataFrame(tracecorr1_all_dates)\n",
    "    #\n",
    "    tracecorr1_all_dates.plot(kind = 'box',ax=axs[ivari*2], positions=np.arange(0,ndates_sorted,1))\n",
    "    axs[ivari*2].plot(np.arange(0,ndates_sorted,1),tracecorr1_mean_all_dates,'r*',markersize=10)\n",
    "    #\n",
    "    axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2].set_ylim([-0.2,1.2])\n",
    "    #\n",
    "    axs[ivari*2].set_xticklabels([])\n",
    "    #\n",
    "    axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "    #\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "        \n",
    "    # animal 2\n",
    "    #\n",
    "    tracecorr2_all_dates = pd.DataFrame(tracecorr2_all_dates)\n",
    "    #\n",
    "    tracecorr2_all_dates.plot(kind = 'box',ax=axs[ivari*2+1], positions=np.arange(0,ndates_sorted,1))\n",
    "    axs[ivari*2+1].plot(np.arange(0,ndates_sorted,1),tracecorr2_mean_all_dates,'r*',markersize=10)\n",
    "    #\n",
    "    axs[ivari*2+1].set_ylabel(\"paiwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "    #\n",
    "    if ivari == nvaris-1:\n",
    "        axs[ivari*2+1].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2+1].set_xticklabels(dates_list_sorted,rotation=90, fontsize=10)\n",
    "    else:\n",
    "        axs[ivari*2+1].set_xticklabels([])\n",
    "    #\n",
    "    axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "    #\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2+1].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2+1].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_succpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0ab79",
   "metadata": {},
   "source": [
    "#### plot the correlation among pull trigger traces - failed pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "varis_ylabel = ['degree','degree','pixel/s','degree/s']\n",
    "varis_tracecolor = ['r','#458B74','#FFC710','#FF1493'] \n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,1)\n",
    "fig.set_figheight(5*nvaris*2)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "\n",
    "# \n",
    "tracecorr1_errpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "tracecorr2_errpull_trigevent_sum = dict.fromkeys((varis_toplot),[])\n",
    "\n",
    "for ivari in np.arange(0,nvaris,1):\n",
    "    \n",
    "    vari_toplot = varis_toplot[ivari]\n",
    "    vari_ylabel = varis_ylabel[ivari]\n",
    "\n",
    "    # step 1 - prepare the data\n",
    "    \n",
    "    tracecorr1_all_dates = dict.fromkeys(dates_list,[])\n",
    "    tracecorr2_all_dates = dict.fromkeys(dates_list,[])\n",
    "    tracecorr1_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "    tracecorr2_mean_all_dates = np.zeros((1,ndates_sorted))[0]\n",
    "\n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        \n",
    "        date_tgt = dates_list_sorted[idate]\n",
    "        \n",
    "        # \n",
    "        # animal 1\n",
    "        pull1_trig_events_idate = pull_trig_events_errtrials_all_dates[date_tgt][(animal1_toplot,vari_toplot)]\n",
    "        ntrigs = np.shape(pull1_trig_events_idate)[0]\n",
    "        #\n",
    "        pull1_trig_events_corr_idates = []\n",
    "        #\n",
    "        for itrig in np.arange(0,ntrigs-1,1):\n",
    "            for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                # corr_tracepair = np.corrcoef(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig])[0,1]\n",
    "                corr_tracepair,pp = scipy.stats.spearmanr(pull1_trig_events_idate[itrig],pull1_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                if pp>0.05:\n",
    "                    corr_tracepair = np.nan\n",
    "                corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                # \n",
    "                if (itrig==0) & (jtrig==0):\n",
    "                    pull1_trig_events_corr_idates = corr_tracepair\n",
    "                else:\n",
    "                    pull1_trig_events_corr_idates = np.append(pull1_trig_events_corr_idates,corr_tracepair)\n",
    "        #\n",
    "        tracecorr1_all_dates[date_tgt]=pd.Series(pull1_trig_events_corr_idates)\n",
    "        tracecorr1_mean_all_dates[idate] = np.nanmean(pull1_trig_events_corr_idates)\n",
    "        \n",
    "        # animal 2\n",
    "        pull2_trig_events_idate = pull_trig_events_errtrials_all_dates[date_tgt][(animal2_toplot,vari_toplot)]\n",
    "        ntrigs = np.shape(pull2_trig_events_idate)[0]\n",
    "        #\n",
    "        pull2_trig_events_corr_idates = []\n",
    "        #\n",
    "        for itrig in np.arange(0,ntrigs-1,1):\n",
    "            for jtrig in np.arange(itrig+1,ntrigs,1):\n",
    "                # corr_tracepair = np.corrcoef(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig])[0,1]\n",
    "                corr_tracepair,pp = scipy.stats.spearmanr(pull2_trig_events_idate[itrig],pull2_trig_events_idate[jtrig],nan_policy='propagate') \n",
    "                if pp>0.05:\n",
    "                    corr_tracepair = np.nan\n",
    "                corr_tracepair = corr_tracepair*corr_tracepair\n",
    "                # \n",
    "                if (itrig==0) & (jtrig==0):\n",
    "                    pull2_trig_events_corr_idates = corr_tracepair\n",
    "                else:\n",
    "                    pull2_trig_events_corr_idates = np.append(pull2_trig_events_corr_idates,corr_tracepair)\n",
    "        #\n",
    "        tracecorr2_all_dates[date_tgt]=pd.Series(pull2_trig_events_corr_idates)\n",
    "        tracecorr2_mean_all_dates[idate] = np.nanmean(pull2_trig_events_corr_idates)\n",
    "    # \n",
    "    tracecorr1_errpull_trigevent_sum[vari_toplot] = tracecorr1_all_dates\n",
    "    tracecorr2_errpull_trigevent_sum[vari_toplot] = tracecorr2_all_dates                \n",
    "        \n",
    "    # step 2 - plot\n",
    "    # animal 1\n",
    "    #\n",
    "    tracecorr1_all_dates = pd.DataFrame(tracecorr1_all_dates)\n",
    "    #\n",
    "    tracecorr1_all_dates.plot(kind = 'box',ax=axs[ivari*2], positions=np.arange(0,ndates_sorted,1))\n",
    "    axs[ivari*2].plot(np.arange(0,ndates_sorted,1),tracecorr1_mean_all_dates,'r*',markersize=10)\n",
    "    #\n",
    "    axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2].set_ylim([-0.2,1.2])\n",
    "    #\n",
    "    axs[ivari*2].set_xticklabels([])\n",
    "    #\n",
    "    axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "    #\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "        \n",
    "    # animal 2\n",
    "    #\n",
    "    tracecorr2_all_dates = pd.DataFrame(tracecorr2_all_dates)\n",
    "    #\n",
    "    tracecorr2_all_dates.plot(kind = 'box',ax=axs[ivari*2+1], positions=np.arange(0,ndates_sorted,1))\n",
    "    axs[ivari*2+1].plot(np.arange(0,ndates_sorted,1),tracecorr2_mean_all_dates,'r*',markersize=10)\n",
    "    #\n",
    "    axs[ivari*2+1].set_ylabel(\"paiwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "    #\n",
    "    if ivari == nvaris-1:\n",
    "        axs[ivari*2+1].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs[ivari*2+1].set_xticklabels(dates_list_sorted,rotation=90, fontsize=10)\n",
    "    else:\n",
    "        axs[ivari*2+1].set_xticklabels([])\n",
    "    #\n",
    "    axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot)\n",
    "    #\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2+1].plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs[ivari*2+1].text(taskswitch+0.25,-0.1,tasktypes[itaskswitch],fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_failedpull_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82e86a",
   "metadata": {},
   "source": [
    "#### put the correlation result together in one figure - average across sessions within training condition; all pulls, add statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,1)\n",
    "fig.set_figheight(5*nvaris*2)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "for ivari in np.arange(0,nvaris,1):\n",
    "    \n",
    "    vari_toplot = varis_toplot[ivari]\n",
    "    \n",
    "    tracecorr1_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    tracecorr2_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "\n",
    "    tracecorr1_signiP_matrx = np.ones((ngroups,ngroups))\n",
    "    tracecorr2_signiP_matrx = np.ones((ngroups,ngroups))\n",
    "    \n",
    "    for igroup in np.arange(0,ngroups,1):\n",
    "\n",
    "        igroup_typename = group_typenames[igroup] \n",
    "        igroup_typeID =  group_typeIDs[igroup] \n",
    "        igroup_cothres = group_coopthres[igroup]\n",
    "        igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "        igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "        \n",
    "        #\n",
    "        tracecorr1_allpull_trigevent_ivari = tracecorr1_allpull_trigevent_sum[vari_toplot]\n",
    "        tracecorr2_allpull_trigevent_ivari = tracecorr2_allpull_trigevent_sum[vari_toplot]\n",
    "        #\n",
    "        mergeddata = [list(tracecorr1_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr1_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        mergeddata = [list(tracecorr2_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr2_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "\n",
    "\n",
    "    # test the significance\n",
    "    for igroup in np.arange(0,ngroups,1):\n",
    "        igroup_typename = group_typenames[igroup] \n",
    "        for jgroup in np.arange(0,ngroups,1):\n",
    "            jgroup_typename = group_typenames[jgroup] \n",
    "            #\n",
    "            xxx = tracecorr1_allpull_trigevent_ivari_allgroups[igroup_typename]\n",
    "            xxx = np.array(xxx[~pd.isna(xxx)])\n",
    "            yyy = tracecorr1_allpull_trigevent_ivari_allgroups[jgroup_typename]\n",
    "            yyy = np.array(yyy[~pd.isna(yyy)])\n",
    "            \n",
    "            _,pp = scipy.stats.ranksums(xxx,yyy)\n",
    "            #\n",
    "            tracecorr1_signiP_matrx[igroup,jgroup]=pp\n",
    "            #\n",
    "            xxx = tracecorr2_allpull_trigevent_ivari_allgroups[igroup_typename]\n",
    "            xxx = np.array(xxx[~pd.isna(xxx)])\n",
    "            yyy = tracecorr2_allpull_trigevent_ivari_allgroups[jgroup_typename]   \n",
    "            yyy = np.array(yyy[~pd.isna(yyy)])\n",
    "            _,pp = scipy.stats.ranksums(xxx,yyy)\n",
    "            #\n",
    "            tracecorr2_signiP_matrx[igroup,jgroup]=pp\n",
    "            \n",
    "        \n",
    "    # do the plot \n",
    "    # animal 1\n",
    "    tracecorr1_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_allpull_trigevent_ivari_allgroups)\n",
    "    #\n",
    "    bx1 = tracecorr1_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                positions=np.arange(0,ngroups,1),color='r')\n",
    "    # axs[ivari*2].legend([bx1,bx2,bx3],['all pulls','successful pulls','failed pulls'])    \n",
    "    axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2].set_ylim([-0.2,1])\n",
    "    axs[ivari*2].set_xticklabels([])\n",
    "    axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "    #\n",
    "    print(np.round(tracecorr1_signiP_matrx,3))\n",
    "    \n",
    "    # animal 2\n",
    "    tracecorr2_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_allpull_trigevent_ivari_allgroups)\n",
    "    #\n",
    "    bx1 = tracecorr2_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                 positions=np.arange(0,ngroups,1),color='r')\n",
    "    # axs[ivari*2+1].legend(['all pulls','successful pulls','failed pulls'])    \n",
    "    axs[ivari*2+1].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2+1].set_ylim([-0.2,1.2])\n",
    "    axs[ivari*2+1].set_xticklabels(group_typenames)\n",
    "    axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "    #\n",
    "    print(np.round(tracecorr2_signiP_matrx,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39aa15a",
   "metadata": {},
   "source": [
    "#### put the correlation result together in one figure - average across sessions within training condition; put the all successful and failed pulls in one plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17672eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis_toplot = ['gaze_other_angle','othergaze_self_angle','mass_move_speed','gaze_angle_speed']\n",
    "nvaris = np.shape(varis_toplot)[0]\n",
    "animal1_toplot = animal1_fixedorder[0]\n",
    "animal2_toplot = animal2_fixedorder[0]\n",
    "\n",
    "# different session conditions\n",
    "#group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "#group_coopthres = [0,3,2,1.5,1,0]\n",
    "#group_sorting_df_coopthres = [100,3,2,1.5,1,-1]\n",
    "group_typenames = ['self','coop(1s)','no-vision']\n",
    "group_typeIDs  =  [1,3,5]\n",
    "group_coopthres = [0,1,0]\n",
    "group_sorting_df_coopthres = [100,1,-1]\n",
    "ngroups = np.shape(group_typenames)[0]\n",
    "#\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "\n",
    "fig, axs = plt.subplots(nvaris*2,1)\n",
    "fig.set_figheight(5*nvaris*2)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "for ivari in np.arange(0,nvaris,1):\n",
    "    \n",
    "    vari_toplot = varis_toplot[ivari]\n",
    "    \n",
    "    tracecorr1_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    tracecorr2_allpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    tracecorr1_succpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    tracecorr2_succpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    tracecorr1_errpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    tracecorr2_errpull_trigevent_ivari_allgroups = dict.fromkeys(group_typenames,[])\n",
    "    \n",
    "    for igroup in np.arange(0,ngroups,1):\n",
    "\n",
    "        igroup_typename = group_typenames[igroup] \n",
    "        igroup_typeID =  group_typeIDs[igroup] \n",
    "        igroup_cothres = group_coopthres[igroup]\n",
    "        igroup_sorting_df_coopthres = group_sorting_df_coopthres[igroup]\n",
    "\n",
    "        igroup_dates_tgt = sorting_df['dates'][sorting_df['coopthres']==igroup_sorting_df_coopthres]\n",
    "        \n",
    "        #\n",
    "        tracecorr1_allpull_trigevent_ivari = tracecorr1_allpull_trigevent_sum[vari_toplot]\n",
    "        tracecorr2_allpull_trigevent_ivari = tracecorr2_allpull_trigevent_sum[vari_toplot]\n",
    "        tracecorr1_succpull_trigevent_ivari = tracecorr1_succpull_trigevent_sum[vari_toplot]\n",
    "        tracecorr2_succpull_trigevent_ivari = tracecorr2_succpull_trigevent_sum[vari_toplot]\n",
    "        tracecorr1_errpull_trigevent_ivari = tracecorr1_errpull_trigevent_sum[vari_toplot]\n",
    "        tracecorr2_errpull_trigevent_ivari = tracecorr2_errpull_trigevent_sum[vari_toplot]\n",
    "\n",
    "        mergeddata = [list(tracecorr1_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr1_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        mergeddata = [list(tracecorr2_allpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr2_allpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        mergeddata = [list(tracecorr1_succpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr1_succpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        mergeddata = [list(tracecorr2_succpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr2_succpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        mergeddata = [list(tracecorr1_errpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr1_errpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        mergeddata = [list(tracecorr2_errpull_trigevent_ivari.get(key)) for key in list(igroup_dates_tgt)]\n",
    "        tracecorr2_errpull_trigevent_ivari_allgroups[igroup_typename] = pd.Series([y for x in mergeddata for y in x])\n",
    "        \n",
    "        \n",
    "    # do the plot \n",
    "    # animal 1\n",
    "    tracecorr1_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_allpull_trigevent_ivari_allgroups)\n",
    "    tracecorr1_succpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_succpull_trigevent_ivari_allgroups)\n",
    "    tracecorr1_errpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr1_errpull_trigevent_ivari_allgroups)\n",
    "    #\n",
    "    bx1 = tracecorr1_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                positions=np.arange(0,ngroups*4,4),color='r')\n",
    "    axs[ivari*2].set_xticks([])\n",
    "    bx2 = tracecorr1_succpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                positions=np.arange(1,ngroups*4+1,4),color='b')\n",
    "    axs[ivari*2].set_xticks([])\n",
    "    bx3 = tracecorr1_errpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2], \n",
    "                                                positions=np.arange(2,ngroups*4+2,4),color='k')\n",
    "    axs[ivari*2].set_xticks(np.arange(1,ngroups*4+1,4))\n",
    "    # axs[ivari*2].legend([bx1,bx2,bx3],['all pulls','successful pulls','failed pulls'])    \n",
    "    axs[ivari*2].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2].set_ylim([-0.2,1])\n",
    "    axs[ivari*2].set_xticklabels([])\n",
    "    axs[ivari*2].set_title(animal1_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "    \n",
    "    # animal 2\n",
    "    tracecorr2_allpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_allpull_trigevent_ivari_allgroups)\n",
    "    tracecorr2_succpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_succpull_trigevent_ivari_allgroups)\n",
    "    tracecorr2_errpull_trigevent_ivari_allgroups = pd.DataFrame(tracecorr2_errpull_trigevent_ivari_allgroups)\n",
    "    #\n",
    "    bx1 = tracecorr2_allpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                 positions=np.arange(0,ngroups*4,4),color='r')\n",
    "    axs[ivari*2+1].set_xticks([])\n",
    "    bx2 = tracecorr2_succpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                positions=np.arange(1,ngroups*4+1,4),color='b')\n",
    "    axs[ivari*2+1].set_xticks([])\n",
    "    bx3 = tracecorr2_errpull_trigevent_ivari_allgroups.plot(kind='box',ax=axs[ivari*2+1], \n",
    "                                                positions=np.arange(2,ngroups*4+2,4),color='k')\n",
    "    axs[ivari*2+1].set_xticks(np.arange(1,ngroups*4+1,4))\n",
    "    # axs[ivari*2+1].legend(['all pulls','successful pulls','failed pulls'])    \n",
    "    axs[ivari*2+1].set_ylabel(\"pairwise correlation R^2\",fontsize=13)\n",
    "    axs[ivari*2+1].set_ylim([-0.2,1])\n",
    "    axs[ivari*2+1].set_xticklabels(group_typenames)\n",
    "    axs[ivari*2+1].set_title(animal2_toplot+'; pairwise R^2 for pull triggered '+vari_toplot )\n",
    "    \n",
    "    \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"pairwiseCorr_PullTrigEvent_summary_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99967b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d593b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3057d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86494ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50925eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6379d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ec914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aaf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e995eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
