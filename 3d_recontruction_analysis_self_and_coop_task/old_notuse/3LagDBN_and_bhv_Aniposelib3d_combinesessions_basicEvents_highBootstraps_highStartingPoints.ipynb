{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c2ebd5",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the combined sessions, combined for each condition\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In thie script, for each bootstrap more number of starting points is used, and the highest score one is selected for that bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa3a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20220909\",\"20220912\",\"20220915\",\"20220920\",\"20220922\",\"20220923\",\"20221010\",\n",
    "                      \"20221011\",\"20221013\",\"20221014\",\"20221015\",\"20221017\",\"20230215\",     \n",
    "                      \"20221018\",\"20221019\",\"20221020\",\"20221021\",\"20221022\",\"20221026\",\"20221028\",\"20221030\",\n",
    "                      \"20221107\",\"20221108\",\"20221109\",\"20221110\",\"20221111\",\"20221114\",\"20221115\",\"20221116\",\n",
    "                      \"20221117\",\"20221118\",\"20221121\",\"20221122\",\"20221123\",\"20221125\",\"20221128\",\"20221129\",              \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221212\",\"20221214\",\"20221216\",\"20221219\",\"20221220\",\n",
    "                      \"20221221\",\"20230208\",\"20230209\",\"20230213\",\"20230214\",\"20230111\",\"20230112\",\"20230201\",\n",
    "\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 6.50, 18.10, 0,      33.03, 549.0, 116.80, 6.50,\n",
    "                                 2.80, 27.80, 272.50, 27.90, 27.00,  33.00,\n",
    "                                28.70, 45.30, 21.10,  27.10, 51.90,  21.00, 30.80, 17.50,                      \n",
    "                                15.70,  2.65, 27.30,   0.00,  0.00,  71.80,  0.00,  0.00, \n",
    "                                75.50, 20.20,  0.00,  24.20, 36.70,  26.40, 22.50, 28.50,                       \n",
    "                                 0.00,  0.00, 21.70,  84.70, 17.00,  19.80, 23.50, 25.20,  \n",
    "                                 0.00,  0.00,  0.00,   0.00,  0.00, 130.00, 14.20, 24.20, \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20220912\",\"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                      \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                      \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                      \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                      \"20230117\",\"20230118\",\"20230124\",\"20230126\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                18.10,  0.00, 33.03,  6.50,  0.00, \n",
    "                                 2.80, 27.80, 27.90, 27.00,  \n",
    "                                51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                 0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                 0.00,  0.00,  0.00,  0.00,  \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# eddie sparkle\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20221122\",\"20221125\",\"20221128\",\"20221129\",\"20221130\",\"20221202\",\"20221206\",\n",
    "                      \"20221207\",\"20221208\",\"20221209\",\"20230126\",\"20230127\",\"20230130\",\"20230201\",\"20230203-1\",\n",
    "                      \"20230206\",\"20230207\",\"20230208-1\",\"20230209\",\"20230222\",\"20230223-1\",\"20230227-1\",\n",
    "                      \"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230321\",\"20230322\",\"20230324\",\"20230327\",\"20230328\",\n",
    "                      \"20230330\",\"20230331\",\"20230403\",\"20230404\",\"20230405\",\"20230406\",\"20230407\",\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 8.00,38.00,1.00,3.00,5.00,9.50,1.00,\n",
    "                                 4.50,4.50,5.00,38.00,166.00,4.20,3.80,3.60,\n",
    "                                 7.50,9.00,7.50,8.50,14.50,7.80,8.00,7.50,\n",
    "                                 8.00,8.00,4.00,123.00,14.00,8.80,\n",
    "                                 7.00,7.50,5.50,11.00,9.00,\n",
    "                                 17.00,4.50,9.30,25.50,20.40,21.30,24.80,\n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20221122\",  \"20221125\",  \n",
    "                      \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                      \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                      \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                      \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                      \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                  8.00,  38.00, \n",
    "                                  9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                  9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                  8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                  7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                  4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "# ginger kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230209\",\"20230213\",\"20230214\",\"20230216\",\"20230222\",\"20230223\",\"20230228\",\"20230302\",\n",
    "                      \"20230303\",\"20230307\",\"20230314\",\"20230315\",\"20230316\",\"20230317\"         \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00,  0.00, 48.00, 26.20, 18.00, 23.00, 28.50,\n",
    "                                34.00, 25.50, 25.50, 31.50, 28.00, 30.50\n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20230213\",\"20230214\",\"20230216\",\n",
    "                      \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                      \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                      \"20230323\",\"20230412\",\"20230413\",\"20230517\",\"20230614\",\"20230615\",\n",
    "                      \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00, 48.00, \n",
    "                                23.00, 28.50, 34.00, 25.50, \n",
    "                                25.50, 31.50, 28.00, 30.50,\n",
    "                                33.50, 22.20, 50.00,  0.00, \n",
    "                                33.00, 18.20, 22.80, 31.00, 24.00, 21.00,\n",
    "                                 0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "    \n",
    "#    \n",
    "#dates_list = [\"20221128\"]\n",
    "#session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# where to save the demo video\n",
    "withboxCorner = 1\n",
    "video_file_dir = data_saved_folder+'/example_videos_Anipose_bhv_demo/'+animal1_filename+'_'+animal2_filename\n",
    "if not os.path.exists(video_file_dir):\n",
    "    os.makedirs(video_file_dir)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6595613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "except:\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        for imergepair in np.arange(0,np.shape(merge_campairs)[0],1):\n",
    "            \n",
    "            # should be only one merge type - \"Anipose\"\n",
    "            merge_campair = merge_campairs[imergepair]\n",
    "\n",
    "            # load camera tracking results\n",
    "            try:\n",
    "                # dummy\n",
    "                if reanalyze_video:\n",
    "                    print(\"re-analyze the data \",date_tgt)\n",
    "                    dummy\n",
    "                ## read\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                    body_part_locs_Anipose = pickle.load(f)                 \n",
    "            except:\n",
    "                print(\"did not save data for Anipose - body part tracking \"+date_tgt)\n",
    "                # analyze and save\n",
    "                Anipose_h5_file = Anipose_analyzed_path +date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "                Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "                body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                    pickle.dump(body_part_locs_Anipose, f)            \n",
    "            \n",
    "            min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "                    \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "            tasktypes_all_dates[idate] = tasktype\n",
    "            coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # analyze behavior results\n",
    "            # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "            succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "\n",
    "            trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "            #\n",
    "            pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<10]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "            # load behavioral event results\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                    output_key_locations = pickle.load(f)\n",
    "            except:\n",
    "                print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "                output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "               \n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_key_locations, f)\n",
    "                \n",
    "             \n",
    "            look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "            look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "            look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "            look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "            look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "            look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                               \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                               \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "            timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "            timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "            timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "              \n",
    "            #\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            \n",
    "            \n",
    "            # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "            # could be used for define time bin for DBN\n",
    "            if 1:\n",
    "                _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                             oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #\n",
    "                pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "                bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                                'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "           \n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158391c",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2541be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9be41",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe0aaea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.73838863  2.02484076  1.45720721  2.09630872  2.05283688  3.61925926\n",
      "  3.032       1.48945869  2.12715232  2.27323944  2.26950355  1.73636364\n",
      "  2.18776371  4.20660377  2.3004329   1.78047809  1.47537994  1.48511327\n",
      "  1.77420635  1.50655738  1.95544554 22.86020408  3.02537313 17.18030303\n",
      "  2.02808989  2.51621622]\n",
      "2.550191637630662\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAFuCAYAAADNvh8+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABqTElEQVR4nO3deXgUVfbw8e9JCKvI7riw6rDEJBAQFxQFBETEfUNcBgRFcIy7AkbHV8coIKMoM4qOIG4TcXB+gKKCsoygjoqAyCZuIIgLIiCyB877R3faTkhCp7uru6r6fJ6nn3RXdd86fW5X903VrXtFVTHGGGOMMcmRluwAjDHGGGNSmTXGjDHGGGOSyBpjxhhjjDFJZI0xY4wxxpgkssaYMcYYY0wSWWPMGGOMMSaJEtoYE5GJIvKTiCwrtTxPRD4XkeUiMjqRMRljjDHGJFOij4xNAs4MXyAi3YDzgLaqmgWMSXBMxhhjjDFJk9DGmKq+C/xSavFQYKSq7g4+56dExmSMMcYYk0xu6DPWCjhVRD4Ukf+KyPHJDsgYY4wxJlGqJDsAAjHUA04CjgdeEZGjtYx5mkRkMDAYoFatWse1adMmoYEaY4wxbvfZd1vJOapO3J/rBC/FWuyTTz6hffv2pKX9fjxr//79LF68mOOOO66i1/2sqo3KWueGxth64D/BxtdHIrIfaAhsLP1EVX0aeBqgY8eOunDhwoQGaowxxrhd8+EzWDiyT9yf6wQvxVqsevXqXHnlldx6662hZY888ggrVqygonaJiKwtb50bGmNTgdOBeSLSCqgK/JzUiIwxxhhjynDttdcybNgwAIYMGcL48eMZNmwYQ4YMibrMhDbGRKQQ6Ao0FJH1wL3ARGBicLiLPUD/sk5RGmOMMcYk27hx4wC46667uO2226hWrRpDhgwJLY9GQhtjqtqvnFVXJjIOY4wxxphojRs3LqbGV2luuJrSGGOMMSZlWWPMGGOMMSaJrDFmjDHGNQYMGMCUKVMqfM68efN4//33477tSZMmccMNN8RUxo4dO+jTpw9t2rQhKyuL4cOHl/m8NWvWUKNGDXJzc8nNzS3R+btr1660bt06tO6nnwJjoe/evZu+ffvyxz/+kRNPPJE1a9aEXnPmmWdSt25dzj777BLbmTNnDh06dCA7O5v+/ftTVFQEwKpVq+jUqRNrx5zPmDElJ77ZsmULF198MW3atCEzM5MPPvgAgHvuuYe2bduSm5vLGWecwYYNGwB46aWXQrHm5uaSlpbGkiVLANizZw+DBw+mVatWtGnThldffRWA8ePHk5OTw4Zn8+jcuTMrVqwIbf+5556jZcuWtGzZkueeey60XFXJz8+nVatWZGZm8vjjj4fWzZs3j9zcXLKysujSpQsA69ato1u3bmRmZpKVlcVjjz12kNpLIlX15O24445TY4wx/tK/f3/997//XeFz7r33Xn344Yfjvu1nn31W//znP8dUxvbt23XOnDmqqrp7927t3LmzvvHGGwc875tvvtGsrKwyy+jSpYt+/PHHByz/xz/+odddd52qqhYWFuqll14aWvfOO+/o9OnTtU+fPtps2Ouqqrpv3z5t3Lixfv7556qqes899+gzzzyjqqo//vijfvTRR3pop0sPyOWf/vQn/ec//xl6D5s3b1ZV1a1bt4ae89hjj4ViCbd06VJt0aJF6PFf/vIXzc/PD8WzcePGEmU1G/a6Tps2TXv16qWqqps2bdIWLVropk2b9JdfftEWLVroL7/8oqqqDXrfpFdddZXu27cv9B5UVTdv3qyZmZm6du3aEss3bNign3zyiaqq/vrrr9qyZUtdvnx5GRlPDGChltOmsSNjxhhjQtasWUObNm245ppryM7O5oorruCdd97hlFNOoWXLlnz00UcAbN++nYEDB3L88cfTvn17pk2bFnr9qaeeSocOHejQoUPoCNa8efPo2rVr6IjLFVdcgR7kwvnmzZtz77330qFDB3Jycli1ahVr1qxh/PjxPProo+Tm5jJ//nw2btzIRRddxPHHH8/xxx/Pe++9x/79+2nevDlbtmwJlffHP/6RH3/8kddee40TTzyR9u3b06NHD3788ce45a9mzZp069YNgKpVq9KhQwfWr18fl7KnTZtG//79Abj44ouZPXt2KIfdu3endu3aJZ6/adMmqlWrRqtWrQDo2bNn6MjUYYcdxvHHH4+kpZd4za+//sq7777LoEGDQu+hbt26ABx66KGh523fvh0ROSDGwsJC+vX7/Vq9iRMnMmLECADS0tJo2LBhhWXNnDmTnj17Ur9+ferVq0fPnj156623ANi25A3+8pe/hAZbPeywwwD417/+xYUXXkjTpk1LLD/iiCPo0KEDALVr1yYzM5Pvvvuu/AQnkTXGjDHGlPDll19y0003sXTpUlatWsW//vUvFixYwJgxY3jwwQcBKCgo4PTTT+fjjz9m7ty53HHHHWzfvp3DDjuMt99+m0WLFjF58mRuvPHGULmLFy9m7NixrFixgq+//pr33nvvoLE0bNiQRYsWMXToUMaMGUPz5s0ZMmQIt9xyC0uWLOHUU0/lpptu4pZbbuHjjz/m1Vdf5ZprriEtLY3zzjuP//u//wPgww8/pHnz5vzhD3+gc+fO/O9//2Px4sVcdtlljB49usIY5s6dW+I0XPHt5JNPrvB1W7Zs4bXXXqN79+5lrv/mm29o3749Xbp0Yf78+SXWXX311eTm5vLXv/411OD67rvvaNKkCQBVqlShTp06bNq0qcLc7d27NzQQ6ZQpU1i3bl2FMX/99dc0atSIq6++mvbt23PNNdewffv20Pr8/HyaNGnCSy+9xP3333/A6ydPnhxqjBU3hO+55x46dOjAJZdcUqLh+49//IPvnrqGO++8M3TKMfw9AjRu3DjUgCra/AOTJ0+mY8eO9O7dmy+++AKA1atXs3nzZrp27cpxxx3H888/f0Bca9asYfHixZx44okVvv9kscaYMcaYElq0aEFOTg5paWlkZWXRvXt3RIScnJxQP6VZs2YxcuRIcnNz6dq1K7t27eLbb79l7969XHvtteTk5HDJJZeU6At0wgkn0LhxY9LS0sjNzS3R56k8F154IQDHHXdcuc9/5513uOGGG8jNzeXcc8/l119/Zdu2bfTt25fJkycD8PLLL9O3b18A1q9fT69evcjJyeHhhx9m+fLlFcbQrVs3lixZcsCton5rRUVF9OvXjxtvvJGjjz76gPVHHHEE3377LYsXL+aRRx7h8ssv59dffwUCfbA+++wz5s+fz/z583nhhRcAyjySWNbRqfB1L7/8MrfccgsnnHACtWvXpkqVike0KioqCjV+Fy9eTK1atRg5cmRofUFBAevWreOKK67g73//e4nXfvjhh9SsWZPs7OxQWevXr+eUU05h0aJFdOrUidtvvz30/D//+c8cdd0zjBo1igceeOCg71H37aV69eosXLiQa6+9loEDB4a288knnzBjxgxmzpzJX//6V1avXh16/W+//cZFF13E2LFjSxyRcxNrjBljjCmhWrVqoftpaWmhx2lpaaEO4KrKq6++GmqYfPvtt2RmZvLoo4/yhz/8gU8//ZSFCxeyZ8+eMstNT08PlRVJLBU9f//+/XzwwQehWL777jtq165Np06d+PLLL9m4cSNTp04NNezy8vK44YYb+Oyzz3jqqafYtWtXhTFEc2Rs8ODBtGzZkptvvrnc99WgQQMg0NA85phjQg2Io446CgicWrv88stDp4YbN24cOrJVVFTE1q1bqV+/foWxd+rUifnz5/PRRx9x2mmn0bJlywqf37hxYxo3bhw6gnTxxRezaNGiA553+eWXh055Fnv55ZdLnKJs0KABNWvW5IILLgDgkksuKbOsyy67jKlTpx7wHiHQcD7yyCMBSK/dkIsuugiACy64gKVLl4Zec+aZZ1KrVi0aNmzIaaedxqeffgrA3r17ueiii7jiiitC9e9G1hgzxhhTab169WLcuHGhIxmLFy8GYOvWrRxxxBGkpaXxwgsvsG/fvrhvu3bt2mzbti30+IwzzihxlKb4Sj4R4YILLuDWW28lMzMz1PjZunVrqMETfrVeeSp7ZOzuu+9m69atjB07ttwyN27cGMrN119/zRdffMHRRx9NUVERP/8cmBFw7969vP7666EjTeeee24o3ilTpnD66adXeGQMKHEl5qhRow46Zc/hhx9OkyZN+PzzzwGYPXs2xx57LEDotCDA9OnTadOmTejx/v37+fe//81ll10WWiYinHPOOcybN6/CsmbMmBFqJPbq1YtZs2axefNmNm/ezKxZs+jVqxcANVuexJw5cwD473//G+oLd9555zF//nyKiorYsWMHH374IZmZmagqgwYNIjMzs8Q8kq5UXs9+t9/sakpjjIm/0lf5hV/dGL5ux44dOnjwYM3OztasrCzt06ePqqquXr1ac3Jy9MQTT9Thw4drrVq1VFV17ty5oeeoqv75z3/WZ5999oDth2+vWbNmoavvPv74Y+3SpYuqqn7++eeak5Oj7dq103fffVc3btyol156qebk5GhmZmaJq/w+/vhjBXTSpEmhZVOnTtUWLVpo586d9fbbbw+VG4+rKdetW6eAtmnTRtu1a6ft2rULXZk4bdo0veeee1RVdcqUKXrsscdq27ZttX379jp9+nRVVf3tt9+0Q4cOmpOTo8cee6zeeOONWlRUpKqqO3fu1IsvvliPOeYYPf744/Wrr74Kbbdz587asGFDrV69uqYf0kDfeustVVW9/fbbtU2bNtqqVSt99NFHQ8///vvv9aijjlKpWkPr1KmjRx11VOgKx8WLF+txxx2nOTk5et5554WuZrzwwgs1KytLc3Jy9Oyzz9b169eHyps7d66eeOKJB+RjzZo1euqpp2pOTo6efvrpoSseb7zxRj322GM147AW2rVrV122bFnoNRMmTNBjjjlGjznmGJ04cWJoeZObXtazzjpLs7Oz9aSTTtIlS5aE1o0ePVozMzM1Kysr9D7nz5+vQOiz0q5dO50xY0ZlqjOuqOBqSlGPTgPZsWNHrWh2dD/r2rUrQOi/DRMflldnWF69w+rKW8qrr+bDZ7BmZJ+IyqjMc53gpVhjJSKfqGrHstYldG5KEx+lB/Uz8WF5dYbl1TusrrzF6ss/rDHmQeFXo5j4sbw6w/LqHVZX3mL15R/Wgd8YY4wxJomsMeZBXbt2DfUVMPFjeXWG5dU7rK68xeoreQoLC8nOziY9PZ3s7GwKCwtjKs9OUxpjjDHGRKiwsJD8/HwmTJhA586dWbBgQWj6qPBx1irDjowZY4wxxkSooKCACRMm0K1bNzIyMujWrRsTJkygoKAg6jKtMWaMMcYYE6GVK1fSuXPnEss6d+7MypUroy7TGmPGGGOMMRHKzMxkwYIFJZYtWLCAzMzMqMu0PmMedOmllyY7BF+yvDrD8uodVlfeYvWVHPn5+QwaNOiAPmOxnKa0xpgHXX/99ckOwZcsr86wvHqH1ZW3JLO+CgsLKSgoYOXKlWRmZpKfnx9153WvKX6feXl5ofdfUFAQ0/tPaGNMRCYCZwM/qWp2qXW3Aw8DjVT150TG5TU7duwAoGbNmkmOxF8sr86wvHqH1ZW3JKu+nLia0Gv69esX1/ea6D5jk4AzSy8UkSZAT+DbBMfjSWeddRZnnXVWssPwHcurMyyv3mF15S3Jqq+CggIuv/xy8vLyqF69Onl5eVx++eUxnaZLdQk9Mqaq74pI8zJWPQrcCUxLZDzGGGOMqZwVK1bw448/csghhwCwfft2nnrqKTZt2pTkyLwr6VdTisi5wHeq+mmyYzHGGGNMxdLT09m/fz8TJ05k165dTJw4kf3795Oenp7s0DwrqR34RaQmkA+cEeHzBwODAZo2bepgZMYYY4wpS1FREVWrVi2xrGrVqhQVFSUpIu9L9tWUxwAtgE9FBKAxsEhETlDVH0o/WVWfBp4G6NixoyYyUOOc5sNnlLtuzcg+CYzEGGNMJAYMGFDiasIBAwYwcuTIZIflWUltjKnqZ8BhxY9FZA3Q0a6mrNiAAQOSHUJchTe4mg+fkbQGmN/y6haWV++wuvKWZNVX48aNmTRpEv/6179CV1NefvnlNG7cOCnx+MFBG2Mi0hU4H+gA1Ad+ARYDU1V1bmU2JiKFQFegoYisB+5V1QmVitjYF6ZDLK/OsLx6h9WVtySrvkaPHs1NN93EwIEDWbt2Lc2aNWPfvn088sgjSYnHD8ptjIlIN2AsUA+YDUwFfgUOBbKBSSKyBbg50kaZqlY4KIeqNo+knFT388+BA4cNGzZMciT+Ynl1huXVO6yuvCVZ9VU8vlZBQQEiQq1atXjwwQdTZowxJ1R0ZKwAuAN4W1XL7J8lImcAfwU6l7XeOOPiiy8GYN68eckNxGcsr86wvHqH1ZW3JLO+4j3oaaortzGmqicf7MWqOguYFdeIjDHGGGNSSFTjjIlICxGxsSWMMcaYFFRYWEh2djbp6elkZ2dTWFiY7JA8LaLGmIhMFJFTgvf7AV8CX4vI5U4GZ4wxxhh3KZ6bcty4cezatYtx48aRn59vDbIYRHpkrDewKHj/VuAiAnNJ3uVEUMYYY4xxp4KCAiZMmEC3bt3IyMigW7duTJgwweamjEGk44zVVNWdIlKPwECt01RVgxN8mwQbOnRoskPwJcurMyyv3mF15S3Jqq+VK1fSuXPJ6/Y6d+7MypUrkxKPH0TaGPtORLoAmcD8YEPsUMDmPkiCvn37JjsEX7K8OsPy6h1WV96SrPrKzMxkwYIFdOvWLbRswYIFZGZmJiUeP4j0NOX9wNvAGOBvwWU9gCUOxGQOYt26daxbty7ZYfiO5dUZllfvsLrylmTVV35+PoMGDWLu3Lns3buXuXPnMmjQIPLz8xMeS7LE+wKGiI6MqerLIjIteH9ncPEC4P2Ytm6ictVVVwE2FlC8WV6dYXn1Dqsrb0lWfRWPLxY+N2VBQUHKjDtWfAHDhAkTQtNBDRo0CCDqHEQ8N2VYI6z48U9RbdEYY4wxnpbKg76GX8AAhC5gyMvLi39jTES+AMoceT+cqraKasvGGGOMMR7jxAUMFR0ZeyDqUo0xxhhjfCgzM5P77ruPqVOnhk7Tnn/++TFdwFDRdEjPRV2qMcYYY4wPdevWjVGjRjFq1CiGDBnC+PHjGTZsGEOGDIm6zIpOUx4ZSQGquiHqrZuo3HbbbckOwZcsr86wvHqH1ZW3WH0lx9y5cxk2bBgTJ07kjjvuIDMzk2HDhjF16tSoy6zoNOV6Ku4zJsH16VFv3UTlnHPOSXYIvmR5dYbl1TusrrwlmfVVWFhIQUFB6DRdfn5+ynToX7lyJYsXL+aBB37vzbV3714eeuihqMusqDHWIupSjaM+//xzAFq3bp3kSPzF8uoMy6t3WF15S7Lqy4mhHbzEiUFvK+oztjbqUo2jrrvuOsDGAoo3y6szLK/eYXXlLcmqLyeGdvCS4kFvSzdGY5mbM6JxxkSk3AnBVfXBqLdujDHGGE9J9bkpnRj0NtJBX3uWenwkgdOYCwBrjBljjDEpwuamjP+gt5FOh9St9DIRuQFoFLdIjDHGGON6TpymS3URT4dUhieBDcC9cYrFGGOMMS6X6nNTOiGWxlg7AsNbmAS7++67kx2CL1lenWF59Q6rK29JZn2l8tyUToi0A//blBxzrBbQAfhbZTYmIhOBs4GfVDU7uOxh4BxgD/AVcLWqbqlMuammR48eyQ7BlyyvzvBbXpsPn1Hm8jUj+yQ4kvjzW135ndWXf0R6ZGxBqce/AXep6n8rub1JwN+B58OWvQ2MUNUiERkFjACGVbLclLJkyRIAcnNzkxqH31heneG3vIY3upoPn+GLRlgxv9WV31l9+UekHfjvi8fGVPVdEWleatmssIf/Ay6Ox7b87OabbwZsLKB4s7w6w/LqHVZX3mL15R9pkT5RRJqKyHAR+Xvwb3MH4hkIvFlBDINFZKGILNy4caMDmzd+UlhYSHZ2Nunp6WRnZ1NYWJjskIwxxpgDRNQYE5Ezgc+BPkCd4N+VweVxISL5QBHwUnnPUdWnVbWjqnZs1MhG1TDlK56uY9y4cezatYtx48aRn59vDTJjjDGuE+mRsYeBQap6qqpepaqnEjiKVakO/OURkf4EOvZfoaoVTU5uTEQKCgq4/PLLycvLo3r16uTl5XH55ZfbODjGGGNcJ9IO/M2Bl0stmww8HWsAwaNrw4Auqroj1vKMAVixYgXbt29n4sSJoUEJBw4cyNq1NuWqMcYYd4m0MTYP6ArMCVvWBajU1ZQiUhgsp6GIrCcwYOwIoBrwtogA/E9Vh1Sm3FTz4IM2A9XBVK1alby8vBIT2ebl5XHXXeVOs2p5dYjl1TusrrzF6ss/Im2MfQn8n4hMBdYQOFJ2PjAhfBLxg00arqpljRA3IcIYTNDJJ5+c7BBcb8+ePYwcOZJx48axdu1amjVrxvbt29mzZ0+5r7G8OsPy6h1WV95i9eUfkTbGcoFFQNPgjeDj9mHPUWzS8IR4//33AdsRK3LUUUexbds2atWqRfCIK3v27OGoo44q9zWWV2dYXr3D6spbrL78I+qJwk3yFJ9qs7FlKlazZs0SfcauuOKKCp9veXWG5dU7rK68xerLPyIeZ8wYL9mwYQOjRo0qcTXlqFGj2LBhQ7JDM8YYY0qIZaJwY1wrMzOTxo0bs2zZstCyuXPnkpmZmcSojDHGmAPZkTHjS/n5+QwaNIi5c+eyd+9e5s6dy6BBg8jPz092aMYYY0wJdmTM+FK/foELd/Py8li5ciWZmZkUFBSElhtjjDFuYY0xDxo7dmyyQ/CEfv36VarxZXl1huXVO6yuvMXqyz/KbYyJyDcEhquokKoeHdeIzEHl5uYmOwRfsrw6w/LqHVZX3mL15R8VHRm7O+z+0cD1BAZo/Sb4+GrgCedCM+V55513AOjRo0eSI/EXy6szLK/eYXXlLVZf/lFuY0xVXyq+LyLvAueo6sKwZa8CY4EHnAzQHOiBBwIptx2wYs2Hzyhz+ZqRfcpcbnl1huXVO6yuvMXqyz8qMwL/klLLlgaXG+NKxY2u5sNnlNsAM8YYY5It0qEtPgduKbXsZmB1XKMxxhhjjEkxkR4Z+zPwhoj8GVgLNAMOAexwgzHGGGNMDCKdm/IjETkaOAc4CvgOeF1VtzoZnDHGGGOM30U8zpiq/gq8dNAnGsc99dRTyQ7BlyyvzrC8eofVlbckq77KuzgKyr9AylQsosaYiAhwGdARqB2+TlUHOxCXqUDr1q2THYIvWV6dYXn1Dqsrb0lWfYU3uOwCqfiItAP/k8DfgSZARqmbSbDXXnuN1157Ldlh+I7l1RmWV++wuvIWqy//iPQ05SXACar6lZPBmMj87W9/A+Ccc85JciT+Ynl1huXVO6yuvMXqK3kqO47lwUTaGNsBfBvVFowxxhhjfCTe41hGeppyNPCXYN8xY4wxxhgTJ5EeGbuRwNhieSLyU/gKVW0V96iMMcYYY1JEpI2xuMw/KSITgbOBn1Q1O7isPjAZaA6sAS5V1c3x2J4xxhhjjNtFOujrc3Ha3iQCV2U+H7ZsODBbVUeKyPDg42Fx2p4vvfDCC8kOwZcsr86wvHqH1ZW3WH35R6TjjF1e3jpV/VekG1PVd0WkeanF5wFdg/efA+ZhjbEKNWnSJNkh+JLl1RmWV++wuvIWqy//iPQ0ZUGpx4cFX/sdEHFjrBx/UNXvAVT1exE5rLwnishgYDBA06ZNY9ysd02ePBmAvn37JjkSf7G8OsPy6h1WV95i9eUfkZ6mbBH+WESqEGigrXEgporieBp4GqBjx46ayG27yZNPPgnYDhhvlldnWF69w+rKW6y+/CPSoS1KUNUi4B5gRBxi+FFEjgAI/v3pIM83xhhjjPGNqBpjQUcCh8QhhulA/+D9/sC0OJRpjDHGeEphYSHZ2dmkp6eTnZ1NYWFhskMyCRJpB/6nSy2qBXQHplRmYyJSSKCzfkMRWQ/cC4wEXhGRQQRG+b+kMmUaY4wxXldYWEh+fj4TJkygc+fOLFiwgEGDBgHQr1+/JEdnnBbpkbHSk4P/QuCKx+srszFV7aeqR6hqhqo2VtUJqrpJVburasvg318q9Q6MMcYYjysoKGDChAl069aNjIwMunXrxoQJEygoKH39nKksLxxxjLQD/9VOB2IiN2VKpQ5ImghZXp1hefUOq6vkWblyJZ07dy6xrHPnzqxcubLc11h9HZxXjjhG3GdMRA4Rkb4icruIXCoi8egvZqLQsGFDGjZsmOwwfMfy6gzLq3dYXSVPZmYmCxYsKLFswYIFZGZmlvsaq6+D88oRx4gaYyKSBawGxhAYpPVvwGoRyXYwNlOOSZMmMWnSpGSH4TuWV2dYXr3D6ip58vPzGTRoEHPnzmXv3r3MnTuXQYMGkZ+fX+5rrL4OLpojjskQ6ZGxscBTQFNVPRVoCjwJPOZQXKYCtgM6w/LqDMurd1hdJU+/fv3o06cPvXv3pmrVqvTu3Zs+ffpUeCrN6uvgojnimAyRNsbaAw+qqgIE/44Ech2KyxhjjEkZhYWFzJgxgzfffJM9e/bw5ptvMmPGDFd2NveSaI44JkOk0yFtBZoDX4Qtaw78Gud4jDHGmJQT3rcJCPVtysvLc1VHc68pzl1eXh4rV64kMzOTgoIC1+U00sbYc8AMERkJfAO0AO4EJjkUlzHGGJMyvNK3yYv69evnusZXaZWZKHwvgbHFmgDrCDTEHnYmLGOMMSZ1FPdtKj4yBu7s22SccdDGWHBS8FuBR1X1IedDMgfzxhtvJDsEX7K8OsPy6h1WV8lT3Lep9HhYFQ3BYPXlHwdtjKlqkYjcpaqjExGQObiaNWsmOwRfsrw6w/LqHVZXyRNN3yarL/+I9DTlXBHpoqr/dTQaE5EnnngCgOuvr9RsVOYgLK/OsLx6h9VVclW2b5PVl39E2hhbA0wTkSnB+/uLV6jqg/EPy1TklVdeAWwHjDfLqzMsr95hdeUtVl/+EWljLBdYDBwTvBVTwBpjxhhjjHGd5sNnlLl8zcg+CY6kYpFOFN7t4M8yxhhjjHGP8EZX8+EzXNcIKxbxROHGGGOMMSb+Ip0ovJGIvCQiP4jIvvCb0wEaY4wxqaBXr16kpaUhIqSlpdGrV69kh2QSJNI+Y48DRwCDgEKgHzAceMWhuEwF5s2bl+wQfMny6gzLq3dYXSVPr169mDVrFkOHDuWhhx5ixIgRPPnkk/Tq1YuZM2eW+RqrL/+ItDF2OpCjqj+JyH5VnSEinwFTgHHOhWeMMcb439tvv02PHj149913qV+/PpmZmfTo0YO333472aGZBIi0z1gGsDF4f6eI1FLVb4E2zoRlKjJmzBjGjBmT7DB8x/LqDMurd1hdJY+q8sUXXzBu3Dh27drFuHHj+OKLL1DVcl9j9eUfkTbGVgMdgvc/Be4SkTuBHx2JylTo9ddf5/XXX092GL5jeXWG5dU7rK6Sq127dnTr1o2MjAy6detGu3btKny+1Zd/RNoYuwuoFnb/EuAWAnNWGmOMMSZG06dP5/rrr2fr1q1cf/31TJ8+PdkhmQSJqDGmqnNU9f3g/UWq2kpVj1DV1+IViIjcIiLLRWSZiBSKSPV4lW2MSU2FhYVkZ2eTnp5OdnY2hYWFyQ7JmDJlZWXRsWNHxo8fT926dRk/fjwdO3YkKysr2aGZBCi3MSYi6ZEUEOnzDlLGUcCNQEdVzQbSgctiLdcYk7oKCwvJz88v0QcnPz/fGmTGlfLz89m0aROzZ89mz549zJ49m02bNpGfn5/s0EwCVHQ15XIReQiYrKq7Sq8UkWoEGkzDgGPjFEsNEdkL1AQ2xKFMX6pRo0ayQ/Aly6szkpXXgoICJkyYQLdugQlEunXrxoQJE8jLy6vUZMypxPaB5Cn+TObl5bFy5UoyMzMpKCio8LNq9eUfFTXGLgIeBh4TkfeBFcCvwKEEGl+dgA8I9B+Liap+JyJjgG+BncAsVZ1V+nkiMhgYDNC0adNYN+tZb775ZrJD8CW/5bW8OdkgsfOyJSuvK1eupHPnziWWde7cmZUrVyYlHi/w2z7gNf369avUPwpWX/5RbmNMVZcDZ4lIa+A8AldT1gM2A/8FblXVVfEIQkTqBbfRAtgC/FtErlTVF0vF9DTwNEDHjh3Lv97XGOOZOdmckpmZyYIFC0JHxgAWLFhAZmZmEqMyxpgDHXTQV1X9HBjtcBw9gG9UdSOAiPwHOBl4scJXpai//vWvANxzzz1JjsRfLK/OSFZe8/PzGTRoEBMmTKBz584sWLCAQYMGUVBQkNA4vMT2AW+x+vIPt0wU/i1wkojUFBEBugOuPJfghquzZs+ezezZsxO+Xb+zvDojWXnt168fBQUF5OXlUb16dfLy8g7aByfV2T6QXJX9fbH68o9yj4yJyBfAQU8FqmqrWINQ1Q9FZAqwCCgCFhM8HekmxVdnlf5PG7AveGNcqLJ9cIxJFvt9SW0VnaZ8IGFRAKp6L3BvIrdZWXZ1ljHlXxjgxj5pXorVSwoLCykoKAhd9Zefn2/fgTGy35fUVlEH/ucSGYgX2NVZxileajQUx+SFiwK8FKtX2BEcZ9jvS2o7aAf+YsHBXVsCjQApXq6q7zoQlyu55eqsBg0aJHR7qSKZefVzo8E+r94RSV3ZERxnRPP7YvuWf0TUGBORDsB/gKYE+pFJ8O8+oKpj0blMfn4+ffv2pVatWqxdu5ZmzZqxfft2HnvssYTG8eqrryZ0e6nC8uoMy6t3RFJXK1euZP369WRnZ4dOUw4bNsy1R3C8cko1/Pfl22+/pWnTpgf9fbF9yz8iPTI2Fvg/4C8ErnxsQmC4iwXOhOV+gYs+jTEmtRx55JEMGzaMl156KXSa8oorruDII49MdmgH8OopVVUbRjPVRDq0RQ4wXFW3AaKqvwF3Avc7FlkcxHsYioKCAgYPHkytWrUAqFWrFoMHD074uEUjRoxgxIgRCd1mKrC8OsPy6h2R1lXpxoJbGw/hp1QzMjJCp1TdONZcQUEBkydP5ptvvmH//v188803TJ48ucJYbd/yj0iPjO0Nu79VRA4DtgKHxz+k+HDiP6IVK1awY8eOA8pcs2ZNHCM/uA8++CCh20sVlldnWF69I5K62rBhA5MmTSoxh+Lo0aMZMGCA8wFWkpdOqUbTgd/2Lf+I9MjYJ0DP4P15wAvAy8BSB2KKCyf+I6patSo33HBDiTJvuOEGqlZNmW5zxpgUl5mZSePGjVm2bBn79u1j2bJlNG7c2JXTTBWfUh03bhy7du1i3LhxDBs2zJWnVIs78Iez6btSR6RHxq7h94bbrcBDBCYMv9qJoOLBicuE9+zZw7hx42jfvn3oyNi4cePYs2dPrOEaY1KYk5O6x7sDu9emmdqyZQu9evVi7969ZGRkUKVKFVdehfjj0WdxxgX9aND7Jqo1Ppbd61ew6c3HqHvan5IdmkmAiBpjqvpd2P1NwGDHIoqTzMxM7rvvPqZOnRr6Ejr//PNj+i/j2GOP5fzzzy9xeP6KK65g6tSp8Qs8Tpz6cvfSeFjGeIVTk7o70V2j+HXh34NunWZq/fr1pKWl0ahRI3766Sfq16/Pxo0bWb9+fbJDO8DG6aMpLGxPQUEByyevJOvYTB598lFX5tXEX6RDW3wJTASeC2+YuVm3bt0YNWoUo0aNYsiQIYwfP55hw4YxZMiQqMvMz88v84st0f8RNm7c+KDPcerL3aly3SCSvJrK80Ne2903i60795a5rvQ/KHVqZPDpvWckIqyDquyYYJHWlVemmRIRrrvuOp544onQsuuvv57x48cnMaryFee1+fAZLIvgu9UP+5YJiPQ0ZQHQH/h/IjKbQMNsqqqW/e3kAnPnzmXYsGFMnDiRO+64I9RxM5ajWCM+PZTt2RfT69IB7N20nowGjanTqS8jPj2URH4vvfjii4nbWAqxvDoj0ry6eTyorTv3RvyPR0VHpROtsh3Y/VBX4VSVF154gWeeeSZ0mrJatWquvfqzsuw7yz8i6sCvqs+qalcgE1gIPAx8LyKJHe20ElauXEnr1q1LLGvdunVMfcbWjOzDxumj2bNxLc3unM6ejWvZOH20r44OGZMMhYWF3HTTTWzfvh1VZfv27dx0000xD0eT6o488kjuvPPOEh3Y77zzzpg6sBef+gwvMz8/35V1lZ6ezm+//UaDBg1IS0ujQYMG/Pbbb6Snpyc7NGNKiPRqSgBU9StVvQc4CfgQuMGRqOLAiS8ht7j55pu5+eabkx2G71henRFJXu+8807S09OZOHEiu3fvZuLEiaSnp3PnnXcmJkgfKz1AdUUDVkdSV14au0tVSUtL44477mDbtm3ccccdpKWl+ebImH1n+UfEjTERSReR80RkKrAGqAtc50xY8bFr1y4GDhxItWrVGDhwILt27Up2SHGxZMkSlixZkuwwfMfy6oxI8rp+/Xqef/75Ej/wzz//vCs7WnvJhg0byMrKonv37lStWpXu3buTlZXFhg0bynx+JHUVfuqzeEDt9evXx2XsrngP1L1//36uueYa7rrrLmrVqsVdd93FNddcw/79+2OO1Q3sO8s/Iu3A/whwBbAHeBG4U1VXOxlYrL777jsOOeQQvvvuO1SV7777jurVq/Pdd564/sAYY2JWt25dZs+eTVpaGvv27SMtLY3Zs2dTr169qMt0ajokJ678rFatGq1bty7xj/gjjzxCtWrVYorVmHiL9MhYE2AA0ExVR7i9IQaBvgIZGRnMnDmTPXv2MHPmTDIyMqyvgDEu1LhxY/r378/cuXPZu3cvc+fOpX///na1WIw2b96MqjJ48GC2bNnC4MGDUVU2b94cU7lOTIfkxOnPa6+9lmHDhvHII4+wY8cOHnnkEYYNG8a1114bc7zGxNNBG2MiUgWoCcxVVc8c2y0qKjpgZPyqVatSVFSUpIiMMeUZPXo0RUVFDBw4kOrVqzNw4ECKiooYPXp0skPzNFXlsssu491336V+/fq8++67XHbZZTE1njZs2MDo0aPJy8ujevXq5OXlMXr06HJPfUbKiYG6x40bx5AhQ0qcphwyZAjjxo2LKVZj4u2gjTFVLQKOAzzXijnhhBPo3bs3VatWpXfv3pxwwgnJDikuWrVqRatWrZIdhu9YXp0RSV779etH3759+f7779m/fz/ff/89ffv2deVwCV7Trl27ElMXtWvXrtznRlJXTk2HVDxQd3ifsfvuuy/mclevXh2aJWXPnj2sXu36EzsRs+8s/4j0NOULuPjKybLUr1+f119/nQcffJDt27fz4IMP8vrrr1O/fv1khxazp59+mqeffjrZYfiO5dUZkeS1sLCQGTNm8Oabb7Jnzx7efPNNZsyY4crhErwkLS2N/Pz8Eqfp8vPzSUsr+6s/kroqng4p/JTyoEGDyM/PjynW4oG6Bw4cyLZt2xg4cCCjRo0KDVgbjV69ejFr1iyGDBnCli1bGDJkCLNmzaJXr14xxeoW9p3lH5EO+toBuElEbiBwJWXodKWqumOo6VJq1qzJ/v37GTduHLfffjvNmjXj0EMPpWbNmskOzRhTSkFBAe3ataN3797s3r2batWq0bt3b9dOs+MVtXLPYtui17nt9ju47bbbQNJA93PDDdH/b92vXz/ef//9EnV17bXXxlxPTgzU/fbbbzN06NDQCPzFf906Ar9JXZEeGXuXwCj8LwDzgffCbq60YcMGHn/8cWrVqoWIUKtWLR5//PGY+zW4weDBgxk82PXTg3qO5dUZkeR1+fLlTJ06ld27dwOwe/dupk6dyvLlyxMRom/9+slr3HDDDVSrmgFAtaoZ3HDDDeX2mYqkrgoLC5k8eTJHHHEEIsIRRxzB5MmTYz6K6cRA3arKQw89VGLZQw895Jtxxuw7yz8iHYH/vvJu8QpEROqKyBQRWSUiK0WkUyzlOdWvwQ1Wr17tq34PbmF5dUZl8nruueeyceNGzj33XIejSh3FA183G/Z6aADs8kRSV04N0Fs8ZEb4QN3Dhg2LacgMEWHEiBEllo0YMaLCgW+9xL6z/CPS05SIyNHAZcCRqnqDiLQCMlQ1Xv+6Pga8paoXi0hVAldwRu3Ho8/ijAv60aD3TVRrfCy7169g05uP8fyTj8YnWhOz8iZfdvPEyxXNO2jTYsUmIyODpUuXcthhh9GsWTMyMjLYu9e109+mrPXr1zNr1qwSk48///zznHFG7PtovIfM6NmzJ08++SQvv/wyW7ZsoW7dumzevDkusRoTT5EO+toT+A8wF+hKoDN/I+BuoHesQYjIocBpBMYyQ1X3EBhgNmobp4+msLA9BQUFLJ+8kqxjM3n0yUet/4mLRDr5spsmXg6Pt/nwGdYAi6PiTuXFRy3K62Rukm/OnDnccsstoYnCzznnnJjL3LBhA6effjrdu3dHVRERunfvzpw5c6Iuc8CAAcyePTs0rtrmzZtJT09nwIABMcXatm1bPvvss9DjnJwcli5dGlOZJrVF+m03ErhEVc8F9gWXLSLQsT8ejgY2As+KyGIReUZEasVaaL9+/Vi2bBnN7pzOsmXLrCFmjIvt3r2bvLw8tm3bRl5eXqj/mHGX+vXr8/DDD5e46vHhhx+O+Ur1unXrMmfOHMaMGcP27dsZM2YMc+bMoW7dulGXWXyhwt/+9je2b9/O3/72txLLo1HcEAs/pf7ZZ5/Rtm3bqMs0JtLTlMeo6lvB+wqgqjtFJCOOcXQA8lT1QxF5DBgO3BP+JBEZDAwGaNq0aZw27T25ubnJDsGXLK/OqExeb7vttsBVfyYpIqmrmjVrsnXr1hJ1lZ6eHvOV6r/++isZGRkMHz6c2267jYyMDDIyMvj111+jLvOXX35h9OjR3HrrrQDceuut7Nu3L6b+bZ999hnNmzdn5syZNGrUiGrVqtG8efMSR8oSxb6z/CPSxtg6EclW1WXFC0SkHYFhLuJhPbBeVT8MPp5CoDFWgqo+DTwN0LFjR39cDhOFsWPHJjsEX7K8OiOSvFarVq3MI2E2h2BiRVJXZU3evm/fvpgndS8qKioxQ0q8+gtmZ2dX+Dgaa9asCd3fvXt3iceJZN9Z/hHpacrHgf+IyJVAuohcRGDC8Lj0hlfVHwg0+Iqva+4OrIhH2cYY9ytuiJ188sls2LCBk08+ucRy4z5OXfk6dOhQtmzZwtChQ2Muq0qVKlx55ZUlBqi98sorqVIl4mvXymVX/pp4iugTqar/lECv2mFAOnAfMFZVX4hjLHnAS8ErKb8Gro5j2b5y5ZVXAvDiiy8mORJ/sbw6I9K8NmnShE8++YQjjzySatWq0aRJE9atW5eIEE1QZfaB6dOn06hRo7jH8OSTT/Lkk0/Gpawabc/kl8Uz6HHOhezfvpW0WnXYv2Mrtdv748Ib+87yj4j/PQg/RegEVV0CdHSqfD+J9XSAKZvl1RmR5vWnn37izTffpHPnzixYsIDevWO+UNtUUmX2gbS0NPbv3x/660a/fvIaeXl5/POf/2Q3SkbRTq79859jnii8Ro0aJRqjNWrUYOfOnfEIuVLsO8s/IjpNKSJlDoEsIonvsWiM8aXdu3czduxYtm7dytixY+0Upctdd911bNmyheuuuy6u5cb79F9lBr2N1M6dO0vEmYyGmPGXSI+MNa7kcmOMqTSnTn2Z+Ivn6cRwbv8MiAiqekCcfhnV3yRHhY0xEbmr+Hlh94v9EbAOHcaYmGU0bMr+PbvZ9+uPoWXph/6BNk0aJjEqU57iBkl5j/2svPeZKu/fOONgpyl7Bm8ZYfd7ErjasRow0NHoTJk6depEp04xTd1pymB5dUYkeX3u8ZE0bVCTOXPm0PT2qYG/DWqSn5+foCgNRFZXVapUQVWpV68eS5cupV69eqhqXK5QhPheTemUk08+GVUN3Yqv/k00+87yjwr3HlXtBiAi41Q1LzEhmYN56KGHkh2CL1lenRFJXotnx8jLy+PbFSvJezOTgoICmzUjwSKpq9atW7Nq1So2b94cGnU+PT2d1q1bH+SVkXHq9Gc8ffTRRzzyyCMMGTKE8ePH89FHHyUlDvvO8o+IOvBbQ8wY4zSbvswbVqxYcUD/KBFhxYrYhoasUqXKAUfXylqWbCJC69atueuuu6hVqxZ33XUXrVu3tj5jJiaRXk35BxF5SkQ+EZHV4TenAzQHuuiii7jooouSHYbvWF6dYXn1jkjrqqioqMTVhOEj50erWrVqFBUVlThNWVRU5LpZGHr27Mny5csZOHAgW7ZsYeDAgSxfvpyePXsmPBbbt/wj0n85ngdqAROA7c6Fk5qaD59R5vI1I8semHDTpk1OhpOyLK/OsLx6RyR1papUrVqVpUuXcthhh9GsWTOqVq3Knj17Ytr29u3badiwYYnTlA0bNuTnn3+Oqdx4mzlzJjVadCgRZ/Xm7fm8/Y0Jj6Wi+irvd6W0OjXiNcW0iUWkjbGTgKNU9Tcng0lV4Y2u5sNnlNsIM8YYNyg+WlV8aq5atWoxN8YAfv75Z4YOHcpDDz3EiBEjXNt3bOc3iwD3fl+XFZNbYzUBkc5NuZ7AFZXGGGNS3O7du5k4cSK7du1i4sSJcRugt3r16lxyySXUrFmTSy65hOrVq8elXGPcLtIjYw8Bz4nI/wN+CF+hqhviHZQxxhj32rNnD6effnrcy61ZsyYDBw5k7dq1NGvWjJo1a7Jr1664b8cYt6lMnzGAs4Hike0keD893kGZinXv3j3ZIfiS5dUZllfviKSuatWqxfbtB3YdrlWrVszb3167Cdt+24wifPfbftJqN0E2b465XL+yfcs/Im2MtXA0ClMp99xzT7JD8CXLqzMsr94RSV1t376d2rVrM23atNCk7ueddx7btm2LadtnnHEGs2bNYujQobyW0YVz9v6XJ598kjPOOCOmcv3M9i3/iKgxpqprnQ7EmHhod98stu7ce8Dysq4sqlMjg0/vtS96YyqrSpUqJU5T1qtXL+YyZ86cSa9evRg/fjyqTzJehDPOOIOZM2fGXLZxr9qZw8l5bniEzwXw50UI5TbGROR2VR0TvF96XsoQVX3QicBM+Xr37g3Am2++meRI3Gfrzr0RXzFUuoFmeXWG5dU7Iq2rzaVOHZZ+HK3ihpdd+RcZP+xb21aOjPo7208qupoyvHdmz3JuPZwLzZRn586d7Ny5M9lh+I7l1RmWV++oTF1lZWWxdu1asrKyHI7KlMf2Lf8o98iYqp4Vdr9bYsIxxhjjdjVq1GD58uU0a9Ys9NgaBcZEz12TfhljjFds3QoDBsCkSVCnTrKjSaidO3dy+OGH89NPP3HYYYfxww8/HPxFxnick32SrTFmjDHRmD4dpk6F116DK69MdjQJ16BBAz788EPOOussa4yZlBBLn+SDscaYB5199tnJDsGXLK/O8G1eJ078/a9PGmOVqavw05QmOXy7b6Uga4x50O23357sEHzJ8lq+WA7P+yavPXrA7Nm/P65aNfD3vfcgOEcjwIvN2oFHrwSMtK5atmzJl19+iaoiIvzxj3/kiy++cDg6U1oi963yvgPgwO8BGzao8iJqjIlIvqoWlLF8hKo+FK9gRCQdWAh8p6rW5DfGJZw8PO8Z+fnwwQewY0fgcfHE2OETZNesyd9P7kvnxEeXMPXr1+err75izJgxDBkyhPHjx3PHHXdQv379ZIdmHGTfAc6KdKLwYeUsvyNegQTdBKyMc5m+07VrV7p27ZrsMHzH8uoM3+S1Wzd4/XWoWbPs9TVrwowZ/K9p28TGFUeR1FXaKYPQKtW57Y47qVWrFrfdcSdapTpppwxKTJAmxDf7lqn4yJiIHBm8myYiRxCYj7JYS2B3vAIRkcYEhtYtAG6NV7nGGBM33brB5MlwySUQPoF19eqB5V27wluRHxXw4owRG6ePprCwPQUFBSxfsZKszDbk5+fTr1+/ZIdmjGcd7DTlen6fGHx92HIB9gHxnBhrLHAnUDuOZRpjTHxt2QJVqkBaGlSrBrt3Bx5v2VLporx66qdfv37069eP5sNnsMyj/eOMcZODnaZsARwDbAOODrs1A2qr6sh4BCEiZwM/qeonB3neYBFZKCILN27cGI9NG2NM5UyYEOg31q4dTJsW+Ltjx+9XVxpjTCVV2BhT1bWqukZV6wbvF9/Wqequil5bSacA54rIGuBl4HQRebGMeJ5W1Y6q2rFRo0Zx3Lxxha1b4YILAn+Ncas6deDhh2HhQujZEz7+GEaPhkMPTXZkxhiPinhoCxHpBHSk1GnEeEwUrqojgBHB7XQFbldVfwzc44BLL7002SE4I8mDaPo2r0nmu7xOnVrycXo63HZb4OZxvqsrn7P68o9Ih7b4f8BdwBJge9gqBWJujJnKuf7668tdF2mHYLd0Bi4hyYNoVpRXEz3Lq3dYXXmL1Zd/RHpkbAjQWVU/cjIYAFWdB8xzejtetiM4zlHNMi6xj7RDsBs6A7/4cj6MChtOLsmDaFaUV69w48CMfshrqrC68harL/+ItDEmBAZjNS5w1llnATBv3rzkBhKjf3TqS+eNX7hmEE0/5NWNV+f5Ia+pwurKW6y+yubGf0oPJtLG2DPAIOCfDsZiUswHzdoGBtE8++zfG2ThigfRfGv7get8wItjTBljjNu58Z/Sg4m0MXYicLuI3Ah8H75CVe0XwkQvzoNoeokXvzCMMcbEX6SNsfnBmzHxF8dBNI0xxhiviagxpqr3OR2IcS/Hz7+HD6I5ahQMGwaffhqfqyq3boUBA2DSpMD4UMYYY4zLVGacsaOBy4AjVfUGEWkFZKjqcseiM2UaMGBAQrfn+Om04kE0b745cHTs9NNh7FiYH4eDsZUYuyzReU0Vic6rE33xamcOJ+e54RFtv3YmBKbZ9R7bB7wl6fVl/+zGTaTjjPUE/gPMBboCNwCNgLuB3k4FZ8qW9B0w3pwcRLMSY5f5Lq8u4Yd/HratHJkS/ftsH/CWpNdXkgfq9pODzU1ZbCRwiaqeS2CCcIBFQAdHojIV+vnnn/n555+THYYrvfhyfmCMsuLb++8HVhSPXVZ869HjgNdaXp1hefUOqytvSXp9hf+za2IS6WnKY1T1reB9BVDVnSKS4UxY7uK2IQguvvhiwMaWKUukY5dx991QasgMy6szLK/eYXWVeLHMmpLw+urRA2bP/v1xkgfq9pNIG2PrRCRbVZcVLxCRdsAaR6JyGRuCwDsiHbvMz0NmGGO8w0uzppCfDx984JqBuv0k0tOUjwP/EZErgXQRuQh4EXjUsciMiVbx2GXVq5dcHj52mTHGmMrp1i3wz2550y8VD9TdtG1i4/KBiBpjqvpPYAwwDEgH7gMeU9UXHIzNmOiFj11Wo0bgr41dZowxsbF/dh0R6ZExVPVpVc1R1UNUNVtVn3EyMGNiEj522bRpgb87dlhHU2OMiZX9sxuwdStccEHgb4wiHdriGWCiqr4f8xZNzIYOHZrsENwvirHLLK/OsLx6h9WVtyStvpwcqNtL4ji0R6Qd+DOAmSLyHfAs8Lyqfn+Q1yScF2dqj0bfvn2THYL7RTF2meXVGZbXxIrle9DqyluSVl9ODtTtJZUYx/JgIp0Oqb+IXA/0BQYAfxWRWcAEVf2/mCKIo1S56nHdunUANGnSJMmR+EtFeY3l8vNUZ5/XSojDiOaxfA9aXXlL0urLyYG6XezFl/Nh1Nm/LyhnaA+6d4eOt1Sq7IinQ1LV7cBEYKKIHEPgCsspBDr0m0qK5b/Xq666CrCxgOKtorx66vLz0pyYsqQSZdrntRKSPKK51ZW3WH0lVizjWB5MxI0xABFpCFxJ4OhYS6CwUlszIalyFM+4gBM/8F6aBsVL8+fF8bRHKrMj2cYJTo5jGWkH/nOAq4GzgMXAE8DLqvprpbZmjCmfU40GJ37gvdRocHHD0cnTHqnM00eyjbsVD+1xySWwa9fvy2Mc2iPSI2NPAS8Auaq6KqotmeRw6gfeS0cbvCJejQYHpixxotGQsAtuXNxwdPK0hzHGIeFDe1SrBrt3xzy0R6TjjDVR1WHWEAuK49gijgv/gfdCuaksXpPu5ueXHCG7gilLIvWPTn0jKpO77464zOKjF5Hcymu0lSWWyeITLXTa4yAjmttAmsbEKJ6/2w6MYxnpCPz7RKSziDwtIq8BiMhxInJa1Fv2siQ3RG677TZui/SqlXj9wCeq3CSqVF7jwLFGgwNTlsTSaEh0Xp1oODrKRSOaJ7quTBkq0Wiw+qqEeP5uFw/tsXAh9OwJH38Mo0fDoYdGXWSkfcYuB/5OYD7K4gaYAvcDXaPe+u/lNwGeBw4H9gNPq+pjsZbrmCSf9jjnnHPKXRfp6aTKnKICHDn15YTamcPJeW54hM8F+D3WivLqBEdPUUXSr6GyE6VH2Vci0Xn15GTxDpz2iEai68pLIr0oAGI8rV6J7gpWX5UQz99tB4b2iLTPWD5whqouFJGrgsuWAVlRb7mkIuA2VV0kIrWBT0TkbVVdEafyY+K2Traff/45AK1btz5gXaQ/8H8/uS+dK7PR/Hz44IP4lxtn21aOjPoq1Yry6gTHGw1O/MBHUWai8wo41snWMU6PaB5hH8+k1FUiJHn8tkqpRKPBt/UVDx45gFAs0sbYkaq6MHhfg3+LiNMYY8HR/L8P3t8mIiuBo4DYG2Nx2And1sn2uuuuA8oeWybSH/j/RXOkJc7lRnoUq/QRLKdUlFfHONlocOIHPooyK5XXeF4Y4pKjTRFxekTzCI+2JHofSNhFHD64orasRkNSvrO8wiMHEIpF2hj7SkROLjU35cnA5/EOSESaA+2BD8tYNxgYDNC0adPICozDTui50x5OnKJyoNxIj2L5/vJzpxoNTvzAu6TREBEvzZ/n9IjmLr2i1I1HmxLNsbMZqc6BAwhOivRqygeAaSJyN5AhIrcRGPD1/ngGIyKHAK8CN5c1hpmqPq2qHVW1Y6NGjSIrNF4dzV3UyTYi4T/wNWoE/sbjB96pclOZA1fmAIEf+FtvDdQR/P4DX/qHP9llhovnhSEOdLL1jB49Iro45MWX85Mbp0MivTjGDe8/0otjKnPBjQny0O92pFdTTgUuB04E1gKnAwNV9c14BSIiGQQaYi+p6n+iLsjJLyEvNUSc+oF3qtxUZo0GZ/ZXpxuObubA8CZeEukVtTG9/3gOleChRoPneOR3O9IjY6jq26p6jqpmq2ofVX07XkGIiAATgJWq+khMhTn5JeR0QySeO7dTP/Cp3HAIF8+6skbD749TrNHgGAeGN2l33yyaD58RuuXc8gpP/ecBcm55pcTydvfNii32OOxbCTnaFO8hjjzSaPAcjxxAqNTclA46BbgK+ExElgSX3aWqb1S6JCfPE7ukv8zdkYyP5FQfFKf7tiRRRHkt5uIOwW5TYV491q/DU6Lo41lRXR3Qv+uFF2Ds/+h13B648tLQ4pj7eMZr33Kq72yxePdDi6KPY6W+s1KV07/bcRLxkTEnqeoCVRVVbauqucFb5RtixZw65OuS/jI9evSghwtGD/ebSuXVh4PeOuWgebVTNM6p5NEWV+wD8Sw3jkebHJ/ZIYqzDvZbEAGPnHlwRWPMEV445Btlf5klS5awZMmSxMfrcxXl1Usdgt0mos+rF/ZXL6rkKZqk7ANO9huM4ykqx2d2iKLRYL8F/uHfxpgXzhNH2V/m5ptv5uabb3Y+vhRTUV4T0iHYpyL6vHphf/WiSh5tSco+4GS/wTj2cXXjPKL2W+AfkU6H9AwwsdQ4Y+7mhfPE1l/GMxwbTNcExHF/jWVKrERzPNY49vH00oDSIfHu4+q1mR2MZ0TagT8DmCki3wETgReCo+a7l1c6mjvdydTEjwfqyksNkRLiuL/GMiVWonkpVsAzA0o7ykszOxjPiKgxpqr9ReR6oC8wAHhARGYBE1T1/xyMLzXYzu0dLq8rz/24e0ik+apTI8PhSJLMqX3A5ftWiJdmdjCeUZlxxrar6kRVPQ3IBASY4lhkqcT6y3iH1VVKWjOyT5m3stZFPX+iV6T6gNI21qJxQKXGGRORhsCVBI6OtSQwJZKJVSX7yzz44IPlFuXE5NuePfVVSRXlNcQLfRFdJqK8GldI6j7glX3LRV1gbN/yj0g78J8DXA2cBSwGngBeLmv+SBOFSu7cJ598crlFOTH5dqqc+qooryFx/CJOlUZuRHk1rpDofSAh5fqY7Vv+EemRsaeAF4BcVV3lYDyu5LYfzfeD4/DYjhhfic5rqjRyy8ur2/Yr45/vFifOEDglllj9Ul8m8sZYE1Xd52gkceDUl7vbfjTvuusuAObNm+f4tlKJ5bV8sexb5eXVbfuVXzhRV17jxBkCp8QSq1/qyyuc/Acy0qsp94lIEyAXqF1q3b8i3prD7MvdGGfYvuUdVlfGOMPJfSvSPmODgb8DW4DwkfcUcE1jzEvsFI0xxhhjIPLTlPcAfW1Msfix/16NManO/ik1JiDSxtgh1hAzxhgTT/ZPqXGCFxv5kTbG/i0ifVTV9gYXGDt2bLJD8CXLqzMsr95RUV156QpFJ7jxB94v+1a8Z7fwYiO/3MaYiDwd9rA68IqIzAFKzEmpqoMdis2UIzc3t8L1kXy4/DxlS7Q7dkV5TfUfolgc7PNq3KOiuvLSFYpOcOMPfCL3Lacao2XltPnwGRHn2i8qOjIW/ku1D3iljOUmCd555x0AevToccA6r32w491wjOX9V5TXVP8hikVFeTXuYnXlLYmsLzc2Rv2k3MaYql6dyEBM5B544AEgsV+YTkyS7LaGYzLy6gS3TWjtl7w6xU31ZXXlLVZf/hHx3JQicghwNtAYWAe8oarbnArMbdz0hZlo5TWO3HzEzSvi/blysq5SeR9witv+ITHGJEek44x1BN4AdgLfAk2BcSJylqoudDC+SkuFIzjGH7z0ufJSrMYY4zWRHhl7Avibqo4qXiAidwJPAsc7EVg0vHYEx440mFTn1D6Q6vtWqr9/8NaFTF6K1Tgj0sZYJvC3UsseITAYbFyIyJnAY0A68IyqjoxX2W7ktYajsS/MeHNqH0j1o3jJPlXthoazU58BL8Wa6rz2j16kjbElQHbwb7GcUo+jJiLpwD+AnsB64GMRma6qK+JRvt889dRTyQ7BlyrKq31hRs8+r96R6H3AS/+UuvE7wPatsnnxHz1R1bJXiFwe9vAY4DrgGWAt0BwYCDytqn+NOQiRTsD/U9VewccjAFT1ofJe07FjR124sPzuauW1XmNJWkUtYifKtVgt1lSP1akjGW6L1Q915VS5Fqs7Yq3McBV1amTw6b1nVKr8irbhhvcfj3JF5BNV7VjmugoaY99EEI+q6tERPK9CInIxcKaqXhN8fBVwoqreUOp5g4HBAE2bNj1u7dq1sW7ak1577TUAzjnnnCRH4i+WV2dYXr3D6spbrL68JarGWCKJyCVAr1KNsRNUNa+81xzsyJifde3aFYB58+YlNQ6/sbw6w/LqHVZX3mL15S0VNcbSEh1MOdYDTcIeNwY2JCkWY4wxxpiEcUtj7GOgpYi0EJGqwGXA9CTHZIwxxhjjuIhH4HeSqhaJyA3ATAJDW0xU1eVJDssYY4wxxnGuaIwBqOobBEb5N8YYY4xJGa7owB+NVO7Av27dOgCaNGlykGeayrC8OsPy6h1WV95i9eUtrr+aMhqp3BgzxhhjjLd44WpK1xswYABTpkwBYP78+WRlZZGbm8vOnTsTHsvkyZOZPHlywrcLsHjxYq655hoApk2bRtu2bcnNzaVjx44sWLCgwtdedtllfPHFF4kIMypuyetLL71E27Ztadu2LSeffDKffvppha+1vJYvPK+rVq2iU6dOVKtWjTFjxpT7mgEDBtCiRQtyc3PJzc1lyZIl5T5348aNnHnmmfEOO2m8VlfF3L4POCXR9bVw4UJuvPHGctdv2LCBiy++OGHx+IqqevJ23HHHaSL1799f//3vf6uq6nXXXacTJ05M6PbDdenSRbt06ZKUbV988cW6ZMkSVVXdtm2b7t+/X1VVP/30U23dunWFr503b55ec801jscYLbfk9b333tNffvlFVVXfeOMNPeGEEyp8reW1fOF5/fHHH/Wjjz7Su+66Sx9++OFyXxO+r0diwIABumDBgphjdQOv1VUxt+8DTklmfZnKAxZqOW2alD4ytn37dvr06UO7du3Izs5m8uTJfPLJJ3Tp0oXjjjuOXr168f3335d4zTPPPMMrr7zC/fffzxVXXJGkyCv2/PPP07ZtW9q1a8dVV13F2rVr6d69O23btqV79+58++23AOUuHzBgAEOGDOHUU0+lVatWvP766wBs27aNpUuX0q5dOwAOOeQQRAQI5DL8fum8Apx66qm88847FBUVJTQf8ZKovJ588snUq1cPgJNOOon169cDltdY83rYYYdx/PHHk5ER3WTu//3vf0NHy9q3b8+2bdsAOP/883nppZdiTYMnJLuu/LoPOGHNmjVkZmZy7bXXkpWVxRlnnMHOnTtZsmQJJ510Em3btuWCCy5g8+bNB7y2b9++vPHG79fTDRgwgFdffZV58+Zx9tlnA2XvD2vWrCE7OxuAXbt2cfXVV5OTk0P79u2ZO3cuAJMmTeLCCy/kzDPPpGXLltx5550JyIYHlNdKc/stHkfGpkyZUuK/qS1btminTp30p59+UlXVl19+Wa+++mpVLfnfcmX/c463iv4bWrZsmbZq1Uo3btyoqqqbNm3Ss88+WydNmqSqqhMmTNDzzjtPVbXc5f3799devXrpvn37dPXq1XrUUUfpzp07dc6cOXrhhReW2N5//vMfbd26tdarV0/ff/99VS07r8V69OihCxcujDkHTnBTXos9/PDDOmjQIFW1vMYrr/fee+9Bj4y1atVKc3Jy9Oabb9Zdu3aFtl98BGzbtm26d+9eVVVdv369ZmdnVy4pLuX2uvLqPuCUiurrm2++0fT0dF28eLGqql5yySX6wgsvaE5Ojs6bN09VVe+55x696aabDnjtf/7zH/3Tn/6kqqq7d+/Wxo0b644dO3Tu3Lnap08fVS17f/jmm280KytLVVXHjBmjAwYMUFXVlStXapMmTXTnzp367LPPaosWLXTLli26c+dObdq0qX777bfxSomrYUfGypaTk8M777zDsGHDmD9/PuvWrWPZsmX07NmT3NxcHnjggdBRCa+YM2cOF198MQ0bNgSgfv36fPDBB1x+eWDe96uuuirUt6u85QCXXnopaWlptGzZkqOPPppVq1bx/fff06hRoxLbu+CCC1i1ahVTp07lnnvuAQ7Ma506dULPP+yww9iwwXuTKyQ6rwBz585lwoQJjBo1CrC8xiuvB/PQQw+xatUqPv74Y3755ZdQ/k855RRuvfVWHn/8cbZs2UKVKoGRgbya+8pyQ135cR9wUnHfR4DjjjuOr776ii1bttClSxcA+vfvz7vvvnvA63r37s2cOXPYvXs3b775Jqeddho1atQo8Zzy9odiCxYs4KqrrgKgTZs2NGvWjNWrVwPQvXt36tSpQ/Xq1Tn22GNJ1Xmmw6V0Y6xVq1Z88skn5OTkMGLECF599VWysrJYsmQJS5Ys4bPPPmPWrFnJDrNSVDV0urA85a0PX176OSJCjRo12LVrV5mvPe200/jqq6/4+eefD8jr/fffH3rerl27DtipvSDReV26dCnXXHMN06ZNo0GDBsCBn1fLa/Sf14occcQRiAjVqlXj6quv5qOPPgJg+PDhPPPMM+zcuZOTTjqJVatWAd7NfWW5oa78uA84qVq1aqH76enpbNmypczn7du3L3TK8S9/+QvVq1ena9euzJw5k8mTJ3PZZZcd8Jry9odiWsFIDaXjstPLKd4Y27BhAzVr1uTKK6/k9ttv58MPP2Tjxo188MEHAOzdu5fly903EcCUKVNCV3aW1r17d1555RU2bdoEwC+//MLJJ5/Myy+/DASu1OvcuTNAucsB/v3vf7N//36++uorvv76a1q3bk1mZiZffvll6DlffvllaIdbtGgRe/bsoUGDBgfkddGiRaHXrF69mqysrDhmI37cktdvv/2WCy+8kBdeeIFWrVqFllteY8trpIr7iaoqU6dODfWB+eqrr8jJyWHYsGF07Ngx9OOzevXq0HO8zu115dV9wCkV1VdZ6tSpQ7169Zg/fz4AL7zwAl26dCE9PT10EKK4gXvZZZfx7LPPMn/+fHr16nVAWeXtD8VOO+20UF/K1atX8+2339K6deto36rvuWYE/mT47LPPuOOOO0hLSyMjI4Mnn3ySKlWqcOONN7J161aKioq4+eabXbeDF58mKEtWVhb5+fmhHax9+/Y8/vjjDBw4kIcffphGjRrx7LPPApS7HKB169Z06dKFH3/8kfHjx1O9enXatGnD1q1b2bZtG7Vr1+bVV1/l+eefJyMjgxo1ajB58mREpMy8Avz444/UqFGDI444wtkERckteb3//vvZtGkT119/PQBVqlRh4cKFltcY8/rDDz/QsWNHfv31V9LS0hg7diwrVqzg0EMP5ayzzuKZZ57hyCOP5IorrmDjxo2oKrm5uYwfPx6AsWPHMnfuXNLT0zn22GPp3bs3EDid3KdPHwczmDhuryuv7gNOqai+yvPcc88xZMgQduzYwdFHH12ibsKdccYZ/OlPf+Lcc8+latWqB6wva38Iv+Dt+uuvZ8iQIeTk5FClShUmTZpU4oiYKckGffWgSZMmAYErXJwwYMAAzj777DLHi3n00UepXbt2aDygynj00Uc59NBDGTRoUDzCjDvLqzO8mtdInXbaaUybNi10BayXebWu3L4POMXp+jLxZYO++sykSZNCO2GiDR06NOr/burWrUv//v3jHFH8WF6d4dW8RmLjxo3ceuutvmiIgXfryu37gFOSWV8mvuzImAd17doVgHnz5iU1Dr+xvDrD8uodVlfeYvXlLXZkzBhjjDHGpawxZowxxhiTRNYYM8YYY4xJIusz5kE7duwAoGbNmkmOxF8sr86wvHqH1ZW3WH15S0V9xlJ6nDGvsh3PGZZXZ1hevcPqylusvvzDTlN60BNPPMETTzyR7DB8x/LqDMurd1hdeYvVl3/YaUoPssuZnWF5dYbl1TusrrzF6stbbGgLY4wxxhiXSnpjTEQeFpFVIrJURP5PROomOyZjjDHGmERJemMMeBvIVtW2wGpgRJLjMcYYY4xJmKQ3xlR1lqoWBR/+D2iczHiMMcYYYxLJbUNbDAQml7dSRAYDg4MPfxORzyMstyHwc4yxJaLMSpUrInEvs5K8ktdKlZnkvPq2riyvSS2zUuXad0tSy6x0uRHWlytiTWKZTpVbmTKblbciIVdTisg7wOFlrMpX1WnB5+QDHYELNc5BicjC8q5gcFOZTpWb6rGm+vt3qlyL1WJN9VhT/f07VW4qxpqQI2Oq2qOi9SLSHzgb6B7vhpgxxhhjjJsl/TSliJwJDAO6qOqOZMdjjDHGGJNISe/AD/wdqA28LSJLRGS8A9t42iNlOlVuqsea6u/fqXItVos11WNN9ffvVLkpF6tnR+A3xhhjjPEDNxwZM8YYY4xJWdYYM8YYY4xJImuMGWOMMcYkkTXGjDHGGGOSyPeNMRGp71C558a5vD+KyEUicmwcyqoSdv8QEenoYB4OcaLceIrHe0/UBPbx/lyFlRvX+o/X59XJvIrIH0Skg4i0F5E/OLidlNgHEsX21/jWl4jcJCKHSsAEEVkkImfEsXxHchAs++o4lhXzd5ajnytV9c0NOAVYCSwHTiQwCfnXwDqgUwzlXljqdhHwQ/HjKMucCzQM3r+KwCTpzwCfAXkxxDoA2BQsr3fw/c8O5qCfAzn/NobX5hCYj3QdgcuD64Wt+8hln4Ei4B1gEFA3TrmL++cqWO7dYfePDX4WvgHWACe67PPqRF5zg5+rlcGy3wFWBZd1iMc2Sm0vVfYBL8Wa0vtrqfI/Df7tBUwH2gGL3JSDCrYXy74V9+8sJz5XobLjnbxk3oCPgl8YnQjMFdU5uLwD8F4M5RYBrwMTgWeDt23BvxOjLHNZ2P2PgQbB+zWBpTHE+hmBubJaAL8CxwSX/yHacoFby7ndBvwSQ6wLgDOBusDtwS/k4ngXu+wz8BmBWSJeItDYnQZcBtRw0+cqWO6isPszgN7B+ycA77vw8xrvvC6hjB8x4CSCP0xRlGn7gLdiTen9tVT5S4N/HwMuiLG+nPgtXFrO7TNgdwzvO+7fWU58rkJlx1qAm27hHzBgZal1Uf0nEHzt8QSOLg3l97HZvok1VuCo4P25QPXg/XRgeQzlLgm7v6HUumg/gLuAvwL3lnHbEo9Yg4+7AV8Q+NGM9j83pz4D4V+YNYBLgf8Ed8h/ueVzVUasi0utWxxlmU59Xp3I6xcVrPsyyjJtH/BWrCm9v5Yq41lgVrCuahIYZP0Tt+QA+JHA0exmpW7NKfUbVtnPVry/s5z4XBXfkj4dUpyF94EbUWpd1WgLVdWPRaQnkAfMEZFhgEZbXtAtwCwReZXAf5hzROQt4FQCO0+0vhWRhwjscKtE5G8EPiw9gO+jLHMRMFVVPym9QkSuiTpSEBGpo6pbAVR1rohcBLwKRNtnwpHPACDFd1R1J/AK8IqI1AHOj6ZAhz5XAEeLyHQCMTcWkZr6+1RjGVGW6dTnNe55Bd4UkRnA8wROdwE0Af4EvBVlmbYPeCzW4jspur+GG0SgsfO1qu4QkQZAVH2xHMrB68Ahqrqk9AoRmRdDuU58ZznxfRUoONjC84VgR8J3tNQclyJyDHCRqo6OwzaOBMYCHVX16BjLqgNcDrQiME/oemCaqq6KocxDgT8T2EH+TqCfwNXAWuABVa10g0xEWhM4FbOxjHV/UNUfo4z1cgJfEP8rtbwpcI+qXhtFmY58BkTkdlUdE81rIyw/np+rLqUWfaKqvwU7sV+sqv+IslwnPq+O5FVEegPnAUcR+AJdD0xX1TeiLM/2AW/FmvL7a1j5zwPzgfmx7KtllHsU8ChxyEGE26unqpsr+Zq4fmc5+bnyVWPMGGOMMb8TkdOBzgSOCB1NoE/lu6r6WDLjqiwRWaSqHZIdh1N8P7RFMRFxZOJRJ8qNpUwRSReR60TkryJySql1d0dZZpVgmW+JyFIR+VRE3hSRISIS9WF0J8r1UqwH2V5KfF6dKLfUPnByqXXR7gNx36+cKtepWA+yTd9/rhJdZrzKVdU5QAFwD4GrCTsS6PMVTTwJ/2yFb6JST/79O/tNt/++gM+OjEn5Y7MIgauoGrulXAdjfYZAJ82PCFzO+19VvTW4Lqr/LESkENgCPEfgMC9AY6A/UF9V+0YZa9zL9Vis9nl1JlYn9oG4l+nBWFP9c+WZWEuVPxuoBXxA4HTlAlX9KcqyHPlsRbjtSpXvpd8X8F9jbB+BvlHhLWgNPj5KVaPqEOpEuQ7GulRV2wbvVwGeIDDURT/gf6raPooyP1fV1uWsW62qraKMNe7leixW+7w6E6sT+0Dcy/RgrKn+ufJMrKXKfxQ4DtgNvAe8C3wQ7IBe2bIc+WxFuO3KNsY88/sC+O5qyq+B7qr6bekVIrKujOcns1ynYg3tuKpaBAwWkb8Ac4BoRwrfLCKXAK+q6v5gjGnAJUClOlQmoFwvxWqfV2fKdWIfcKJMr8Wa6p8rL8Uaoqq3BMs6hMDFXM8ChwPVoijOqc9WJCp1mhJv/b74rs/YWKBeOetiuZLSiXKdKBNgoYicGb5AVe8nsAM2j7LMy4CLgR9FZLWIfMHvoy5fFkOsTpTrpVjHYp9XJ8p1Yh9wokynynUq1rGk9ufKiTKdLBcAEblBRCYT6Lh/PoEBW3tHWZxTn60Syjl1272SxZT1nf0j8f99ibVMwGenKY3zJDBGjajqz24v10uxGmOME0TkDgKnJj8JHs0qvb7SQ0bEk4jcraoPBO8fC0wlML6aAH1V9cM4bMP1vy9+OzKGBCZEPaaM5W3dVq7HYj1cRA5X1U2Bh3KhiGTFEqdT5Xox1uD9RvGONZ7lpnqsqf7+y9nOgx4ps0UwB21iKKOpiFQP3hcRuVpExonIUAn0n3JVucVU9WFV/bCshljQ7GjLjkdeCRxVKvYwcJOqtiAwuv2jMcQWyivwC3BOrHl1oswQjXGqBTfdCFTeBgKHY5cDx4eti2VqjbiX67FYr+P3iWuHAh8SONT9OTAohljjXq7FarF6pUwPxvp4qds4AleWPQ487pYyg+VODbt/XjAfzwZzMCDKMpcBNYP3RwFTgCuDuY1lbkpHyq3E9hcnOa+OTAflRF6drCtHKznRNwINkCOC908AVhGcST7GSo17uR6L9TMClzM3AH4DDg8ur0ep+eqSXa7FarF6pUwPxroeeJHAtFL9g7eNxffdUmaw3MVh998HWgTvNyT6yeJXhN3/BEgLexxVmU6WW4ntR/xPukN53QJMB14L1n3NsHXLoinTqbw6WVd+u5oyXYPT/ajqRyLSDXhdRBoT2/xZTpTrpVj3amC6kh0i8pWq/hAsf7OIxBKrE+VarBarV8r0WqyZBCZLPxO4Q1W/E5F7VfU5l5UJJb/rqqjqNwCq+rOI7I+yzHUicroGBlFdQ2C+07US6DsUC6fKdYITeT2v1OM0AAlMB/VklGWCM3l1rK781hjbJiLHqOpXAKr6vYh0JdAhMJb+Ek6U66VY94tIhqruBfoULwyeO4+l36ET5VqsFqtXyvRUrKq6DbhZRI4DXpTAZOwx9Tt2osygdiLyK4FO4NUk0NfzBxGpCqRHWeY1wPMi8v+ArcASEVlM4IjjrTHE6lS5karMkBFxz6uq/rec5T8CsczL6UReHasrX11NKSLtgB2q+kWp5RnApar6klvK9VisTYHvg1/u4cuPAjJV9Z0oY417uRarxeqVMr0Wa6myBLge6KSqV8ZanlNllrGNugRy8EEMZWRScuLpjzU45lSMsTlSbgTbra+qv8RYRl2izKsEOr4PIjDsxlEEjr5tAKYBE0p/jqMoP+55daRMPzXGwklgnBLVOF+y60S5FqvFarF6J9ZUf/9OlWuxxrdcEckB/kmggfMmMKy4XBH5SFVPcEOs4uAUQ2HbcP9nQB3uHJjIG9AUeJlAJ8AvgC+Bn4LLmrupXIvVYrVYvRNrqr9/i9VbsQbLXUCgL15d4HYCV9cfE1y32C2xAp9XsG61m/LqVF2pqu8aYx8AfQl0Yi9elk5gZNz/ualci9VitVi9E2uqv3+L1VuxBstYUupxNwINiJOIfpgjJ/L6PwLTCYVfmZgW3M6HbsqrU3Wl6r/G2BfRrEtGuRarxWqxeifWVH//Fqu3Yg2+9lOgTqllbYMNsk1uiZXANEqTCRxtWh2Mb2NwWQs35dWpulL139AWn4jIEwTOPRdPsNqEwLnnxS4r12K1WC1W78Sa6u/fYvVWrBAYlDSTwJEnAFR1qYh0B+5xS6yquobA0SYkvlMMeekz4K8O/MHLawcRGLfkKAKX364nMKDcBFXd7ZZyLVaL1WL1Tqyp/v4tVm/F6hSH8toU+ElVdwWvqB0AdABWAP/U8qdxSkasjtWVrxpjxhhjjAERSScwLlZj4C1VfS9sXWhy7mQTkWXACaq6Q0RGAccQGBfzdABVHZjE8BLGV42xYKv6EgLjlEwhUJnnEZgSaLxGOQ6IE+VarBarxeqdWFP9/Vus3oo1WO4zBKbF+gi4Cvivqt4aXLdIVTu4IVYRWaGqxwbvf0JgPuX9wcefqmq7ypbpYKyO1BX4rzH2BHAYUBX4FahGYL6rs4AfVfUmt5RrsVqsFqt3Yk3192+xeivWYLlLVbVt8H4V4AkCc0j2I3DlX3s3xCoiM4FRqjpHRF4FblXVtcH+Y3NiaIx55jMA+O5qys+CfzOATUDV4OMqxevcUq7FarFarN6JNdXfv8XqrViDr19VxrK/AO8R/dWETuS1CTAXeJdAw2YzMIdAh/jubsqrU3Wl6r+rKYsAVHWviHysqnuCj4tEZJ/LyrVYLVaL1Tuxpvr7t1i9FSvAQhE5U1XfKl6gqveLyAain4A77rGq6jqgm/w+xdAk4jPFkJc+A3GZkNVNfhCRQwBU9czihSJyOLDHZeVarBarxeqdWFP9/Vus3ooVVb0yvCEWtvwZVc2IslincoCqrgTmEzg1+WGMDTHw1mfAX33GyiMitYBaqvqT28u1WC1Wi9U7sab6+3eqXIs1/uWKSAugPbBCVVfFo8ywsqOOVQJDW4wGuhOYo1KAQwmcqhyugXHI4satnwG/HRkDQERKtPpVdTsQayvbkXItVovVYvVOrKn+/p0q12KNf7kiMjXs/nkEGjfnANNEZEC05QbLi2esk4H/Aw5X1Zaq+kfgCALDW7wcS5zgnc+ArxpjItJNRNYDG0Rklog0D1s9y03lWqwWq8XqnVhT/f1brN6KNahZ2P1hwOmqejVwCnBLNAU6FGtDVZ2sqqE+V6q6T1VfBhpEWaanPgOA766m/BjICt6/mOCkqMHHi91UrsVqsVqs3ok11d+/xeqtWIOvXRR2/6NS61yTAwJHv54ATgSODN5ODC57xU15daquVNV3jbFPSz3OAj4HLiDKWeqdKtditVgtVu/Emurv32L1VqzBcvYRGAtrG4HO5YcHl1cFlrol1mA8Q4G3gM+AZcH71wPV3JRXp+pK1X+NsYXFH7iwZY2BJcA2N5VrsVqsFqt3Yk3192+xeivWg2yvLtDJC7HG+D498xlQ9V9jrAfQrozldYF8N5VrsVqsFqt3Yk3192+xeivWMsqqD9SLQzlO5FWASwlMMyQErqp8nMCRsTSXxepYXaXE0BbGGGNMKpEEDxkRLXFyiiEviUer2y03oA4wksCknZuCt5XBZXXdVK7FarFarN6JNdXfv8XqrViD5X4A9AXSw5alA5cRmJvSFbHi3HRQnvkMqKq/hrYAXiEwr1VXVW2gqg2AbsFl/3ZZuRarxWqxeifWVH//Fqu3YgVnhoxwItbQFEMEpkAKTTFE4CKEaHnpM+C7I2OfR7MuGeVarBarxeqdWFP9/Vus3oo1+Nq4DxnhUF7fBA4pY/nhlBqSwwWxOlJXqv47MrZWRO4UkT8ULxCRP4jIMGCdy8q1WC1Wi9U7sab6+7dYvRUrwJ8IDBVxHzCTwKCk9xEYOuIqt8Sqqr1V9bcyVm0Dzo4yTvDWZ8B3jbG+BA6//ldENovIL8A8AleSXOqyci1Wi9Vi9U6sqf7+LVZvxYqq7lHVJ1X1TFXNUdXs4P0nVHW3m2IFR6YY8tJnwF+nKYOHCtsQuPz0kFLLz3RbuRarxWqxeifWVH//FqvnYnVqyIi4xkqgz9V6YCOBo3fNw9bFNpCqlz4DsbzYbTfgRgKj4U4F1gDnxaNSnSjXYrVYLVbvxJrq799i9Vaswdc+AUwBpgMvEuhg/icCfckec0usODcdlGc+A6r+a4x9RrC1CjQnMFruTXGo1LiXa7FarBard2JN9fdvsXor1uJyg3/jNmSEQ3l1ajooz3wGVJUq+Eu6BjsCquoaEekKTBGRZgQO07qpXIvVYrVYvRNrqr9/i9VbsULYkBEiUmLICBGJdsgIJ2LdKyKHq+oPwXKXi0h34HXgmCjLdCpWp+rKdx34fxCR3OIHwaSdDTQEclxWrsVqsVqs3ok11d+/U+VarM6We0iwzDOLF4rI4QQmDo+2zNziB3GKdTjwh/AFqroe6EpgMNVoeekz4LvTlI0pNYln2LpT3FSuxWqxWqzeiTXV37/F6q1YD7K9WsBhXog1xvfpmc+Aqs1NaYwxxviWiGRoYHT78GUNVfXnZMUUTkTqACOA84FGwcU/AdOAkaq6JTmRJZbfTlMaY4wxKU9EuonIemCDiMwSkeZhq2clKayyODfFkIdYY8wYY4zxn9FAL1VtBDwNvC0iJwXXxdTZPM6aq+ooDXbgB1DVH1R1FNA0iXEllDXGjDHGGP+pqqrLAVR1CoHTgM+JyAWAm/onrRWHphjyEmuMGWOMMf6zN3jlJBAYMoLAKPz3Ai2TFtWBnJtiyEOsA78xxhjjMyLSA9ioqp+WWl4X+LOqFiQlsDKISBsCVyr+T8MmDReRM1X1reRFljjWGDPGGGNMUojIjcCfgZVALoER7acF1y1S1Q5JDC9h/DYCvzHGGJPyPDRkxLXAcar6W/CKzyki0lxVH8NdFxo4yvqMGWOMMf7jlSEjSkwxRGDk/d4i8gjWGDPGGGOMh3llyAjnphjyEGuMGWOMMf7jlSEj/gT8EL5AVYtU9U/AackJKfGsA78xxhjjMyJSj8Ak3OcRmIhbgR+B6cAoVf0lieGZUqwxZowxxviQDRnhHXaa0hhjjPGZ4JAR04AbgGUicl7Y6geTE5Upjw1tYYwxxviPDRnhIdYYM8YYY/ynxJARItKVQIOsGdYYcx07TWmMMcb4jw0Z4SHWgd8YY4zxGRFpDBSFjzMWtu4UVX0vCWGZclhjzBhjjDEmiew0pTHGGGNMElljzBhjjDEmiawxZowxxhiTRNYYM8YYY4xJImuMGWOMMcYk0f8HhgGJow4nNPUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-5,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "\n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b000d",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a5dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "DBN_group_coopthres = [0,3,2,1.5,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "prepare_input_data = 0\n",
    "\n",
    "DBN_input_data_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            \n",
    "        # get animal info\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "        \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "            \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "\n",
    "        # load behavioral event results\n",
    "        print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "            output_look_ornot = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "            output_allvectors = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "            output_allangles = pickle.load(f)  \n",
    "        with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "            output_key_locations = pickle.load(f)     \n",
    "        look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "        look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "        look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "        look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "        look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "        # \n",
    "        look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                           \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                           \"othertube\":look_at_othertube_or_not_Anipose}      \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "        timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "        timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "        timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "\n",
    "\n",
    "        if mergetempRos:\n",
    "            temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            # use bhv event to decide temporal resolution\n",
    "            #\n",
    "            #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "        # try different temporal resolutions\n",
    "        for temp_resolu in temp_resolus:\n",
    "            bhv_df = []\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            else:\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "            if len(bhv_df)==0:\n",
    "                bhv_df = bhv_df_itr\n",
    "            else:\n",
    "                bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "            # merge sessions from the same condition\n",
    "            for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                # merge sessions \n",
    "                if (tasktype!=3):\n",
    "                    if (tasktype==iDBN_group_typeID):\n",
    "                        if (len(DBN_input_data_alltypes[iDBN_group_typename])==0):\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = bhv_df\n",
    "                        else:\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = pd.concat([DBN_input_data_alltypes[iDBN_group_typename],bhv_df])\n",
    "                else:\n",
    "                    if (coop_thres==iDBN_group_cothres):\n",
    "                        if (len(DBN_input_data_alltypes[iDBN_group_typename])==0):\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = bhv_df\n",
    "                        else:\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = pd.concat([DBN_input_data_alltypes[iDBN_group_typename],bhv_df])\n",
    "\n",
    "            # save data\n",
    "            if 1:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "                if not os.path.exists(data_saved_subfolder):\n",
    "                    os.makedirs(data_saved_subfolder)\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(DBN_input_data_alltypes, f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                        pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c8d47",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f159213",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd3a9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,5]\n",
    "DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "       \n",
    "        if not moreSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            samplingsizes = [min_samplesize,max_samplesize]\n",
    "            samplingsizes_name = ['min_row_number','max_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            # different session conditions (aka DBN groups)\n",
    "            for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "            \n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "        \n",
    "                # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                colnames = list(bhv_df_all.columns)\n",
    "                eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                nevents = np.size(eventnames)\n",
    "                #\n",
    "                all_pops = list(bhv_df_all.columns)\n",
    "                from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "                #\n",
    "                nFromNodes = np.shape(from_pops)[0]\n",
    "                nToNodes = np.shape(to_pops)[0]\n",
    "                \n",
    "                # for saving DBN related data\n",
    "                DAGs_alltypes[iDBN_group_typename] = np.zeros((nbootstraps, nFromNodes, nToNodes))\n",
    "                DAGs_shuffle_alltypes[iDBN_group_typename] = np.zeros((nbootstraps, nFromNodes, nToNodes))\n",
    "                DAGs_scores_alltypes[iDBN_group_typename] = np.zeros((nbootstraps))\n",
    "                DAGs_shuffle_scores_alltypes[iDBN_group_typename] = np.zeros((nbootstraps))\n",
    "                \n",
    "                # do for each bootstrap\n",
    "                for ibootstp in range(nbootstraps):\n",
    "\n",
    "                    # create the data\n",
    "                    bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = ibootstp) # take the subset for DBN training\n",
    "                    aic = AicScore(bhv_df)\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                    aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "\n",
    "                    # do for each starting point\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # step 1: test on the real data\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "                        #\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "                        #\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "                        #\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "                        #\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    # save the DBN results that have the highest score\n",
    "                    DAGs_alltypes[iDBN_group_typename][ibootstp,:,:] = DAGs_randstart[score_randstart.argmax(),:,:]\n",
    "                    DAGs_shuffle_alltypes[iDBN_group_typename][ibootstp,:,:] = DAGs_randstart_shuffle[score_randstart_shuffle.argmax(),:,:]\n",
    "                    DAGs_scores_alltypes[iDBN_group_typename][ibootstp] = score_randstart.argmax()\n",
    "                    DAGs_shuffle_scores_alltypes[iDBN_group_typename][ibootstp] = score_randstart_shuffle.argmax()\n",
    "\n",
    "                # compare DBN and shuffled DBN    \n",
    "                weighted_graphs_alltypes[iDBN_group_typename] = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                weighted_graphs_shuffled_alltypes[iDBN_group_typename] = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                #\n",
    "                sig_edges_alltypes[iDBN_group_typename] = get_significant_edges(weighted_graphs_alltypes[iDBN_group_typename],\n",
    "                                                                                weighted_graphs_shuffled_alltypes[iDBN_group_typename])\n",
    "\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "    \n",
    "    print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a240805",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5252a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1700, 1700]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab842b3505542e2855ac801594970f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b3e31a299b48c5915adfa74b52758d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3393008/4073764849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdata_saved_subfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_saved_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'data_saved_combinedsessions_Anipose'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msavefile_sufix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_3Lags_highBoots_highStPo/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0manimal1_fixedorder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0manimal2_fixedorder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dumpy' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3393008/4073764849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbhv_df_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0meventnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"pull1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pull2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"owgaze1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"owgaze2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mnevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meventnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,5]\n",
    "DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "try:\n",
    "    dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags_highBoots_highStPo/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "       \n",
    "        if not moreSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            samplingsizes = [min_samplesize,max_samplesize]\n",
    "            samplingsizes_name = ['min_row_number','max_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            # different session conditions (aka DBN groups)\n",
    "            for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "            \n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "        \n",
    "                # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                colnames = list(bhv_df_all.columns)\n",
    "                eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                nevents = np.size(eventnames)\n",
    "                #\n",
    "                all_pops = list(bhv_df_all.columns)\n",
    "                from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "                #\n",
    "                nFromNodes = np.shape(from_pops)[0]\n",
    "                nToNodes = np.shape(to_pops)[0]\n",
    "                \n",
    "                # for saving DBN related data\n",
    "                DAGs_alltypes[iDBN_group_typename] = np.zeros((nbootstraps, nFromNodes, nToNodes))\n",
    "                DAGs_shuffle_alltypes[iDBN_group_typename] = np.zeros((nbootstraps, nFromNodes, nToNodes))\n",
    "                DAGs_scores_alltypes[iDBN_group_typename] = np.zeros((nbootstraps))\n",
    "                DAGs_shuffle_scores_alltypes[iDBN_group_typename] = np.zeros((nbootstraps))\n",
    "                \n",
    "                # do for each bootstrap\n",
    "                for ibootstp in range(nbootstraps):\n",
    "\n",
    "                    # create the data\n",
    "                    bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = ibootstp) # take the subset for DBN training\n",
    "                    aic = AicScore(bhv_df)\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                    aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "\n",
    "                    # do for each starting point\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # step 1: test on the real data\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "                        #\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "                        #\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "                        #\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "                        #\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    # save the DBN results that have the highest score\n",
    "                    DAGs_alltypes[iDBN_group_typename][ibootstp,:,:] = DAGs_randstart[score_randstart.argmax(),:,:]\n",
    "                    DAGs_shuffle_alltypes[iDBN_group_typename][ibootstp,:,:] = DAGs_randstart_shuffle[score_randstart_shuffle.argmax(),:,:]\n",
    "                    DAGs_scores_alltypes[iDBN_group_typename][ibootstp] = score_randstart.argmax()\n",
    "                    DAGs_shuffle_scores_alltypes[iDBN_group_typename][ibootstp] = score_randstart_shuffle.argmax()\n",
    "\n",
    "                # compare DBN and shuffled DBN    \n",
    "                weighted_graphs_alltypes[iDBN_group_typename] = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                weighted_graphs_shuffled_alltypes[iDBN_group_typename] = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                #\n",
    "                sig_edges_alltypes[iDBN_group_typename] = get_significant_edges(weighted_graphs_alltypes[iDBN_group_typename],\n",
    "                                                                                weighted_graphs_shuffled_alltypes[iDBN_group_typename])\n",
    "\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags_highBoots_highStPo/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:  \n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5ca47",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge with arrows; show the best time bin and row number; show the three time lag separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,5]\n",
    "DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "# eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "# eventnodes_color = ['b','r','y','g']\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00']\n",
    "eventnodes_shape = [\"o\",\"o\",\"H\",\"H\"]\n",
    "    \n",
    "savefigs = 1\n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,nDBN_groups)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3],[4,5,6,7],[8,9,10,11]]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "    for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "\n",
    "        try:\n",
    "\n",
    "            iDBN_group_typename = DBN_group_typenames[iDBN_group]\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "            #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "            \n",
    "            sig_avg_dags = weighted_graphs_tgt.mean(axis = 0) * sig_edges_tgt\n",
    "            sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,iDBN_group].set_title(iDBN_group_typename,fontsize=18)\n",
    "            axs[ilag*2+0,iDBN_group].set_xlim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_ylim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,iDBN_group].axis('equal')\n",
    "\n",
    "            \n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                              eventnodes_shape[ieventnode],markersize=60,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                              markeredgecolor='none')              \n",
    "                #axs[ilag*2+0,iDBN_group].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                #                       eventnames[ieventnode],fontsize=15)\n",
    "                \n",
    "                clmap = mpl.cm.get_cmap('Greens')\n",
    "                \n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt>0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        # width=0.04*abs(edge_weight_tgt),\n",
    "                                                        head_width=0.08,\n",
    "                                                        width=0.04,   \n",
    "                                                        color = clmap(edge_weight_tgt))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      # 0.04*abs(edge_weight_tgt),\n",
    "                                                      0.04,\n",
    "                                                      color = clmap(edge_weight_tgt))\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap(edge_weight_tgt), \n",
    "                                    edgecolor=clmap(edge_weight_tgt)\n",
    "                                )\n",
    "                                axs[ilag*2+0,iDBN_group].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap(edge_weight_tgt),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "                                else:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap(edge_weight_tgt),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = 0,1\n",
    "            import matplotlib as mpl\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,iDBN_group].pcolormesh(sig_avg_dags_df,cmap=\"Greens\",norm=norm)\n",
    "            #\n",
    "            if iDBN_group == nDBN_groups-1:\n",
    "                cax = axs[ilag*2+1,iDBN_group].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,iDBN_group], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,iDBN_group].axis('equal')\n",
    "            axs[ilag*2+1,iDBN_group].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,iDBN_group].set_xticks(np.arange(0.5,4.5,1))\n",
    "            axs[ilag*2+1,iDBN_group].set_xticklabels(eventnames)\n",
    "            if iDBN_group == 0:\n",
    "                axs[ilag*2+1,iDBN_group].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks(np.arange(0.5,4.5,1))\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,iDBN_group].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,iDBN_group].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks([])\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels([])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce572531",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge differences, use one condition as the base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69865334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,5]\n",
    "DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "basecondition = 'self'\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "# eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "# eventnodes_color = ['b','r','y','g']\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00']\n",
    "eventnodes_shape = [\"o\",\"o\",\"H\",\"H\"]\n",
    "\n",
    "nFromNodes = nevents\n",
    "nToNodes = nevents\n",
    "    \n",
    "savefigs = 1\n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,nDBN_groups)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3],[4,5,6,7],[8,9,10,11]]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "    \n",
    "weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "#sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "# sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "\n",
    "weighted_graphs_base = weighted_graphs_tgt.mean(axis = 0)\n",
    "\n",
    "sig_edges_base = sig_edges_tgt\n",
    "\n",
    "sig_avg_dags_base =  weighted_graphs_base * sig_edges_base\n",
    "    \n",
    "    \n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "       \n",
    "    for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "\n",
    "        try:\n",
    "\n",
    "            iDBN_group_typename = DBN_group_typenames[iDBN_group]\n",
    "\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "            #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "            \n",
    "            weighted_graphs_delta = (weighted_graphs_tgt-weighted_graphs_base)\n",
    "            weighted_graphs_delta = weighted_graphs_delta.mean(axis=0)\n",
    "            #\n",
    "            sig_edges_delta = ((sig_edges_tgt+sig_edges_base)>0)*1\n",
    "\n",
    "            sig_avg_dags = weighted_graphs_delta * sig_edges_delta\n",
    "\n",
    "            sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,iDBN_group].set_title(iDBN_group_typename,fontsize=18)\n",
    "            axs[ilag*2+0,iDBN_group].set_xlim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_ylim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,iDBN_group].axis('equal')\n",
    "\n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                              eventnodes_shape[ieventnode],markersize=60,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                              markeredgecolor='none')              \n",
    "                \n",
    "                #axs[ilag*2+0,iDBN_group].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                #                       eventnames[ieventnode],fontsize=10)\n",
    "                \n",
    "                clmap = mpl.cm.get_cmap('bwr')\n",
    "                \n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt!=0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        # width=0.04*abs(edge_weight_tgt),\n",
    "                                                        head_width=0.08,\n",
    "                                                        width=0.04,       \n",
    "                                                        color = clmap((1+edge_weight_tgt)/2))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      # 0.04*abs(edge_weight_tgt)\n",
    "                                                      0.04\n",
    "                                                     )\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap((1+edge_weight_tgt)/2), \n",
    "                                    edgecolor=clmap((1+edge_weight_tgt)/2)\n",
    "                                )\n",
    "                                axs[ilag*2+0,iDBN_group].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap((1+edge_weight_tgt)/2),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "                                else:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap((1+edge_weight_tgt)/2),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = -1,1\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,iDBN_group].pcolormesh(sig_avg_dags_df,cmap=\"bwr\",norm=norm)\n",
    "            #-\n",
    "            if iDBN_group == nDBN_groups-1:\n",
    "                cax = axs[ilag*2+1,iDBN_group].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,iDBN_group], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,iDBN_group].axis('equal')\n",
    "            axs[ilag*2+1,iDBN_group].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,iDBN_group].set_xticks(np.arange(0.5,4.5,1))\n",
    "            axs[ilag*2+1,iDBN_group].set_xticklabels(eventnames)\n",
    "            if iDBN_group == 0:\n",
    "                axs[ilag*2+1,iDBN_group].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks(np.arange(0.5,4.5,1))\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,iDBN_group].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,iDBN_group].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks([])\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels([])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_EdgeFifferenceFrom_'+basecondition+'AsBase.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_EdgeFifferenceFrom_'+basecondition+'AsBase.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed62ef",
   "metadata": {},
   "source": [
    "## Plots that include all pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbfff1",
   "metadata": {},
   "source": [
    "### VERSION 1: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis show the time lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "#\n",
    "fig, axs = plt.subplots(2,nanimalpairs)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(5*nanimalpairs)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    #\n",
    "    t3MI_coop_self = MI_coop_self[[0,1,2,3],:]\n",
    "    t2MI_coop_self = MI_coop_self[[4,5,6,7],:]\n",
    "    t1MI_coop_self = MI_coop_self[[8,9,10,11],:]\n",
    "    t3MI_nov_coop = MI_nov_coop[[0,1,2,3],:]\n",
    "    t2MI_nov_coop = MI_nov_coop[[4,5,6,7],:]\n",
    "    t1MI_nov_coop = MI_nov_coop[[8,9,10,11],:]\n",
    "    #\n",
    "    pull_pull_fromNodes = [0,1]\n",
    "    pull_pull_toNodes = [1,0]\n",
    "    #\n",
    "    gaze_gaze_fromNodes = [2,3]\n",
    "    gaze_gaze_toNodes = [3,2]\n",
    "    #\n",
    "    within_pullgaze_fromNodes = [0,1,2,3]\n",
    "    within_pullgaze_toNodes = [2,3,0,1]\n",
    "    #\n",
    "    across_pullgaze_fromNodes = [0,1,2,3]\n",
    "    across_pullgaze_toNodes = [3,2,1,0]\n",
    "\n",
    "    \n",
    "    # plot coop self modulation\n",
    "    # pull-pull\n",
    "    xxx1 = [np.mean(t3MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t2MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t1MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes])]\n",
    "    line1 = axs[0,ianimalpair].plot(xxx1,'*--',markersize = 13,label='pull<->pull')\n",
    "    # gaze-gaze\n",
    "    xxx2 = [np.mean(t3MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes])]\n",
    "    line2 = axs[0,ianimalpair].plot(xxx2,'o--',markersize = 13,label='gaze<->gaze')\n",
    "    # within animal pullgaze\n",
    "    xxx3 = [np.mean(t3MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes])]\n",
    "    line3 = axs[0,ianimalpair].plot(xxx3,'v--',markersize = 13,label='within animal pull<->gaze')\n",
    "    # across animal pullgaze\n",
    "    xxx4 = [np.mean(t3MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes])]\n",
    "    line4 = axs[0,ianimalpair].plot(xxx4,'^--',markersize = 13,label='across animal pull<->gaze')\n",
    "    #\n",
    "    axs[0,ianimalpair].set_ylim([-1.05,1.05])\n",
    "    axs[0,ianimalpair].set_xticks([0,1,2])\n",
    "    axs[0,ianimalpair].set_xticklabels([])\n",
    "    axs[0,ianimalpair].set_yticks([-1,-0.5,0,0.5,1])\n",
    "    if ianimalpair == 0:\n",
    "        axs[0,ianimalpair].tick_params(axis='y', labelsize=13)\n",
    "        axs[0,ianimalpair].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "    else:\n",
    "        axs[0,ianimalpair].set_yticklabels([])\n",
    "    axs[0,ianimalpair].set_title('pair:'+animal1_fixedorder+'-'+animal2_fixedorder,fontsize = 16)\n",
    "\n",
    "\n",
    "    # plot novision coop modulation\n",
    "    # pull-pull\n",
    "    xxx1 = [np.mean(t3MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes])]\n",
    "    line1 = axs[1,ianimalpair].plot(xxx1,'*--',markersize = 13,label='pull<->pull')\n",
    "    # gaze-gaze\n",
    "    xxx2 = [np.mean(t3MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes])]\n",
    "    line2 = axs[1,ianimalpair].plot(xxx2,'o--',markersize = 13,label='gaze<->gaze')\n",
    "    # within animal pullgaze\n",
    "    xxx3 = [np.mean(t3MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes])]\n",
    "    line3 = axs[1,ianimalpair].plot(xxx3,'v--',markersize = 13,label='within animal pull<->gaze')\n",
    "    # across animal pullgaze\n",
    "    xxx4 = [np.mean(t3MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes])]\n",
    "    line4 = axs[1,ianimalpair].plot(xxx4,'^--',markersize = 13,label='across animal pull<->gaze')\n",
    "    #\n",
    "    axs[1,ianimalpair].set_ylim([-1.05,1.05])\n",
    "    axs[1,ianimalpair].set_xticks([0,1,2])\n",
    "    axs[1,ianimalpair].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "    axs[1,ianimalpair].set_xlabel('time lag',fontsize=15)\n",
    "    axs[1,ianimalpair].set_yticks([-1,-0.5,0,0.5,1])\n",
    "    if ianimalpair == 0:\n",
    "        axs[1,ianimalpair].tick_params(axis='y', labelsize=13)\n",
    "        axs[1,ianimalpair].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "        axs[1,ianimalpair].legend()\n",
    "    else:\n",
    "        axs[1,ianimalpair].set_yticklabels([])\n",
    "    axs[1,ianimalpair].set_title('pair:'+animal1_fixedorder+'-'+animal2_fixedorder,fontsize = 16)\n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_summary_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_summary_'+str(temp_resolu)+'_'+j_sampsize_name+'.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33085d8b",
   "metadata": {},
   "source": [
    "### VERSION 2: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis show the time lag; combined into four edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga']\n",
    "animalpairs_datashapes = ['o','v','^']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(2,4)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(5*4)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    #\n",
    "    t3MI_coop_self = MI_coop_self[[0,1,2,3],:]\n",
    "    t2MI_coop_self = MI_coop_self[[4,5,6,7],:]\n",
    "    t1MI_coop_self = MI_coop_self[[8,9,10,11],:]\n",
    "    t3MI_nov_coop = MI_nov_coop[[0,1,2,3],:]\n",
    "    t2MI_nov_coop = MI_nov_coop[[4,5,6,7],:]\n",
    "    t1MI_nov_coop = MI_nov_coop[[8,9,10,11],:]\n",
    "    #\n",
    "    pull_pull_fromNodes = [0,1]\n",
    "    pull_pull_toNodes = [1,0]\n",
    "    #\n",
    "    gaze_gaze_fromNodes = [2,3]\n",
    "    gaze_gaze_toNodes = [3,2]\n",
    "    #\n",
    "    within_pullgaze_fromNodes = [0,1,2,3]\n",
    "    within_pullgaze_toNodes = [2,3,0,1]\n",
    "    #\n",
    "    across_pullgaze_fromNodes = [0,1,2,3]\n",
    "    across_pullgaze_toNodes = [3,2,1,0]\n",
    "\n",
    "    \n",
    "    # plot coop self modulation\n",
    "    # pull-pull\n",
    "    xxx1 = [np.mean(t3MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t2MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t1MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes])]\n",
    "    line1 = axs[0,0].plot(xxx1,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='r')\n",
    "    # gaze-gaze\n",
    "    xxx2 = [np.mean(t3MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes])]\n",
    "    line2 = axs[0,1].plot(xxx2,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='b')\n",
    "    # within animal pullgaze\n",
    "    xxx3 = [np.mean(t3MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes])]\n",
    "    line3 = axs[0,2].plot(xxx3,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='g')\n",
    "    # across animal pullgaze\n",
    "    xxx4 = [np.mean(t3MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes])]\n",
    "    line4 = axs[0,3].plot(xxx4,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='y')\n",
    "    #\n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze','within animal pull<->gaze','across animal pull<->gaze']\n",
    "    for iplot in np.arange(0,4,1):\n",
    "        axs[0,iplot].set_xlim([-0.1,2.1])\n",
    "        axs[0,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[0,iplot].set_xticks([0,1,2])\n",
    "        axs[0,iplot].set_xticklabels([])\n",
    "        axs[0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        #\n",
    "        if iplot == 0:\n",
    "            axs[0,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[0,iplot].set_yticklabels([])\n",
    "        axs[0,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[0,iplot].plot([-1,3],[0,0],'k--')\n",
    "\n",
    "    # plot novision coop modulation\n",
    "    # pull-pull\n",
    "    xxx1 = [np.mean(t3MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes])]\n",
    "    line1 = axs[1,0].plot(xxx1,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='r')\n",
    "    # gaze-gaze\n",
    "    xxx2 = [np.mean(t3MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes])]\n",
    "    line2 = axs[1,1].plot(xxx2,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='b')\n",
    "    # within animal pullgaze\n",
    "    xxx3 = [np.mean(t3MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes])]\n",
    "    line3 = axs[1,2].plot(xxx3,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='g')\n",
    "    # across animal pullgaze\n",
    "    xxx4 = [np.mean(t3MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes])]\n",
    "    line4 = axs[1,3].plot(xxx4,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='y')\n",
    "     #\n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze','within animal pull<->gaze','across animal pull<->gaze']\n",
    "    for iplot in np.arange(0,4,1):\n",
    "        axs[1,iplot].set_xlim([-0.1,2.1])\n",
    "        axs[1,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[1,iplot].set_xticks([0,1,2])\n",
    "        axs[1,iplot].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "        axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "        axs[1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        if iplot == 0:\n",
    "            axs[1,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[1,iplot].set_yticklabels([])\n",
    "        axs[1,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[1,iplot].plot([-1,3],[0,0],'k--')\n",
    "        \n",
    "    axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "                     'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "                     'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_summary_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_v2.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_summary_'+str(temp_resolu)+'_'+j_sampsize_name+'_v2.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cd65a",
   "metadata": {},
   "source": [
    "### VERSION 3: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis show the time lag; combined into six edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga']\n",
    "animalpairs_datashapes = ['o','v','^']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(2,6)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(5*6)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'_3Lags/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "    MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "    \n",
    "    sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "    sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "    \n",
    "    MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "    MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    #\n",
    "    t3MI_coop_self = MI_coop_self[[0,1,2,3],:]\n",
    "    t2MI_coop_self = MI_coop_self[[4,5,6,7],:]\n",
    "    t1MI_coop_self = MI_coop_self[[8,9,10,11],:]\n",
    "    t3MI_nov_coop = MI_nov_coop[[0,1,2,3],:]\n",
    "    t2MI_nov_coop = MI_nov_coop[[4,5,6,7],:]\n",
    "    t1MI_nov_coop = MI_nov_coop[[8,9,10,11],:]\n",
    "    #\n",
    "    pull_pull_fromNodes = [0,1]\n",
    "    pull_pull_toNodes = [1,0]\n",
    "    #\n",
    "    gaze_gaze_fromNodes = [2,3]\n",
    "    gaze_gaze_toNodes = [3,2]\n",
    "    #\n",
    "    within_pullgaze_fromNodes = [0,1]\n",
    "    within_pullgaze_toNodes = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes = [0,1]\n",
    "    across_pullgaze_toNodes = [3,2]\n",
    "    #\n",
    "    within_gazepull_fromNodes = [2,3]\n",
    "    within_gazepull_toNodes = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes = [2,3]\n",
    "    across_gazepull_toNodes = [1,0]\n",
    "\n",
    "    \n",
    "    # plot coop self modulation\n",
    "    # pull-pull\n",
    "    xxx1 = [np.mean(t3MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t2MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t1MI_coop_self[pull_pull_fromNodes,pull_pull_toNodes])]\n",
    "    line1 = axs[0,0].plot(xxx1,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='r')\n",
    "    # gaze-gaze\n",
    "    xxx2 = [np.mean(t3MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[gaze_gaze_fromNodes,gaze_gaze_toNodes])]\n",
    "    line2 = axs[0,1].plot(xxx2,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='b')\n",
    "    # within animal pullgaze\n",
    "    xxx3 = [np.mean(t3MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[within_pullgaze_fromNodes,within_pullgaze_toNodes])]\n",
    "    line3 = axs[0,2].plot(xxx3,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#008080')\n",
    "    # across animal pullgaze\n",
    "    xxx4 = [np.mean(t3MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_coop_self[across_pullgaze_fromNodes,across_pullgaze_toNodes])]\n",
    "    line4 = axs[0,3].plot(xxx4,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#D1B26F')\n",
    "    # within animal gazepull\n",
    "    xxx5 = [np.mean(t3MI_coop_self[within_gazepull_fromNodes,within_gazepull_toNodes]),\n",
    "            np.mean(t2MI_coop_self[within_gazepull_fromNodes,within_gazepull_toNodes]),\n",
    "            np.mean(t1MI_coop_self[within_gazepull_fromNodes,within_gazepull_toNodes])]\n",
    "    line5 = axs[0,4].plot(xxx5,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#7FFF00')\n",
    "    # across animal gazepull\n",
    "    xxx6 = [np.mean(t3MI_coop_self[across_gazepull_fromNodes,across_gazepull_toNodes]),\n",
    "            np.mean(t2MI_coop_self[across_gazepull_fromNodes,across_gazepull_toNodes]),\n",
    "            np.mean(t1MI_coop_self[across_gazepull_fromNodes,across_gazepull_toNodes])]\n",
    "    line6 = axs[0,5].plot(xxx6,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#FAC205')\n",
    "    #\n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze',\n",
    "                 'within animal pull->gaze','across animal pull->gaze',\n",
    "                 'within animal gaze->pull','across animal gaze->pull',]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[0,iplot].set_xlim([-0.1,2.1])\n",
    "        axs[0,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[0,iplot].set_xticks([0,1,2])\n",
    "        axs[0,iplot].set_xticklabels([])\n",
    "        axs[0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        #\n",
    "        if iplot == 0:\n",
    "            axs[0,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[0,iplot].set_yticklabels([])\n",
    "        axs[0,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[0,iplot].plot([-1,3],[0,0],'k--')\n",
    "\n",
    "    # plot novision coop modulation\n",
    "    # pull-pull\n",
    "    xxx1 = [np.mean(t3MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[pull_pull_fromNodes,pull_pull_toNodes])]\n",
    "    line1 = axs[1,0].plot(xxx1,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='r')\n",
    "    # gaze-gaze\n",
    "    xxx2 = [np.mean(t3MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[gaze_gaze_fromNodes,gaze_gaze_toNodes])]\n",
    "    line2 = axs[1,1].plot(xxx2,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='b')\n",
    "    # within animal pullgaze\n",
    "    xxx3 = [np.mean(t3MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[within_pullgaze_fromNodes,within_pullgaze_toNodes])]\n",
    "    line3 = axs[1,2].plot(xxx3,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#008080')\n",
    "    # across animal pullgaze\n",
    "    xxx4 = [np.mean(t3MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[across_pullgaze_fromNodes,across_pullgaze_toNodes])]\n",
    "    line4 = axs[1,3].plot(xxx4,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#D1B26F')\n",
    "    # within animal gazepull\n",
    "    xxx5 = [np.mean(t3MI_nov_coop[within_gazepull_fromNodes,within_gazepull_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[within_gazepull_fromNodes,within_gazepull_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[within_gazepull_fromNodes,within_gazepull_toNodes])]\n",
    "    line5 = axs[1,4].plot(xxx5,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#7FFF00')\n",
    "    # across animal gazepull\n",
    "    xxx6 = [np.mean(t3MI_nov_coop[across_gazepull_fromNodes,across_gazepull_toNodes]),\n",
    "            np.mean(t2MI_nov_coop[across_gazepull_fromNodes,across_gazepull_toNodes]),\n",
    "            np.mean(t1MI_nov_coop[across_gazepull_fromNodes,across_gazepull_toNodes])]\n",
    "    line6 = axs[1,5].plot(xxx6,animalpairs_datashapes[ianimalpair]+'--',markersize = 13,color='#FAC205')\n",
    "    #\n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze',\n",
    "                 'within animal pull->gaze','across animal pull->gaze',\n",
    "                 'within animal gaze->pull','across animal gaze->pull',]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[1,iplot].set_xlim([-0.1,2.1])\n",
    "        axs[1,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[1,iplot].set_xticks([0,1,2])\n",
    "        axs[1,iplot].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "        axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "        axs[1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        if iplot == 0:\n",
    "            axs[1,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[1,iplot].set_yticklabels([])\n",
    "        axs[1,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[1,iplot].plot([-1,3],[0,0],'k--')\n",
    "        \n",
    "    axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "                     'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "                     'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_detailed_summary_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_v2.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_combinesessions_basicEvents_highBoots_highStPo/'+savefile_sufix+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_detailed_summary_'+str(temp_resolu)+'_'+j_sampsize_name+'_v2.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "(weighted_graphs_coop-weighted_graphs_self).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea48b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd43e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99127fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db755f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb002e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f24e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e4f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a2656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3fa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ff85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca400451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da044d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5145a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
