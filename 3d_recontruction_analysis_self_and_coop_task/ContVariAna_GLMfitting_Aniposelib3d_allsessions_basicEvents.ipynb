{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea1560b",
   "metadata": {},
   "source": [
    "### This script runs some basic bhv analysis, and detailed analysis focus on the continuous behavioral variables\n",
    "### this script fits GLM models based on the continuous behavioral variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d0681",
   "metadata": {},
   "source": [
    "#### The output of this script will also be used by the DBN scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044ce3b",
   "metadata": {},
   "source": [
    "### function - GLM fitting based on continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74effba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.continuous_bhv_var_GLM_fitting import get_continuous_bhv_var_for_GLM_fitting\n",
    "from ana_functions.continuous_bhv_var_GLM_fitting import GLM_fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 1*30\n",
    "nframes = 1\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20220909\",\"20220912\",\"20220915\",\"20220920\",\"20220922\",\"20220923\",\"20221010\",\n",
    "                      \"20221011\",\"20221013\",\"20221014\",\"20221015\",\"20221017\",\"20230215\",     \n",
    "                      \"20221018\",\"20221019\",\"20221020\",\"20221021\",\"20221022\",\"20221026\",\"20221028\",\"20221030\",\n",
    "                      \"20221107\",\"20221108\",\"20221109\",\"20221110\",\"20221111\",\"20221114\",\"20221115\",\"20221116\",\n",
    "                      \"20221117\",\"20221118\",\"20221121\",\"20221122\",\"20221123\",\"20221125\",\"20221128\",\"20221129\",              \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221212\",\"20221214\",\"20221216\",\"20221219\",\"20221220\",\n",
    "                      \"20221221\",\"20230208\",\"20230209\",\"20230213\",\"20230214\",\"20230111\",\"20230112\",\"20230201\",\n",
    "\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 6.50, 18.10, 0,      33.03, 549.0, 116.80, 6.50,\n",
    "                                 2.80, 27.80, 272.50, 27.90, 27.00,  33.00,\n",
    "                                28.70, 45.30, 21.10,  27.10, 51.90,  21.00, 30.80, 17.50,                      \n",
    "                                15.70,  2.65, 27.30,   0.00,  0.00,  71.80,  0.00,  0.00, \n",
    "                                75.50, 20.20,  0.00,  24.20, 36.70,  26.40, 22.50, 28.50,                       \n",
    "                                 0.00,  0.00, 21.70,  84.70, 17.00,  19.80, 23.50, 25.20,  \n",
    "                                 0.00,  0.00,  0.00,   0.00,  0.00, 130.00, 14.20, 24.20, \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20220912\",\"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                      \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                      \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                      \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                      \"20230117\",\"20230118\",\"20230124\",\"20230126\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                18.10,  0.00, 33.03,  6.50,  0.00, \n",
    "                                 2.80, 27.80, 27.90, 27.00,  \n",
    "                                51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                 0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                 0.00,  0.00,  0.00,  0.00,  \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# eddie sparkle\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20221122\",\"20221125\",\"20221128\",\"20221129\",\"20221130\",\"20221202\",\"20221206\",\n",
    "                      \"20221207\",\"20221208\",\"20221209\",\"20230126\",\"20230127\",\"20230130\",\"20230201\",\"20230203-1\",\n",
    "                      \"20230206\",\"20230207\",\"20230208-1\",\"20230209\",\"20230222\",\"20230223-1\",\"20230227-1\",\n",
    "                      \"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230321\",\"20230322\",\"20230324\",\"20230327\",\"20230328\",\n",
    "                      \"20230330\",\"20230331\",\"20230403\",\"20230404\",\"20230405\",\"20230406\",\"20230407\",\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 8.00,38.00,1.00,3.00,5.00,9.50,1.00,\n",
    "                                 4.50,4.50,5.00,38.00,166.00,4.20,3.80,3.60,\n",
    "                                 7.50,9.00,7.50,8.50,14.50,7.80,8.00,7.50,\n",
    "                                 8.00,8.00,4.00,123.00,14.00,8.80,\n",
    "                                 7.00,7.50,5.50,11.00,9.00,\n",
    "                                 17.00,4.50,9.30,25.50,20.40,21.30,24.80,\n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20221122\",  \"20221125\",  \n",
    "                      \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                      \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                      \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                      \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                      \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                  8.00,  38.00, \n",
    "                                  9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                  9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                  8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                  7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                  4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "# ginger kanga\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230209\",\"20230213\",\"20230214\",\"20230216\",\"20230222\",\"20230223\",\"20230228\",\"20230302\",\n",
    "                      \"20230303\",\"20230307\",\"20230314\",\"20230315\",\"20230316\",\"20230317\"         \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00,  0.00, 48.00, 26.20, 18.00, 23.00, 28.50,\n",
    "                                34.00, 25.50, 25.50, 31.50, 28.00, 30.50\n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20230213\", # camera2 is set up wrong\n",
    "                      \"20230214\",\"20230216\",\n",
    "                      \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                      \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                      \"20230323\",\"20230412\",\"20230413\",\"20230517\",\"20230614\",\"20230615\",\n",
    "                      \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00, # camera2 is set up wrong\n",
    "                                 0.00, 48.00, \n",
    "                                23.00, 28.50, 34.00, 25.50, \n",
    "                                25.50, 31.50, 28.00, 30.50,\n",
    "                                33.50, 22.20, 50.00,  0.00, \n",
    "                                33.00, 18.20, 22.80, 31.00, 24.00, 21.00,\n",
    "                                 0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    " # dannon kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230718\",\"20230720\",\"20230914\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                      \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                      \"20231013\",\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0, 0, 0, 0, 0, 0, \n",
    "                                 0, 0, 0, 0, 0, 0,\n",
    "                                 0,\n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20230718\",\"20230720\",\"20230914\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                      \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                      \"20231013\", \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0, 0, 0, 0, 0, 0, \n",
    "                                 0, 0, 0, 0, 0, 0,\n",
    "                                 0,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "# a test case\n",
    "if 1:\n",
    "    dates_list = [\"20230324\"]\n",
    "    session_start_times = [5.50] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_succtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_errtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# where to save the demo video\n",
    "withboxCorner = 1\n",
    "video_file_dir = data_saved_folder+'/example_videos_Anipose_bhv_demo/'+animal1_filename+'_'+animal2_filename\n",
    "if not os.path.exists(video_file_dir):\n",
    "    os.makedirs(video_file_dir)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "# NOTE: THIS STEP will save the data to the combinedsession_Anipose folder, since they are the same\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    dummy\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_succtrials_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_errtrials_all_dates = pickle.load(f) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "except:\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        for imergepair in np.arange(0,np.shape(merge_campairs)[0],1):\n",
    "            \n",
    "            # should be only one merge type - \"Anipose\"\n",
    "            merge_campair = merge_campairs[imergepair]\n",
    "\n",
    "            # load camera tracking results\n",
    "            try:\n",
    "                # dummy\n",
    "                if reanalyze_video:\n",
    "                    print(\"re-analyze the data \",date_tgt)\n",
    "                    dummy\n",
    "                ## read\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                    body_part_locs_Anipose = pickle.load(f)                 \n",
    "            except:\n",
    "                print(\"did not save data for Anipose - body part tracking \"+date_tgt)\n",
    "                # analyze and save\n",
    "                Anipose_h5_file = Anipose_analyzed_path +date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "                Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "                body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                    pickle.dump(body_part_locs_Anipose, f)            \n",
    "            \n",
    "            min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "                    \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "            tasktypes_all_dates[idate] = tasktype\n",
    "            coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "            # successful trial or not\n",
    "            succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "            succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # analyze behavior results\n",
    "            # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "            succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "\n",
    "            trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "            #\n",
    "            pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<10]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "\n",
    "\n",
    "            # load behavioral event results\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                    output_key_locations = pickle.load(f)\n",
    "            except:\n",
    "                print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "                output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "               \n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_key_locations, f)\n",
    "                \n",
    "             \n",
    "            look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "            look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "            look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "            look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "            look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "            look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                               \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                               \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "            timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "            timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "            timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "                \n",
    "            \n",
    "            # Use the continuous variables to do the GLM fitting\n",
    "            print('run the GLM fitting for individual session: '+date_tgt)\n",
    "            data_summary, data_summary_names = get_continuous_bhv_var_for_GLM_fitting(animal1, animal2, animalnames_videotrack, min_length, \n",
    "                                                                                      output_look_ornot, output_allvectors, output_allangles, output_key_locations)\n",
    "            # pick a subset of variables\n",
    "            # targeted_varis_id = [0,1,2,3,5,6,7,8]\n",
    "            # targeted_varis_id = [0,2,4]\n",
    "            # data_summary_names = np.array(data_summary_names)[targeted_varis_id]\n",
    "            # data_summary[animal1] = np.array(data_summary[animal1])[targeted_varis_id,:]\n",
    "            # data_summary[animal2] = np.array(data_summary[animal2])[targeted_varis_id,:]\n",
    "            \n",
    "            # basic variables for GLM fitting\n",
    "            history_time = 6 # unit of second\n",
    "            nbootstraps = 1\n",
    "            samplesize = 100\n",
    "            doSuccFailPull = 1\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_succtrials_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_errtrials_all_dates, f)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd103009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_summary['sparkle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf9051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9503d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def33c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0844754f",
   "metadata": {},
   "source": [
    "### temporary - code for function\n",
    "#### def GLM_fitting(animal1, animal2, session_start_time, data_summary, data_summary_names, bhv_data, history_time, nbootstraps, samplesize,doSuccFailPull):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30\n",
    "\n",
    "time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]+session_start_time\n",
    "time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]+session_start_time\n",
    "\n",
    "# align the pull time to the same temporal resolution\n",
    "time_point_pull1 = np.round(time_point_pull1*fps)/fps\n",
    "time_point_pull2 = np.round(time_point_pull2*fps)/fps\n",
    "\n",
    "# separate successful and failed pulls\n",
    "succpull_trialnum = trial_record[trial_record['rewarded']>0]['trial_number'].reset_index(drop=True)\n",
    "bhv_data_succpull = bhv_data[bhv_data['trial_number'].isin(succpull_trialnum)]\n",
    "failpull_trialnum = trial_record[trial_record['rewarded']==0]['trial_number'].reset_index(drop=True)\n",
    "bhv_data_failpull = bhv_data[bhv_data['trial_number'].isin(failpull_trialnum)]\n",
    "#\n",
    "# successful pulls\n",
    "time_point_pull1_succ = bhv_data_succpull[\"time_points\"][bhv_data_succpull[\"behavior_events\"]==1]+session_start_time\n",
    "time_point_pull2_succ = bhv_data_succpull[\"time_points\"][bhv_data_succpull[\"behavior_events\"]==2]+session_start_time\n",
    "#\n",
    "time_point_pull1_succ = np.round(time_point_pull1_succ*fps)/fps\n",
    "time_point_pull2_succ = np.round(time_point_pull2_succ*fps)/fps\n",
    "#\n",
    "# failed pulls\n",
    "time_point_pull1_fail = bhv_data_failpull[\"time_points\"][bhv_data_failpull[\"behavior_events\"]==1]+session_start_time\n",
    "time_point_pull2_fail = bhv_data_failpull[\"time_points\"][bhv_data_failpull[\"behavior_events\"]==2]+session_start_time\n",
    "#\n",
    "time_point_pull1_fail = np.round(time_point_pull1_fail*fps)/fps\n",
    "time_point_pull2_fail = np.round(time_point_pull2_fail*fps)/fps\n",
    "\n",
    "\n",
    "nanimals = 2\n",
    "\n",
    "for ianimal in np.arange(0,nanimals,1):\n",
    "\n",
    "    if ianimal == 0:\n",
    "        data_summary_iani = data_summary[animal1]\n",
    "        time_point_pulls = time_point_pull1\n",
    "        time_point_pulls_succ = time_point_pull1_succ\n",
    "        time_point_pulls_fail = time_point_pull1_fail\n",
    "    elif ianimal == 1:\n",
    "        data_summary_iani = data_summary[animal2]\n",
    "        time_point_pulls = time_point_pull2\n",
    "        time_point_pulls_succ = time_point_pull2_succ\n",
    "        time_point_pulls_fail = time_point_pull2_fail\n",
    "\n",
    "    # total time point number based on the 30Hz fps videos    \n",
    "    total_timepoints = np.shape(data_summary_iani)[1]\n",
    "\n",
    "    #\n",
    "    event_pulls = np.zeros((1,total_timepoints))[0]\n",
    "    ind_pulltimepoint = np.array(np.round(time_point_pulls*fps),dtype=int)\n",
    "    event_pulls[ind_pulltimepoint[ind_pulltimepoint<=total_timepoints]]=1\n",
    "    # successful pulls\n",
    "    event_pulls_succ = np.zeros((1,total_timepoints))[0]\n",
    "    ind_pulltimepoint_succ = np.array(np.round(time_point_pulls_succ*fps),dtype=int)\n",
    "    event_pulls_succ[ind_pulltimepoint_succ[ind_pulltimepoint_succ<=total_timepoints]]=1\n",
    "    # failed pulls\n",
    "    event_pulls_fail = np.zeros((1,total_timepoints))[0]\n",
    "    ind_pulltimepoint_fail = np.array(np.round(time_point_pulls_fail*fps),dtype=int)\n",
    "    event_pulls_fail[ind_pulltimepoint_fail[ind_pulltimepoint_fail<=total_timepoints]]=1\n",
    "    \n",
    "\n",
    "    # prepare the GLM input and output data\n",
    "    n_variables = np.shape(data_summary_names)[0]\n",
    "    input_datas = np.zeros((n_variables,total_timepoints-int(np.round(history_time*fps)),int(np.round(history_time*fps))))\n",
    "    output_datas = np.zeros((1,total_timepoints-int(np.round(history_time*fps))))[0]\n",
    "    # successful pulls\n",
    "    output_datas_succ = np.zeros((1,total_timepoints-int(np.round(history_time*fps))))[0]\n",
    "    # failed pulls\n",
    "    output_datas_fail = np.zeros((1,total_timepoints-int(np.round(history_time*fps))))[0]\n",
    "    \n",
    "    # prepare the input and output data\n",
    "    for i_timepoint in np.arange(int(np.round(history_time*fps)),total_timepoints,1):\n",
    "\n",
    "        #for i_var in np.arange(0,n_variables,1):\n",
    "\n",
    "        #    inputdata_xx = np.array(data_summary_iani)[i_var,np.arange(i_timepoint-int(np.round(history_time*fps)),i_timepoint,1)]\n",
    "            \n",
    "        #    input_datas[i_var,i_timepoint-int(np.round(history_time*fps)),:] = inputdata_xx\n",
    "        #    output_datas[i_timepoint-int(np.round(history_time*fps))] = event_pulls[i_timepoint]\n",
    "            \n",
    "        #    output_datas_succ[i_timepoint-int(np.round(history_time*fps))] = event_pulls_succ[i_timepoint]\n",
    "        #    output_datas_fail[i_timepoint-int(np.round(history_time*fps))] = event_pulls_fail[i_timepoint]\n",
    "        \n",
    "        inputdata_xx = np.array(data_summary_iani)[:,np.arange(i_timepoint-int(np.round(history_time*fps)),i_timepoint,1)]\n",
    "\n",
    "        input_datas[:,i_timepoint-int(np.round(history_time*fps)),:] = inputdata_xx\n",
    "        output_datas[i_timepoint-int(np.round(history_time*fps))] = event_pulls[i_timepoint]\n",
    "\n",
    "        output_datas_succ[i_timepoint-int(np.round(history_time*fps))] = event_pulls_succ[i_timepoint]\n",
    "        output_datas_fail[i_timepoint-int(np.round(history_time*fps))] = event_pulls_fail[i_timepoint]    \n",
    "        \n",
    "\n",
    "    # clean up the input_datas and output_datas\n",
    "    ind_withnan = np.isnan(np.sum(np.sum(input_datas,axis=2),axis=0))\n",
    "    input_datas = input_datas[:,~ind_withnan,:]\n",
    "    output_datas = output_datas[~ind_withnan]\n",
    "    output_datas_succ = output_datas_succ[~ind_withnan]\n",
    "    output_datas_fail = output_datas_fail[~ind_withnan]\n",
    "            \n",
    "    # run for nbootstraps repeat of GLM fitting\n",
    "    for ibootstrap in np.arange(0,nbootstraps,1):\n",
    "        \n",
    "        if not doSuccFailPull:\n",
    "            # create sub-samples for pull and no-pull events, with the same sample size\n",
    "            # for pull events\n",
    "            ind_pullevents = np.where(output_datas==1)[0]\n",
    "            np.random.seed(ibootstrap)\n",
    "            ind_pullevents_sampled = np.random.choice(ind_pullevents,size=samplesize,replace = True)\n",
    "\n",
    "            # for no pull events\n",
    "            ind_nopullevents = np.where(output_datas==0)[0]\n",
    "            np.random.seed(ibootstrap)\n",
    "            ind_nopullevents_sampled = np.random.choice(ind_nopullevents,size=samplesize,replace = True)\n",
    "\n",
    "            # put the two sampled events together \n",
    "            ind_events_sampled = np.append(ind_pullevents_sampled,ind_nopullevents_sampled)\n",
    "            #\n",
    "            output_datas_sampled = output_datas[ind_events_sampled]\n",
    "            input_datas_sampled = input_datas[:,ind_events_sampled,:]\n",
    "            \n",
    "        elif doSuccFailPull:\n",
    "            # create sub-samples for succ pull and fail pull events, with the same sample size\n",
    "            # for succ pull events\n",
    "            ind_succpullevents = np.where(output_datas_succ==1)[0]\n",
    "            np.random.seed(ibootstrap)\n",
    "            ind_succpullevents_sampled = np.random.choice(ind_succpullevents,size=samplesize,replace = True)\n",
    "\n",
    "            # for fail pull events\n",
    "            ind_failpullevents = np.where(output_datas_fail==1)[0]\n",
    "            np.random.seed(ibootstrap)\n",
    "            ind_failpullevents_sampled = np.random.choice(ind_failpullevents,size=samplesize,replace = True)\n",
    "            \n",
    "            # put the two sampled events together \n",
    "            ind_events_sampled = np.append(ind_succpullevents_sampled,ind_failpullevents_sampled)\n",
    "            #\n",
    "            output_datas_sampled = output_datas_succ[ind_events_sampled] # 1 successful pull; 0: failed pull\n",
    "            input_datas_sampled = input_datas[:,ind_events_sampled,:]\n",
    "        \n",
    "        \n",
    "        # GLM logsistic regression fitting\n",
    "        input_datas_sampled_combined = input_datas_sampled[0,:,:]\n",
    "        for i_var in np.arange(1,n_variables,1):\n",
    "            input_datas_sampled_combined = np.concatenate((input_datas_sampled_combined,input_datas_sampled[i_var,:,:]),axis=1)\n",
    "        input_datas_final = np.hstack((np.ones((samplesize*2,1)),input_datas_sampled_combined))\n",
    "\n",
    "\n",
    "        # GLM_model = sm.GLM(endog=output_datas_sampled, exog=input_datas_final,family=sm.families.Binomial())\n",
    "        #GLM_results = GLM_model.fit(max_iter=500, tol=1e-6, tol_criterion='params')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(input_datas[1,:,:][ind_failpullevents,:],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(input_datas_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add110a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(event_pulls_fail)+np.sum(event_pulls_succ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0223e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.nanmean(input_datas_final[:,np.arange(1+history_time*fps*6,1+history_time*fps*7,1)][np.arange(0,samplesize,1),:],axis=0))\n",
    "plt.plot(np.nanmean(input_datas_final[:,np.arange(1+history_time*fps*6,1+history_time*fps*7,1)][np.arange(samplesize,samplesize*2,1),:],axis=0))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((input_datas_final[:,np.arange(1+history_time*fps*6,1+history_time*fps*7,1)][np.arange(0,samplesize,1),:]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((input_datas_final[:,np.arange(1+history_time*fps*6,1+history_time*fps*7,1)][np.arange(samplesize,samplesize*2,1),:]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b40807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac3595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_subset_index = np.concatenate([np.array([0]),np.arange(1+history_time*fps*8,1+history_time*fps*9,3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pGLM_model = sm.GLM(endog=output_datas_sampled, \n",
    "                    exog=input_datas_final[:,input_data_subset_index],\n",
    "                    family=sm.families.Binomial())\n",
    "pGLM_results = pGLM_model.fit(max_iter=35, tol=1e-6, tol_criterion='params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e651a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pGLM_results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeeb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pGLM_results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc568cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from firthlogist import FirthLogisticRegression\n",
    "fl = FirthLogisticRegression()\n",
    "fl.fit(input_datas_final[:,input_data_subset_index], output_datas_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fl.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d773a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.discrete.discrete_model import Probit\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "model = Probit(output_datas_sampled, input_datas_final[:,input_data_subset_index])\n",
    "probit_model = model.fit(maxiter=100)\n",
    "betas = probit_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99967b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d593b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3057d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86494ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50925eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6379d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ec914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aaf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e995eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
