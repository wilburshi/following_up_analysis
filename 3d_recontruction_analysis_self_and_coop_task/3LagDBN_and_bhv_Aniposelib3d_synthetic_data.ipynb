{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c2ebd5",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on synthetic data\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## DBN on the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b000d",
   "metadata": {},
   "source": [
    "### prepare the synthetic data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntimepoint_synth = 5*600*1 # estimate for a 5 sessions (600s long) combined data\n",
    "\n",
    "eventnames = ['pull1','pull2','gaze1','gaze2']\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "# edge probability for behavioral events\n",
    "# rows: from node; column: to nodes\n",
    "if 0:\n",
    "    edgeProb_3lag = np.array([[0.0, 0.1, 0.9, 0.5],\n",
    "                              [0.0, 0.0, 0.6, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.6, 0.0, 0.0]])\n",
    "    edgeProb_2lag = np.array([[0.0, 0.9, 0.2, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.4],\n",
    "                              [0.7, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.7, 0.0, 0.0]])\n",
    "    edgeProb_1lag = np.array([[0.0, 0.0, 0.0, 0.4],\n",
    "                              [0.0, 0.0, 0.9, 0.0],\n",
    "                              [0.0, 0.7, 0.0, 0.0],\n",
    "                              [0.2, 0.0, 0.5, 0.0]])\n",
    "    # low probability\n",
    "    animal1_fixedorder = ['Diversed'] \n",
    "    animal2_fixedorder = ['Prob']\n",
    "if 1:\n",
    "    edgeProb_3lag = np.array([[0.0, 0.1, 0.3, 0.2],\n",
    "                              [0.0, 0.0, 0.2, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.2, 0.0, 0.0]])\n",
    "    edgeProb_2lag = np.array([[0.0, 0.3, 0.2, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.1],\n",
    "                              [0.2, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.2, 0.0, 0.0]])\n",
    "    edgeProb_1lag = np.array([[0.0, 0.0, 0.0, 0.1],\n",
    "                              [0.0, 0.0, 0.3, 0.0],\n",
    "                              [0.0, 0.2, 0.0, 0.0],\n",
    "                              [0.1, 0.0, 0.2, 0.0]])\n",
    "    # low probability\n",
    "    animal1_fixedorder = ['Low'] \n",
    "    animal2_fixedorder = ['Prob']\n",
    "\n",
    "\n",
    "DBN_group_typenames = ['synth_data_1','synth_data_2','synth_data_3']\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "DBN_input_data_synth = dict.fromkeys(DBN_group_typenames,[])\n",
    "\n",
    "pgazes_all = [0.05,0.01,0.001] # estimate based on Eddie/Sparkle: gaze time ~ 27s gaze in a 600s session\n",
    "ppulls_all = [0.05,0.01,0.001] # estimate based on Eddie/Sparkle: 150 pulls in a 600s (18,000 frames) session\n",
    "\n",
    "\n",
    "for igroup in np.arange(0,nDBN_groups,1):\n",
    "    \n",
    "    DBNgroup_name = DBN_group_typenames[igroup]\n",
    "    pgazes = pgazes_all[igroup]\n",
    "    ppulls = ppulls_all[igroup]\n",
    "    \n",
    "    # initialize the trace\n",
    "    pull1_synth_init = (np.random.rand(ntimepoint_synth)<ppulls).astype(int)\n",
    "    pull2_synth_init = (np.random.rand(ntimepoint_synth)<ppulls).astype(int)\n",
    "    gaze1_synth_init = (np.random.rand(ntimepoint_synth)<pgazes).astype(int)\n",
    "    gaze2_synth_init = (np.random.rand(ntimepoint_synth)<pgazes).astype(int)\n",
    "    #\n",
    "    alltrace_synth = dict.fromkeys(eventnames,[])\n",
    "    alltrace_synth['pull1'] = copy.deepcopy(pull1_synth_init)\n",
    "    alltrace_synth['pull2'] = copy.deepcopy(pull2_synth_init)\n",
    "    alltrace_synth['gaze1'] = copy.deepcopy(gaze1_synth_init)\n",
    "    alltrace_synth['gaze2'] = copy.deepcopy(gaze2_synth_init)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # using 3s sliding time window to turn on the events (to reflect the edges)\n",
    "    for ievent in np.arange(0,nevents,1):\n",
    "        # ievent: from event nodes\n",
    "        ieventname = eventnames[ievent]\n",
    "        for jevent in np.arange(0,nevents,1):\n",
    "            # jevent: to event nodes\n",
    "            jeventname = eventnames[jevent]\n",
    "            #\n",
    "            trace_synth_i = alltrace_synth[ieventname]\n",
    "            trace_synth_j = alltrace_synth[jeventname]\n",
    "            #\n",
    "\n",
    "            # 3s sliding time window\n",
    "            for itwin in np.arange(0,ntimepoint_synth-3,1):\n",
    "                # 3s lag\n",
    "                p_sample = np.random.rand(1)[0]\n",
    "                if (p_sample < edgeProb_3lag[ievent,jevent]) & ((trace_synth_i[itwin]==1)|(trace_synth_j[itwin+3]==1)):\n",
    "                    trace_synth_i[itwin] = 1\n",
    "                    trace_synth_j[itwin+3] = 1\n",
    "                #else:\n",
    "                #    trace_synth_i[itwin] = 0\n",
    "                #    trace_synth_j[itwin+3] = 0\n",
    "                # 2s lag\n",
    "                p_sample = np.random.rand(1)[0]\n",
    "                if (p_sample < edgeProb_2lag[ievent,jevent]) & ((trace_synth_i[itwin]==1)|(trace_synth_j[itwin+2]==1)):\n",
    "                    trace_synth_i[itwin] = 1\n",
    "                    trace_synth_j[itwin+2] = 1\n",
    "                #else:\n",
    "                #    trace_synth_i[itwin] = 0\n",
    "                #    trace_synth_j[itwin+2] = 0\n",
    "                # 1s lag\n",
    "                p_sample = np.random.rand(1)[0]\n",
    "                if (p_sample < edgeProb_1lag[ievent,jevent]) & ((trace_synth_i[itwin]==1)|(trace_synth_j[itwin+1]==1)):\n",
    "                    trace_synth_i[itwin] = 1\n",
    "                    trace_synth_j[itwin+1] = 1\n",
    "                #else:\n",
    "                #    trace_synth_i[itwin] = 0\n",
    "                #    trace_synth_j[itwin+1] = 0\n",
    "\n",
    "            alltrace_synth[ieventname] = trace_synth_i\n",
    "            alltrace_synth[jeventname] = trace_synth_j\n",
    "\n",
    "    pull1_synth_final = alltrace_synth['pull1']\n",
    "    pull2_synth_final = alltrace_synth['pull2'] \n",
    "    gaze1_synth_final = alltrace_synth['gaze1']\n",
    "    gaze2_synth_final = alltrace_synth['gaze2']\n",
    "\n",
    "    # same function used \n",
    "    time_point_pull1 = pd.DataFrame(np.where(pull1_synth_final)[0])\n",
    "    time_point_pull2 = pd.DataFrame(np.where(pull2_synth_final)[0])\n",
    "    oneway_gaze1 = np.where(gaze1_synth_final)[0]\n",
    "    oneway_gaze2 = np.where(gaze2_synth_final)[0]\n",
    "    mutual_gaze1 = np.array([])\n",
    "    mutual_gaze2 = np.array([])\n",
    "    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(ntimepoint_synth,0,1,time_point_pull1,time_point_pull2,oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2)\n",
    "\n",
    "\n",
    "    DBN_input_data_synth[DBNgroup_name] = bhv_df_itr\n",
    "\n",
    "print(np.sum(pull1_synth_final)/3000)\n",
    "print(np.sum(pull2_synth_final)/3000)\n",
    "print(np.sum(gaze1_synth_final)/3000)\n",
    "print(np.sum(gaze2_synth_final)/3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c8d47",
   "metadata": {},
   "source": [
    "### run the DBN model on the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 1 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_DBN_synth_data_3Lags/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(2000,3500,500)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        DBN_input_data_alltypes = DBN_input_data_synth\n",
    "\n",
    "                \n",
    "        # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "       \n",
    "        if not moreSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            samplingsizes = [min_samplesize,max_samplesize]\n",
    "            samplingsizes_name = ['min_row_number','max_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            # different session conditions (aka DBN groups)\n",
    "            for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                \n",
    "                try:\n",
    "                    bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "                    # bhv_df = bhv_df_all.sample(30*100,replace = True, random_state = round(time())) # take the subset for DBN training\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    # bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                    colnames = list(bhv_df_all.columns)\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = np.shape(from_pops)[0]\n",
    "                    nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[iDBN_group_typename] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[iDBN_group_typename] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[iDBN_group_typename] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[iDBN_group_typename] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[iDBN_group_typename] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[iDBN_group_typename] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[iDBN_group_typename] = sig_edges\n",
    "                    \n",
    "                except:\n",
    "                    DAGs_alltypes[iDBN_group_typename] = [] \n",
    "                    DAGs_shuffle_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                    DAGs_scores_alltypes[iDBN_group_typename] = []\n",
    "                    DAGs_shuffle_scores_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                    weighted_graphs_alltypes[iDBN_group_typename] = []\n",
    "                    weighted_graphs_shuffled_alltypes[iDBN_group_typename] = []\n",
    "                    sig_edges_alltypes[iDBN_group_typename] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_DBN_synth_data_3Lags/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:  \n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5ca47",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge with arrows; show the best time bin and row number; show the three time lag separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [3000]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "# eventnames = [\"M1pull\",\"M2pull\",\"M1gazeM2\",\"M2gazeM1\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "eventnodes_color = ['b','r','y','g']\n",
    "    \n",
    "savefigs = 1\n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,nDBN_groups+1)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*(nDBN_groups+1))\n",
    "\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3],[4,5,6,7],[8,9,10,11]]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "# add the real probability figure\n",
    "edgeProb_real = np.concatenate((edgeProb_3lag,edgeProb_2lag,edgeProb_1lag))\n",
    "\n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "    for iDBN_group in np.arange(0,nDBN_groups+1,1):\n",
    "\n",
    "        #try:\n",
    "            if iDBN_group < nDBN_groups:\n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group]\n",
    "\n",
    "                weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "                sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "\n",
    "                sig_avg_dags = weighted_graphs_tgt.mean(axis = 0) * sig_edges_tgt\n",
    "                sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "            else:\n",
    "                iDBN_group_typename = 'real edge prob'\n",
    "                sig_avg_dags = edgeProb_real[fromRowID,:]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,iDBN_group].set_title(iDBN_group_typename,fontsize=18)\n",
    "            axs[ilag*2+0,iDBN_group].set_xlim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_ylim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,iDBN_group].axis('equal')\n",
    "\n",
    "            \n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],'.',markersize=60,markerfacecolor=eventnodes_color[ieventnode],markeredgecolor='none')              \n",
    "                axs[ilag*2+0,iDBN_group].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                                       eventnames[ieventnode],fontsize=15)\n",
    "                \n",
    "                clmap = mpl.cm.get_cmap('Blues')\n",
    "                \n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt>0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        width=0.04*abs(edge_weight_tgt),\n",
    "                                                        color = clmap(edge_weight_tgt))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      0.04*abs(edge_weight_tgt),\n",
    "                                                      color = clmap(edge_weight_tgt))\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap(edge_weight_tgt), \n",
    "                                    edgecolor=clmap(edge_weight_tgt)\n",
    "                                )\n",
    "                                axs[ilag*2+0,iDBN_group].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap(edge_weight_tgt),\n",
    "                                                            head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt)\n",
    "                                else:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap(edge_weight_tgt),\n",
    "                                                            head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt)\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = 0,1\n",
    "            import matplotlib as mpl\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,iDBN_group].pcolormesh(sig_avg_dags_df,cmap=\"Blues\",norm=norm)\n",
    "            #\n",
    "            if iDBN_group == nDBN_groups-1:\n",
    "                cax = axs[ilag*2+1,iDBN_group].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,iDBN_group], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,iDBN_group].axis('equal')\n",
    "            axs[ilag*2+1,iDBN_group].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,iDBN_group].set_xticks(np.arange(0.5,4.5,1))\n",
    "            axs[ilag*2+1,iDBN_group].set_xticklabels(eventnames)\n",
    "            if iDBN_group == 0:\n",
    "                axs[ilag*2+1,iDBN_group].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks(np.arange(0.5,4.5,1))\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,iDBN_group].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,iDBN_group].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks([])\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels([])\n",
    "\n",
    "        #except:\n",
    "        #    continue\n",
    "    \n",
    "if savefigs:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_synthetic_data/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows.jpg')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_Aniposelib3d_synthetic_data/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'.jpg')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267a788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778d1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a760892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d652a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d450fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46539506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82775e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734e85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea9082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
