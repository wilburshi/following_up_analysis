{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN has run and this script is used to make predictions\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval\n",
    "from ana_functions.bhv_events_interval import bhv_events_interval_certainEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 2*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20220909\",\"20220912\",\"20220915\",\"20220920\",\"20220922\",\"20220923\",\"20221010\",\n",
    "                      \"20221011\",\"20221013\",\"20221014\",\"20221015\",\"20221017\",\"20230215\",     \n",
    "                      \"20221018\",\"20221019\",\"20221020\",\"20221021\",\"20221022\",\"20221026\",\"20221028\",\"20221030\",\n",
    "                      \"20221107\",\"20221108\",\"20221109\",\"20221110\",\"20221111\",\"20221114\",\"20221115\",\"20221116\",\n",
    "                      \"20221117\",\"20221118\",\"20221121\",\"20221122\",\"20221123\",\"20221125\",\"20221128\",\"20221129\",              \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221212\",\"20221214\",\"20221216\",\"20221219\",\"20221220\",\n",
    "                      \"20221221\",\"20230208\",\"20230209\",\"20230213\",\"20230214\",\"20230111\",\"20230112\",\"20230201\",\n",
    "\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 6.50, 18.10, 0,      33.03, 549.0, 116.80, 6.50,\n",
    "                                 2.80, 27.80, 272.50, 27.90, 27.00,  33.00,\n",
    "                                28.70, 45.30, 21.10,  27.10, 51.90,  21.00, 30.80, 17.50,                      \n",
    "                                15.70,  2.65, 27.30,   0.00,  0.00,  71.80,  0.00,  0.00, \n",
    "                                75.50, 20.20,  0.00,  24.20, 36.70,  26.40, 22.50, 28.50,                       \n",
    "                                 0.00,  0.00, 21.70,  84.70, 17.00,  19.80, 23.50, 25.20,  \n",
    "                                 0.00,  0.00,  0.00,   0.00,  0.00, 130.00, 14.20, 24.20, \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      # \"20220912\",\n",
    "                      \"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                      \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                      \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                      \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                      \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                      \"20230117\",\"20230118\",\"20230124\",\n",
    "                      # \"20230126\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                # 18.10, \n",
    "                                 0.00, 33.03,  6.50,  0.00, \n",
    "                                 2.80, 27.80, 27.90, 27.00,  \n",
    "                                51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                 0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                 0.00,  0.00,  0.00, \n",
    "                                 # 0.00,  \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# eddie sparkle\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20221122\",\"20221125\",\"20221128\",\"20221129\",\"20221130\",\"20221202\",\"20221206\",\n",
    "                      \"20221207\",\"20221208\",\"20221209\",\"20230126\",\"20230127\",\"20230130\",\"20230201\",\"20230203-1\",\n",
    "                      \"20230206\",\"20230207\",\"20230208-1\",\"20230209\",\"20230222\",\"20230223-1\",\"20230227-1\",\n",
    "                      \"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230321\",\"20230322\",\"20230324\",\"20230327\",\"20230328\",\n",
    "                      \"20230330\",\"20230331\",\"20230403\",\"20230404\",\"20230405\",\"20230406\",\"20230407\",\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 8.00,38.00,1.00,3.00,5.00,9.50,1.00,\n",
    "                                 4.50,4.50,5.00,38.00,166.00,4.20,3.80,3.60,\n",
    "                                 7.50,9.00,7.50,8.50,14.50,7.80,8.00,7.50,\n",
    "                                 8.00,8.00,4.00,123.00,14.00,8.80,\n",
    "                                 7.00,7.50,5.50,11.00,9.00,\n",
    "                                 17.00,4.50,9.30,25.50,20.40,21.30,24.80,\n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20221122\",  \"20221125\",  \n",
    "                      \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                      \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                      \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                      \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                      \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                  8.00,  38.00, \n",
    "                                  9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                  9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                  8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                  7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                  4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "# ginger kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230209\",\"20230213\",\"20230214\",\"20230216\",\"20230222\",\"20230223\",\"20230228\",\"20230302\",\n",
    "                      \"20230303\",\"20230307\",\"20230314\",\"20230315\",\"20230316\",\"20230317\"         \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00,  0.00, 48.00, 26.20, 18.00, 23.00, 28.50,\n",
    "                                34.00, 25.50, 25.50, 31.50, 28.00, 30.50\n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      #\"20230213\",\n",
    "                      \"20230214\",\"20230216\",\n",
    "                      \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                      \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                      \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                      \"20230323\",\"20230412\",\"20230413\",\"20230517\",\n",
    "                      \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                # 0.00, \n",
    "                                 0.00, 48.00, \n",
    "                                23.00, 28.50, 34.00, 25.50, \n",
    "                                25.50, 31.50, 28.00, 30.50,\n",
    "                                 0.00,  0.00,  0.00,  0.00, \n",
    "                                 0.00,  0.00,  0.00,  0.00, \n",
    "                                 0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20230718\",\"20230720\",\"20230914\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                      \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                      \"20231013\",\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 0, 0, 0, 0, 0, 0, \n",
    "                                 0, 0, 0, 0, 0, 0,\n",
    "                                 0, 0, 0, 0, \n",
    "                              ] # in second \n",
    "    elif do_bestsession:   \n",
    "        dates_list = [\n",
    "                      \"20230718\",\"20230720\",\"20230914\",\"20230726\",\"20230727\",\"20230809\",\n",
    "                      \"20230810\",\"20230811\",\"20230814\",\"20230816\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                      \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                      \"20231013\",\"20231020\",\"20231024\",\"20231025\",\n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                    0,    0,    0, 32.2, 27.2, 37.5,\n",
    "                                 21.0, 21.5, 19.8, 32.0,    0,    0,   0, \n",
    "                                    0,    0,    0,    0,    0,    0,\n",
    "                                    0,    0,    0,    0, \n",
    "                              ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "    \n",
    "# Koala Vermelho\n",
    "if 0:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \"20231221\",  \"20231222\",\"20231226\",\"20231227\",  \"20231229\",\"20231230\",\n",
    "                      \"20231231\",  \"20240102\",\"20240104\",\"20240104-2\",\"20240105\",\"20240108\",\n",
    "                      \"20240109\",  \"20240115\",\"20240116\",\"20240117\",  \"20240118\",\"20240119\",\n",
    "                      \"20240122\",  \"20240123\",\"20240124\",\"20240125\",  \"20240126\",\"20240129\",\n",
    "                      \"20240130\",  \"20240131\",\"20240201\",\"20240202\",  \"20240205\",\"20240206_1\",\n",
    "                      \"20240206_2\",\"20240207\",\"20240208\",\"20240209\",  \"20240212\",\"20240213\",\n",
    "                      \"20240214\",  \"20240215\",\"20240216\",\"20240219\",  \"20240220\",\"20240222\", \n",
    "                      \"20240223\",  \"20240226\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00, \n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00, \n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                0.00,  0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                0.00,  0.00,  \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20231222\",\"20231226\",\"20231227\",  \"20231229\",\"20231230\",\n",
    "                      \"20231231\",\"20240102\",\"20240104-2\",\"20240105\",\"20240108\",\n",
    "                      \"20240109\",\"20240115\",\"20240116\",  \"20240117\",\"20240118\",\"20240119\",\n",
    "                      \"20240207\",\"20240208\",\"20240209\",  \"20240212\",\"20240213\",\n",
    "                      \"20240214\",\"20240215\",\"20240216\", \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                21.5,  0.00,  0.00,  0.00,  0.00, \n",
    "                                0.00,  12.2,  0.00,  18.8,  31.2,  \n",
    "                                32.5,  0.00,  50.0,  0.00,  37.5,  29.5,\n",
    "                                58.5,  72.0,  0.00,  71.5,  70.5,\n",
    "                                86.8,  94.0,  65.0,  \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['koala']\n",
    "    animal2_fixedorder = ['vermelho']\n",
    "\n",
    "    animal1_filename = \"Koala\"\n",
    "    animal2_filename = \"Vermelho\"\n",
    "    \n",
    "    \n",
    "#    \n",
    "# dates_list = [\"20230718\"]\n",
    "# session_start_times = [0.00] # in second\n",
    "\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_edges_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467f2e8",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### For each condition, only use the hypothetical dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53984386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "redoFitting = 1\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['coop(1s)']\n",
    "# DBN_group_typeIDs  =  [3]\n",
    "# DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN input data - to and from Nodes\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = '' # '' means 1secondlag, otherwise will be specificed\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "    \n",
    "# hypothetical graph structure that reflect the strategies\n",
    "# hypothetical graph structure that reflect the strategies\n",
    "strategynames = ['threeMains','sync_pulls','gaze_lead_pull','social_attention',\n",
    "                 'other_dependencies','other_noself_dependcies']\n",
    "# strategynames = ['threeMains',]\n",
    "# strategynames = ['gaze_lead_pull'] # ['all_threes','sync_pulls','gaze_lead_pull','social_attention']\n",
    "bina_graphs_specific_strategy = {\n",
    "    'threeMains': np.array([[0,1,0,1],[1,0,1,0],[1,0,0,0],[0,1,0,0]]),\n",
    "    'sync_pulls': np.array([[0,1,0,0],[1,0,0,0],[0,0,0,0],[0,0,0,0]]),\n",
    "    'gaze_lead_pull':np.array([[0,0,0,0],[0,0,0,0],[1,0,0,0],[0,1,0,0]]),\n",
    "    'social_attention':np.array([[0,0,0,1],[0,0,1,0],[0,0,0,0],[0,0,0,0]]),\n",
    "    'other_dependencies': np.array([[1,0,1,0],[0,1,0,1],[0,1,1,1],[1,0,1,1]]),\n",
    "    'other_noself_dependcies': np.array([[0,0,1,0],[0,0,0,1],[0,1,0,1],[1,0,1,0]]),\n",
    "}\n",
    "nstrategies_forplot = np.shape(strategynames)[0]\n",
    "\n",
    "\n",
    "for istrg in np.arange(0,nstrategies_forplot,1):\n",
    "    \n",
    "    strategyname = strategynames[istrg]\n",
    "\n",
    "    #\n",
    "    bina_graph_mean_strg = bina_graphs_specific_strategy[strategyname]\n",
    "    \n",
    "    # translate the binary DAGs to edge\n",
    "    nrows,ncols = np.shape(bina_graph_mean_strg)\n",
    "    edgenames = []\n",
    "    for irow in np.arange(0,nrows,1):\n",
    "        for icol in np.arange(0,ncols,1):\n",
    "            if bina_graph_mean_strg[irow,icol] > 0:\n",
    "                edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "    # define the DBN predicting model\n",
    "    bn = BayesianNetwork()\n",
    "    bn.add_nodes_from(fromNodes)\n",
    "    bn.add_nodes_from(toNodes)\n",
    "    bn.add_edges_from(edgenames)\n",
    "    \n",
    "    effect_slice = toNodes\n",
    "    \n",
    "    # load ROC_summary_all data\n",
    "    try:\n",
    "        if redoFitting:\n",
    "            dumpy\n",
    "        \n",
    "        print('load all ROC data for hypothetical dependencies, and only plot the summary figure')\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_'+strategyname+timelagtype+'.pkl', 'rb') as f:\n",
    "            ROC_summary_all = pickle.load(f)\n",
    "   \n",
    "    except:  \n",
    "    \n",
    "        # initialize a summary dataframe for plotting the summary figure across animals \n",
    "        ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "        fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "        fig.set_figheight(8*nanimalpairs)\n",
    "        fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "            # initiate figure\n",
    "            # fig, axs = plt.subplots(nDBN_groups,2) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "            # fig.set_figheight(8*nDBN_groups)\n",
    "            # fig.set_figwidth(15*2)\n",
    "\n",
    "            # load the DBN input data\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "            #\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            # load the DBN training outcome\n",
    "            if moreSampSize:\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "            # make sure these variables are the same as in the previous steps\n",
    "            # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            ntemp_reses = np.shape(temp_resolus)[0]\n",
    "            #\n",
    "            if moreSampSize:\n",
    "                # different data (down/re)sampling numbers\n",
    "                # samplingsizes = np.arange(1100,3000,100)\n",
    "                samplingsizes = [1100]\n",
    "                # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "                # samplingsizes = [100,500]\n",
    "                # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "                samplingsizes_name = list(map(str, samplingsizes))\n",
    "            else:\n",
    "                samplingsizes_name = ['min_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "            #\n",
    "            temp_resolu = temp_resolus[0]\n",
    "            j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "            for igroup in np.arange(0,nDBN_groups,1):\n",
    "                DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "                DBN_input_data_tgt = DBN_input_data_alltypes[DBN_group_typename]\n",
    "                weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "                weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "                # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "                sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "                # mean weight \n",
    "                weighted_graphs_mean_tgt = np.nanmean(weighted_graphs_tgt,axis=0)\n",
    "                weighted_graphs_mean_tgt = weighted_graphs_mean_tgt * sig_edges_tgt\n",
    "\n",
    "                bina_graphs_mean_tgt = sig_edges_tgt\n",
    "\n",
    "\n",
    "                # run niters iterations for each condition\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                    # Split data into training and testing sets\n",
    "                    train_data, test_data = train_test_split(DBN_input_data_tgt, test_size=0.2)\n",
    "                    # test_data = DBN_input_data_tgt\n",
    "\n",
    "                    # Perform parameter learning for each time slice\n",
    "                    bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                    # Perform inference\n",
    "                    infer = VariableElimination(bn)\n",
    "\n",
    "                    # Prediction for each behavioral events\n",
    "                    for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                        var = effect_slice[ievent]\n",
    "                        Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                        for index, row in test_data.iterrows():\n",
    "                            evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                        fromNodes[1]: row[fromNodes[1]], \n",
    "                                        fromNodes[2]: row[fromNodes[2]], \n",
    "                                        fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                            # Query the probability distribution for Pulls given evidence\n",
    "                            aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                            # Extract the probability of outcome = 1\n",
    "                            prob = aucPpredBehavior.values[1]\n",
    "                            Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                        # Calculate the AUC score\n",
    "                        trueBeh = test_data[var].values\n",
    "                        try:\n",
    "                            auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                        except:\n",
    "                            auc = np.nan\n",
    "                        print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                        # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                        if iiter == 0:\n",
    "                            fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                            axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                         label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                        # put data in the summarizing data frame\n",
    "                        if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                            ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc\n",
    "                                                                     }, ignore_index=True)\n",
    "                        else:\n",
    "                            ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "                    if iiter == 0:\n",
    "                        axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                        axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                        axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                        axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                          +'\\n'+DBN_group_typename+'; with dependency:'+strategyname+timelagtype)\n",
    "                        axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            fig.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_onlydependency'+strategyname+timelagtype+'.pdf')\n",
    "\n",
    "        \n",
    "        \n",
    "        # save the summarizing data ROC_summary_all\n",
    "        savedata = 1\n",
    "        if savedata:\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "\n",
    "            with open(data_saved_subfolder+'/ROC_summary_all_dependencies_'+strategyname+timelagtype+'.pkl', 'wb') as f:\n",
    "                pickle.dump(ROC_summary_all, f)\n",
    "\n",
    "            \n",
    "            \n",
    "    # average ROC across all iterations\n",
    "    # separate Kanga into two groups based on her partner\n",
    "    ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    #\n",
    "    animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "    nanis = np.shape(animals_ROC)[0]\n",
    "    actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "    nacts = np.shape(actions_ROC)[0]\n",
    "    testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "    ntcons = np.shape(testCons_ROC)[0]\n",
    "    ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "    #\n",
    "    for iani in np.arange(0,nanis,1):\n",
    "        animal_ROC = animals_ROC[iani]\n",
    "        ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "        ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "        #\n",
    "        for iact in np.arange(0,nacts,1):\n",
    "            action_ROC = actions_ROC[iact]\n",
    "            ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "            ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "            #\n",
    "            for itcon in np.arange(0,ntcons,1):\n",
    "                testCon_ROC = testCons_ROC[itcon]\n",
    "                ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "                ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "                #\n",
    "                ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                    'action':action_ROC,\n",
    "                                                                    'testCondition':testCon_ROC,\n",
    "                                                                    'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                     }, ignore_index=True)        \n",
    "    \n",
    "    # plot the summarizing figure\n",
    "    fig2, axs2 = plt.subplots(1,1)\n",
    "    fig2.set_figheight(5)\n",
    "    fig2.set_figwidth(5)\n",
    "    seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                    errorbar='ci',alpha=.5)\n",
    "    seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                      alpha=.9,size=6,dodge=True,legend=False)\n",
    "    plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "    axs2.set_xlim([-0.5,1.5])\n",
    "    axs2.set_title('all animal'+'; with dependency:'+strategyname+timelagtype)\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_onlydependency'+strategyname+timelagtype+'_summarizingplot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629042f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623a1652",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### For each condition, only use the hypothetical dependencies together with the self dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d697948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "redoFitting = 1\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['coop(1s)']\n",
    "# DBN_group_typeIDs  =  [3]\n",
    "# DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN input data - to and from Nodes\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = '' # '' means 1secondlag, otherwise will be specificed\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "# hypothetical graph structure that reflect the strategies\n",
    "# hypothetical graph structure that reflect the strategies\n",
    "strategynames = ['threeMains','sync_pulls','gaze_lead_pull','social_attention',\n",
    "                 ]\n",
    "# strategynames = ['threeMains',]\n",
    "# strategynames = ['gaze_lead_pull'] # ['all_threes','sync_pulls','gaze_lead_pull','social_attention']\n",
    "bina_graphs_specific_strategy = {\n",
    "    'threeMains': np.array([[1,1,0,1],[1,1,1,0],[1,0,1,0],[0,1,0,1]]),\n",
    "    'sync_pulls': np.array([[1,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1]]),\n",
    "    'gaze_lead_pull':np.array([[1,0,0,0],[0,1,0,0],[1,0,1,0],[0,1,0,1]]),\n",
    "    'social_attention':np.array([[1,0,0,1],[0,1,1,0],[0,0,1,0],[0,0,0,1]]),\n",
    "}\n",
    "nstrategies_forplot = np.shape(strategynames)[0]\n",
    "\n",
    "\n",
    "for istrg in np.arange(0,nstrategies_forplot,1):\n",
    "    \n",
    "    strategyname = strategynames[istrg]\n",
    "\n",
    "    #\n",
    "    bina_graph_mean_strg = bina_graphs_specific_strategy[strategyname]\n",
    "    \n",
    "    # translate the binary DAGs to edge\n",
    "    nrows,ncols = np.shape(bina_graph_mean_strg)\n",
    "    edgenames = []\n",
    "    for irow in np.arange(0,nrows,1):\n",
    "        for icol in np.arange(0,ncols,1):\n",
    "            if bina_graph_mean_strg[irow,icol] > 0:\n",
    "                edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "    # define the DBN predicting model\n",
    "    bn = BayesianNetwork()\n",
    "    bn.add_nodes_from(fromNodes)\n",
    "    bn.add_nodes_from(toNodes)\n",
    "    bn.add_edges_from(edgenames)\n",
    "    \n",
    "    effect_slice = toNodes\n",
    "    \n",
    "    # load ROC_summary_all data\n",
    "    try:\n",
    "        if redoFitting:\n",
    "            dumpy\n",
    "        \n",
    "        print('load all ROC data for hypothetical dependencies (with self edges), and only plot the summary figure')\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_'+strategyname+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "            ROC_summary_all = pickle.load(f)\n",
    "   \n",
    "    except:  \n",
    "    \n",
    "        # initialize a summary dataframe for plotting the summary figure across animals \n",
    "        ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "        fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "        fig.set_figheight(8*nanimalpairs)\n",
    "        fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "        for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "            # initiate figure\n",
    "            # fig, axs = plt.subplots(nDBN_groups,2) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "            # fig.set_figheight(8*nDBN_groups)\n",
    "            # fig.set_figwidth(15*2)\n",
    "\n",
    "            # load the DBN input data\n",
    "            animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "            animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "            #\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            # load the DBN training outcome\n",
    "            if moreSampSize:\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "            # make sure these variables are the same as in the previous steps\n",
    "            # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            ntemp_reses = np.shape(temp_resolus)[0]\n",
    "            #\n",
    "            if moreSampSize:\n",
    "                # different data (down/re)sampling numbers\n",
    "                # samplingsizes = np.arange(1100,3000,100)\n",
    "                samplingsizes = [1100]\n",
    "                # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "                # samplingsizes = [100,500]\n",
    "                # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "                samplingsizes_name = list(map(str, samplingsizes))\n",
    "            else:\n",
    "                samplingsizes_name = ['min_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "            #\n",
    "            temp_resolu = temp_resolus[0]\n",
    "            j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "            for igroup in np.arange(0,nDBN_groups,1):\n",
    "                DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "                DBN_input_data_tgt = DBN_input_data_alltypes[DBN_group_typename]\n",
    "                weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "                weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "                # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "                sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "                # mean weight \n",
    "                weighted_graphs_mean_tgt = np.nanmean(weighted_graphs_tgt,axis=0)\n",
    "                weighted_graphs_mean_tgt = weighted_graphs_mean_tgt * sig_edges_tgt\n",
    "\n",
    "                bina_graphs_mean_tgt = sig_edges_tgt\n",
    "\n",
    "\n",
    "                # run niters iterations for each condition\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                    # Split data into training and testing sets\n",
    "                    train_data, test_data = train_test_split(DBN_input_data_tgt, test_size=0.2)\n",
    "                    # test_data = DBN_input_data_tgt\n",
    "\n",
    "                    # Perform parameter learning for each time slice\n",
    "                    bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                    # Perform inference\n",
    "                    infer = VariableElimination(bn)\n",
    "\n",
    "                    # Prediction for each behavioral events\n",
    "                    for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                        var = effect_slice[ievent]\n",
    "                        Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                        for index, row in test_data.iterrows():\n",
    "                            evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                        fromNodes[1]: row[fromNodes[1]], \n",
    "                                        fromNodes[2]: row[fromNodes[2]], \n",
    "                                        fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                            # Query the probability distribution for Pulls given evidence\n",
    "                            aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                            # Extract the probability of outcome = 1\n",
    "                            prob = aucPpredBehavior.values[1]\n",
    "                            Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                        # Calculate the AUC score\n",
    "                        trueBeh = test_data[var].values\n",
    "                        try:\n",
    "                            auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                        except:\n",
    "                            auc = np.nan\n",
    "                        print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                        # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                        if iiter == 0:\n",
    "                            fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                            axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                         label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                        # put data in the summarizing data frame\n",
    "                        if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                            ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc\n",
    "                                                                     }, ignore_index=True)\n",
    "                        else:\n",
    "                            ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                      'action':eventnames[ievent][2:],\n",
    "                                                                      'testCondition':DBN_group_typename,\n",
    "                                                                      'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "                    if iiter == 0:\n",
    "                        axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                        axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                        axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                        axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                          +'\\n'+DBN_group_typename+'; with dependency:'+strategyname+timelagtype)\n",
    "                        axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            fig.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_onlydependency'+strategyname+timelagtype+'_withSelfEdge.pdf')\n",
    "\n",
    "        \n",
    "        \n",
    "        # save the summarizing data ROC_summary_all\n",
    "        savedata = 1\n",
    "        if savedata:\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "            if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "\n",
    "            with open(data_saved_subfolder+'/ROC_summary_all_dependencies_'+strategyname+timelagtype+'_withSelfEdge.pkl', 'wb') as f:\n",
    "                pickle.dump(ROC_summary_all, f)\n",
    "\n",
    "            \n",
    "            \n",
    "    # average ROC across all iterations\n",
    "    # separate Kanga into two groups based on her partner\n",
    "    ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    #\n",
    "    animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "    nanis = np.shape(animals_ROC)[0]\n",
    "    actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "    nacts = np.shape(actions_ROC)[0]\n",
    "    testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "    ntcons = np.shape(testCons_ROC)[0]\n",
    "    ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "    #\n",
    "    for iani in np.arange(0,nanis,1):\n",
    "        animal_ROC = animals_ROC[iani]\n",
    "        ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "        ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "        #\n",
    "        for iact in np.arange(0,nacts,1):\n",
    "            action_ROC = actions_ROC[iact]\n",
    "            ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "            ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "            #\n",
    "            for itcon in np.arange(0,ntcons,1):\n",
    "                testCon_ROC = testCons_ROC[itcon]\n",
    "                ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "                ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "                #\n",
    "                ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                    'action':action_ROC,\n",
    "                                                                    'testCondition':testCon_ROC,\n",
    "                                                                    'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                     }, ignore_index=True)        \n",
    "    \n",
    "    # plot the summarizing figure\n",
    "    fig2, axs2 = plt.subplots(1,1)\n",
    "    fig2.set_figheight(5)\n",
    "    fig2.set_figwidth(5)\n",
    "    seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                    errorbar='ci',alpha=.5)\n",
    "    seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                      alpha=.9,size=6,dodge=True,legend=False)\n",
    "    plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "    axs2.set_xlim([-0.5,1.5])\n",
    "    axs2.set_title('all animal'+'; with dependency:'+strategyname+timelagtype)\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_onlydependency'+strategyname+timelagtype+'_withSelfEdge_summarizingplot.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67567d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fefcfe67",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### training set and testing set are from the same conditions\n",
    "### use the DBN learned structure, only the 0 1 DAG, do not consider the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "redoFitting = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['coop(1s)']\n",
    "# DBN_group_typeIDs  =  [3]\n",
    "# DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN model\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = 'allthreelags'\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3], [4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1and2secondlag'\n",
    "# time_lags = ['t_-2','t_-1']\n",
    "# fromRowIDs =[[4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1secondlag'\n",
    "# time_lags = ['t_-1']\n",
    "# fromRowIDs =[[8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '2secondlag'\n",
    "# time_lags = ['t_-2']\n",
    "# fromRowIDs =[[4,5,6,7]]\n",
    "#\n",
    "# timelagtype = '3secondlag'\n",
    "# time_lags = ['t_-3']\n",
    "# fromRowIDs =[[0,1,2,3]]\n",
    "#\n",
    "nlags = np.shape(fromRowIDs)[0]\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "\n",
    "# load ROC_summary_all data\n",
    "try:\n",
    "    if redoFitting:\n",
    "        dumpy\n",
    "    print('load all ROC data for within task condition (only binary dependencies), and only plot the summary figure')\n",
    "        \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_'+timelagtype+'.pkl', 'rb') as f:\n",
    "        ROC_summary_all = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    # initialize a summary dataframe for plotting the summary figure across animals \n",
    "    ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "    fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "    fig.set_figheight(7*nanimalpairs)\n",
    "    fig.set_figwidth(7*nDBN_groups)\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        # load the DBN input data\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "        #\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "        # load the DBN training outcome\n",
    "        if moreSampSize:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "        # make sure these variables are the same as in the previous steps\n",
    "        # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]\n",
    "        #\n",
    "        if moreSampSize:\n",
    "            # different data (down/re)sampling numbers\n",
    "            # samplingsizes = np.arange(1100,3000,100)\n",
    "            samplingsizes = [1100]\n",
    "            # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "            # samplingsizes = [100,500]\n",
    "            # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "            samplingsizes_name = list(map(str, samplingsizes))\n",
    "        else:\n",
    "            samplingsizes_name = ['min_row_number']   \n",
    "        nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "        for igroup in np.arange(0,nDBN_groups,1):\n",
    "            DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "            DBN_input_data_tgt = DBN_input_data_alltypes[DBN_group_typename]\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "            # mean weight \n",
    "            weighted_graphs_mean_tgt = np.nanmean(weighted_graphs_tgt,axis=0)\n",
    "            weighted_graphs_mean_tgt = weighted_graphs_mean_tgt * sig_edges_tgt\n",
    "\n",
    "            bina_graphs_mean_tgt = sig_edges_tgt\n",
    "            #\n",
    "            # consider the time lags\n",
    "            if nlags == 1:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:] \n",
    "            elif nlags == 2:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]\n",
    "            elif nlags == 3:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]+bina_graphs_mean_tgt[fromRowIDs[2],:]\n",
    "\n",
    "            \n",
    "\n",
    "            # translate the binary DAGs to edge\n",
    "            nrows,ncols = np.shape(bina_graphs_mean_tgt)\n",
    "            edgenames = []\n",
    "            for irow in np.arange(0,nrows,1):\n",
    "                for icol in np.arange(0,ncols,1):\n",
    "                    if bina_graphs_mean_tgt[irow,icol] > 0:\n",
    "                        edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "            # define the DBN predicting model\n",
    "            bn = BayesianNetwork()\n",
    "            bn.add_nodes_from(fromNodes)\n",
    "            bn.add_nodes_from(toNodes)\n",
    "            bn.add_edges_from(edgenames)\n",
    "\n",
    "            effect_slice = toNodes        \n",
    "\n",
    "            # run niters iterations for each condition\n",
    "            for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                # Split data into training and testing sets\n",
    "                train_data, test_data = train_test_split(DBN_input_data_tgt, test_size=0.2)\n",
    "                # test_data = DBN_input_data_tgt\n",
    "\n",
    "                # Perform parameter learning for each time slice\n",
    "                bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                # Perform inference\n",
    "                infer = VariableElimination(bn)\n",
    "\n",
    "                # Prediction for each behavioral events\n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                    var = effect_slice[ievent]\n",
    "                    Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                    fromNodes[1]: row[fromNodes[1]], \n",
    "                                    fromNodes[2]: row[fromNodes[2]], \n",
    "                                    fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                        # Query the probability distribution for Pulls given evidence\n",
    "                        aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                        # Extract the probability of outcome = 1\n",
    "                        prob = aucPpredBehavior.values[1]\n",
    "                        Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                    # Calculate the AUC score\n",
    "                    trueBeh = test_data[var].values\n",
    "                    try:\n",
    "                        auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                    except:\n",
    "                        auc = np.nan\n",
    "                    print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                    # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                    if iiter == 0:\n",
    "                        fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                        axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                     label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                    # put data in the summarizing data frame\n",
    "                    if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "                    else:\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                             }, ignore_index=True)\n",
    "\n",
    "                if iiter == 0:\n",
    "                    axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                    axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                      +'\\n'+DBN_group_typename+'; with dependency:'+strategyname)\n",
    "                    axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_'+timelagtype+'.pdf')\n",
    "\n",
    "        \n",
    "    # save the summarizing data ROC_summary_all\n",
    "    savedata = 1\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_'+timelagtype+'.pkl', 'wb') as f:\n",
    "            pickle.dump(ROC_summary_all, f)\n",
    "        \n",
    "\n",
    "# average ROC across all iterations\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                'action':action_ROC,\n",
    "                                                                'testCondition':testCon_ROC,\n",
    "                                                                'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "        \n",
    "# plot the summarizing figure\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(5)\n",
    "seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                errorbar='ci',alpha=.5)\n",
    "seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                  alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_'+timelagtype+'_summarizingplot.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a41c6",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### training set and testing set are from the same conditions\n",
    "### use the DBN learned structure, consider the weight as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "redoFitting = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['coop(1s)']\n",
    "# DBN_group_typeIDs  =  [3]\n",
    "# DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN model\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = 'allthreelags'\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3], [4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1and2secondlag'\n",
    "# time_lags = ['t_-2','t_-1']\n",
    "# fromRowIDs =[[4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1secondlag'\n",
    "# time_lags = ['t_-1']\n",
    "# fromRowIDs =[[8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '2secondlag'\n",
    "# time_lags = ['t_-2']\n",
    "# fromRowIDs =[[4,5,6,7]]\n",
    "#\n",
    "# timelagtype = '3secondlag'\n",
    "# time_lags = ['t_-3']\n",
    "# fromRowIDs =[[0,1,2,3]]\n",
    "#\n",
    "nlags = np.shape(fromRowIDs)[0]\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "\n",
    "# load ROC_summary_all data\n",
    "try:\n",
    "    if redoFitting:\n",
    "        dumpy\n",
    "    print('load all ROC data for within task condition (weighted dependencies), and only plot the summary figure')\n",
    "        \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_'+timelagtype+'.pkl', 'rb') as f:\n",
    "        ROC_summary_all = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    # initialize a summary dataframe for plotting the summary figure across animals \n",
    "    ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "    fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "    fig.set_figheight(7*nanimalpairs)\n",
    "    fig.set_figwidth(7*nDBN_groups)\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        # load the DBN input data\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "        #\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "        # load the DBN training outcome\n",
    "        if moreSampSize:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "        # make sure these variables are the same as in the previous steps\n",
    "        # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]\n",
    "        #\n",
    "        if moreSampSize:\n",
    "            # different data (down/re)sampling numbers\n",
    "            # samplingsizes = np.arange(1100,3000,100)\n",
    "            samplingsizes = [1100]\n",
    "            # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "            # samplingsizes = [100,500]\n",
    "            # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "            samplingsizes_name = list(map(str, samplingsizes))\n",
    "        else:\n",
    "            samplingsizes_name = ['min_row_number']   \n",
    "        nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "        for igroup in np.arange(0,nDBN_groups,1):\n",
    "            DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "            DBN_input_data_tgt = DBN_input_data_alltypes[DBN_group_typename]\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "            # mean weight \n",
    "            weighted_graphs_mean_tgt = np.nanmean(weighted_graphs_tgt,axis=0)\n",
    "            weighted_graphs_mean_tgt = weighted_graphs_mean_tgt * sig_edges_tgt\n",
    "\n",
    "            bina_graphs_mean_tgt = (np.random.random(size=np.shape(weighted_graphs_mean_tgt))<weighted_graphs_mean_tgt).astype(int)\n",
    "            \n",
    "            #\n",
    "            # consider the time lags\n",
    "            if nlags == 1:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:] \n",
    "            elif nlags == 2:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]\n",
    "            elif nlags == 3:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]+bina_graphs_mean_tgt[fromRowIDs[2],:]\n",
    "\n",
    "            \n",
    "\n",
    "            # translate the binary DAGs to edge\n",
    "            nrows,ncols = np.shape(bina_graphs_mean_tgt)\n",
    "            edgenames = []\n",
    "            for irow in np.arange(0,nrows,1):\n",
    "                for icol in np.arange(0,ncols,1):\n",
    "                    if bina_graphs_mean_tgt[irow,icol] > 0:\n",
    "                        edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "            # define the DBN predicting model\n",
    "            bn = BayesianNetwork()\n",
    "            bn.add_nodes_from(fromNodes)\n",
    "            bn.add_nodes_from(toNodes)\n",
    "            bn.add_edges_from(edgenames)\n",
    "\n",
    "            effect_slice = toNodes        \n",
    "\n",
    "            # run niters iterations for each condition\n",
    "            for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                # Split data into training and testing sets\n",
    "                train_data, test_data = train_test_split(DBN_input_data_tgt, test_size=0.2)\n",
    "                # test_data = DBN_input_data_tgt\n",
    "\n",
    "                # Perform parameter learning for each time slice\n",
    "                bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                # Perform inference\n",
    "                infer = VariableElimination(bn)\n",
    "\n",
    "                # Prediction for each behavioral events\n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                    var = effect_slice[ievent]\n",
    "                    Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                    fromNodes[1]: row[fromNodes[1]], \n",
    "                                    fromNodes[2]: row[fromNodes[2]], \n",
    "                                    fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                        # Query the probability distribution for Pulls given evidence\n",
    "                        aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                        # Extract the probability of outcome = 1\n",
    "                        prob = aucPpredBehavior.values[1]\n",
    "                        Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                    # Calculate the AUC score\n",
    "                    trueBeh = test_data[var].values\n",
    "                    try:\n",
    "                        auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                    except:\n",
    "                        auc = np.nan\n",
    "                    print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                    # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                    if iiter == 0:\n",
    "                        fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                        axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                     label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                    # put data in the summarizing data frame\n",
    "                    if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "                    else:\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                             }, ignore_index=True)\n",
    "\n",
    "                if iiter == 0:\n",
    "                    axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                    axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                      +'\\n'+DBN_group_typename+'; with dependency:'+strategyname)\n",
    "                    axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_weightedGraph_'+timelagtype+'.pdf')\n",
    "\n",
    "        \n",
    "    # save the summarizing data ROC_summary_all\n",
    "    savedata = 1\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_'+timelagtype+'.pkl', 'wb') as f:\n",
    "            pickle.dump(ROC_summary_all, f)\n",
    "        \n",
    "\n",
    "# average ROC across all iterations\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                'action':action_ROC,\n",
    "                                                                'testCondition':testCon_ROC,\n",
    "                                                                'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "        \n",
    "# plot the summarizing figure\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(5)\n",
    "seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                errorbar='ci',alpha=.5)\n",
    "seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                  alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_weightedGraph_'+timelagtype+'_summarizingplot.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279a191",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### training set and testing set are from the same conditions\n",
    "### use the DBN learned structure, only the 0 1 DAG, do not consider the weights\n",
    "### do not consider the self dependencies (dependencies to variables themselves, no diagonal dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "redoFitting = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['coop(1s)']\n",
    "# DBN_group_typeIDs  =  [3]\n",
    "# DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN model\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = 'allthreelags'\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3], [4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1and2secondlag'\n",
    "# time_lags = ['t_-2','t_-1']\n",
    "# fromRowIDs =[[4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1secondlag'\n",
    "# time_lags = ['t_-1']\n",
    "# fromRowIDs =[[8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '2secondlag'\n",
    "# time_lags = ['t_-2']\n",
    "# fromRowIDs =[[4,5,6,7]]\n",
    "#\n",
    "# timelagtype = '3secondlag'\n",
    "# time_lags = ['t_-3']\n",
    "# fromRowIDs =[[0,1,2,3]]\n",
    "#\n",
    "nlags = np.shape(fromRowIDs)[0]\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "\n",
    "# load ROC_summary_all data\n",
    "try:\n",
    "    if redoFitting:\n",
    "        dumpy\n",
    "    print('load all ROC data for within task condition (only binary dependencies without self dependencies), and only plot the summary figure')\n",
    "        \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "        ROC_summary_all = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    # initialize a summary dataframe for plotting the summary figure across animals \n",
    "    ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "    fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "    fig.set_figheight(7*nanimalpairs)\n",
    "    fig.set_figwidth(7*nDBN_groups)\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        # load the DBN input data\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "        #\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "        # load the DBN training outcome\n",
    "        if moreSampSize:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "        # make sure these variables are the same as in the previous steps\n",
    "        # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]\n",
    "        #\n",
    "        if moreSampSize:\n",
    "            # different data (down/re)sampling numbers\n",
    "            # samplingsizes = np.arange(1100,3000,100)\n",
    "            samplingsizes = [1100]\n",
    "            # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "            # samplingsizes = [100,500]\n",
    "            # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "            samplingsizes_name = list(map(str, samplingsizes))\n",
    "        else:\n",
    "            samplingsizes_name = ['min_row_number']   \n",
    "        nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "        for igroup in np.arange(0,nDBN_groups,1):\n",
    "            DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "            DBN_input_data_tgt = DBN_input_data_alltypes[DBN_group_typename]\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "            # mean weight \n",
    "            weighted_graphs_mean_tgt = np.nanmean(weighted_graphs_tgt,axis=0)\n",
    "            weighted_graphs_mean_tgt = weighted_graphs_mean_tgt * sig_edges_tgt\n",
    "\n",
    "            bina_graphs_mean_tgt = sig_edges_tgt\n",
    "            #\n",
    "            # consider the time lags\n",
    "            if nlags == 1:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:] \n",
    "            elif nlags == 2:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]\n",
    "            elif nlags == 3:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]+bina_graphs_mean_tgt[fromRowIDs[2],:]\n",
    "\n",
    "            \n",
    "\n",
    "            # translate the binary DAGs to edge\n",
    "            nrows,ncols = np.shape(bina_graphs_mean_tgt)\n",
    "            edgenames = []\n",
    "            for irow in np.arange(0,nrows,1):\n",
    "                for icol in np.arange(0,ncols,1):\n",
    "                    \n",
    "                    # remove the self dependencies\n",
    "                    if irow == icol:\n",
    "                        bina_graphs_mean_tgt[irow,icol] = 0\n",
    "                    \n",
    "                    if bina_graphs_mean_tgt[irow,icol] > 0:\n",
    "                        edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "            # define the DBN predicting model\n",
    "            bn = BayesianNetwork()\n",
    "            bn.add_nodes_from(fromNodes)\n",
    "            bn.add_nodes_from(toNodes)\n",
    "            bn.add_edges_from(edgenames)\n",
    "\n",
    "            effect_slice = toNodes        \n",
    "\n",
    "            # run niters iterations for each condition\n",
    "            for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                # Split data into training and testing sets\n",
    "                train_data, test_data = train_test_split(DBN_input_data_tgt, test_size=0.2)\n",
    "                # test_data = DBN_input_data_tgt\n",
    "\n",
    "                # Perform parameter learning for each time slice\n",
    "                bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                # Perform inference\n",
    "                infer = VariableElimination(bn)\n",
    "\n",
    "                # Prediction for each behavioral events\n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                    var = effect_slice[ievent]\n",
    "                    Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                    fromNodes[1]: row[fromNodes[1]], \n",
    "                                    fromNodes[2]: row[fromNodes[2]], \n",
    "                                    fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                        # Query the probability distribution for Pulls given evidence\n",
    "                        aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                        # Extract the probability of outcome = 1\n",
    "                        prob = aucPpredBehavior.values[1]\n",
    "                        Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                    # Calculate the AUC score\n",
    "                    trueBeh = test_data[var].values\n",
    "                    try:\n",
    "                        auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                    except:\n",
    "                        auc = np.nan\n",
    "                    print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                    # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                    if iiter == 0:\n",
    "                        fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                        axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                     label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                    # put data in the summarizing data frame\n",
    "                    if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "                    else:\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                             }, ignore_index=True)\n",
    "\n",
    "                if iiter == 0:\n",
    "                    axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                    axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                      +'\\n'+DBN_group_typename+'; with dependency:'+strategyname)\n",
    "                    axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_noself_'+timelagtype+'.pdf')\n",
    "\n",
    "        \n",
    "    # save the summarizing data ROC_summary_all\n",
    "    savedata = 1\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_noself_'+timelagtype+'.pkl', 'wb') as f:\n",
    "            pickle.dump(ROC_summary_all, f)\n",
    "        \n",
    "\n",
    "# average ROC across all iterations\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                'action':action_ROC,\n",
    "                                                                'testCondition':testCon_ROC,\n",
    "                                                                'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "        \n",
    "# plot the summarizing figure\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(5)\n",
    "seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                errorbar='ci',alpha=.5)\n",
    "seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                  alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_binaryGraph_noself_'+timelagtype+'_summarizingplot.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff414f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514ba067",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### training set and testing set are from the same conditions\n",
    "### use the DBN learned structure, consider the weight as well\n",
    "### do not consider the self dependencies (dependencies to variables themselves, no diagonal dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc70038",
   "metadata": {},
   "outputs": [],
   "source": [
    "redoFitting = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,3,5]\n",
    "DBN_group_coopthres = [0,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['coop(1s)']\n",
    "# DBN_group_typeIDs  =  [3]\n",
    "# DBN_group_coopthres = [1]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "# DBN model\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = 'allthreelags'\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3], [4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1and2secondlag'\n",
    "# time_lags = ['t_-2','t_-1']\n",
    "# fromRowIDs =[[4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1secondlag'\n",
    "# time_lags = ['t_-1']\n",
    "# fromRowIDs =[[8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '2secondlag'\n",
    "# time_lags = ['t_-2']\n",
    "# fromRowIDs =[[4,5,6,7]]\n",
    "#\n",
    "# timelagtype = '3secondlag'\n",
    "# time_lags = ['t_-3']\n",
    "# fromRowIDs =[[0,1,2,3]]\n",
    "#\n",
    "nlags = np.shape(fromRowIDs)[0]\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "\n",
    "# load ROC_summary_all data\n",
    "try:\n",
    "    if redoFitting:\n",
    "        dumpy\n",
    "    print('load all ROC data for within task condition (weighted dependencies without self dependencies), and only plot the summary figure')\n",
    "        \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "        ROC_summary_all = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    # initialize a summary dataframe for plotting the summary figure across animals \n",
    "    ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "    fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "    fig.set_figheight(7*nanimalpairs)\n",
    "    fig.set_figwidth(7*nDBN_groups)\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        # load the DBN input data\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "        #\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "        # load the DBN training outcome\n",
    "        if moreSampSize:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "        # make sure these variables are the same as in the previous steps\n",
    "        # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]\n",
    "        #\n",
    "        if moreSampSize:\n",
    "            # different data (down/re)sampling numbers\n",
    "            # samplingsizes = np.arange(1100,3000,100)\n",
    "            samplingsizes = [1100]\n",
    "            # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "            # samplingsizes = [100,500]\n",
    "            # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "            samplingsizes_name = list(map(str, samplingsizes))\n",
    "        else:\n",
    "            samplingsizes_name = ['min_row_number']   \n",
    "        nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "        for igroup in np.arange(0,nDBN_groups,1):\n",
    "            DBN_group_typename = DBN_group_typenames[igroup]\n",
    "\n",
    "            DBN_input_data_tgt = DBN_input_data_alltypes[DBN_group_typename]\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "            # mean weight \n",
    "            weighted_graphs_mean_tgt = np.nanmean(weighted_graphs_tgt,axis=0)\n",
    "            weighted_graphs_mean_tgt = weighted_graphs_mean_tgt * sig_edges_tgt\n",
    "\n",
    "            bina_graphs_mean_tgt = (np.random.random(size=np.shape(weighted_graphs_mean_tgt))<weighted_graphs_mean_tgt).astype(int)\n",
    "            \n",
    "            #\n",
    "            # consider the time lags\n",
    "            if nlags == 1:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:] \n",
    "            elif nlags == 2:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]\n",
    "            elif nlags == 3:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]+bina_graphs_mean_tgt[fromRowIDs[2],:]\n",
    "\n",
    "            \n",
    "\n",
    "            # translate the binary DAGs to edge\n",
    "            nrows,ncols = np.shape(bina_graphs_mean_tgt)\n",
    "            edgenames = []\n",
    "            for irow in np.arange(0,nrows,1):\n",
    "                for icol in np.arange(0,ncols,1):\n",
    "                    \n",
    "                    # remove the self dependencies\n",
    "                    if irow == icol:\n",
    "                        bina_graphs_mean_tgt[irow,icol] = 0\n",
    "                    \n",
    "                    if bina_graphs_mean_tgt[irow,icol] > 0:\n",
    "                        edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "            # define the DBN predicting model\n",
    "            bn = BayesianNetwork()\n",
    "            bn.add_nodes_from(fromNodes)\n",
    "            bn.add_nodes_from(toNodes)\n",
    "            bn.add_edges_from(edgenames)\n",
    "\n",
    "            effect_slice = toNodes        \n",
    "\n",
    "            # run niters iterations for each condition\n",
    "            for iiter in np.arange(0,niters,1):\n",
    "\n",
    "\n",
    "                # Split data into training and testing sets\n",
    "                train_data, test_data = train_test_split(DBN_input_data_tgt, test_size=0.2)\n",
    "                # test_data = DBN_input_data_tgt\n",
    "\n",
    "                # Perform parameter learning for each time slice\n",
    "                bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                # Perform inference\n",
    "                infer = VariableElimination(bn)\n",
    "\n",
    "                # Prediction for each behavioral events\n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                    var = effect_slice[ievent]\n",
    "                    Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                    fromNodes[1]: row[fromNodes[1]], \n",
    "                                    fromNodes[2]: row[fromNodes[2]], \n",
    "                                    fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                        # Query the probability distribution for Pulls given evidence\n",
    "                        aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                        # Extract the probability of outcome = 1\n",
    "                        prob = aucPpredBehavior.values[1]\n",
    "                        Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                    # Calculate the AUC score\n",
    "                    trueBeh = test_data[var].values\n",
    "                    try:\n",
    "                        auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                    except:\n",
    "                        auc = np.nan\n",
    "                    print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                    # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                    if iiter == 0:\n",
    "                        fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                        axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                     label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                    # put data in the summarizing data frame\n",
    "                    if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "                    else:\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename,\n",
    "                                                                  'predROC':auc\n",
    "                                                             }, ignore_index=True)\n",
    "\n",
    "                if iiter == 0:\n",
    "                    axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                    axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                      +'\\n'+DBN_group_typename+'; with dependency:'+strategyname)\n",
    "                    axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_weightedGraph_noself_'+timelagtype+'.pdf')\n",
    "\n",
    "        \n",
    "    # save the summarizing data ROC_summary_all\n",
    "    savedata = 1\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_noself_'+timelagtype+'.pkl', 'wb') as f:\n",
    "            pickle.dump(ROC_summary_all, f)\n",
    "        \n",
    "\n",
    "# average ROC across all iterations\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                'action':action_ROC,\n",
    "                                                                'testCondition':testCon_ROC,\n",
    "                                                                'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "        \n",
    "# plot the summarizing figure\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(5)\n",
    "seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                errorbar='ci',alpha=.5)\n",
    "seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',\n",
    "                  alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_fullDBN_weightedGraph_noself_'+timelagtype+'_summarizingplot.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d2f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb4f6702",
   "metadata": {},
   "source": [
    "### load the DBN related data for each dyad and run the prediction\n",
    "### from one task condition to test on another task condition\n",
    "### use the DBN learned structure, only the 0 1 DAG, do not consider the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82422f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "redoFitting = 0\n",
    "\n",
    "niters = 100\n",
    "\n",
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "# animal1_fixedorders = ['eddie',]\n",
    "# animal2_fixedorders = ['sparkle',]\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "temp_resolu = 1\n",
    "\n",
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames_train = ['coop(1s)','coop(1s)','coop(1s)']\n",
    "DBN_group_typeIDs_train  =  [3,3,3]\n",
    "DBN_group_coopthres_train = [1,1,1]\n",
    "#\n",
    "DBN_group_typenames_test = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs_test  =  [1,3,5]\n",
    "DBN_group_coopthres_test = [0,1,0]\n",
    "#\n",
    "nDBN_groups = np.shape(DBN_group_typenames_train)[0]\n",
    "\n",
    "\n",
    "# DBN model\n",
    "toNodes = ['pull1_t3','pull2_t3','owgaze1_t3','owgaze2_t3']\n",
    "fromNodes = ['pull1_t2','pull2_t2','owgaze1_t2','owgaze2_t2']\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "nevents = np.shape(eventnames)[0]\n",
    "\n",
    "timelagtype = 'allthreelags'\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3], [4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1and2secondlag'\n",
    "# time_lags = ['t_-2','t_-1']\n",
    "# fromRowIDs =[[4,5,6,7], [8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '1secondlag'\n",
    "# time_lags = ['t_-1']\n",
    "# fromRowIDs =[[8,9,10,11]]\n",
    "#\n",
    "# timelagtype = '2secondlag'\n",
    "# time_lags = ['t_-2']\n",
    "# fromRowIDs =[[4,5,6,7]]\n",
    "#\n",
    "# timelagtype = '3secondlag'\n",
    "# time_lags = ['t_-3']\n",
    "# fromRowIDs =[[0,1,2,3]]\n",
    "#\n",
    "nlags = np.shape(fromRowIDs)[0]\n",
    "#\n",
    "if timelagtype == '2secondlag':\n",
    "    fromNodes = ['pull1_t1','pull2_t1','owgaze1_t1','owgaze2_t1']\n",
    "if timelagtype == '3secondlag':\n",
    "    fromNodes = ['pull1_t0','pull2_t0','owgaze1_t0','owgaze2_t0']\n",
    "\n",
    "\n",
    "# load ROC_summary_all data\n",
    "try:\n",
    "    if redoFitting:\n",
    "        dumpy\n",
    "    print('load all ROC data for across task conditions (only binary dependencies), and only plot the summary figure')\n",
    "\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_'+timelagtype+'_trainedon'+DBN_group_typenames_train[0]+'.pkl', 'rb') as f:\n",
    "        ROC_summary_all = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    # initialize a summary dataframe for plotting the summary figure across animals \n",
    "    ROC_summary_all = pd.DataFrame(columns=['animal','action','testCondition','predROC'])\n",
    "\n",
    "    fig, axs = plt.subplots(nanimalpairs,nDBN_groups) # nDBN_groups(3) task conditions; 2 animal individual\n",
    "    fig.set_figheight(7*nanimalpairs)\n",
    "    fig.set_figwidth(7*nDBN_groups)\n",
    "\n",
    "    for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "\n",
    "        # load the DBN input data\n",
    "        animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "        animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "        #\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder+animal2_fixedorder+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "        # load the DBN training outcome\n",
    "        if moreSampSize:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "                sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "        # make sure these variables are the same as in the previous steps\n",
    "        # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]\n",
    "        #\n",
    "        if moreSampSize:\n",
    "            # different data (down/re)sampling numbers\n",
    "            # samplingsizes = np.arange(1100,3000,100)\n",
    "            samplingsizes = [1100]\n",
    "            # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "            # samplingsizes = [100,500]\n",
    "            # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "            samplingsizes_name = list(map(str, samplingsizes))\n",
    "        else:\n",
    "            samplingsizes_name = ['min_row_number']   \n",
    "        nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "\n",
    "\n",
    "        for igroup in np.arange(0,nDBN_groups,1):\n",
    "\n",
    "            DBN_group_typename_train = DBN_group_typenames_train[igroup]\n",
    "            DBN_group_typename_test  = DBN_group_typenames_test[igroup]\n",
    "\n",
    "            #\n",
    "            # training dataset\n",
    "            DBN_input_data_tgt_train = DBN_input_data_alltypes[DBN_group_typename_train]\n",
    "            weighted_graphs_tgt_train = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename_train]\n",
    "            weighted_graphs_shuffled_tgt_train = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename_train]\n",
    "            # sig_edges_tgt_train = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename_train]\n",
    "            sig_edges_tgt_train = get_significant_edges(weighted_graphs_tgt_train,weighted_graphs_shuffled_tgt_train)\n",
    "\n",
    "            # mean weight \n",
    "            weighted_graphs_mean_tgt_train = np.nanmean(weighted_graphs_tgt_train,axis=0)\n",
    "            weighted_graphs_mean_tgt_train = weighted_graphs_mean_tgt_train * sig_edges_tgt_train\n",
    "\n",
    "            bina_graphs_mean_tgt_train = sig_edges_tgt_train\n",
    "\n",
    "            # train_data = DBN_input_data_tgt_train\n",
    "\n",
    "            #\n",
    "            # testing dataset\n",
    "            DBN_input_data_tgt_test = DBN_input_data_alltypes[DBN_group_typename_test]\n",
    "            weighted_graphs_tgt_test = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename_test]\n",
    "            weighted_graphs_shuffled_tgt_test = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename_test]\n",
    "            # sig_edges_tgt_test = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][DBN_group_typename_test]\n",
    "            sig_edges_tgt_test = get_significant_edges(weighted_graphs_tgt_test,weighted_graphs_shuffled_tgt_test)\n",
    "\n",
    "            # mean weight \n",
    "            weighted_graphs_mean_tgt_test = np.nanmean(weighted_graphs_tgt_test,axis=0)\n",
    "            weighted_graphs_mean_tgt_test = weighted_graphs_mean_tgt_test * sig_edges_tgt_test\n",
    "\n",
    "            bina_graphs_mean_tgt_test = sig_edges_tgt_test\n",
    "\n",
    "            # test_data = DBN_input_data_tgt_test\n",
    "            \n",
    "            \n",
    "            bina_graphs_mean_tgt = bina_graphs_mean_tgt_train\n",
    "            # consider the time lags\n",
    "            #\n",
    "            if nlags == 1:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:] \n",
    "            elif nlags == 2:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]\n",
    "            elif nlags == 3:\n",
    "                bina_graphs_mean_tgt = bina_graphs_mean_tgt[fromRowIDs[0],:]+bina_graphs_mean_tgt[fromRowIDs[1],:]+bina_graphs_mean_tgt[fromRowIDs[2],:]\n",
    "\n",
    "            \n",
    "\n",
    "            # translate the binary DAGs to edge\n",
    "            nrows,ncols = np.shape(bina_graphs_mean_tgt)\n",
    "            edgenames = []\n",
    "            for irow in np.arange(0,nrows,1):\n",
    "                for icol in np.arange(0,ncols,1):\n",
    "                    if bina_graphs_mean_tgt[irow,icol] > 0:\n",
    "                        edgenames.append((fromNodes[irow],toNodes[icol]))\n",
    "\n",
    "            # define the DBN predicting model\n",
    "            bn = BayesianNetwork()\n",
    "            bn.add_nodes_from(fromNodes)\n",
    "            bn.add_nodes_from(toNodes)\n",
    "            bn.add_edges_from(edgenames)\n",
    "\n",
    "            effect_slice = toNodes        \n",
    "\n",
    "            # run niters iterations for each condition\n",
    "            for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                # To match other analysis case, split data randomly here well \n",
    "                # Split data into training and testing sets\n",
    "                train_data, _ = train_test_split(DBN_input_data_tgt_train, test_size=0.2)\n",
    "                _, test_data  = train_test_split(DBN_input_data_tgt_test, test_size=0.2)\n",
    "\n",
    "                # Perform parameter learning for each time slice\n",
    "                bn.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "                # Perform inference\n",
    "                infer = VariableElimination(bn)\n",
    "\n",
    "                # Prediction for each behavioral events\n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "\n",
    "                    var = effect_slice[ievent]\n",
    "                    Pbehavior = [] # Initialize log-likelihood\n",
    "\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        evidence = {fromNodes[0]: row[fromNodes[0]], \n",
    "                                    fromNodes[1]: row[fromNodes[1]], \n",
    "                                    fromNodes[2]: row[fromNodes[2]], \n",
    "                                    fromNodes[3]: row[fromNodes[3]], }\n",
    "\n",
    "                        # Query the probability distribution for Pulls given evidence\n",
    "                        aucPpredBehavior = infer.query(variables=[var], evidence=evidence) \n",
    "\n",
    "                        # Extract the probability of outcome = 1\n",
    "                        prob = aucPpredBehavior.values[1]\n",
    "                        Pbehavior = np.append(Pbehavior, prob)\n",
    "\n",
    "                    # Calculate the AUC score\n",
    "                    trueBeh = test_data[var].values\n",
    "                    try:\n",
    "                        auc = roc_auc_score(trueBeh, Pbehavior)\n",
    "                    except:\n",
    "                        auc = np.nan\n",
    "                    print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "                    # Optionally, plot the ROC curve - only plot for the first iteration\n",
    "                    if iiter == 0:\n",
    "                        fpr, tpr, _ = roc_curve(trueBeh, Pbehavior)\n",
    "\n",
    "                        axs[ianimalpair,igroup].plot(fpr, tpr, \n",
    "                                                     label=f'AUC for '+eventnames[ievent]+' auc = '+\"{:.2f}\".format(auc))\n",
    "\n",
    "                    # put data in the summarizing data frame\n",
    "                    if (ievent == 0) | (ievent == 2): # for animal1\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal1_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename_test,\n",
    "                                                                  'predROC':auc\n",
    "                                                                 }, ignore_index=True)\n",
    "                    else:\n",
    "                        ROC_summary_all = ROC_summary_all.append({'animal':animal2_fixedorder,\n",
    "                                                                  'action':eventnames[ievent][2:],\n",
    "                                                                  'testCondition':DBN_group_typename_test,\n",
    "                                                                  'predROC':auc\n",
    "                                                             }, ignore_index=True)\n",
    "\n",
    "                if iiter == 0:\n",
    "                    axs[ianimalpair,igroup].plot([0, 1], [0, 1], 'k--')\n",
    "                    axs[ianimalpair,igroup].set_xlabel('False Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_ylabel('True Positive Rate')\n",
    "                    axs[ianimalpair,igroup].set_title('ROC Curve '+animal1_fixedorder+'&'+animal2_fixedorder\n",
    "                                                      +'\\n'+DBN_group_typename_train+' to '+DBN_group_typename_test)\n",
    "                    axs[ianimalpair,igroup].legend(loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        fig.savefig(figsavefolder+'acrossCondition_DBNpredicition_fullDBN_binaryGraph_'+timelagtype+'_DBNtrainedon'+DBN_group_typename_train+'.pdf')\n",
    "\n",
    "        \n",
    "    # save the summarizing data ROC_summary_all\n",
    "    savedata = 1\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_'+timelagtype+'_trainedon'+DBN_group_typenames_train[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(ROC_summary_all, f)\n",
    "        \n",
    "        \n",
    "# average ROC across all iterations\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all.iloc[np.where([ROC_summary_all['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                'action':action_ROC,\n",
    "                                                                'testCondition':testCon_ROC,\n",
    "                                                                'predROC':np.nanmean(ROC_summary_all_itcon['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "        \n",
    "# plot the summarizing figure\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(5)\n",
    "seaborn.barplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',errorbar='ci',alpha=.5)\n",
    "seaborn.swarmplot(ax=axs2,data=ROC_summary_all_mean,x='action',y='predROC',hue='testCondition',alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'acrossCondition_DBNpredicition_fullDBN_binaryGraph_'+timelagtype+'_DBNtrainedon'+DBN_group_typenames_train[0]+'_summarizingplot.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0bfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e68b7b",
   "metadata": {},
   "source": [
    "### plot the ROC across all analysis conditions\n",
    "### only do this after all the previous analysis were done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBN model information\n",
    "timelagtype = 'allthreelags'\n",
    "\n",
    "# load all the ROC_all data\n",
    "#\n",
    "print('load all ROC data for within task condition (only binary dependencies)')\n",
    "#         \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_binary = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_binary.iloc[np.where([ROC_summary_all_full_binary['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_binary.iloc[np.where([ROC_summary_all_full_binary['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for within task condition (only binary dependencies without self dependencies)')\n",
    "#         \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_binary_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_binary_noself = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_binary_noself.iloc[np.where([ROC_summary_all_full_binary_noself['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_binary_noself.iloc[np.where([ROC_summary_all_full_binary_noself['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for within task condition (weighted dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted.iloc[np.where([ROC_summary_all_full_weighted['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted.iloc[np.where([ROC_summary_all_full_weighted['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for within task condition (weighted dependencies without self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_noself = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_noself.iloc[np.where([ROC_summary_all_full_weighted_noself['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_noself.iloc[np.where([ROC_summary_all_full_weighted_noself['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "\n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - three main dependencies')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains.iloc[np.where([ROC_summary_all_threeMains['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains.iloc[np.where([ROC_summary_all_threeMains['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - sync_pulls')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls.iloc[np.where([ROC_summary_all_sync_pulls['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls.iloc[np.where([ROC_summary_all_sync_pulls['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - gaze_lead_pull')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull.iloc[np.where([ROC_summary_all_gaze_lead_pull['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull.iloc[np.where([ROC_summary_all_gaze_lead_pull['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - social_attention')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention.iloc[np.where([ROC_summary_all_social_attention['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention.iloc[np.where([ROC_summary_all_social_attention['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - three main dependencies with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_withSelfEdge = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_withSelfEdge.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_withSelfEdge.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - sync_pulls with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_withSelfEdge = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_withSelfEdge.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_withSelfEdge.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - gaze_lead_pull with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_withSelfEdge = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_withSelfEdge.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for hypothetical dependencies - social_attention with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_withSelfEdge = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_withSelfEdge.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_withSelfEdge.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "        \n",
    "    \n",
    "# put all data together\n",
    "ROC_summary_all_full_binary['type']='binary'\n",
    "ROC_summary_all_full_binary_noself['type']='binary_noself'\n",
    "ROC_summary_all_full_weighted['type']='weighted'\n",
    "ROC_summary_all_full_weighted_noself['type']='weighted_noself'\n",
    "ROC_summary_all_threeMains['type']='threeMains'\n",
    "ROC_summary_all_sync_pulls['type']='sync_pulls'\n",
    "ROC_summary_all_gaze_lead_pull['type']='gaze_lead_pull'\n",
    "ROC_summary_all_social_attention['type']='social_attention'\n",
    "#\n",
    "ROC_summary_all_threeMains_withSelfEdge['type']='threeMains_withSelfEdge'\n",
    "ROC_summary_all_sync_pulls_withSelfEdge['type']='sync_pulls_withSelfEdge'\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge['type']='gaze_lead_pull_withSelfEdge'\n",
    "ROC_summary_all_social_attention_withSelfEdge['type']='social_attention_withSelfEdge'\n",
    "\n",
    "# \n",
    "ROC_summary_all_merged = pd.concat([ROC_summary_all_full_binary,\n",
    "                                    ROC_summary_all_full_binary_noself,\n",
    "                                    ROC_summary_all_full_weighted,\n",
    "                                    ROC_summary_all_full_weighted_noself,\n",
    "                                    ROC_summary_all_threeMains,\n",
    "                                    ROC_summary_all_sync_pulls,\n",
    "                                    ROC_summary_all_gaze_lead_pull,\n",
    "                                    ROC_summary_all_social_attention,\n",
    "                                    #\n",
    "                                    ROC_summary_all_threeMains_withSelfEdge,\n",
    "                                    ROC_summary_all_sync_pulls_withSelfEdge,\n",
    "                                    ROC_summary_all_gaze_lead_pull_withSelfEdge,\n",
    "                                    ROC_summary_all_social_attention_withSelfEdge,\n",
    "                                   ])\n",
    "\n",
    "\n",
    "# for each animal calculate the mean\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all_merged['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all_merged['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all_merged['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "types_ROC = np.unique(ROC_summary_all_merged['type'])\n",
    "ntypes = np.shape(types_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all_merged.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all_merged['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all_merged[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            for itype in np.arange(0,ntypes,1):\n",
    "                type_ROC = types_ROC[itype]\n",
    "                ind_type = ROC_summary_all_itcon['type'] == type_ROC\n",
    "                ROC_summary_all_itype = ROC_summary_all_itcon[ind_type]\n",
    "                #\n",
    "                ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                    'action':action_ROC,\n",
    "                                                                    'testCondition':testCon_ROC,\n",
    "                                                                    'type':type_ROC,\n",
    "                                                                    'predROC':np.nanmean(ROC_summary_all_itype['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "\n",
    "\n",
    "# plot\n",
    "testCond_forplot = 'coop(1s)' # 'no-vision','self','coop(1s)'\n",
    "# ROC_summary_all_forplot = ROC_summary_all_merged[ROC_summary_all_merged['testCondition']==testCond_forplot]\n",
    "ROC_summary_all_forplot = ROC_summary_all_mean[ROC_summary_all_mean['testCondition']==testCond_forplot]\n",
    "\n",
    "# plot the summarizing figure - pool all animals together\n",
    "#\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(15)\n",
    "# s=seaborn.barplot(ax=axs2,data=ROC_summary_all_forplot,x='action',y='predROC',hue='type',errorbar='ci',alpha=.5)\n",
    "s=seaborn.boxplot(ax=axs2,data=ROC_summary_all_forplot,x='action',y='predROC',hue='type',whis=3)\n",
    "# s=seaborn.swarmplot(ax=axs2,data=ROC_summary_all_forplot,x='action',y='predROC',hue='type',\n",
    "#                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "\n",
    "# plot the summarizing figure - separate each animal\n",
    "#\n",
    "fig3, axs3 = plt.subplots(2,1)\n",
    "fig3.set_figheight(10)\n",
    "fig3.set_figwidth(10)\n",
    "#\n",
    "events_forplot = ['pull','gaze'] \n",
    "nevents = np.shape(events_forplot)[0]\n",
    "\n",
    "for ievent in np.arange(0,nevents,1):\n",
    "    ievent_forplot = events_forplot[ievent]\n",
    "    \n",
    "    ROC_summary_all_ievent = ROC_summary_all_forplot[ROC_summary_all_forplot['action']==ievent_forplot]\n",
    "    #\n",
    "    s=seaborn.barplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "                    errorbar='ci',alpha=.5)\n",
    "    # s=seaborn.boxplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type')\n",
    "    # s=seaborn.swarmplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "    #                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "    axs3[ievent].plot([-0.5,9.5],[0.5,0.5],'k--')\n",
    "    axs3[ievent].set_xlim([-0.5,9.5])\n",
    "    axs3[ievent].set_title('all animal; events of '+ievent_forplot)\n",
    "    s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "# plot the summarizing figure - separate male and female\n",
    "maleanimals = np.array(['eddie','dannon','dodson','vermelho'])\n",
    "allanimalnames = np.array(ROC_summary_all_forplot['animal'])\n",
    "allanimalsexes = allanimalnames.copy()\n",
    "#\n",
    "allanimalsexes[np.isin(allanimalnames,maleanimals)]='male'\n",
    "allanimalsexes[~np.isin(allanimalnames,maleanimals)]='female'\n",
    "#\n",
    "ROC_summary_all_forplot['sex'] = allanimalsexes\n",
    "\n",
    "#\n",
    "fig4, axs4 = plt.subplots(1,2)\n",
    "fig4.set_figheight(5)\n",
    "fig4.set_figwidth(20)\n",
    "#\n",
    "events_forplot = ['pull','gaze'] \n",
    "nevents = np.shape(events_forplot)[0]\n",
    "\n",
    "for ievent in np.arange(0,nevents,1):\n",
    "    ievent_forplot = events_forplot[ievent]\n",
    "    \n",
    "    ROC_summary_all_ievent = ROC_summary_all_forplot[ROC_summary_all_forplot['action']==ievent_forplot]\n",
    "    #\n",
    "    # s=seaborn.barplot(ax=axs4[ievent],data=ROC_summary_all_ievent,x='sex',y='predROC',hue='type',\n",
    "    #                 errorbar='ci',alpha=.5)\n",
    "    s=seaborn.boxplot(ax=axs4[ievent],data=ROC_summary_all_ievent,x='sex',y='predROC',hue='type',whis=3)\n",
    "    # s=seaborn.swarmplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "    #                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "    axs4[ievent].plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "    axs4[ievent].set_xlim([-0.5,1.5])\n",
    "    axs4[ievent].set_title('all animal; events of '+ievent_forplot)\n",
    "    s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# plot the summarizing figure - only plot the speficified condition; plot all animals together\n",
    "fig5, axs5 = plt.subplots(1,1)\n",
    "fig5.set_figheight(5)\n",
    "fig5.set_figwidth(15)\n",
    "#\n",
    "testCond_forplot = 'coop(1s)' # 'no-vision','self','coop(1s)'\n",
    "# ROC_summary_all_forplot = ROC_summary_all_merged[ROC_summary_all_merged['testCondition']==testCond_forplot]\n",
    "ROC_summary_all_forplot = ROC_summary_all_mean[ROC_summary_all_mean['testCondition']==testCond_forplot]\n",
    "#\n",
    "plottypes = ['threeMains','sync_pulls','gaze_lead_pull','social_attention',\n",
    "    'threeMains_withSelfEdge','sync_pulls_withSelfEdge','gaze_lead_pull_withSelfEdge','social_attention_withSelfEdge']\n",
    "ind_toplot = np.isin(ROC_summary_all_forplot['type'],plottypes)\n",
    "ROC_summary_all_forplot2 = ROC_summary_all_forplot[ind_toplot]\n",
    "\n",
    "# s=seaborn.barplot(ax=axs5,data=ROC_summary_all_forplot2,x='action',y='predROC',hue='type',errorbar='ci',alpha=.5)\n",
    "s=seaborn.boxplot(ax=axs5,data=ROC_summary_all_forplot2,x='action',y='predROC',hue='type',whis=3)\n",
    "# s=seaborn.swarmplot(ax=axs5,data=ROC_summary_all_forplot2,x='action',y='predROC',hue='type',\n",
    "#                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs5.set_xlim([-0.5,1.5])\n",
    "axs5.set_title('all animal, compared with and without self edges')\n",
    "s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_'+timelagtype+'DBN_andHypotheticalDependencies_'+testCond_forplot+'_summarizingplot_allanimals.pdf')\n",
    "    fig3.savefig(figsavefolder+'withinCondition_DBNpredicition_'+timelagtype+'DBN_andHypotheticalDependencies_'+testCond_forplot+'_summarizingplot_eachanimals.pdf')\n",
    "    fig4.savefig(figsavefolder+'withinCondition_DBNpredicition_'+timelagtype+'DBN_andHypotheticalDependencies_'+testCond_forplot+'_summarizingplot_malefemales.pdf')\n",
    "    fig5.savefig(figsavefolder+'withinCondition_DBNpredicition_'+timelagtype+'DBN_andHypotheticalDependencies_'+testCond_forplot+'_summarizingplot_allanimals_withandwithoutselfedge.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iind=(ROC_summary_all_forplot['action']=='pull')&(ROC_summary_all_forplot['testCondition']=='coop(1s)')&(ROC_summary_all_forplot['type']=='social_attention')\n",
    "\n",
    "ROCtgt = np.array(ROC_summary_all_forplot[iind]['predROC'])\n",
    "\n",
    "st.ttest_1samp(ROCtgt,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afe4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_summary_all_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc10876",
   "metadata": {},
   "source": [
    "### plot the ROC across all analysis conditions\n",
    "### only do this after all the previous analysis were done \n",
    "#### compare 1s 2s and 3s time lags (different timelagtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a985b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the ROC_all data\n",
    "\n",
    "# 1s time lags \n",
    "timelagtype = '1secondlag'\n",
    "\n",
    "print('load all ROC data for 1secondlag within task condition (weighted dependencies with self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_1secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_1secondlag.iloc[np.where([ROC_summary_all_full_weighted_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_1secondlag.iloc[np.where([ROC_summary_all_full_weighted_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 1secondlag within task condition (weighted dependencies without self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_noself_1secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_noself_1secondlag.iloc[np.where([ROC_summary_all_full_weighted_noself_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_noself_1secondlag.iloc[np.where([ROC_summary_all_full_weighted_noself_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - three main dependencies')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_1secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_1secondlag.iloc[np.where([ROC_summary_all_threeMains_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_1secondlag.iloc[np.where([ROC_summary_all_threeMains_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - sync_pulls')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_1secondlag = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_1secondlag.iloc[np.where([ROC_summary_all_sync_pulls_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_1secondlag.iloc[np.where([ROC_summary_all_sync_pulls_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - gaze_lead_pull')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_1secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_1secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_1secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - social_attention')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_1secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_1secondlag.iloc[np.where([ROC_summary_all_social_attention_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_1secondlag.iloc[np.where([ROC_summary_all_social_attention_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - three main dependencies with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_withSelfEdge_1secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - sync_pulls with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_withSelfEdge_1secondlag = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 1secondlag hypothetical dependencies - gaze_lead_pull with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - social_attention with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_withSelfEdge_1secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_withSelfEdge_1secondlag.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge_1secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "        \n",
    "\n",
    "# 2s time lags \n",
    "timelagtype = '2secondlag'\n",
    "\n",
    "print('load all ROC data for 2secondlag within task condition (weighted dependencies with self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_2secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_2secondlag.iloc[np.where([ROC_summary_all_full_weighted_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_2secondlag.iloc[np.where([ROC_summary_all_full_weighted_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 2secondlag within task condition (weighted dependencies without self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_noself_2secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_noself_2secondlag.iloc[np.where([ROC_summary_all_full_weighted_noself_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_noself_2secondlag.iloc[np.where([ROC_summary_all_full_weighted_noself_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - three main dependencies')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_2secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_2secondlag.iloc[np.where([ROC_summary_all_threeMains_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_2secondlag.iloc[np.where([ROC_summary_all_threeMains_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - sync_pulls')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_2secondlag = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_2secondlag.iloc[np.where([ROC_summary_all_sync_pulls_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_2secondlag.iloc[np.where([ROC_summary_all_sync_pulls_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - gaze_lead_pull')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_2secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_2secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_2secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - social_attention')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_2secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_2secondlag.iloc[np.where([ROC_summary_all_social_attention_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_2secondlag.iloc[np.where([ROC_summary_all_social_attention_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - three main dependencies with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_withSelfEdge_2secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - sync_pulls with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_withSelfEdge_2secondlag = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - gaze_lead_pull with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 2secondlag hypothetical dependencies - social_attention with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_withSelfEdge_2secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_withSelfEdge_2secondlag.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge_2secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "\n",
    "# 3s time lags \n",
    "timelagtype = '3secondlag'\n",
    "\n",
    "print('load all ROC data for 3secondlag within task condition (weighted dependencies with self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_3secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_3secondlag.iloc[np.where([ROC_summary_all_full_weighted_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_3secondlag.iloc[np.where([ROC_summary_all_full_weighted_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 3secondlag within task condition (weighted dependencies without self dependencies)')\n",
    "#\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_fullDBNdependencies_weighted_noself_'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_full_weighted_noself_3secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_full_weighted_noself_3secondlag.iloc[np.where([ROC_summary_all_full_weighted_noself_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_full_weighted_noself_3secondlag.iloc[np.where([ROC_summary_all_full_weighted_noself_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - three main dependencies')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_3secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_3secondlag.iloc[np.where([ROC_summary_all_threeMains_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_3secondlag.iloc[np.where([ROC_summary_all_threeMains_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - sync_pulls')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_3secondlag = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_3secondlag.iloc[np.where([ROC_summary_all_sync_pulls_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_3secondlag.iloc[np.where([ROC_summary_all_sync_pulls_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - gaze_lead_pull')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_3secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_3secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_3secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - social_attention')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention'+timelagtype+'.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_3secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_3secondlag.iloc[np.where([ROC_summary_all_social_attention_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_3secondlag.iloc[np.where([ROC_summary_all_social_attention_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - three main dependencies with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_threeMains'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_threeMains_withSelfEdge_3secondlag = pickle.load(f)\n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_threeMains_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_threeMains_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_threeMains_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - sync_pulls with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_sync_pulls'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_sync_pulls_withSelfEdge_3secondlag = pickle.load(f)    \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_sync_pulls_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_sync_pulls_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_sync_pulls_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "    \n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - gaze_lead_pull with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_gaze_lead_pull'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "#\n",
    "print('load all ROC data for 3secondlag hypothetical dependencies - social_attention with Self Edge')\n",
    "#        \n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "with open(data_saved_subfolder+'/ROC_summary_all_dependencies_social_attention'+timelagtype+'_withSelfEdge.pkl', 'rb') as f:\n",
    "    ROC_summary_all_social_attention_withSelfEdge_3secondlag = pickle.load(f) \n",
    "# separate Kanga into two groups based on her partner\n",
    "ROC_summary_all_social_attention_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]] = ROC_summary_all_social_attention_withSelfEdge_3secondlag.iloc[np.where([ROC_summary_all_social_attention_withSelfEdge_3secondlag['animal']=='kanga'])[1][0:niters*nDBN_groups*2]].replace('kanga','kanga1')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184be3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all data together\n",
    "ROC_summary_all_full_weighted_1secondlag['type']='weighted_1secondlag'\n",
    "ROC_summary_all_full_weighted_noself_1secondlag['type']='weighted_noself_1secondlag'\n",
    "ROC_summary_all_threeMains_1secondlag['type']='threeMains_1secondlag'\n",
    "ROC_summary_all_sync_pulls_1secondlag['type']='sync_pulls_1secondlag'\n",
    "ROC_summary_all_gaze_lead_pull_1secondlag['type']='gaze_lead_pull_1secondlag'\n",
    "ROC_summary_all_social_attention_1secondlag['type']='social_attention_1secondlag'\n",
    "ROC_summary_all_threeMains_withSelfEdge_1secondlag['type']='threeMains_withSelfEdge_1secondlag'\n",
    "ROC_summary_all_sync_pulls_withSelfEdge_1secondlag['type']='sync_pulls_withSelfEdge_1secondlag'\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag['type']='gaze_lead_pull_withSelfEdge_1secondlag'\n",
    "ROC_summary_all_social_attention_withSelfEdge_1secondlag['type']='social_attention_withSelfEdge_1secondlag'\n",
    "#\n",
    "ROC_summary_all_full_weighted_2secondlag['type']='weighted_2secondlag'\n",
    "ROC_summary_all_full_weighted_noself_2secondlag['type']='weighted_noself_2secondlag'\n",
    "ROC_summary_all_threeMains_2secondlag['type']='threeMains_2secondlag'\n",
    "ROC_summary_all_sync_pulls_2secondlag['type']='sync_pulls_2secondlag'\n",
    "ROC_summary_all_gaze_lead_pull_2secondlag['type']='gaze_lead_pull_2secondlag'\n",
    "ROC_summary_all_social_attention_2secondlag['type']='social_attention_2secondlag'\n",
    "ROC_summary_all_threeMains_withSelfEdge_2secondlag['type']='threeMains_withSelfEdge_2secondlag'\n",
    "ROC_summary_all_sync_pulls_withSelfEdge_2secondlag['type']='sync_pulls_withSelfEdge_2secondlag'\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag['type']='gaze_lead_pull_withSelfEdge_2secondlag'\n",
    "ROC_summary_all_social_attention_withSelfEdge_2secondlag['type']='social_attention_withSelfEdge_2secondlag'\n",
    "#\n",
    "ROC_summary_all_full_weighted_3secondlag['type']='weighted_3secondlag'\n",
    "ROC_summary_all_full_weighted_noself_3secondlag['type']='weighted_noself_3secondlag'\n",
    "ROC_summary_all_threeMains_3secondlag['type']='threeMains_3secondlag'\n",
    "ROC_summary_all_sync_pulls_3secondlag['type']='sync_pulls_3secondlag'\n",
    "ROC_summary_all_gaze_lead_pull_3secondlag['type']='gaze_lead_pull_3secondlag'\n",
    "ROC_summary_all_social_attention_3secondlag['type']='social_attention_3secondlag'\n",
    "ROC_summary_all_threeMains_withSelfEdge_3secondlag['type']='threeMains_withSelfEdge_3secondlag'\n",
    "ROC_summary_all_sync_pulls_withSelfEdge_3secondlag['type']='sync_pulls_withSelfEdge_3secondlag'\n",
    "ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag['type']='gaze_lead_pull_withSelfEdge_3secondlag'\n",
    "ROC_summary_all_social_attention_withSelfEdge_3secondlag['type']='social_attention_withSelfEdge_3secondlag'\n",
    "\n",
    "\n",
    "#\n",
    "ROC_summary_all_merged = pd.concat([\n",
    "                                    ROC_summary_all_full_weighted_1secondlag,\n",
    "                                    ROC_summary_all_full_weighted_noself_1secondlag,\n",
    "                                    ROC_summary_all_threeMains_1secondlag,\n",
    "                                    ROC_summary_all_sync_pulls_1secondlag,\n",
    "                                    ROC_summary_all_gaze_lead_pull_1secondlag,\n",
    "                                    ROC_summary_all_social_attention_1secondlag,\n",
    "                                    ROC_summary_all_threeMains_withSelfEdge_1secondlag,\n",
    "                                    ROC_summary_all_sync_pulls_withSelfEdge_1secondlag,\n",
    "                                    ROC_summary_all_gaze_lead_pull_withSelfEdge_1secondlag,\n",
    "                                    ROC_summary_all_social_attention_withSelfEdge_1secondlag,\n",
    "                                    #\n",
    "                                    ROC_summary_all_full_weighted_2secondlag,\n",
    "                                    ROC_summary_all_full_weighted_noself_2secondlag,\n",
    "                                    ROC_summary_all_threeMains_2secondlag,\n",
    "                                    ROC_summary_all_sync_pulls_2secondlag,\n",
    "                                    ROC_summary_all_gaze_lead_pull_2secondlag,\n",
    "                                    ROC_summary_all_social_attention_2secondlag,\n",
    "                                    ROC_summary_all_threeMains_withSelfEdge_2secondlag,\n",
    "                                    ROC_summary_all_sync_pulls_withSelfEdge_2secondlag,\n",
    "                                    ROC_summary_all_gaze_lead_pull_withSelfEdge_2secondlag,\n",
    "                                    ROC_summary_all_social_attention_withSelfEdge_2secondlag,\n",
    "                                    #\n",
    "                                    ROC_summary_all_full_weighted_3secondlag,\n",
    "                                    ROC_summary_all_full_weighted_noself_3secondlag,\n",
    "                                    ROC_summary_all_threeMains_3secondlag,\n",
    "                                    ROC_summary_all_sync_pulls_3secondlag,\n",
    "                                    ROC_summary_all_gaze_lead_pull_3secondlag,\n",
    "                                    ROC_summary_all_social_attention_3secondlag,\n",
    "                                    ROC_summary_all_threeMains_withSelfEdge_3secondlag,\n",
    "                                    ROC_summary_all_sync_pulls_withSelfEdge_3secondlag,\n",
    "                                    ROC_summary_all_gaze_lead_pull_withSelfEdge_3secondlag,\n",
    "                                    ROC_summary_all_social_attention_withSelfEdge_3secondlag,\n",
    "                                   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for each animal calculate the mean\n",
    "#\n",
    "animals_ROC = np.unique(ROC_summary_all_merged['animal'])\n",
    "nanis = np.shape(animals_ROC)[0]\n",
    "actions_ROC = np.unique(ROC_summary_all_merged['action'])\n",
    "nacts = np.shape(actions_ROC)[0]\n",
    "testCons_ROC = np.unique(ROC_summary_all_merged['testCondition'])\n",
    "ntcons = np.shape(testCons_ROC)[0]\n",
    "types_ROC = np.unique(ROC_summary_all_merged['type'])\n",
    "ntypes = np.shape(types_ROC)[0]\n",
    "ROC_summary_all_mean = pd.DataFrame(columns=ROC_summary_all_merged.keys())\n",
    "#\n",
    "for iani in np.arange(0,nanis,1):\n",
    "    animal_ROC = animals_ROC[iani]\n",
    "    ind_ani = ROC_summary_all_merged['animal']==animal_ROC\n",
    "    ROC_summary_all_iani = ROC_summary_all_merged[ind_ani]\n",
    "    #\n",
    "    for iact in np.arange(0,nacts,1):\n",
    "        action_ROC = actions_ROC[iact]\n",
    "        ind_act = ROC_summary_all_iani['action']==action_ROC\n",
    "        ROC_summary_all_iact = ROC_summary_all_iani[ind_act]\n",
    "        #\n",
    "        for itcon in np.arange(0,ntcons,1):\n",
    "            testCon_ROC = testCons_ROC[itcon]\n",
    "            ind_tcon = ROC_summary_all_iact['testCondition']==testCon_ROC\n",
    "            ROC_summary_all_itcon = ROC_summary_all_iact[ind_tcon]\n",
    "            #\n",
    "            for itype in np.arange(0,ntypes,1):\n",
    "                type_ROC = types_ROC[itype]\n",
    "                ind_type = ROC_summary_all_itcon['type'] == type_ROC\n",
    "                ROC_summary_all_itype = ROC_summary_all_itcon[ind_type]\n",
    "                #\n",
    "                ROC_summary_all_mean = ROC_summary_all_mean.append({'animal':animal_ROC,\n",
    "                                                                    'action':action_ROC,\n",
    "                                                                    'testCondition':testCon_ROC,\n",
    "                                                                    'type':type_ROC,\n",
    "                                                                    'predROC':np.nanmean(ROC_summary_all_itype['predROC'])\n",
    "                                                                 }, ignore_index=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4526e4c",
   "metadata": {},
   "source": [
    "#### plot for one specific task condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot\n",
    "testCond_forplot = 'coop(1.5s)' # 'no-vision','self','coop(1s)'\n",
    "# testCond_forplot = 'self' # 'no-vision','self','coop(1s)'\n",
    "# ROC_summary_all_forplot = ROC_summary_all_merged[ROC_summary_all_merged['testCondition']==testCond_forplot]\n",
    "ROC_summary_all_forplot = ROC_summary_all_mean[ROC_summary_all_mean['testCondition']==testCond_forplot]\n",
    "\n",
    "# plot the summarizing figure - pool all animals together\n",
    "#\n",
    "fig2, axs2 = plt.subplots(1,1)\n",
    "fig2.set_figheight(5)\n",
    "fig2.set_figwidth(15)\n",
    "# s=seaborn.barplot(ax=axs2,data=ROC_summary_all_forplot,x='action',y='predROC',hue='type',errorbar='ci',alpha=.5)\n",
    "s=seaborn.boxplot(ax=axs2,data=ROC_summary_all_forplot,x='action',y='predROC',hue='type',whis=3)\n",
    "# s=seaborn.swarmplot(ax=axs2,data=ROC_summary_all_forplot,x='action',y='predROC',hue='type',\n",
    "#                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "axs2.set_xlim([-0.5,1.5])\n",
    "axs2.set_title('all animal')\n",
    "s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "\n",
    "# plot the summarizing figure - separate each animal\n",
    "#\n",
    "fig3, axs3 = plt.subplots(2,1)\n",
    "fig3.set_figheight(10)\n",
    "fig3.set_figwidth(10)\n",
    "#\n",
    "events_forplot = ['pull','gaze'] \n",
    "nevents = np.shape(events_forplot)[0]\n",
    "\n",
    "for ievent in np.arange(0,nevents,1):\n",
    "    ievent_forplot = events_forplot[ievent]\n",
    "    \n",
    "    ROC_summary_all_ievent = ROC_summary_all_forplot[ROC_summary_all_forplot['action']==ievent_forplot]\n",
    "    #\n",
    "    s=seaborn.barplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "                    errorbar='ci',alpha=.5)\n",
    "    # s=seaborn.boxplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type')\n",
    "    # s=seaborn.swarmplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "    #                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "    axs3[ievent].plot([-0.5,9.5],[0.5,0.5],'k--')\n",
    "    axs3[ievent].set_xlim([-0.5,9.5])\n",
    "    axs3[ievent].set_title('all animal; events of '+ievent_forplot)\n",
    "    s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "# plot the summarizing figure - separate male and female\n",
    "maleanimals = np.array(['eddie','dannon','dodson','vermelho'])\n",
    "allanimalnames = np.array(ROC_summary_all_forplot['animal'])\n",
    "allanimalsexes = allanimalnames.copy()\n",
    "#\n",
    "allanimalsexes[np.isin(allanimalnames,maleanimals)]='male'\n",
    "allanimalsexes[~np.isin(allanimalnames,maleanimals)]='female'\n",
    "#\n",
    "ROC_summary_all_forplot['sex'] = allanimalsexes\n",
    "\n",
    "#\n",
    "fig4, axs4 = plt.subplots(1,2)\n",
    "fig4.set_figheight(5)\n",
    "fig4.set_figwidth(20)\n",
    "#\n",
    "events_forplot = ['pull','gaze'] \n",
    "nevents = np.shape(events_forplot)[0]\n",
    "\n",
    "for ievent in np.arange(0,nevents,1):\n",
    "    ievent_forplot = events_forplot[ievent]\n",
    "    \n",
    "    ROC_summary_all_ievent = ROC_summary_all_forplot[ROC_summary_all_forplot['action']==ievent_forplot]\n",
    "    #\n",
    "    # s=seaborn.barplot(ax=axs4[ievent],data=ROC_summary_all_ievent,x='sex',y='predROC',hue='type',\n",
    "    #                 errorbar='ci',alpha=.5)\n",
    "    s=seaborn.boxplot(ax=axs4[ievent],data=ROC_summary_all_ievent,x='sex',y='predROC',hue='type',whis=3)\n",
    "    # s=seaborn.swarmplot(ax=axs3[ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "    #                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "    axs4[ievent].plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "    axs4[ievent].set_xlim([-0.5,1.5])\n",
    "    axs4[ievent].set_title('all animal; events of '+ievent_forplot)\n",
    "    if ievent == nevents-1:\n",
    "        s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    else:\n",
    "        axs4[ievent].legend([])\n",
    "\n",
    "if 0:\n",
    "    # plot the summarizing figure - only plot the speficified condition; plot all animals together\n",
    "    fig5, axs5 = plt.subplots(1,1)\n",
    "    fig5.set_figheight(5)\n",
    "    fig5.set_figwidth(15)\n",
    "    #\n",
    "    testCond_forplot = 'coop(1s)' # 'no-vision','self','coop(1s)'\n",
    "    # ROC_summary_all_forplot = ROC_summary_all_merged[ROC_summary_all_merged['testCondition']==testCond_forplot]\n",
    "    ROC_summary_all_forplot = ROC_summary_all_mean[ROC_summary_all_mean['testCondition']==testCond_forplot]\n",
    "    #\n",
    "    plottypes = ['threeMains','sync_pulls','gaze_lead_pull','social_attention',\n",
    "        'threeMains_withSelfEdge','sync_pulls_withSelfEdge','gaze_lead_pull_withSelfEdge','social_attention_withSelfEdge']\n",
    "    ind_toplot = np.isin(ROC_summary_all_forplot['type'],plottypes)\n",
    "    ROC_summary_all_forplot2 = ROC_summary_all_forplot[ind_toplot]\n",
    "\n",
    "    # s=seaborn.barplot(ax=axs5,data=ROC_summary_all_forplot2,x='action',y='predROC',hue='type',errorbar='ci',alpha=.5)\n",
    "    s=seaborn.boxplot(ax=axs5,data=ROC_summary_all_forplot2,x='action',y='predROC',hue='type',whis=3)\n",
    "    # s=seaborn.swarmplot(ax=axs5,data=ROC_summary_all_forplot2,x='action',y='predROC',hue='type',\n",
    "    #                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "    plt.plot([-0.5,1.5],[0.5,0.5],'k--')\n",
    "    axs5.set_xlim([-0.5,1.5])\n",
    "    axs5.set_title('all animal, compared with and without self edges')\n",
    "    s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    fig2.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_differenttimelag_'+testCond_forplot+'_summarizingplot_allanimals.pdf')\n",
    "    fig3.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_differenttimelag_'+testCond_forplot+'_summarizingplot_eachanimals.pdf')\n",
    "    fig4.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_differenttimelag_'+testCond_forplot+'_summarizingplot_malefemales.pdf')\n",
    "    if 0:\n",
    "        fig5.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_differenttimelag_'+testCond_forplot+'_summarizingplot_allanimals_withandwithoutselfedge.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf2de2",
   "metadata": {},
   "source": [
    "#### plot, for multiple task condition, across multiple time lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_summary_all_forplot = ROC_summary_all_mean.copy()\n",
    "\n",
    "taskconds_order = ['self','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "ROC_summary_all_forplot['testCondition'] = pd.Categorical(ROC_summary_all_forplot['testCondition'], \n",
    "                                                          categories=taskconds_order, ordered=True)\n",
    "ROC_summary_all_forplot = ROC_summary_all_forplot.sort_values(by='testCondition')\n",
    "\n",
    "#\n",
    "events_forplot = ['pull','gaze'] \n",
    "nevents = np.shape(events_forplot)[0]\n",
    "\n",
    "# \n",
    "# \n",
    "examinewithselfedge = 0\n",
    "if examinewithselfedge:\n",
    "    dependencies_forplot = ['weighted','threeMains_withSelfEdge','sync_pulls_withSelfEdge',\n",
    "                            'gaze_lead_pull_withSelfEdge','social_attention_withSelfEdge']\n",
    "else:\n",
    "    dependencies_forplot = ['weighted_noself','threeMains','sync_pulls','gaze_lead_pull','social_attention']\n",
    "    \n",
    "ndepends = np.shape(dependencies_forplot)[0]\n",
    "\n",
    "\n",
    "# separate the action and stretagy structure; box plot\n",
    "fig4, axs4 = plt.subplots(ndepends,nevents)\n",
    "fig4.set_figheight(5*ndepends)\n",
    "fig4.set_figwidth(10*nevents)\n",
    "\n",
    "for idepend in np.arange(0,ndepends,1):\n",
    "    idepend_forplot = dependencies_forplot[idepend]\n",
    "    \n",
    "    idepend_alllags_forplot = [idepend_forplot+'_1secondlag',\n",
    "                               idepend_forplot+'_2secondlag',\n",
    "                               idepend_forplot+'_3secondlag',]\n",
    "    \n",
    "    \n",
    "    ROC_summary_all_idepend = ROC_summary_all_forplot[np.isin(ROC_summary_all_forplot['type'],idepend_alllags_forplot)]\n",
    "    ROC_summary_all_idepend['type'] = pd.Categorical(ROC_summary_all_idepend['type'],\n",
    "                                                     categories = idepend_alllags_forplot, ordered=True)\n",
    "    ROC_summary_all_idepend = ROC_summary_all_idepend.sort_values(by='type')\n",
    "    \n",
    "    \n",
    "    for ievent in np.arange(0,nevents,1):\n",
    "        ievent_forplot = events_forplot[ievent]\n",
    "\n",
    "        ROC_summary_all_ievent = ROC_summary_all_idepend[ROC_summary_all_idepend['action']==ievent_forplot]\n",
    "        #\n",
    "        # s=seaborn.barplot(ax=axs4[idepend,ievent],data=ROC_summary_all_ievent,x='sex',y='predROC',hue='type',\n",
    "        #                 errorbar='ci',alpha=.5)\n",
    "        s=seaborn.boxplot(ax=axs4[idepend,ievent],data=ROC_summary_all_ievent,\n",
    "                          x='testCondition',y='predROC',hue='type',whis=3)\n",
    "        # s=seaborn.swarmplot(ax=axs3[idepend,ievent],data=ROC_summary_all_ievent,x='animal',y='predROC',hue='type',\n",
    "        #                   alpha=.9,size=6,dodge=True,legend=False)\n",
    "        # s=seaborn.lineplot(ax=axs4[idepend,ievent],data=ROC_summary_all_ievent,\n",
    "        #                   x='testCondition',y='predROC',hue='type')\n",
    "        axs4[idepend,ievent].plot([-0.5,4.5],[0.5,0.5],'k--')\n",
    "        axs4[idepend,ievent].set_xlim([-0.5,4.5])\n",
    "        axs4[idepend,ievent].set_ylim([0.45,0.8])\n",
    "        axs4[idepend,ievent].set_title('all animal; events of '+ievent_forplot)\n",
    "        if ievent == nevents-1:\n",
    "            s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        else:\n",
    "            axs4[idepend,ievent].legend([])\n",
    "            \n",
    "        if 0:\n",
    "            # Connect the same individuals with lines across categories\n",
    "            x_ticks_offset = [-0.33,0,0.33]\n",
    "            linecolors = ['b','r','g']\n",
    "            for animal in ROC_summary_all_idepend['animal'].unique():\n",
    "                individual_data = ROC_summary_all_ievent[ROC_summary_all_idepend['animal'] == animal]\n",
    "                \n",
    "                types_toplot = np.unique(individual_data['type'])\n",
    "                ntypes = np.shape(types_toplot)[0]\n",
    "                \n",
    "                for itype in np.arange(0,ntypes,1):\n",
    "                    indidivual_data_itype = individual_data[individual_data['type']==types_toplot[itype]]\n",
    "                \n",
    "                    indidivual_data_itype['testCondition'] = pd.Categorical(indidivual_data_itype['testCondition'], \n",
    "                                                          categories=taskconds_order, ordered=True)\n",
    "                    indidivual_data_itype =indidivual_data_itype.sort_values(by='testCondition')\n",
    "                \n",
    "                    xxx = axs4[idepend,ievent].get_xticks()\n",
    "                    xticklabels = axs4[idepend,ievent].get_xticklabels()\n",
    "                    xtick_labels_text = [label.get_text() for label in xticklabels]\n",
    "                    nticks = np.shape(xtick_labels_text)[0]\n",
    "                    #\n",
    "                    xxx_toplots = np.zeros(np.shape(indidivual_data_itype['testCondition']))\n",
    "                    #\n",
    "                    for itick in np.arange(0,nticks,1):\n",
    "                        indd = np.array(indidivual_data_itype['testCondition']) == xtick_labels_text[itick]\n",
    "                        xxx_toplots[indd] = xxx[itick]\n",
    "                                       \n",
    "                    \n",
    "                    axs4[idepend,ievent].plot(xxx_toplots+x_ticks_offset[itype], indidivual_data_itype['predROC'],\n",
    "                                              '-', color=linecolors[itype])\n",
    "\n",
    "\n",
    "\n",
    "# separate the dependency structures and time lags, similar to the DBN fitting summarizing results; line plot\n",
    "timelags_forplot = ['1secondlag','2secondlag','3secondlag']\n",
    "nlags = np.shape(timelags_forplot)[0]\n",
    "fig5, axs5 = plt.subplots(nlags,ndepends)\n",
    "fig5.set_figheight(6*nlags)\n",
    "fig5.set_figwidth(6*ndepends)\n",
    "     \n",
    "for idepend in np.arange(0,ndepends,1):\n",
    "    idepend_forplot = dependencies_forplot[idepend]\n",
    "    \n",
    "    for ilag in np.arange(0,nlags,1):\n",
    "        ilag_forplot = timelags_forplot[ilag]\n",
    "        \n",
    "        idepend_ilag_forplot = idepend_forplot+'_'+ilag_forplot\n",
    "\n",
    "        ROC_summary_all_idepend_ilag = ROC_summary_all_forplot[np.isin(ROC_summary_all_forplot['type'],idepend_ilag_forplot)]\n",
    "        ROC_summary_all_idepend_ilag['action'] = pd.Categorical(ROC_summary_all_idepend_ilag['action'],\n",
    "                                                     categories = events_forplot, ordered=True)\n",
    "        ROC_summary_all_idepend_ilag = ROC_summary_all_idepend_ilag.sort_values(by='action')\n",
    "        \n",
    "        \n",
    "        s=seaborn.lineplot(ax=axs5[ilag,idepend],data=ROC_summary_all_idepend_ilag,\n",
    "                          x='testCondition',y='predROC',hue='action')\n",
    "        axs5[ilag,idepend].plot([-0.5,4.5],[0.5,0.5],'k--')\n",
    "        axs5[ilag,idepend].set_xlim([-0.5,4.5])\n",
    "        axs5[ilag,idepend].set_ylim([0.49,0.7])\n",
    "        axs5[ilag,idepend].set_title('all animal; events of '+idepend_ilag_forplot)\n",
    "        if idepend == ndepends-1:\n",
    "            s.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        else:\n",
    "            axs5[ilag,idepend].legend([])\n",
    "        \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents_DBNpredictions/'+savefile_sufix+'/'+cameraID+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if examinewithselfedge:\n",
    "        fig4.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_withselfedge_differenttimelag_multiconditions_summarizingplot.pdf')\n",
    "        fig5.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_withselfedge_differenttimelag_multiconditions_summarizingplot_lineplot.pdf')       \n",
    "    else:\n",
    "        fig4.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_noselfedge_differenttimelag_multiconditions_summarizingplot.pdf')\n",
    "        fig5.savefig(figsavefolder+'withinCondition_DBNpredicition_andHypotheticalDependencies_noselfedge_differenttimelag_multiconditions_summarizingplot_lineplot.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02894f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdcdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DLC] *",
   "language": "python",
   "name": "conda-env-.conda-DLC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
