{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the combined sessions, combined for each condition\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) \n",
    "### This script analyze the mixed pairs - unfamiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval\n",
    "from ana_functions.bhv_events_interval import bhv_events_interval_certainEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 2*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# Dodson Kanga\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230414\",\"20230418\",\"20230419\",\"20230420\",\"20230421\",  \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  164.00,  223.20,  255.00,  0, \n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "    #\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "# Eddie Kanga\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230424\",\"20230425\",\"20230426\",  \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  0,  0,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "    #\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "# Eddie and Scorch\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230421\",\"20230429\",\"20230629\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  0,  0,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "    #\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "    \n",
    "# Ginger and Dodson\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230508\",\"20230512\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  370.5,  \n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['dodson']\n",
    "    #\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Dodson\"\n",
    "\n",
    "# Sparkle and Dodson\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230426\",\"20230623\",\"20230629\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  0,  0,  \n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['sparkle']\n",
    "    animal2_fixedorder = ['dodson']\n",
    "    #\n",
    "    animal1_filename = \"Sparkle\"\n",
    "    animal2_filename = \"Dodson\"\n",
    "    \n",
    "# Eddie and Ginger\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230414\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                19.00, \n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['ginger']\n",
    "    #\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Ginger\"    \n",
    "    \n",
    "# Dannon and Dodson\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230912\", #\"20230927\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0, # 0,  \n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['dodson']\n",
    "    #\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Dodson\"\n",
    "    \n",
    "# Eddie and Dodson\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230427\",\"20230428\",\"20230501\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  0,  0,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['dodson']\n",
    "    #\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Dodson\"\n",
    "    \n",
    "# Ginger and Scorch\n",
    "if 1:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230424\",\"20230425\",\"20230426\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                545.50,  384.00,  290.80,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "    #\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# Ginger and Sparkle\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230419_2\",\"20230420_2\",#\"20230421_2\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0,  0, # 0,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    #\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "# Kanga and Scorch\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230427\",\"20230428\",\"20230501_2\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                475.5,  314,  0,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['kanga']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "    #\n",
    "    animal1_filename = \"Kanga\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "# Kanga and Sparkle - no pull; no analysis\n",
    "if 0:\n",
    "    if do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        dates_list = [\n",
    "                      \"20230510\",\"20230512\",\"20230515\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                469.8,  376,  390.5,\n",
    "                              ] # in second\n",
    "    #\n",
    "    animal1_fixedorder = ['kanga']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    #\n",
    "    animal1_filename = \"Kanga\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#    \n",
    "#dates_list = [\"20231010\"]\n",
    "#session_start_times = [0.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_edges_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "\n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull_edges_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull_edges_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "        try: \n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "            video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "            video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "        pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 1:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "        owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "        mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "        mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "\n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "            \n",
    "            all_pull_edges_intervals = bhv_events_interval_certainEdges(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            pull_edges_intv_all_dates[date_tgt] = all_pull_edges_intervals\n",
    "        else:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull2, time_point_pull1, \n",
    "                                                                         oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "            \n",
    "            all_pull_edges_intervals = bhv_events_interval_certainEdges(totalsess_time, session_start_time, time_point_pull2, time_point_pull1, \n",
    "                                                                        oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "            pull_edges_intv_all_dates[date_tgt] = all_pull_edges_intervals\n",
    "   \n",
    "\n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                              lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                              animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                              animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                              video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_edges_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_edges_intv_all_dates, f)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "succ_rate_all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aada694",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "# combine all the cooperation sessions\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "# coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort = tasktypes_all_dates\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900d890",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b179dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "\n",
    "#\n",
    "#tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#tasktypes = ['self','coop(1s)','no-vision']\n",
    "tasktypes = ['coop']\n",
    "\n",
    "#\n",
    "# ind=(sorting_df['coopthres']==100)|(sorting_df['coopthres']==1)|(sorting_df['coopthres']==-1)\n",
    "ind=(sorting_df['coopthres']==3)\n",
    "sorting_df = sorting_df[ind]\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    # pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    pull_other_intv_ii[pull_other_intv_ii>25]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "pull_other_intv_forplots_df = pd.DataFrame(pull_other_intv_forplots)\n",
    "pull_other_intv_forplots_df.columns = list(dates_list_sorted)\n",
    "\n",
    "#\n",
    "# plot\n",
    "# pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "seaborn.violinplot(ax=ax1,data=pull_other_intv_forplots_df,color='skyblue')\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "# plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch,15,'mean='+\"{:.3f}\".format(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "\n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3831554",
   "metadata": {},
   "source": [
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval; pool sessions within a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "\n",
    "#\n",
    "#tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "#tasktypes = ['self','coop(1s)','no-vision']\n",
    "tasktypes = ['coop']\n",
    "#\n",
    "# ind=(sorting_df['coopthres']==100)|(sorting_df['coopthres']==1)|(sorting_df['coopthres']==-1)\n",
    "ind=(sorting_df['coopthres']==3)\n",
    "sorting_df = sorting_df[ind]\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    # pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    pull_other_intv_ii[pull_other_intv_ii>25]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "pull_other_intv_forplots.columns = dates_list_sorted \n",
    "\n",
    "# pull_other_intv_forplots = [\n",
    "#     np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==100]['dates'])].stack().reset_index(drop=True)),\n",
    "#     np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==1]['dates'])].stack().reset_index(drop=True)),\n",
    "#     np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==-1]['dates'])].stack().reset_index(drop=True))\n",
    "# ]\n",
    "pull_other_intv_forplots = [\n",
    "    np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==3]['dates'])].stack().reset_index(drop=True)),\n",
    "]\n",
    "#\n",
    "pull_other_intv_forplots_df = pd.DataFrame(pull_other_intv_forplots).T\n",
    "pull_other_intv_forplots_df.columns = tasktypes\n",
    "\n",
    "# plt.boxplot(pull_other_intv_forplots,whis=1.5, meanline=True)\n",
    "seaborn.violinplot(ax=ax1,data=pull_other_intv_forplots_df)\n",
    "\n",
    "plt.xticks(np.arange(0, len(tasktypes), 1), tasktypes, fontsize = 14);\n",
    "ax1.set_ylim([-2,20])\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=14)\n",
    "ax1.set_title(\"animal pair: \"+animal1_fixedorder[0]+' '+animal2_fixedorder[0],fontsize=15)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_combinedsessions_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15978aac",
   "metadata": {},
   "source": [
    "#### focus on different pull edges intervals\n",
    "#### seperate individual animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b937f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "\n",
    "#\n",
    "# tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "# tasktypes = ['self','coop(1s)','no-vision']\n",
    "tasktypes = ['coop']\n",
    "#\n",
    "# ind=(sorting_df['coopthres']==100)|(sorting_df['coopthres']==1)|(sorting_df['coopthres']==-1)\n",
    "ind=(sorting_df['coopthres']==3)\n",
    "sorting_df = sorting_df[ind]\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "    \n",
    "#\n",
    "plotanimals = [animal1_fixedorder[0],animal2_fixedorder[0]]\n",
    "nanimals = np.shape(plotanimals)[0]\n",
    "#\n",
    "plottypes = [['pull_to_pull_interval','pull_to_pull_interval'],\n",
    "             ['pull2_to_pull1_interval','pull1_to_pull2_interval'],           \n",
    "             ['pull2_to_gaze1_interval','pull1_to_gaze2_interval'],\n",
    "             ['gaze2_to_pull1_interval','gaze1_to_pull2_interval'],\n",
    "             ['gaze1_to_pull1_interval','gaze2_to_pull2_interval'],\n",
    "             ['pull1_to_gaze1_interval','pull2_to_gaze2_interval'],\n",
    "           ]\n",
    "nplottypes = np.shape(plottypes)[0]\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(nplottypes,nanimals)\n",
    "fig.set_figheight(nplottypes*5)\n",
    "fig.set_figwidth(nanimals*10)\n",
    "\n",
    "\n",
    "for ianimal in np.arange(0,nanimals,1):\n",
    "    plotanimal = plotanimals[ianimal]\n",
    "    \n",
    "    for iplottype in np.arange(0,nplottypes,1):\n",
    "        plottype = plottypes[iplottype][ianimal]\n",
    "    \n",
    "        pull_other_intv_forplots = {}\n",
    "        pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "        pull_other_intv_ii = []\n",
    "        for ii in np.arange(0,ndates_sorted,1):\n",
    "            pull_other_intv_ii = pd.Series(pull_edges_intv_all_dates[dates_list_sorted[ii]][plottype])\n",
    "            # remove the interval that is too large\n",
    "            # pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "            pull_other_intv_ii[pull_other_intv_ii>25]= np.nan\n",
    "            pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "            pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "\n",
    "\n",
    "        #\n",
    "        pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "        #\n",
    "        pull_other_intv_forplots_df = pd.DataFrame(pull_other_intv_forplots)\n",
    "        pull_other_intv_forplots_df.columns = list(dates_list_sorted)\n",
    "\n",
    "        #\n",
    "        # plot\n",
    "        # pull_other_intv_forplots.plot(kind = 'box',ax=axs[iplottype,ianimal], positions=np.arange(0,ndates_sorted,1))\n",
    "        seaborn.violinplot(ax=axs[iplottype,ianimal],data=pull_other_intv_forplots_df,color='skyblue')\n",
    "        # plt.boxplot(pull_other_intv_forplots)\n",
    "        #axs[iplottype,ianimal].plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "        #\n",
    "        axs[iplottype,ianimal].set_ylabel(plottype,fontsize=13)\n",
    "        axs[iplottype,ianimal].set_ylim([-2,25])\n",
    "        #\n",
    "        axs[iplottype,ianimal].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        if iplottype == nplottypes-1:\n",
    "            axs[iplottype,ianimal].set_xticklabels(dates_list_sorted, rotation=45,fontsize=10)\n",
    "        else:\n",
    "            axs[iplottype,ianimal].set_xticklabels('')\n",
    "        axs[iplottype,ianimal].set_yticks(np.arange(-2,24,2))\n",
    "        axs[iplottype,ianimal].set_title('to animal:'+plotanimal)\n",
    "        #\n",
    "        taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[iplottype,ianimal].plot([taskswitch,taskswitch],[-2,25],'k--')\n",
    "        taskswitches = np.concatenate(([0],taskswitches))\n",
    "        for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "            taskswitch = taskswitches[itaskswitch]\n",
    "            axs[iplottype,ianimal].text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "        axs[iplottype,ianimal].text(taskswitch-0.25,23,'mean='+\"{:.3f}\".format(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "\n",
    "        #print(pull_other_intv_mean)\n",
    "        #print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"Pull_Edge_Interval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5cfe5",
   "metadata": {},
   "source": [
    "#### focus on different pull edges intervals\n",
    "#### seperate individual animals pool sessions within a task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5915c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "if 0:\n",
    "    # sort the data based on task type and dates\n",
    "    sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "    sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "\n",
    "    #\n",
    "    # tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    # tasktypes = ['self','coop(1s)','no-vision']\n",
    "    tasktypes = ['coop']\n",
    "    #\n",
    "    # ind=(sorting_df['coopthres']==100)|(sorting_df['coopthres']==1)|(sorting_df['coopthres']==-1)\n",
    "    ind=(sorting_df['coopthres']==3)\n",
    "    sorting_df = sorting_df[ind]\n",
    "    dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "    ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "    #\n",
    "    plotanimals = [animal1_fixedorder[0],animal2_fixedorder[0]]\n",
    "    nanimals = np.shape(plotanimals)[0]\n",
    "    #\n",
    "    plottypes = [['pull_to_pull_interval','pull_to_pull_interval'],\n",
    "                 ['pull2_to_pull1_interval','pull1_to_pull2_interval'],           \n",
    "                 ['pull2_to_gaze1_interval','pull1_to_gaze2_interval'],\n",
    "                 ['gaze2_to_pull1_interval','gaze1_to_pull2_interval'],\n",
    "                 ['gaze1_to_pull1_interval','gaze2_to_pull2_interval'],\n",
    "                 ['pull1_to_gaze1_interval','pull2_to_gaze2_interval'],\n",
    "               ]\n",
    "    nplottypes = np.shape(plottypes)[0]\n",
    "\n",
    "    #\n",
    "    fig, axs = plt.subplots(nplottypes,nanimals)\n",
    "    fig.set_figheight(nplottypes*5)\n",
    "    fig.set_figwidth(nanimals*5)\n",
    "\n",
    "\n",
    "    for ianimal in np.arange(0,nanimals,1):\n",
    "        plotanimal = plotanimals[ianimal]\n",
    "\n",
    "        for iplottype in np.arange(0,nplottypes,1):\n",
    "            plottype = plottypes[iplottype][ianimal]\n",
    "\n",
    "            pull_other_intv_forplots = {}\n",
    "            pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "            pull_other_intv_ii = []\n",
    "            for ii in np.arange(0,ndates_sorted,1):\n",
    "                pull_other_intv_ii = pd.Series(pull_edges_intv_all_dates[dates_list_sorted[ii]][plottype])\n",
    "                # remove the interval that is too large\n",
    "                # pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "                pull_other_intv_ii[pull_other_intv_ii>25]= np.nan\n",
    "                pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "                pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "\n",
    "            #\n",
    "            pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "            pull_other_intv_forplots.columns = dates_list_sorted \n",
    "\n",
    "            # pull_other_intv_forplots = [\n",
    "            #     np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==100]['dates'])].stack().reset_index(drop=True)),\n",
    "            #     np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==1]['dates'])].stack().reset_index(drop=True)),\n",
    "            #     np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==-1]['dates'])].stack().reset_index(drop=True))\n",
    "            # ]\n",
    "            pull_other_intv_forplots = [\n",
    "                np.array(pull_other_intv_forplots[list(sorting_df[sorting_df['coopthres']==3]['dates'])].stack().reset_index(drop=True)),\n",
    "            ]\n",
    "            #\n",
    "            pull_other_intv_forplots_df = pd.DataFrame(pull_other_intv_forplots).T\n",
    "            pull_other_intv_forplots_df.columns = tasktypes\n",
    "\n",
    "            #\n",
    "            if plottype == 'pull_to_pull_interval':\n",
    "                pull_other_intv_base_df = pd.DataFrame.copy(pull_other_intv_forplots_df)\n",
    "                pull_other_intv_base_df['interval_type']='cross animal pulls'\n",
    "\n",
    "            pull_other_intv_forplots_df['interval_type']='y axis dependency'\n",
    "            df_long=pd.concat([pull_other_intv_base_df,pull_other_intv_forplots_df])\n",
    "            # df_long2 = df_long.melt(id_vars=['interval_type'], value_vars=['self', 'coop(1s)', 'no-vision'],var_name='condition', value_name='value')\n",
    "            df_long2 = df_long.melt(id_vars=['interval_type'], value_vars=['coop'],var_name='condition', value_name='value')\n",
    "\n",
    "            # axs[iplottype,ianimal].boxplot(pull_other_intv_forplots,whis=1.5)\n",
    "            # seaborn.violinplot(ax=axs[iplottype,ianimal],data=pull_other_intv_forplots_df)\n",
    "            seaborn.violinplot(ax=axs[iplottype,ianimal],data=df_long2,x='condition',y='value',hue='interval_type',split=True,gap=5)\n",
    "\n",
    "            axs[iplottype,ianimal].set_xticks(np.arange(0, len(tasktypes), 1))\n",
    "            if iplottype == nplottypes-1:\n",
    "                axs[iplottype,ianimal].set_xticklabels(tasktypes, fontsize = 14)\n",
    "            else:\n",
    "                axs[iplottype,ianimal].set_xticklabels('')\n",
    "            ax=axs[iplottype,ianimal].set_ylim([-2,25])\n",
    "            ax=axs[iplottype,ianimal].set_ylabel(plottype,fontsize=13)\n",
    "            ax=axs[iplottype,ianimal].set_title('to animal:'+plotanimal,fontsize=15)\n",
    "\n",
    "\n",
    "    savefigs = 1\n",
    "    if savefigs:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+\"Pull_Edge_Interval_hist_combinedsessions_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c061dd",
   "metadata": {},
   "source": [
    "### plot some other basis behavioral measures\n",
    "#### social gaze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_numbers = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/30\n",
    "gaze_pull_ratios = (owgaze1_num_all_dates+owgaze2_num_all_dates+mtgaze1_num_all_dates+mtgaze2_num_all_dates)/(pull1_num_all_dates+pull2_num_all_dates)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# grouptypes = ['self reward','3s threshold','2s threshold','1.5s threshold','1s threshold','novision']\n",
    "# grouptypes = ['self reward','cooperative','no-vision']\n",
    "grouptypes = ['cooperative']\n",
    "\n",
    "gaze_numbers_groups = [# np.transpose(gaze_numbers[np.transpose(coopthres_forsort==100)[0]])[0],\n",
    "                       np.transpose(gaze_numbers[np.transpose(coopthres_forsort==3)[0]])[0],\n",
    "                       # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==2)[0]])[0],\n",
    "                       # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1.5)[0]])[0],\n",
    "                       # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==1)[0]])[0],\n",
    "                       # np.transpose(gaze_numbers[np.transpose(coopthres_forsort==-1)[0]])[0]\n",
    "                      ]\n",
    "\n",
    "gaze_numbers_plot = plt.boxplot(gaze_numbers_groups,whis=1.5, meanline=True)\n",
    "# gaze_numbers_plot = seaborn.violinplot(gaze_numbers_groups)\n",
    "# seaborn.swarmplot(gaze_numbers_groups)\n",
    "\n",
    "plt.xticks(np.arange(0+1, len(grouptypes)+1, 1), grouptypes, fontsize = 14);\n",
    "ax1.set_ylim([240/30,3000/30])\n",
    "ax1.set_ylabel(\"average social gaze time (s)\",fontsize=14)\n",
    "\n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"averaged_gazenumbers_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "# DBN_group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "# DBN_group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "# DBN_group_coopthres = [0,3,2,1.5,1,0]\n",
    "# DBN_group_typenames = ['self','coop','no-vision']\n",
    "# DBN_group_typeIDs  =  [1,3,5]\n",
    "# DBN_group_coopthres = [0,3,0] # combine all the cooperation conditions\n",
    "DBN_group_typenames = ['coop']\n",
    "DBN_group_typeIDs  =  [3]\n",
    "DBN_group_coopthres = [3] # combine all the cooperation conditions\n",
    "\n",
    "\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "prepare_input_data = 0\n",
    "\n",
    "DBN_input_data_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 1000 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            \n",
    "        # get animal info\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "        \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "            \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "\n",
    "        # load behavioral event results\n",
    "        print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "            output_look_ornot = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "            output_allvectors = pickle.load(f)\n",
    "        with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "            output_allangles = pickle.load(f)  \n",
    "        #\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']   \n",
    "\n",
    "\n",
    "        if mergetempRos:\n",
    "            temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "            # use bhv event to decide temporal resolution\n",
    "            #\n",
    "            #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "\n",
    "        ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "        # try different temporal resolutions\n",
    "        for temp_resolu in temp_resolus:\n",
    "            bhv_df = []\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            else:\n",
    "                bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "            if len(bhv_df)==0:\n",
    "                bhv_df = bhv_df_itr\n",
    "            else:\n",
    "                bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "            # merge sessions from the same condition\n",
    "            for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                # merge sessions \n",
    "                if (tasktype!=3):\n",
    "                    if (tasktype==iDBN_group_typeID):\n",
    "                        if (len(DBN_input_data_alltypes[iDBN_group_typename])==0):\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = bhv_df\n",
    "                        else:\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = pd.concat([DBN_input_data_alltypes[iDBN_group_typename],bhv_df])\n",
    "                else:\n",
    "                    # if (coop_thres==iDBN_group_cothres):\n",
    "                    if (coop_thres<=iDBN_group_cothres): # combine all the cooperation conditions\n",
    "                        if (len(DBN_input_data_alltypes[iDBN_group_typename])==0):\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = bhv_df\n",
    "                        else:\n",
    "                            DBN_input_data_alltypes[iDBN_group_typename] = pd.concat([DBN_input_data_alltypes[iDBN_group_typename],bhv_df])\n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a743731",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7d323",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d13d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "       \n",
    "        if not moreSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            samplingsizes = [min_samplesize,max_samplesize]\n",
    "            samplingsizes_name = ['min_row_number','max_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        # for jj in np.arange(0,nsamplings,1):\n",
    "        for jj in np.arange(0,1,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            # different session conditions (aka DBN groups)\n",
    "            # for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "            for iDBN_group in np.arange(1,2,1):\n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                try:\n",
    "                    bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "                    # bhv_df = bhv_df_all.sample(30*100,replace = True, random_state = round(time())) # take the subset for DBN training\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    # bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "\n",
    "\n",
    "                    # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                    colnames = list(bhv_df_all.columns)\n",
    "                    eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                    nevents = np.size(eventnames)\n",
    "\n",
    "                    all_pops = list(bhv_df_all.columns)\n",
    "                    from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                    to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                    causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                    nFromNodes = np.shape(from_pops)[0]\n",
    "                    nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                    DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                    score_randstart = np.zeros((num_starting_points))\n",
    "                    score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                    # step 1: randomize the starting point for num_starting_points times\n",
    "                    for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                        # try different down/re-sampling size\n",
    "                        bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                        aic = AicScore(bhv_df)\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                        aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                        np.random.seed(istarting_points)\n",
    "                        random.seed(istarting_points)\n",
    "                        starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                        starting_graph = DAG()\n",
    "                        starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                        starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                        # step 2: add the shffled data results\n",
    "                        # shuffled bhv_df\n",
    "                        best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                        DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                        DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                        score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                    DAGs_alltypes[iDBN_group_typename] = DAGs_randstart \n",
    "                    DAGs_shuffle_alltypes[iDBN_group_typename] = DAGs_randstart_shuffle\n",
    "\n",
    "                    DAGs_scores_alltypes[iDBN_group_typename] = score_randstart\n",
    "                    DAGs_shuffle_scores_alltypes[iDBN_group_typename] = score_randstart_shuffle\n",
    "\n",
    "                    weighted_graphs = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                    weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                    sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                    weighted_graphs_alltypes[iDBN_group_typename] = weighted_graphs\n",
    "                    weighted_graphs_shuffled_alltypes[iDBN_group_typename] = weighted_graphs_shuffled\n",
    "                    sig_edges_alltypes[iDBN_group_typename] = sig_edges\n",
    "                    \n",
    "                except:\n",
    "                    DAGs_alltypes[iDBN_group_typename] = [] \n",
    "                    DAGs_shuffle_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                    DAGs_scores_alltypes[iDBN_group_typename] = []\n",
    "                    DAGs_shuffle_scores_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                    weighted_graphs_alltypes[iDBN_group_typename] = []\n",
    "                    weighted_graphs_shuffled_alltypes[iDBN_group_typename] = []\n",
    "                    sig_edges_alltypes[iDBN_group_typename] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "    print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647783a",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "    for temp_resolu in temp_resolus:\n",
    "\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "                \n",
    "        # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "       \n",
    "        if not moreSampSize:\n",
    "            key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "            key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "            key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "            min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "            min_samplesize = int(min_samplesize/100)*100\n",
    "            max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "            max_samplesize = int(max_samplesize/100)*100\n",
    "            # samplingsizes = [min_samplesize,max_samplesize]\n",
    "            # samplingsizes_name = ['min_row_number','max_row_number']   \n",
    "            samplingsizes = [min_samplesize]\n",
    "            samplingsizes_name = ['min_row_number']   \n",
    "            nsamplings = np.shape(samplingsizes)[0]\n",
    "            print(samplingsizes)\n",
    "                \n",
    "        # try different down/re-sampling size\n",
    "        for jj in np.arange(0,nsamplings,1):\n",
    "            \n",
    "            isamplingsize = samplingsizes[jj]\n",
    "            \n",
    "            DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "            sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "            # different session conditions (aka DBN groups)\n",
    "            for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                # try:\n",
    "                bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "                # bhv_df = bhv_df_all.sample(30*100,replace = True, random_state = round(time())) # take the subset for DBN training\n",
    "\n",
    "                #Anirban(Alec) shuffle, slow\n",
    "                # bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "\n",
    "\n",
    "                # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                colnames = list(bhv_df_all.columns)\n",
    "                eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                nevents = np.size(eventnames)\n",
    "\n",
    "                all_pops = list(bhv_df_all.columns)\n",
    "                from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                nFromNodes = np.shape(from_pops)[0]\n",
    "                nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                score_randstart = np.zeros((num_starting_points))\n",
    "                score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                # step 1: randomize the starting point for num_starting_points times\n",
    "                for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                    # try different down/re-sampling size\n",
    "                    bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                    aic = AicScore(bhv_df)\n",
    "\n",
    "                    #Anirban(Alec) shuffle, slow\n",
    "                    bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                    aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                    np.random.seed(istarting_points)\n",
    "                    random.seed(istarting_points)\n",
    "                    starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                    starting_graph = DAG()\n",
    "                    starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                    starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                    best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                    DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                    DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                    score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                    # step 2: add the shffled data results\n",
    "                    # shuffled bhv_df\n",
    "                    best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                    DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                    DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                    score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                DAGs_alltypes[iDBN_group_typename] = DAGs_randstart \n",
    "                DAGs_shuffle_alltypes[iDBN_group_typename] = DAGs_randstart_shuffle\n",
    "\n",
    "                DAGs_scores_alltypes[iDBN_group_typename] = score_randstart\n",
    "                DAGs_shuffle_scores_alltypes[iDBN_group_typename] = score_randstart_shuffle\n",
    "\n",
    "                weighted_graphs = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                weighted_graphs_alltypes[iDBN_group_typename] = weighted_graphs\n",
    "                weighted_graphs_shuffled_alltypes[iDBN_group_typename] = weighted_graphs_shuffled\n",
    "                sig_edges_alltypes[iDBN_group_typename] = sig_edges\n",
    "                    \n",
    "                # except:\n",
    "                #     DAGs_alltypes[iDBN_group_typename] = [] \n",
    "                #     DAGs_shuffle_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                #     DAGs_scores_alltypes[iDBN_group_typename] = []\n",
    "                #     DAGs_shuffle_scores_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                #     weighted_graphs_alltypes[iDBN_group_typename] = []\n",
    "                #     weighted_graphs_shuffled_alltypes[iDBN_group_typename] = []\n",
    "                #     sig_edges_alltypes[iDBN_group_typename] = []\n",
    "                \n",
    "            DAGscores_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "            weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "            sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    save_data = 0\n",
    "    if save_data:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if moreSampSize:  \n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf1eea",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge with arrows; show the best time bin and row number; show the three time lag separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "# DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "# DBN_group_typeIDs  =  [1,3,5]\n",
    "# DBN_group_coopthres = [0,1,0]\n",
    "# DBN_group_typenames = ['self','coop','no-vision']\n",
    "# DBN_group_typeIDs  =  [1,3,5]\n",
    "# DBN_group_coopthres = [0,3,0]\n",
    "DBN_group_typenames = ['coop']\n",
    "DBN_group_typeIDs  =  [3]\n",
    "DBN_group_coopthres = [3]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "# eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "# eventnodes_color = ['b','r','y','g']\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00']\n",
    "eventnodes_shape = [\"o\",\"o\",\"^\",\"^\"]\n",
    "    \n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,nDBN_groups)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3],[4,5,6,7],[8,9,10,11]]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "    for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "\n",
    "       # try:\n",
    "\n",
    "        iDBN_group_typename = DBN_group_typenames[iDBN_group]\n",
    "\n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "        # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "        sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "        #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "\n",
    "        sig_avg_dags = weighted_graphs_tgt.mean(axis = 0) * sig_edges_tgt\n",
    "        sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "\n",
    "        # plot\n",
    "        axs[ilag*2+0].set_title(iDBN_group_typename,fontsize=18)\n",
    "        axs[ilag*2+0].set_xlim([-0.5,1.5])\n",
    "        axs[ilag*2+0].set_ylim([-0.5,1.5])\n",
    "        axs[ilag*2+0].set_xticks([])\n",
    "        axs[ilag*2+0].set_xticklabels([])\n",
    "        axs[ilag*2+0].set_yticks([])\n",
    "        axs[ilag*2+0].set_yticklabels([])\n",
    "        axs[ilag*2+0].spines['top'].set_visible(False)\n",
    "        axs[ilag*2+0].spines['right'].set_visible(False)\n",
    "        axs[ilag*2+0].spines['bottom'].set_visible(False)\n",
    "        axs[ilag*2+0].spines['left'].set_visible(False)\n",
    "        # axs[ilag*2+0].axis('equal')\n",
    "\n",
    "\n",
    "        for ieventnode in np.arange(0,nevents,1):\n",
    "            # plot the event nodes\n",
    "            axs[ilag*2+0].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                          eventnodes_shape[ieventnode],markersize=60,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                          markeredgecolor='none')              \n",
    "            #axs[ilag*2+0].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "            #                       eventnames[ieventnode],fontsize=15)\n",
    "\n",
    "            clmap = mpl.cm.get_cmap('Greens')\n",
    "\n",
    "            # plot the event edges\n",
    "            for ifromNode in np.arange(0,nevents,1):\n",
    "                for itoNode in np.arange(0,nevents,1):\n",
    "                    edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                    if edge_weight_tgt>0:\n",
    "                        if not ifromNode == itoNode:\n",
    "                            #axs[ilag*2+0].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                            axs[ilag*2+0].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                    nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                    nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                    nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                    # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                    # width=0.04*abs(edge_weight_tgt),\n",
    "                                                    head_width=0.08,\n",
    "                                                    width=0.04,   \n",
    "                                                    color = clmap(edge_weight_tgt))\n",
    "                        if ifromNode == itoNode:\n",
    "                            ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                  .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                  nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                  # 0.04*abs(edge_weight_tgt),\n",
    "                                                  0.04,\n",
    "                                                  color = clmap(edge_weight_tgt))\n",
    "                            p = PatchCollection(\n",
    "                                [ring], \n",
    "                                facecolor=clmap(edge_weight_tgt), \n",
    "                                edgecolor=clmap(edge_weight_tgt)\n",
    "                            )\n",
    "                            axs[ilag*2+0].add_collection(p)\n",
    "                            # add arrow head\n",
    "                            if ifromNode < 2:\n",
    "                                axs[ilag*2+0].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        0,-0.05,color=clmap(edge_weight_tgt),\n",
    "                                                        # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                        head_width=0.08,width=0.04      \n",
    "                                                        )\n",
    "                            else:\n",
    "                                axs[ilag*2+0].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        0,0.02,color=clmap(edge_weight_tgt),\n",
    "                                                        # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                        head_width=0.08,width=0.04      \n",
    "                                                        )\n",
    "\n",
    "        # heatmap for the weights\n",
    "        sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "        sig_avg_dags_df.columns = eventnames\n",
    "        sig_avg_dags_df.index = eventnames\n",
    "        vmin,vmax = 0,1\n",
    "        import matplotlib as mpl\n",
    "        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        im = axs[ilag*2+1].pcolormesh(sig_avg_dags_df,cmap=\"Greens\",norm=norm)\n",
    "        #\n",
    "        if iDBN_group == nDBN_groups-1:\n",
    "            cax = axs[ilag*2+1].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "            fig.colorbar(im, ax=axs[ilag*2+1], cax=cax,label='edge confidence')\n",
    "\n",
    "        axs[ilag*2+1].axis('equal')\n",
    "        axs[ilag*2+1].set_xlabel('to Node',fontsize=14)\n",
    "        axs[ilag*2+1].set_xticks(np.arange(0.5,4.5,1))\n",
    "        axs[ilag*2+1].set_xticklabels(eventnames)\n",
    "        if iDBN_group == 0:\n",
    "            axs[ilag*2+1].set_ylabel('from Node',fontsize=14)\n",
    "            axs[ilag*2+1].set_yticks(np.arange(0.5,4.5,1))\n",
    "            axs[ilag*2+1].set_yticklabels(eventnames)\n",
    "            axs[ilag*2+1].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            axs[ilag*2+1].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "        else:\n",
    "            axs[ilag*2+1].set_yticks([])\n",
    "            axs[ilag*2+1].set_yticklabels([])\n",
    "\n",
    "        #except:\n",
    "        #    continue\n",
    "    \n",
    "savefigs = 1    \n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if moreSampSize:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows.pdf')\n",
    "    else:  \n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed62ef",
   "metadata": {},
   "source": [
    "## Plots that include all pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e9217",
   "metadata": {},
   "source": [
    "### version 1: \n",
    "#### plot the raw weight of the key edges \n",
    "#### only show the coop; (coop(1s) for familiar pairs) \n",
    "#### separate animal 1 and 2, plot individual animal; \n",
    "#### put all animal in one plot - based on the \"to Node\"; for one time lag or merged all time lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "# each row means the \"to Node\" id\n",
    "moreSampSize = 0\n",
    "\n",
    "# to Node animal ID\n",
    "animal1_toNodes = ['eddie','dodson','dannon','ginger','sparkle','scorch','kanga']\n",
    "nanimal1_toNodes = np.shape(animal1_toNodes)[0]\n",
    "\n",
    "# the other animals' ID for each of the toNode animal\n",
    "animal2_familiars = [['sparkle'],\n",
    "                     ['scorch'],\n",
    "                     ['kanga'],\n",
    "                     ['kanga'],\n",
    "                     ['eddie'],\n",
    "                     ['dodson'],\n",
    "                     ['dannon','ginger']]\n",
    "animal2_unfamiliars = [['scorch','ginger','kanga','dodson'],\n",
    "                       ['kanga','ginger','sparkle','dannon','eddie'],\n",
    "                       ['dodson'],\n",
    "                       ['scorch','sparkle','eddie','dodson'],\n",
    "                       ['dodson','ginger'],\n",
    "                       ['eddie','ginger','kanga'],\n",
    "                       ['dodson','eddie','scorch']]\n",
    "\n",
    "\n",
    "timelag = 1 # 1 or 2 or 3 or 0(merged - merge all three lags) or 12 (merged lag 1 and 2)\n",
    "timelagname = '1second' # '1/2/3second' or 'merged' or '12merged'\n",
    "# timelagname = 'merged' # together with timelag = 0\n",
    "# timelagname = '12merged' # together with timelag = 12\n",
    "\n",
    "# define some parameters\n",
    "#\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# define how to load from the matrix\n",
    "if timelag == 1:\n",
    "    pull_pull_fromNodes_all = [9,8]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [11,10]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [8,9]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [9,8]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [10,11]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [11,10]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 2:\n",
    "    pull_pull_fromNodes_all = [5,4]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [7,6]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [4,5]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [5,4]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [6,7]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [7,6]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 3:\n",
    "    pull_pull_fromNodes_all = [1,0]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [3,2]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [0,1]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [1,0]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [2,3]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [3,2]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 0:\n",
    "    pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "    pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "    gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "    across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "    across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "elif timelag == 12:\n",
    "    pull_pull_fromNodes_all = [[5,9],[4,8]]\n",
    "    pull_pull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[7,11],[6,10]]\n",
    "    gaze_gaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[4,8],[5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[5,9],[4,8]]\n",
    "    across_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[6,10],[7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[7,11],[6,10]]\n",
    "    across_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "\n",
    "# initiate figures\n",
    "fig, axs = plt.subplots(nanimal1_toNodes,6)\n",
    "fig.set_figheight(8*nanimal1_toNodes)\n",
    "fig.set_figwidth(8*6)\n",
    "\n",
    "\n",
    "# loop all the animals\n",
    "for ianimal1 in np.arange(0,nanimal1_toNodes,1):\n",
    "    animal1_name = animal1_toNodes[ianimal1]\n",
    "    animal1_name_init = str.upper(animal1_name[0:2])\n",
    "    animal2_names = np.concatenate((animal2_familiars[ianimal1],animal2_unfamiliars[ianimal1]))\n",
    "    #\n",
    "    nanimal2 = np.shape(animal2_names)[0]\n",
    "    animal2_names_init = []\n",
    "    \n",
    "    # loop all the partner\n",
    "    for ianimal2 in np.arange(0,nanimal2,1):\n",
    "        animal2_name = animal2_names[ianimal2]\n",
    "        animal2_names_init.append(str.upper(animal2_name[0:2]))\n",
    "        \n",
    "        # load the raw weight data - the order of the animal1 and animal2 decide how to load matrix \n",
    "        try: # normal order - animal1_name _ animal2_name\n",
    "            try: # familiar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_name+animal2_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            except: # unfamiliar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_name+animal2_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            #\n",
    "            martrix_order = 0\n",
    "        #\n",
    "        except: # normal order - animal2_name _ animal1_name\n",
    "            try: # familiar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal2_name+animal1_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            except: # unfamiliar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal2_name+animal1_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            #\n",
    "            martrix_order = 1\n",
    "                        \n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "        # load edge weight data    \n",
    "        #\n",
    "        try:\n",
    "            weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "            weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "            sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "        except:\n",
    "            weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "            weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "            sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "\n",
    "        # organize the key edge data\n",
    "        weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "        #\n",
    "        \n",
    "\n",
    "        # plot raw coop eight weight\n",
    "\n",
    "        # pull-pull\n",
    "        a = (weighted_graphs_coop[:,pull_pull_fromNodes_all[martrix_order],pull_pull_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[ianimal1,0].errorbar(ianimal2,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[ianimal1,0].plot(ianimal2,xxx1,'o',markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (weighted_graphs_coop[:,gaze_gaze_fromNodes_all[martrix_order],gaze_gaze_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[ianimal1,1].errorbar(ianimal2,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[ianimal1,1].plot(ianimal2,xxx2,'o',markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (weighted_graphs_coop[:,within_gazepull_fromNodes_all[martrix_order],within_gazepull_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[ianimal1,2].errorbar(ianimal2,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[ianimal1,2].plot(ianimal2,xxx3,'o',markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (weighted_graphs_coop[:,across_gazepull_fromNodes_all[martrix_order],across_gazepull_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[ianimal1,3].errorbar(ianimal2,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[ianimal1,3].plot(ianimal2,xxx4,'o',markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (weighted_graphs_coop[:,within_pullgaze_fromNodes_all[martrix_order],within_pullgaze_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[ianimal1,4].errorbar(ianimal2,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[ianimal1,4].plot(ianimal2,xxx5,'o',markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (weighted_graphs_coop[:,across_pullgaze_fromNodes_all[martrix_order],across_pullgaze_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[ianimal1,5].errorbar(ianimal2,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[ianimal1,5].plot(ianimal2,xxx6,'o',markersize = 13,color='k')\n",
    "\n",
    "    #\n",
    "    plottypes = [animal1_name_init+' as To Node; across animal pull<->pull',\n",
    "                 animal1_name_init+' as To Node; across animal gaze<->gaze',\n",
    "                 animal1_name_init+' as To Node; within animal gaze->pull',\n",
    "                 animal1_name_init+' as To Node; across animal gaze->pull',\n",
    "                 animal1_name_init+' as To Node; within animal pull->gaze',\n",
    "                 animal1_name_init+' as To Node; across animal pull->gaze',\n",
    "                 ]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[ianimal1,iplot].set_xlim([-0.3,nanimal2-0.7])\n",
    "        axs[ianimal1,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[ianimal1,iplot].set_xticks(np.arange(0,nanimal2,1))\n",
    "        axs[ianimal1,iplot].set_xticklabels(animal2_names_init,fontsize = 20)\n",
    "        axs[ianimal1,iplot].set_xlabel('partner marmoset',fontsize = 20)\n",
    "        axs[ianimal1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        #\n",
    "        if iplot == 0:\n",
    "            axs[ianimal1,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[ianimal1,iplot].set_ylabel('Cooperative edge weights',fontsize=22)\n",
    "        else:\n",
    "            axs[ianimal1,iplot].set_yticklabels([])\n",
    "        axs[ianimal1,iplot].set_title(plottypes[iplot],fontsize = 21)\n",
    "        #\n",
    "        axs[ianimal1,iplot].plot([-1,nanimal2*2],[0,0],'k--')\n",
    "\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_CoopEdgeWeights_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI_basedonToNodes.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_CoopEdgeWeights_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI_basedonToNodes.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8907b",
   "metadata": {},
   "source": [
    "### version 1-1: \n",
    "#### plot the raw weight of the key edges \n",
    "#### only show the coop; (coop(1s) for familiar pairs) \n",
    "#### separate animal 1 and 2, plot individual animal; \n",
    "#### put all animal in one plot - based on the \"to Node\"; for one time lag or merged all time lags\n",
    "#### group into different groups: male action with familiar male; male action with unfamiliar male; male action with familiar female; male action with unfamiliar female; female action with familiar male; female action with unfamiliar male; female action with familiar female; female action with unfamiliar female; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec539601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "# each row means the \"to Node\" id\n",
    "moreSampSize = 0\n",
    "\n",
    "# to Node animal ID\n",
    "animal1_toNodes = ['eddie','dodson','dannon','ginger','sparkle','scorch','kanga']\n",
    "nanimal1_toNodes = np.shape(animal1_toNodes)[0]\n",
    "\n",
    "# the other animals' ID for each of the toNode animal\n",
    "animal2_familiars = [['sparkle'],\n",
    "                     ['scorch'],\n",
    "                     ['kanga'],\n",
    "                     ['kanga'],\n",
    "                     ['eddie'],\n",
    "                     ['dodson'],\n",
    "                     ['dannon','ginger']]\n",
    "animal2_unfamiliars = [['scorch','ginger','kanga','dodson'],\n",
    "                       ['kanga','ginger','sparkle','dannon','eddie'],\n",
    "                       ['dodson'],\n",
    "                       ['scorch','sparkle','eddie','dodson'],\n",
    "                       ['dodson','ginger'],\n",
    "                       ['eddie','ginger','kanga'],\n",
    "                       ['dodson','eddie','scorch']]\n",
    "\n",
    "\n",
    "timelag = 1 # 1 or 2 or 3 or 0(merged - merge all three lags) or 12 (merged lag 1 and 2)\n",
    "timelagname = '1second' # '1/2/3second' or 'merged' or '12merged'\n",
    "# timelagname = 'merged' # together with timelag = 0\n",
    "# timelagname = '12merged' # together with timelag = 12\n",
    "\n",
    "# define some parameters\n",
    "#\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# define how to load from the matrix\n",
    "if timelag == 1:\n",
    "    pull_pull_fromNodes_all = [9,8]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [11,10]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [8,9]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [9,8]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [10,11]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [11,10]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 2:\n",
    "    pull_pull_fromNodes_all = [5,4]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [7,6]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [4,5]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [5,4]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [6,7]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [7,6]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 3:\n",
    "    pull_pull_fromNodes_all = [1,0]\n",
    "    pull_pull_toNodes_all = [0,1]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [3,2]\n",
    "    gaze_gaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [0,1]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [1,0]\n",
    "    across_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [2,3]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [3,2]\n",
    "    across_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "elif timelag == 0:\n",
    "    pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "    pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "    gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "    across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "    across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "elif timelag == 12:\n",
    "    pull_pull_fromNodes_all = [[5,9],[4,8]]\n",
    "    pull_pull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[7,11],[6,10]]\n",
    "    gaze_gaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[4,8],[5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[5,9],[4,8]]\n",
    "    across_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[6,10],[7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[7,11],[6,10]]\n",
    "    across_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "    #\n",
    "\n",
    "# save the summarizing data\n",
    "edgetype_names = ['across animal pull<->pull','across animal gaze<->gaze','within animal gaze->pull',\n",
    "                  'across animal gaze->pull', 'within animal pull->gaze', 'across animal pull->gaze',]\n",
    "nedgetypes = np.shape(edgetype_names)[0]\n",
    "mean_edge_weight_all = dict.fromkeys(edgetype_names,{})\n",
    "mean_pull_pull = {}\n",
    "mean_gaze_gaze = {}\n",
    "mean_within_gaze_pull = {}\n",
    "mean_cross_gaze_pull ={}\n",
    "mean_within_pull_gaze = {}\n",
    "mean_cross_pull_gaze ={}\n",
    "\n",
    "\n",
    "# loop all the animals\n",
    "for ianimal1 in np.arange(0,nanimal1_toNodes,1):\n",
    "    animal1_name = animal1_toNodes[ianimal1]\n",
    "    animal1_name_init = str.upper(animal1_name[0:2])\n",
    "    animal2_names = np.concatenate((animal2_familiars[ianimal1],animal2_unfamiliars[ianimal1]))\n",
    "    #\n",
    "    nanimal2 = np.shape(animal2_names)[0]\n",
    "    animal2_names_init = []\n",
    "    \n",
    "    # loop all the partner\n",
    "    for ianimal2 in np.arange(0,nanimal2,1):\n",
    "        animal2_name = animal2_names[ianimal2]\n",
    "        animal2_names_init.append(str.upper(animal2_name[0:2]))\n",
    "                \n",
    "        # load the raw weight data - the order of the animal1 and animal2 decide how to load matrix \n",
    "        try: # normal order - animal1_name _ animal2_name\n",
    "            try: # familiar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_name+animal2_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            except: # unfamiliar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_name+animal2_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            #\n",
    "            martrix_order = 0\n",
    "        #\n",
    "        except: # normal order - animal2_name _ animal1_name\n",
    "            try: # familiar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal2_name+animal1_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            except: # unfamiliar pairs\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal2_name+animal1_name+'/'\n",
    "                if moreSampSize:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            #\n",
    "            martrix_order = 1\n",
    "                        \n",
    "        #\n",
    "        temp_resolu = temp_resolus[0]\n",
    "        j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "        # load edge weight data    \n",
    "        #\n",
    "        try:\n",
    "            weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "            weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "            sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "        except:\n",
    "            weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "            weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "            sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "\n",
    "        # organize the key edge data\n",
    "        weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "        #\n",
    "        \n",
    "        # calculate the mean edge weight\n",
    "        # pull-pull\n",
    "        a = (weighted_graphs_coop[:,pull_pull_fromNodes_all[martrix_order],pull_pull_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        mean_pull_pull[animal2_name+'_to_'+animal1_name]=xxx1\n",
    "        # gaze-gaze\n",
    "        a = (weighted_graphs_coop[:,gaze_gaze_fromNodes_all[martrix_order],gaze_gaze_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        mean_gaze_gaze[animal2_name+'_to_'+animal1_name]=xxx2\n",
    "        # within animal gazepull\n",
    "        a = (weighted_graphs_coop[:,within_gazepull_fromNodes_all[martrix_order],within_gazepull_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        mean_within_gaze_pull[animal2_name+'_to_'+animal1_name]=xxx3\n",
    "        # across animal gazepull\n",
    "        a = (weighted_graphs_coop[:,across_gazepull_fromNodes_all[martrix_order],across_gazepull_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        mean_cross_gaze_pull[animal2_name+'_to_'+animal1_name]=xxx4\n",
    "        # within animal pullgaze\n",
    "        a = (weighted_graphs_coop[:,within_pullgaze_fromNodes_all[martrix_order],within_pullgaze_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        mean_within_pull_gaze[animal2_name+'_to_'+animal1_name]=xxx5\n",
    "        # across animal pullgaze\n",
    "        a = (weighted_graphs_coop[:,across_pullgaze_fromNodes_all[martrix_order],across_pullgaze_toNodes_all[martrix_order]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        mean_cross_pull_gaze[animal2_name+'_to_'+animal1_name]=xxx6\n",
    "\n",
    "mean_edge_weight_all['across animal pull<->pull'] = mean_pull_pull\n",
    "mean_edge_weight_all['across animal gaze<->gaze'] = mean_gaze_gaze\n",
    "mean_edge_weight_all['within animal gaze->pull'] = mean_within_gaze_pull\n",
    "mean_edge_weight_all['across animal gaze->pull'] = mean_cross_gaze_pull\n",
    "mean_edge_weight_all['within animal pull->gaze'] = mean_within_pull_gaze\n",
    "mean_edge_weight_all['across animal pull->gaze'] = mean_cross_pull_gaze\n",
    "        \n",
    "        \n",
    "# plot the summarizing figures\n",
    "if 1:\n",
    "    grouptypes = ['male action with unfamiliar male',\n",
    "                  'male action with familiar female',\n",
    "                  'male action with unfamiliar female',\n",
    "                  'female action with familiar male',\n",
    "                  'female action with unfamiliar male',\n",
    "                  'female action with familiar female',\n",
    "                  'female action with unfamiliar female',\n",
    "                 ]    \n",
    "    ngroupttypes = np.shape(grouptypes)[0]\n",
    "    #\n",
    "    animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie'],\n",
    "                        ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon'],\n",
    "                        ['kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie'],\n",
    "                        ['eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga'],\n",
    "                        ['dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger'],\n",
    "                        ['kanga_to_ginger','ginger_to_kanga'],\n",
    "                        ['ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga'],\n",
    "                      ]\n",
    "    grouptypename = 'MaleFemaleFamiliarUnfamiliar'\n",
    "#\n",
    "if 0:\n",
    "    grouptypes = ['individual in unfamiliar pairs',\n",
    "                  'individual in familiar pairs',\n",
    "                 ]    \n",
    "    ngroupttypes = np.shape(grouptypes)[0]\n",
    "    #\n",
    "    animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie',\n",
    "                         'kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie',\n",
    "                         'dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger',\n",
    "                         'ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga',\n",
    "                         ],\n",
    "                        ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon',\n",
    "                         'eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga',\n",
    "                         'kanga_to_ginger','ginger_to_kanga'\n",
    "                        ],\n",
    "                      ]\n",
    "    grouptypename = 'FamiliarUnfamiliar'\n",
    "#\n",
    "if 0:\n",
    "    grouptypes = ['male action with unfamiliar pair',\n",
    "                  'male action with familiar pair',                  \n",
    "                  'female action with unfamiliar pair',\n",
    "                  'female action with familiar pair',\n",
    "                 ]    \n",
    "    ngroupttypes = np.shape(grouptypes)[0]\n",
    "    #\n",
    "    animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie',\n",
    "                         'kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie'],\n",
    "                        ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon'],\n",
    "                        ['dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger',\n",
    "                         'ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga'],\n",
    "                        ['eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga','kanga_to_ginger','ginger_to_kanga'],\n",
    "                      ]\n",
    "    grouptypename = 'MaleFemale'\n",
    "    \n",
    "# edgetype for plot only\n",
    "# edgetype_names_forplot = ['across animal pull<->pull','within animal gaze->pull','across animal pull->gaze',]\n",
    "edgetype_names_forplot = ['across animal pull<->pull','across animal gaze<->gaze','within animal gaze->pull',\n",
    "                          'across animal gaze->pull', 'within animal pull->gaze', 'across animal pull->gaze',\n",
    "                         ]\n",
    "nedgetypes_forplot = np.shape(edgetype_names_forplot)[0]\n",
    "\n",
    "# initiate figures\n",
    "fig, axs = plt.subplots(1,nedgetypes_forplot) # 6: six types of edges\n",
    "fig.set_figheight(8*1)\n",
    "fig.set_figwidth(8*nedgetypes_forplot)\n",
    "\n",
    "for iedgetype in np.arange(0,nedgetypes_forplot,1):\n",
    "    edgetype_name = edgetype_names_forplot[iedgetype]\n",
    "    mean_edge_weight_iedgetype = mean_edge_weight_all[edgetype_name]\n",
    "    \n",
    "    mean_edge_weight_grouptypes = dict.fromkeys(grouptypes,())\n",
    "    \n",
    "    # reorganize the data\n",
    "    for igrouptype in np.arange(0,ngroupttypes,1):\n",
    "        animalpairtypes_igroup = animalpairtypes[igrouptype]\n",
    "        #\n",
    "        mean_edge_weight_grouptypes[grouptypes[igrouptype]] = np.array([mean_edge_weight_iedgetype[x] for x in animalpairtypes_igroup])\n",
    "    mean_edge_weight_grouptypes_df = pd.DataFrame.from_dict(mean_edge_weight_grouptypes,orient='index')\n",
    "    mean_edge_weight_grouptypes_df = mean_edge_weight_grouptypes_df.transpose()\n",
    "    \n",
    "    seaborn.barplot(ax=axs.ravel()[iedgetype],data=mean_edge_weight_grouptypes_df,errorbar='se',alpha=.5,capsize=0.1)\n",
    "    seaborn.swarmplot(ax=axs.ravel()[iedgetype],data=mean_edge_weight_grouptypes_df,alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs.ravel()[iedgetype].set_xlabel('')\n",
    "    axs.ravel()[iedgetype].set_ylabel('cooperation edge weights',fontsize=20)\n",
    "    axs.ravel()[iedgetype].set_title(edgetype_name,fontsize=24)\n",
    "    axs.ravel()[iedgetype].set_ylim([-0.05,1.05])\n",
    "    axs.ravel()[iedgetype].set_xticklabels(axs.ravel()[iedgetype].get_xticklabels(), rotation=90)\n",
    "        \n",
    "        \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_CoopEdgeWeights_'+grouptypename+'_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI_basedonToNodes.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_CoopEdgeWeights_'+grouptypename+'_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI_basedonToNodes.pdf',bbox_inches=\"tight\")\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989dec2",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "### plot the social gaze and separate in different condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c171f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "# each row means the \"to Node\" id\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 \n",
    "\n",
    "temp_resolu = 1 # 1s \n",
    "\n",
    "# to Node animal ID\n",
    "animal1_toNodes = ['eddie','dodson','dannon','ginger','sparkle','scorch','kanga']\n",
    "nanimal1_toNodes = np.shape(animal1_toNodes)[0]\n",
    "\n",
    "# the other animals' ID for each of the toNode animal\n",
    "animal2_familiars = [['sparkle'],\n",
    "                     ['scorch'],\n",
    "                     ['kanga'],\n",
    "                     ['kanga'],\n",
    "                     ['eddie'],\n",
    "                     ['dodson'],\n",
    "                     ['dannon','ginger']]\n",
    "animal2_unfamiliars = [['scorch','ginger','kanga','dodson'],\n",
    "                       ['kanga','ginger','sparkle','dannon','eddie'],\n",
    "                       ['dodson'],\n",
    "                       ['scorch','sparkle','eddie','dodson'],\n",
    "                       ['dodson','ginger'],\n",
    "                       ['eddie','ginger','kanga'],\n",
    "                       ['dodson','eddie','scorch']]\n",
    "\n",
    "# save the summarizing data\n",
    "mean_social_gaze_num_all = {}\n",
    "mean_social_gaze_per_second_all = {}\n",
    "\n",
    "# loop all the animals\n",
    "for ianimal1 in np.arange(0,nanimal1_toNodes,1):\n",
    "    animal1_name = animal1_toNodes[ianimal1]\n",
    "    animal1_name_init = str.upper(animal1_name[0:2])\n",
    "    animal2_names = np.concatenate((animal2_familiars[ianimal1],animal2_unfamiliars[ianimal1]))\n",
    "    #\n",
    "    nanimal2 = np.shape(animal2_names)[0]\n",
    "    animal2_names_init = []\n",
    "    \n",
    "    # loop all the partner\n",
    "    for ianimal2 in np.arange(0,nanimal2,1):\n",
    "        animal2_name = animal2_names[ianimal2]\n",
    "        animal2_names_init.append(str.upper(animal2_name[0:2]))\n",
    "\n",
    "        # load data - DBN_input_data_alltypes\n",
    "        try: # familiar pairs\n",
    "            try:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_name+animal2_name+'/'\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_name+animal2_name+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_name+animal2_name+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                colname = 'owgaze1_t0'\n",
    "                tasktype = 'coop(1s)'\n",
    "                #\n",
    "            except:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal2_name+animal1_name+'/'\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal2_name+animal1_name+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal2_name+animal1_name+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                colname = 'owgaze2_t0'\n",
    "                tasktype = 'coop(1s)'\n",
    "                #\n",
    "        except: # unfamiliar pairs\n",
    "            try:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_name+animal2_name+'/'\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_name+animal2_name+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_name+animal2_name+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                colname = 'owgaze1_t0'\n",
    "                tasktype = 'coop'\n",
    "                #\n",
    "            except:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal2_name+animal1_name+'/'\n",
    "                if not mergetempRos:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal2_name+animal1_name+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal2_name+animal1_name+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                colname = 'owgaze2_t0'\n",
    "                tasktype = 'coop'\n",
    "                \n",
    "        # gaze column for DBN_input_data\n",
    "        gaze_series = DBN_input_data_alltypes[tasktype][colname]\n",
    "        #\n",
    "        nsessions = np.sum(gaze_series.index==0)\n",
    "        session_endIDs = np.where((gaze_series.index[1:]-(gaze_series.index[0:-1]+1))!=0)[0]\n",
    "        session_startIDs = np.concatenate([np.array([0]),session_endIDs+1,np.array([np.shape(gaze_series)[0]+1])])\n",
    "        #\n",
    "        gaze_num_allsessions = np.zeros((1,nsessions))[0]\n",
    "        gaze_per_sec_allsessions = np.zeros((1,nsessions))[0]\n",
    "        #\n",
    "        # count gaze numbers (1s time bin) for each session\n",
    "        for isession in np.arange(0,nsessions,1):\n",
    "            gaze_series_isess = gaze_series.reset_index(drop=True)[session_startIDs[isession]:session_startIDs[isession+1]]\n",
    "            gaze_num_allsessions[isession] = np.sum(gaze_series_isess)\n",
    "            gaze_per_sec_allsessions[isession] = np.sum(gaze_series_isess)/np.shape(gaze_series_isess)[0]\n",
    "            # \n",
    "        mean_social_gaze_num_all[animal2_name+'_to_'+animal1_name] = gaze_num_allsessions\n",
    "        mean_social_gaze_per_second_all[animal2_name+'_to_'+animal1_name] = gaze_per_sec_allsessions\n",
    "            \n",
    "            \n",
    "# plot the summarizing figures\n",
    "if 1:\n",
    "    grouptypes = ['male action with unfamiliar male',\n",
    "                  'male action with familiar female',\n",
    "                  'male action with unfamiliar female',\n",
    "                  'female action with familiar male',\n",
    "                  'female action with unfamiliar male',\n",
    "                  'female action with familiar female',\n",
    "                  'female action with unfamiliar female',\n",
    "                 ]    \n",
    "    ngroupttypes = np.shape(grouptypes)[0]\n",
    "    #\n",
    "    animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie'],\n",
    "                        ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon'],\n",
    "                        ['kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie'],\n",
    "                        ['eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga'],\n",
    "                        ['dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger'],\n",
    "                        ['kanga_to_ginger','ginger_to_kanga'],\n",
    "                        ['ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga'],\n",
    "                      ]\n",
    "    grouptypename = 'MaleFemaleFamiliarUnfamiliar'\n",
    "#\n",
    "if 0:\n",
    "    grouptypes = ['individual in unfamiliar pairs',\n",
    "                  'individual in familiar pairs',\n",
    "                 ]    \n",
    "    ngroupttypes = np.shape(grouptypes)[0]\n",
    "    #\n",
    "    animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie',\n",
    "                         'kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie',\n",
    "                         'dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger',\n",
    "                         'ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga',\n",
    "                         ],\n",
    "                        ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon',\n",
    "                         'eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga',\n",
    "                         'kanga_to_ginger','ginger_to_kanga'\n",
    "                        ],\n",
    "                      ]\n",
    "    grouptypename = 'FamiliarUnfamiliar'\n",
    "#\n",
    "if 0:\n",
    "    grouptypes = ['male action with unfamiliar pair',\n",
    "                  'male action with familiar pair',                  \n",
    "                  'female action with unfamiliar pair',\n",
    "                  'female action with familiar pair',\n",
    "                 ]    \n",
    "    ngroupttypes = np.shape(grouptypes)[0]\n",
    "    #\n",
    "    animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie',\n",
    "                         'kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie'],\n",
    "                        ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon'],\n",
    "                        ['dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger',\n",
    "                         'ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga'],\n",
    "                        ['eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga','kanga_to_ginger','ginger_to_kanga'],\n",
    "                      ]\n",
    "    grouptypename = 'MaleFemale'\n",
    "    \n",
    "# initiate figures\n",
    "nplots = 1\n",
    "fig, axs = plt.subplots(1,nplots) # \n",
    "fig.set_figheight(8*1)\n",
    "fig.set_figwidth(8*nplots)\n",
    "\n",
    "#\n",
    "for iplot in np.arange(0,nplots,1):\n",
    "    # \n",
    "    mean_social_gaze_grouptypes = dict.fromkeys(grouptypes,())\n",
    "    \n",
    "    # reorganize the data\n",
    "    for igrouptype in np.arange(0,ngroupttypes,1):\n",
    "        animalpairtypes_igroup = animalpairtypes[igrouptype]\n",
    "        #\n",
    "        mean_social_gaze_grouptypes[grouptypes[igrouptype]] = np.concatenate([mean_social_gaze_per_second_all[x] for x in animalpairtypes_igroup])\n",
    "        # mean_social_gaze_grouptypes[grouptypes[igrouptype]] = np.concatenate([mean_social_gaze_num_all[x] for x in animalpairtypes_igroup])\n",
    "        \n",
    "    #\n",
    "    mean_social_gaze_grouptypes_df = pd.DataFrame.from_dict(mean_social_gaze_grouptypes,orient='index')\n",
    "    mean_social_gaze_grouptypes_df = mean_social_gaze_grouptypes_df.transpose()\n",
    "    \n",
    "    seaborn.barplot(ax=axs,data=mean_social_gaze_grouptypes_df,errorbar='se',alpha=.5,capsize=0.1)\n",
    "    seaborn.swarmplot(ax=axs,data=mean_social_gaze_grouptypes_df,alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    axs.set_xlabel('')\n",
    "    axs.set_ylabel('mean social gaze per second',fontsize=20)\n",
    "    axs.set_title('mean social gaze per second',fontsize=24)\n",
    "    # axs.set_ylim([-0.05,1.05])\n",
    "    axs.set_xticklabels(axs.get_xticklabels(), rotation=90)\n",
    "        \n",
    "        \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_SocialGazeNums_'+grouptypename+'_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI_basedonToNodes.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_SocialGazeNums_'+grouptypename+'_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI_basedonToNodes.pdf',bbox_inches=\"tight\")\n",
    "           \n",
    "        \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49ecf0",
   "metadata": {},
   "source": [
    "### plot the success rate and separate in different condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "# each row means the \"to Node\" id\n",
    "moreSampSize = 0\n",
    "mergetempRos = 0 \n",
    "\n",
    "temp_resolu = 1 # 1s \n",
    "\n",
    "# to Node animal ID\n",
    "animal1_toNodes = ['eddie','dodson','dannon','ginger','sparkle','scorch','kanga']\n",
    "nanimal1_toNodes = np.shape(animal1_toNodes)[0]\n",
    "\n",
    "# the other animals' ID for each of the toNode animal\n",
    "animal2_familiars = [['sparkle'],\n",
    "                     ['scorch'],\n",
    "                     ['kanga'],\n",
    "                     ['kanga'],\n",
    "                     ['eddie'],\n",
    "                     ['dodson'],\n",
    "                     ['dannon','ginger']]\n",
    "animal2_unfamiliars = [['scorch','ginger','kanga','dodson'],\n",
    "                       ['kanga','ginger','sparkle','dannon','eddie'],\n",
    "                       ['dodson'],\n",
    "                       ['scorch','sparkle','eddie','dodson'],\n",
    "                       ['dodson','ginger'],\n",
    "                       ['eddie','ginger','kanga'],\n",
    "                       ['dodson','eddie','scorch']]\n",
    "\n",
    "#\n",
    "malenames = ['eddie','dodson','dannon']\n",
    "\n",
    "# save the summarizing data\n",
    "succrate_inMC_all = pd.DataFrame(columns=['animal1name','animal2name','animal1sex','animal2sex',\n",
    "                                          'pairtype','succrate'])\n",
    "\n",
    "# initiate figures\n",
    "nplots = 1\n",
    "fig, axs = plt.subplots(1,nplots) # \n",
    "fig.set_figheight(8*1)\n",
    "fig.set_figwidth(8*nplots)\n",
    "\n",
    "# loop all the animals\n",
    "for ianimal1 in np.arange(0,nanimal1_toNodes,1):\n",
    "    animal1_name = animal1_toNodes[ianimal1]\n",
    "    animal1_name_init = str.upper(animal1_name[0:2])\n",
    "    animal2_names = np.concatenate((animal2_familiars[ianimal1],animal2_unfamiliars[ianimal1]))\n",
    "    #\n",
    "    nanimal2 = np.shape(animal2_names)[0]\n",
    "    animal2_names_init = []\n",
    "    \n",
    "    if np.isin(animal1_name,malenames):\n",
    "        animal1_sex = 'male'\n",
    "    else:\n",
    "        animal1_sex = 'female'\n",
    "    \n",
    "    \n",
    "    # loop all the partner\n",
    "    for ianimal2 in np.arange(0,nanimal2,1):\n",
    "        animal2_name = animal2_names[ianimal2]\n",
    "        animal2_names_init.append(str.upper(animal2_name[0:2]))\n",
    "        \n",
    "        if np.isin(animal2_name,malenames):\n",
    "            animal2_sex = 'male'\n",
    "        else:\n",
    "            animal2_sex = 'female'\n",
    "\n",
    "        # load data - DBN_input_data_alltypes\n",
    "        try: # familiar pairs\n",
    "            try:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_name+animal2_name+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                    succ_rate_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                    tasktypes_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                    coopthres_all_dates = pickle.load(f)\n",
    "                #\n",
    "            except:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal2_name+animal1_name+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                    succ_rate_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                    tasktypes_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/coopthres_all_dates_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                    coopthres_all_dates = pickle.load(f)\n",
    "            #\n",
    "            ind_MC = (coopthres_all_dates==1)&(tasktypes_all_dates==3)\n",
    "            pairtype = 'familiar'\n",
    "                #\n",
    "        except: # unfamiliar pairs\n",
    "            try:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal1_name+animal2_name+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                    succ_rate_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                    tasktypes_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                    coopthres_all_dates = pickle.load(f)\n",
    "                #\n",
    "            except:\n",
    "                data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/mixedpairs/'+animal2_name+animal1_name+'/'\n",
    "                #\n",
    "                with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                    succ_rate_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                    tasktypes_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/coopthres_all_dates_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                    coopthres_all_dates = pickle.load(f)\n",
    "            #\n",
    "            ind_MC = (tasktypes_all_dates==3)\n",
    "            pairtype = 'unfamiliar'\n",
    "        \n",
    "        # get the succ rate\n",
    "        nsessions = np.sum(ind_MC)\n",
    "        succrates_MC = succ_rate_all_dates[ind_MC]\n",
    "        \n",
    "        for isession in np.arange(0,nsessions,1):\n",
    "                        \n",
    "            succrate_inMC_all = succrate_inMC_all.append({'animal1name':animal1_name,\n",
    "                                                          'animal2name':animal2_name,\n",
    "                                                          'animal1sex':animal1_sex,\n",
    "                                                          'animal2sex':animal2_sex,\n",
    "                                                          'pairtype':pairtype,\n",
    "                                                          'succrate':succrates_MC[isession],                \n",
    "                                                          },ignore_index=True)\n",
    "            \n",
    "# calculate and make some plot       \n",
    "df = succrate_inMC_all.copy()\n",
    "\n",
    "# Function to sort animal names and swap sexes accordingly\n",
    "def sort_animals(row):\n",
    "    if row['animal1name'] > row['animal2name']:\n",
    "        # Swap names\n",
    "        row['animal1name'], row['animal2name'] = row['animal2name'], row['animal1name']\n",
    "        # Swap sexes\n",
    "        row['animal1sex'], row['animal2sex'] = row['animal2sex'], row['animal1sex']\n",
    "    return row\n",
    "\n",
    "# Apply to each row\n",
    "df_sorted = df.apply(sort_animals, axis=1)    \n",
    "df_sorted['pair_sex_type'] = df_sorted['animal1sex'] + '-' + df_sorted['animal2sex']\n",
    "\n",
    "\n",
    "# manually add the pairs that had really bad performance\n",
    "for ii in np.arange(0,3,1):\n",
    "    df_sorted=df_sorted.append({'animal1name':'kanga',\n",
    "                                'animal2name':'sparkle',\n",
    "                                'animal1sex':'female',\n",
    "                                'animal2sex':'female',\n",
    "                                'pairtype':'unfamiliar',\n",
    "                                'pair_sex_type':'female-female',\n",
    "                                'succrate':0    \n",
    "                               },ignore_index=True)\n",
    "    #\n",
    "    df_sorted=df_sorted.append({'animal1name':'scorch',\n",
    "                                'animal2name':'sparkle',\n",
    "                                'animal1sex':'female',\n",
    "                                'animal2sex':'female',\n",
    "                                'pairtype':'unfamiliar',\n",
    "                                'pair_sex_type':'female-female',\n",
    "                                'succrate':0    \n",
    "                               },ignore_index=True)\n",
    "\n",
    "\n",
    "# Group and compute the mean while ignoring NaNs\n",
    "df_grouped = df_sorted.groupby(['animal1name', 'animal2name'], as_index=False)['succrate'].mean()\n",
    "# Merge back in the first row of the original info for each pair\n",
    "extra_info = df_sorted.drop_duplicates(subset=['animal1name', 'animal2name'])[\n",
    "    ['animal1name', 'animal2name', 'animal1sex', 'animal2sex', 'pairtype','pair_sex_type']\n",
    "]\n",
    "# Merge with the averaged succrate\n",
    "df_final = pd.merge(df_grouped, extra_info, on=['animal1name', 'animal2name'])\n",
    "\n",
    "df_target = df_final.copy()\n",
    "# df_target = df_sorted.copy()\n",
    "\n",
    "seaborn.boxplot(ax=axs, data=df_target, y='succrate', x='pair_sex_type',hue='pairtype')\n",
    "\n",
    "# anova + posthoc\n",
    "# Fit a two-way ANOVA model with interaction\n",
    "model = smf.ols('succrate ~ C(pair_sex_type) * C(pairtype)', data=df_target).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "#\n",
    "print(anova_table)\n",
    "\n",
    "# post hoc\n",
    "df_target['interaction_group'] = df_target['pair_sex_type'] + ' | ' + df_target['pairtype']\n",
    "#\n",
    "tukey = pairwise_tukeyhsd(endog=df_target['succrate'],\n",
    "                          groups=df_target['interaction_group'],\n",
    "                          alpha=0.05)\n",
    "\n",
    "print(tukey.summary())\n",
    "\n",
    "# run a power analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3c888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a2267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910a5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9335d02",
   "metadata": {},
   "source": [
    "## Analysis including all the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    # each row means the \"to Node\" id\n",
    "    moreSampSize = 0\n",
    "\n",
    "    # to Node animal ID\n",
    "    animal1_toNodes = ['eddie','dodson','dannon','ginger','sparkle','scorch','kanga']\n",
    "    nanimal1_toNodes = np.shape(animal1_toNodes)[0]\n",
    "\n",
    "    # the other animals' ID for each of the toNode animal\n",
    "    animal2_familiars = [['sparkle'],\n",
    "                         ['scorch'],\n",
    "                         ['kanga'],\n",
    "                         ['kanga'],\n",
    "                         ['eddie'],\n",
    "                         ['dodson'],\n",
    "                         ['dannon','ginger']]\n",
    "    animal2_unfamiliars = [['scorch','ginger','kanga','dodson'],\n",
    "                           ['kanga','ginger','sparkle','dannon','eddie'],\n",
    "                           ['dodson'],\n",
    "                           ['scorch','sparkle','eddie','dodson'],\n",
    "                           ['dodson','ginger'],\n",
    "                           ['eddie','ginger','kanga'],\n",
    "                           ['dodson','eddie','scorch']]\n",
    "\n",
    "\n",
    "    timelag = 1 # 1 or 2 or 3 or 0(merged - merge all three lags) or 12 (merged lag 1 and 2)\n",
    "    timelagname = '1second' # '1/2/3second' or 'merged' or '12merged'\n",
    "    # timelagname = 'merged' # together with timelag = 0\n",
    "    # timelagname = '12merged' # together with timelag = 12\n",
    "\n",
    "    # define some parameters\n",
    "    #\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    # define how to load from the matrix\n",
    "    if timelag == 1:\n",
    "        pull_pull_fromNodes_all = [9,8]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [11,10]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [8,9]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [9,8]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [10,11]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [11,10]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "    elif timelag == 2:\n",
    "        pull_pull_fromNodes_all = [5,4]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [7,6]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [4,5]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [5,4]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [6,7]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [7,6]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "    elif timelag == 3:\n",
    "        pull_pull_fromNodes_all = [1,0]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [3,2]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [0,1]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [1,0]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [2,3]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [3,2]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "    elif timelag == 0:\n",
    "        pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "    elif timelag == 12:\n",
    "        pull_pull_fromNodes_all = [[5,9],[4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[7,11],[6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[4,8],[5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[5,9],[4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[6,10],[7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[7,11],[6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "\n",
    "    #\n",
    "    edgetype_names = ['across animal pull<->pull','across animal gaze<->gaze','within animal gaze->pull',\n",
    "                      'across animal gaze->pull', 'within animal pull->gaze', 'across animal pull->gaze',]\n",
    "    nedgetypes = np.shape(edgetype_names)[0]\n",
    "\n",
    "    #\n",
    "    nanimalpairs = np.shape(np.concatenate(animal2_unfamiliars))[0] + np.shape(np.concatenate(animal2_familiars))[0]\n",
    "    ianimal_pair = 0\n",
    "    #\n",
    "    animal_pairs_names = np.empty((1,nanimalpairs),dtype='object')[0]\n",
    "    edgeweight_animalpair_sum = np.zeros((nanimalpairs,nedgetypes))\n",
    "\n",
    "\n",
    "    # loop all the animals\n",
    "    for ianimal1 in np.arange(0,nanimal1_toNodes,1):\n",
    "        animal1_name = animal1_toNodes[ianimal1]\n",
    "        animal1_name_init = str.upper(animal1_name[0:2])\n",
    "        animal2_names = np.concatenate((animal2_familiars[ianimal1],animal2_unfamiliars[ianimal1]))\n",
    "        #\n",
    "        nanimal2 = np.shape(animal2_names)[0]\n",
    "        animal2_names_init = []\n",
    "\n",
    "        # loop all the partner\n",
    "        for ianimal2 in np.arange(0,nanimal2,1):\n",
    "            animal2_name = animal2_names[ianimal2]\n",
    "            animal2_names_init.append(str.upper(animal2_name[0:2]))\n",
    "\n",
    "            # load the raw weight data - the order of the animal1 and animal2 decide how to load matrix \n",
    "            try: # normal order - animal1_name _ animal2_name\n",
    "                try: # familiar pairs\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_name+animal2_name+'/'\n",
    "                    if moreSampSize:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    else:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                except: # unfamiliar pairs\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal1_name+animal2_name+'/'\n",
    "                    if moreSampSize:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    else:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_name+animal2_name+'.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                #\n",
    "                martrix_order = 0\n",
    "            #\n",
    "            except: # normal order - animal2_name _ animal1_name\n",
    "                try: # familiar pairs\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/'+animal2_name+animal1_name+'/'\n",
    "                    if moreSampSize:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    else:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                except: # unfamiliar pairs\n",
    "                    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'_3lags/'+cameraID+'/mixedpairs/'+animal2_name+animal1_name+'/'\n",
    "                    if moreSampSize:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'_moreSampSize.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    else:\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal2_name+animal1_name+'.pkl', 'rb') as f:\n",
    "                            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                #\n",
    "                martrix_order = 1\n",
    "\n",
    "            #\n",
    "            temp_resolu = temp_resolus[0]\n",
    "            j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "            # load edge weight data    \n",
    "            #\n",
    "            try:\n",
    "                weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "                weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "                sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "            except:\n",
    "                weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "                weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "                sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)]['coop']\n",
    "\n",
    "            # organize the key edge data\n",
    "            weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "\n",
    "            # calculate the mean edge weight\n",
    "            # pull-pull\n",
    "            a = (weighted_graphs_coop[:,pull_pull_fromNodes_all[martrix_order],pull_pull_toNodes_all[martrix_order]]).flatten()\n",
    "            xxx1 = np.mean(a)\n",
    "            # gaze-gaze\n",
    "            a = (weighted_graphs_coop[:,gaze_gaze_fromNodes_all[martrix_order],gaze_gaze_toNodes_all[martrix_order]]).flatten()\n",
    "            xxx2 = np.mean(a)\n",
    "            # within animal gazepull\n",
    "            a = (weighted_graphs_coop[:,within_gazepull_fromNodes_all[martrix_order],within_gazepull_toNodes_all[martrix_order]]).flatten()\n",
    "            xxx3 = np.mean(a)\n",
    "            # across animal gazepull\n",
    "            a = (weighted_graphs_coop[:,across_gazepull_fromNodes_all[martrix_order],across_gazepull_toNodes_all[martrix_order]]).flatten()\n",
    "            xxx4 = np.mean(a)\n",
    "            # within animal pullgaze\n",
    "            a = (weighted_graphs_coop[:,within_pullgaze_fromNodes_all[martrix_order],within_pullgaze_toNodes_all[martrix_order]]).flatten()\n",
    "            xxx5 = np.mean(a)\n",
    "            # across animal pullgaze\n",
    "            a = (weighted_graphs_coop[:,across_pullgaze_fromNodes_all[martrix_order],across_pullgaze_toNodes_all[martrix_order]]).flatten()\n",
    "            xxx6 = np.mean(a)\n",
    "\n",
    "            edgeweight_ianimal = np.array([xxx1,xxx2,xxx3,xxx4,xxx5,xxx6])\n",
    "\n",
    "            #\n",
    "            edgeweight_animalpair_sum[ianimal_pair,:] = edgeweight_ianimal\n",
    "            animal_pairs_names[ianimal_pair] = animal2_name+'_to_'+animal1_name\n",
    "\n",
    "            ianimal_pair = ianimal_pair + 1\n",
    "\n",
    "    # run PCA         \n",
    "    pca = PCA(n_components=3)\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    edgeweight_animalpair_sum_PCA = pca.fit_transform(edgeweight_animalpair_sum)        \n",
    "\n",
    "    # initialize figure   \n",
    "    fig = plt.figure(figsize = (5*3, 5*2))\n",
    "    ax1 = fig.add_subplot(2,3,1)\n",
    "    ax2 = fig.add_subplot(2,3,2,projection='3d')\n",
    "    ax4 = fig.add_subplot(2,3,4)\n",
    "    ax5 = fig.add_subplot(2,3,5)\n",
    "    ax6 = fig.add_subplot(2,3,6)\n",
    "\n",
    "\n",
    "    # plot the PCA results\n",
    "    ax1.scatter(edgeweight_animalpair_sum_PCA[:,0],edgeweight_animalpair_sum_PCA[:,1])    \n",
    "    ax2.scatter(edgeweight_animalpair_sum_PCA[:,0],edgeweight_animalpair_sum_PCA[:,1],edgeweight_animalpair_sum_PCA[:,2])  \n",
    "\n",
    "\n",
    "    # plot the PC1 and PC2 with different separation\n",
    "    if 1:\n",
    "        grouptypes = ['male action with unfamiliar male',\n",
    "                      'male action with familiar female',\n",
    "                      'male action with unfamiliar female',\n",
    "                      'female action with familiar male',\n",
    "                      'female action with unfamiliar male',\n",
    "                      'female action with familiar female',\n",
    "                      'female action with unfamiliar female',\n",
    "                     ]    \n",
    "        ngroupttypes = np.shape(grouptypes)[0]\n",
    "        #\n",
    "        animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie'],\n",
    "                            ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon'],\n",
    "                            ['kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie'],\n",
    "                            ['eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga'],\n",
    "                            ['dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger'],\n",
    "                            ['kanga_to_ginger','ginger_to_kanga'],\n",
    "                            ['ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga'],\n",
    "                          ]\n",
    "        grouptypename = 'MaleFemaleFamiliarUnfamiliar'\n",
    "    #\n",
    "    if 0:\n",
    "        grouptypes = ['individual in unfamiliar pairs',\n",
    "                      'individual in familiar pairs',\n",
    "                     ]    \n",
    "        ngroupttypes = np.shape(grouptypes)[0]\n",
    "        #\n",
    "        animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie',\n",
    "                             'kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie',\n",
    "                             'dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger',\n",
    "                             'ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga',\n",
    "                             ],\n",
    "                            ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon',\n",
    "                             'eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga',\n",
    "                             'kanga_to_ginger','ginger_to_kanga'\n",
    "                            ],\n",
    "                          ]\n",
    "        grouptypename = 'FamiliarUnfamiliar'\n",
    "    #\n",
    "    if 0:\n",
    "        grouptypes = ['male action with unfamiliar pair',\n",
    "                      'male action with familiar pair',                  \n",
    "                      'female action with unfamiliar pair',\n",
    "                      'female action with familiar pair',\n",
    "                     ]    \n",
    "        ngroupttypes = np.shape(grouptypes)[0]\n",
    "        #\n",
    "        animalpairtypes = [ ['dannon_to_dodson','dodson_to_dannon','eddie_to_dodson','dodson_to_eddie',\n",
    "                             'kanga_to_dodson','kanga_to_eddie','scorch_to_eddie','ginger_to_dodson','sparkle_to_dodson','ginger_to_eddie'],\n",
    "                            ['sparkle_to_eddie','scorch_to_dodson','kanga_to_dannon'],\n",
    "                            ['dodson_to_kanga','eddie_to_kanga','eddie_to_scorch','dodson_to_ginger','dodson_to_sparkle','eddie_to_ginger',\n",
    "                             'ginger_to_scorch','scorch_to_ginger','ginger_to_sparkle','sparkle_to_ginger','kanga_to_scorch','scorch_to_kanga'],\n",
    "                            ['eddie_to_sparkle','dodson_to_scorch','dannon_to_kanga','kanga_to_ginger','ginger_to_kanga'],\n",
    "                          ]\n",
    "        grouptypename = 'MaleFemale'\n",
    "\n",
    "\n",
    "    # reorganize the data\n",
    "    mean_PC1_grouptypes = dict.fromkeys(grouptypes,())\n",
    "    mean_PC2_grouptypes = dict.fromkeys(grouptypes,())\n",
    "    mean_PC3_grouptypes = dict.fromkeys(grouptypes,())\n",
    "    #\n",
    "    for igrouptype in np.arange(0,ngroupttypes,1):\n",
    "        animalpairtypes_igroup = animalpairtypes[igrouptype]\n",
    "        #\n",
    "        mean_PC1_grouptypes[grouptypes[igrouptype]] = edgeweight_animalpair_sum_PCA[np.isin(animal_pairs_names,animalpairtypes_igroup),0]\n",
    "        mean_PC2_grouptypes[grouptypes[igrouptype]] = edgeweight_animalpair_sum_PCA[np.isin(animal_pairs_names,animalpairtypes_igroup),1]\n",
    "        mean_PC3_grouptypes[grouptypes[igrouptype]] = edgeweight_animalpair_sum_PCA[np.isin(animal_pairs_names,animalpairtypes_igroup),2]\n",
    "    #\n",
    "    mean_PC1_grouptypes_df = pd.DataFrame.from_dict(mean_PC1_grouptypes,orient='index')\n",
    "    mean_PC1_grouptypes_df = mean_PC1_grouptypes_df.transpose()\n",
    "    #\n",
    "    seaborn.barplot(ax=ax4,data=mean_PC1_grouptypes_df,errorbar='se',alpha=.5,capsize=0.1)\n",
    "    seaborn.swarmplot(ax=ax4,data=mean_PC1_grouptypes_df,alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    ax4.set_xlabel('')\n",
    "    ax4.set_ylabel('PC1',fontsize=10)\n",
    "    ax4.set_title(edgetype_name,fontsize=14)\n",
    "    # ax4.set_ylim([-0.05,1.05])\n",
    "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=90)  \n",
    "\n",
    "    #\n",
    "    mean_PC2_grouptypes_df = pd.DataFrame.from_dict(mean_PC2_grouptypes,orient='index')\n",
    "    mean_PC2_grouptypes_df = mean_PC2_grouptypes_df.transpose()\n",
    "    #\n",
    "    seaborn.barplot(ax=ax5,data=mean_PC2_grouptypes_df,errorbar='se',alpha=.5,capsize=0.1)\n",
    "    seaborn.swarmplot(ax=ax5,data=mean_PC2_grouptypes_df,alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    ax5.set_xlabel('')\n",
    "    ax5.set_ylabel('PC2',fontsize=10)\n",
    "    ax5.set_title(edgetype_name,fontsize=14)\n",
    "    # ax5.set_ylim([-0.05,1.05])\n",
    "    ax5.set_xticklabels(ax5.get_xticklabels(), rotation=90) \n",
    "\n",
    "    #\n",
    "    mean_PC3_grouptypes_df = pd.DataFrame.from_dict(mean_PC3_grouptypes,orient='index')\n",
    "    mean_PC3_grouptypes_df = mean_PC3_grouptypes_df.transpose()\n",
    "    #\n",
    "    seaborn.barplot(ax=ax6,data=mean_PC3_grouptypes_df,errorbar='se',alpha=.5,capsize=0.1)\n",
    "    seaborn.swarmplot(ax=ax6,data=mean_PC3_grouptypes_df,alpha=.9,size= 9,dodge=True,legend=False)\n",
    "    ax6.set_xlabel('')\n",
    "    ax6.set_ylabel('PC3',fontsize=10)\n",
    "    ax6.set_title(edgetype_name,fontsize=14)\n",
    "    # ax6.set_ylim([-0.05,1.05])\n",
    "    ax6.set_xticklabels(ax6.get_xticklabels(), rotation=90);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        if moreSampSize:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            plt.savefig(figsavefolder+'threeTimeLag_edgeweightPCA_'+grouptypename+'_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI_basedonToNodes.pdf')\n",
    "        else:\n",
    "            figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_bhv_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/mixedpairs/'\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "            plt.savefig(figsavefolder+'threeTimeLag_edgeweightPCA_'+grouptypename+'_'+timelagname+'Lag_IndiAnimal_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI_basedonToNodes.pdf',bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c82e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32ab25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67c329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862775aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37f09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4803d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acdce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b12d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
