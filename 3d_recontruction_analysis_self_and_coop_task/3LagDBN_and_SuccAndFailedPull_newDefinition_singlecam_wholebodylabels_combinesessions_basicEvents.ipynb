{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the combined sessions, combined for each condition\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, DBN is run with sucessful and failed pulls seperately\n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 5*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# session list options\n",
    "do_bestsession = 1 # only analyze the best (five) sessions for each conditions during the training phase\n",
    "do_trainedMCs = 1 # the list that only consider trained (1s) MC, together with SR and NV as controls\n",
    "if do_bestsession:\n",
    "    if not do_trainedMCs:\n",
    "        savefile_sufix = '_bestsessions'\n",
    "    elif do_trainedMCs:\n",
    "        savefile_sufix = '_trainedMCsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson scorch\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "            \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "            \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          # \"20220912\",\n",
    "                          \"20220915\",\"20220920\",\"20221010\",\"20230208\",\n",
    "                          \"20221011\",\"20221013\",\"20221015\",\"20221017\",\n",
    "                          \"20221022\",\"20221026\",\"20221028\",\"20221030\",\"20230209\",\n",
    "                          \"20221125\",\"20221128\",\"20221129\",\"20230214\",\"20230215\",                  \n",
    "                          \"20221205\",\"20221206\",\"20221209\",\"20221214\",\"20230112\",\n",
    "                          \"20230117\",\"20230118\",\"20230124\",\n",
    "                          # \"20230126\",\n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    # 18.10, \n",
    "                                     0.00, 33.03,  6.50,  0.00, \n",
    "                                     2.80, 27.80, 27.90, 27.00,  \n",
    "                                    51.90, 21.00, 30.80, 17.50,  0.00,                    \n",
    "                                    26.40, 22.50, 28.50,  0.00, 33.00,                     \n",
    "                                     0.00,  0.00, 21.70, 17.00, 14.20, \n",
    "                                     0.00,  0.00,  0.00, \n",
    "                                     # 0.00,  \n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20220915\",\"20220920\",\"20221010\",\"20230208\", # SR\n",
    "                          \n",
    "                          \"20230321\",\"20230322\",\"20230323\",\"20230324\",\"20230412\",\"20230413\", # trained MC\n",
    "                          \n",
    "                          \"20230117\",\"20230118\",\"20230124\", # NV \n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                     0.00, 33.03,  6.50,  0.00, \n",
    "                                     \n",
    "                                     20.5,  21.4,  21.0,  24.5,  20.5,  26.6,\n",
    "                    \n",
    "                                     0.00,  0.00,  0.00,  \n",
    "                                  ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['scorch']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Scorch\"\n",
    "    \n",
    "    \n",
    "# eddie sparkle\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                 \n",
    "                              ] # in second\n",
    "    elif do_bestsession:   \n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20221122\",  \"20221125\",  \n",
    "                          \"20221202\",  \"20221206\",  \"20230126\",  \"20230130\",  \"20230201\",\n",
    "                          \"20230207\",  \"20230208-1\",\"20230209\",  \"20230222\",  \"20230223-1\",\n",
    "                          \"20230227-1\",\"20230228-1\",\"20230302-1\",\"20230307-2\",\"20230313\",\n",
    "                          \"20230321\",  \"20230322\",  \"20230324\",  \"20230327\",  \"20230328\",\n",
    "                          \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\"\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                      8.00,  38.00, \n",
    "                                      9.50,   1.00, 38.00,  4.20,  3.80,\n",
    "                                      9.00,   7.50,  8.50, 14.50,  7.80,\n",
    "                                      8.00,   7.50,  8.00,  8.00,  4.00,\n",
    "                                      7.00,   7.50,  5.50, 11.00,  9.00,\n",
    "                                      4.50,   9.30, 25.50, 20.40, 21.30,\n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20221122\",  \"20221125\",  # sr\n",
    "                \n",
    "                          \"20230410\",  \"20230411\",  \"20230412\",  \"20230413\",  \"20230616\", # trained MC\n",
    "                \n",
    "                          \"20230331\",  \"20230403\",  \"20230404\",  \"20230405\",  \"20230406\", # nv\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                      8.00, 38.00, \n",
    "                \n",
    "                                      23.2,  23.0,  21.2,  25.0,  23.0,   \n",
    "                \n",
    "                                      4.50,  9.30, 25.50, 20.40, 21.30,\n",
    "                \n",
    "                                  ] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "    \n",
    "    \n",
    "# ginger kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                      \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second \n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          #\"20230213\",\n",
    "                          \"20230214\",\"20230216\",\n",
    "                          \"20230228\",\"20230302\",\"20230303\",\"20230307\",          \n",
    "                          \"20230314\",\"20230315\",\"20230316\",\"20230317\",\n",
    "                          \"20230301\",\"20230320\",\"20230321\",\"20230322\",\n",
    "                          \"20230323\",\"20230412\",\"20230413\",\"20230517\",\n",
    "                          \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\"\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                    # 0.00, \n",
    "                                     0.00, 48.00, \n",
    "                                    23.00, 28.50, 34.00, 25.50, \n",
    "                                    25.50, 31.50, 28.00, 30.50,\n",
    "                                     0.00,  0.00,  0.00,  0.00, \n",
    "                                     0.00,  0.00,  0.00,  0.00, \n",
    "                                     0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                  ] # in second \n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20230214\",   \"20230216\",  # SR\n",
    "                          \n",
    "                          \"20230614\",   \"20230615\",  \"20230711\",\"20230712\", # trained MC\n",
    "                \n",
    "                          \"20230522_ws\",\"20230524\",\"20230605_1\",\"20230606\",\"20230607\", # nv  \n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                     0.00, 48.00, \n",
    "                                    \n",
    "                                     0.00,  0.00,  54.5,  24.7,\n",
    "                \n",
    "                                     0.00,  0.00,  0.00,  0.00,  0.00,\n",
    "                                  ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['ginger']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Ginger\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                    \n",
    "                   ]\n",
    "        session_start_times = [ \n",
    "                              \n",
    "                              ] # in second \n",
    "    elif do_bestsession: \n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20230718\",\"20230720\",\"20230914\",\"20230726\",\"20230727\",\"20230809\",\n",
    "                          \"20230810\",\"20230811\",\"20230814\",\"20230816\",\"20230829\",\"20230907\",\"20230915\",\n",
    "                          \"20230918\",\"20230926\",\"20230928\",\"20231002\",\"20231010\",\"20231011\",\n",
    "                          \"20231013\",\"20231020\",\"20231024\",\"20231025\",\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                        0,    0,    0, 32.2, 27.2, 37.5,\n",
    "                                     21.0, 21.5, 19.8, 32.0,    0,    0,   0, \n",
    "                                        0,    0,    0,    0,    0,    0,\n",
    "                                        0,    0,    0,    0, \n",
    "                                  ] # in second \n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20230718\",\"20230720\",\"20230914\", # sr\n",
    "                \n",
    "                          \"20231030\",\"20231031\",\"20231101\",\"20231102\",\"20240304\",\"20240305\", # trained MC\n",
    "                \n",
    "                          \"20231011\",\"20231013\",\"20231020\",\"20231024\",\"20231025\", # nv\n",
    "                       ]\n",
    "            session_start_times = [ \n",
    "                                       0,    0,    0,\n",
    "                \n",
    "                                    18.2, 14.0, 15.8, 15.2, 16.3, 37.9,\n",
    "                \n",
    "                                       0,    0,    0,    0,    0, \n",
    "                                  ] # in second \n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "\n",
    "# Koala Vermelho\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        dates_list = [\n",
    "                     \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                               \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        if not do_trainedMCs:\n",
    "            # pick only five sessions for each conditions during the training phase\n",
    "            dates_list = [\n",
    "                          \"20231222\",\"20231226\",\"20231227\",  \"20231229\",\"20231230\",\n",
    "                          \"20231231\",\"20240102\",\"20240104-2\",\"20240105\",\"20240108\",\n",
    "                          \"20240109\",\"20240115\",\"20240116\",  \"20240117\",\"20240118\",\"20240119\",\n",
    "                          \"20240207\",\"20240208\",\"20240209\",  \"20240212\",\"20240213\",\n",
    "                          \"20240214\",\"20240215\",\"20240216\",  \n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    21.5,  0.00,  0.00,  0.00,  0.00, \n",
    "                                    0.00,  12.2,  0.00,  18.8,  31.2,  \n",
    "                                    32.5,  0.00,  50.0,  0.00,  37.5,  29.5,\n",
    "                                    58.5,  72.0,  0.00,  71.5,  70.5,\n",
    "                                    86.8,  94.0,  65.0, \n",
    "                                  ] # in second\n",
    "        elif do_trainedMCs:\n",
    "            dates_list = [\n",
    "                          \"20231222\",\"20231226\",\"20231227\", # SR\n",
    "                          \n",
    "                          \"20240220\",\"20240222\",\"20240223\",\"20240226\", # trained MC\n",
    "                 \n",
    "                          \"20240214\",\"20240215\",\"20240216\",  # NV\n",
    "                         ]\n",
    "            session_start_times = [ \n",
    "                                    21.5,  0.00,  0.00, \n",
    "                                    \n",
    "                                    68.8,  43.8,  13.2,  47.5,\n",
    "                \n",
    "                                    86.8,  94.0,  65.0, \n",
    "                                  ] # in second\n",
    "\n",
    "    animal1_fixedorder = ['koala']\n",
    "    animal2_fixedorder = ['vermelho']\n",
    "\n",
    "    animal1_filename = \"Koala\"\n",
    "    animal2_filename = \"Vermelho\"\n",
    "    \n",
    "#    \n",
    "#dates_list = [\"20221128\"]\n",
    "#session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "93c7a76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data from all dates are loaded\n"
     ]
    }
   ],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        try:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            try: \n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "                video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "         # load behavioral results\n",
    "        try:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "        pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "        owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "        mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "        mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "\n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 1:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                              lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                              animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                              animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                              video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebdc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aada694",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1d0e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900d890",
   "metadata": {},
   "source": [
    "### plot behavioral events interval to get a sense about time bin\n",
    "#### only focus on pull_to_other_bhv_interval and other_bhv_to_pull_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a7b179dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.75222222 3.639      3.00148148 1.17300275 0.44110787 0.49111111\n",
      " 0.48381295 1.29574468 0.78442029 1.0699115 ]\n",
      "1.1778628877824586\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]\n",
    "\n",
    "pull_other_intv_forplots = {}\n",
    "pull_other_intv_mean = np.zeros((1,ndates_sorted))[0]\n",
    "pull_other_intv_ii = []\n",
    "for ii in np.arange(0,ndates_sorted,1):\n",
    "    pull_other_intv_ii = pd.Series(bhv_intv_all_dates[dates_list_sorted[ii]]['pull_other_pooled'])\n",
    "    # remove the interval that is too large\n",
    "    pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "    # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "    pull_other_intv_forplots[ii] = pull_other_intv_ii\n",
    "    pull_other_intv_mean[ii] = np.nanmean(pull_other_intv_ii)\n",
    "    \n",
    "    \n",
    "#\n",
    "pull_other_intv_forplots = pd.DataFrame(pull_other_intv_forplots)\n",
    "\n",
    "#\n",
    "# plot\n",
    "pull_other_intv_forplots.plot(kind = 'box',ax=ax1, positions=np.arange(0,ndates_sorted,1))\n",
    "# plt.boxplot(pull_other_intv_forplots)\n",
    "plt.plot(np.arange(0,ndates_sorted,1),pull_other_intv_mean,'r*',markersize=10)\n",
    "#\n",
    "ax1.set_ylabel(\"bhv event interval(around pulls)\",fontsize=13)\n",
    "ax1.set_ylim([-2,16])\n",
    "#\n",
    "plt.xticks(np.arange(0,ndates_sorted,1),dates_list_sorted, rotation=90,fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "#\n",
    "tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.plot([taskswitch,taskswitch],[-2,15],'k--')\n",
    "taskswitches = np.concatenate(([0],taskswitches))\n",
    "for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "    taskswitch = taskswitches[itaskswitch]\n",
    "    ax1.text(taskswitch+0.25,-1,tasktypes[itaskswitch],fontsize=10)\n",
    "ax1.text(taskswitch-5,15,'mean Inteval = '+str(np.nanmean(pull_other_intv_forplots)),fontsize=10)\n",
    "\n",
    "print(pull_other_intv_mean)\n",
    "print(np.nanmean(pull_other_intv_forplots))\n",
    "\n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"bhvInterval_hist_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "DBN_group_coopthres = [0,3,2,1.5,1,0]\n",
    "if do_trainedMCs:\n",
    "    DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "    DBN_group_typeIDs  =  [1,3,5]\n",
    "    DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "pulltypes = ['succpull','failedpull']\n",
    "# pulltypes = ['succpull']\n",
    "npulltypes = np.shape(pulltypes)[0]\n",
    "\n",
    "prepare_input_data = 0\n",
    "\n",
    "DBN_input_data_alltypes = dict.fromkeys(pulltypes, [])\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    for ipulltype in np.arange(0,npulltypes,1):\n",
    "        pulltype = pulltypes[ipulltype]\n",
    "\n",
    "        DBN_input_data_alltypes[pulltype] = dict.fromkeys(DBN_group_typenames, [])\n",
    "            \n",
    "        for idate in np.arange(0,ndates,1):\n",
    "            date_tgt = dates_list[idate]\n",
    "            session_start_time = session_start_times[idate]\n",
    "                 \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "            except:\n",
    "                try:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                except:\n",
    "                    bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_forceManipulation_task/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                    trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                    bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                    session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                    #\n",
    "                    trial_record = pd.read_json(trial_record_json[0])\n",
    "                    bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                    session_info = pd.read_json(session_info_json[0])\n",
    "                \n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1   \n",
    "\n",
    "            # load behavioral event results\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            #\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # redefine the totalsess_time for the length of each recording (NOT! remove the session_start_time)\n",
    "            totalsess_time = int(np.ceil(np.shape(look_at_other_or_not_merge['dodson'])[0]/fps))\n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            # time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            # time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']   \n",
    "\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            if pulltype == 'succpull':\n",
    "                time_point_pull1 = np.round(successful_pull1,1)\n",
    "                time_point_pull2 = np.round(successful_pull2,1)\n",
    "            elif pulltype == 'failedpull':\n",
    "                time_point_pull1 = np.round(failed_pull1,1)\n",
    "                time_point_pull2 = np.round(failed_pull2,1)\n",
    "\n",
    "            if mergetempRos:\n",
    "                temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "                # use bhv event to decide temporal resolution\n",
    "                #\n",
    "                #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "\n",
    "            ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "            # try different temporal resolutions\n",
    "            for temp_resolu in temp_resolus:\n",
    "                bhv_df = []\n",
    "\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                else:\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "                if len(bhv_df)==0:\n",
    "                    bhv_df = bhv_df_itr\n",
    "                else:\n",
    "                    bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                    bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "                # merge sessions from the same condition\n",
    "                for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                    iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                    iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                    iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                    # merge sessions \n",
    "                    if (tasktype!=3):\n",
    "                        if (tasktype==iDBN_group_typeID):\n",
    "                            if (len(DBN_input_data_alltypes[pulltype][iDBN_group_typename])==0):\n",
    "                                DBN_input_data_alltypes[pulltype][iDBN_group_typename] = bhv_df\n",
    "                            else:\n",
    "                                DBN_input_data_alltypes[pulltype][iDBN_group_typename] = pd.concat([DBN_input_data_alltypes[pulltype][iDBN_group_typename],bhv_df])\n",
    "                    else:\n",
    "                        if (coop_thres==iDBN_group_cothres):\n",
    "                            if (len(DBN_input_data_alltypes[pulltype][iDBN_group_typename])==0):\n",
    "                                DBN_input_data_alltypes[pulltype][iDBN_group_typename] = bhv_df\n",
    "                            else:\n",
    "                                DBN_input_data_alltypes[pulltype][iDBN_group_typename] = pd.concat([DBN_input_data_alltypes[pulltype][iDBN_group_typename],bhv_df])\n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0bdc4fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'succpull': [], 'failedpull': []}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBN_input_data_alltypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c72ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1cbc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a743731",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f4d56009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,3,  3,3,5]\n",
    "DBN_group_coopthres = [0,3,2,1.5,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "pulltypes = ['succpull','failedpull']\n",
    "npulltypes = np.shape(pulltypes)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7d323",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68d13d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # analyze successful pull and failed pull separately\n",
    "    for pulltype in pulltypes:\n",
    "        \n",
    "        # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "        for temp_resolu in temp_resolus:\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            \n",
    "            DBN_input_data_alltypes = DBN_input_data_alltypes[pulltype]\n",
    "\n",
    "            # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "\n",
    "            if not moreSampSize:\n",
    "                key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "                key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "                key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "                min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "                min_samplesize = int(min_samplesize/100)*100\n",
    "                max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "                max_samplesize = int(max_samplesize/100)*100\n",
    "                samplingsizes = [min_samplesize,max_samplesize]\n",
    "                samplingsizes_name = ['min_row_number','max_row_number']   \n",
    "                nsamplings = np.shape(samplingsizes)[0]\n",
    "                print(samplingsizes)\n",
    "\n",
    "            # try different down/re-sampling size\n",
    "            # for jj in np.arange(0,nsamplings,1):\n",
    "            for jj in np.arange(0,1,1):\n",
    "\n",
    "                isamplingsize = samplingsizes[jj]\n",
    "\n",
    "                DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "                weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "                # different session conditions (aka DBN groups)\n",
    "                # for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                for iDBN_group in np.arange(0,1,1):\n",
    "                    iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                    iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                    iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                    try:\n",
    "                        bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "                        bhv_df_all = bhv_df_all.reset_index(drop=True)\n",
    "                        # bhv_df = bhv_df_all.sample(30*100,replace = True, random_state = round(time())) # take the subset for DBN training\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        # bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "\n",
    "\n",
    "                        # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                        colnames = list(bhv_df_all.columns)\n",
    "                        eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                        nevents = np.size(eventnames)\n",
    "\n",
    "                        all_pops = list(bhv_df_all.columns)\n",
    "                        from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                        to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                        causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                        nFromNodes = np.shape(from_pops)[0]\n",
    "                        nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                        DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        score_randstart = np.zeros((num_starting_points))\n",
    "                        score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                        # step 1: randomize the starting point for num_starting_points times\n",
    "                        for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                            # try different down/re-sampling size\n",
    "                            bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                            aic = AicScore(bhv_df)\n",
    "\n",
    "                            #Anirban(Alec) shuffle, slow\n",
    "                            bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                            aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                            np.random.seed(istarting_points)\n",
    "                            random.seed(istarting_points)\n",
    "                            starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                            starting_graph = DAG()\n",
    "                            starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                            starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                            # step 2: add the shffled data results\n",
    "                            # shuffled bhv_df\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                        DAGs_alltypes[iDBN_group_typename] = DAGs_randstart \n",
    "                        DAGs_shuffle_alltypes[iDBN_group_typename] = DAGs_randstart_shuffle\n",
    "\n",
    "                        DAGs_scores_alltypes[iDBN_group_typename] = score_randstart\n",
    "                        DAGs_shuffle_scores_alltypes[iDBN_group_typename] = score_randstart_shuffle\n",
    "\n",
    "                        weighted_graphs = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                        weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                        sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                        weighted_graphs_alltypes[iDBN_group_typename] = weighted_graphs\n",
    "                        weighted_graphs_shuffled_alltypes[iDBN_group_typename] = weighted_graphs_shuffled\n",
    "                        sig_edges_alltypes[iDBN_group_typename] = sig_edges\n",
    "\n",
    "                    except:\n",
    "                        DAGs_alltypes[iDBN_group_typename] = [] \n",
    "                        DAGs_shuffle_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                        DAGs_scores_alltypes[iDBN_group_typename] = []\n",
    "                        DAGs_shuffle_scores_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                        weighted_graphs_alltypes[iDBN_group_typename] = []\n",
    "                        weighted_graphs_shuffled_alltypes[iDBN_group_typename] = []\n",
    "                        sig_edges_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                DAGscores_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "                DAGscores_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "                weighted_graphs_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "                sig_edges_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "    print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647783a",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5cafbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "     # analyze successful pull and failed pull separately\n",
    "    for pulltype in pulltypes:\n",
    "        \n",
    "        # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "        for temp_resolu in temp_resolus:\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            if not mergetempRos:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "            #        \n",
    "            DBN_input_data_alltypes = DBN_input_data_alltypes[pulltype]\n",
    "\n",
    "            # only try two sample sizes - minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "\n",
    "            if not moreSampSize:\n",
    "                key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "                key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "                key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "                min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "                min_samplesize = int(min_samplesize/100)*100\n",
    "                max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "                max_samplesize = int(max_samplesize/100)*100\n",
    "                # samplingsizes = [min_samplesize,max_samplesize]\n",
    "                samplingsizes = [min_samplesize]\n",
    "                # samplingsizes_name = ['min_row_number','max_row_number'] \n",
    "                samplingsizes_name = ['min_row_number'] \n",
    "                nsamplings = np.shape(samplingsizes)[0]\n",
    "                print(samplingsizes)\n",
    "\n",
    "            # try different down/re-sampling size\n",
    "            for jj in np.arange(0,nsamplings,1):\n",
    "\n",
    "                isamplingsize = samplingsizes[jj]\n",
    "\n",
    "                DAGs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                DAGs_shuffle_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                DAGs_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                DAGs_shuffle_scores_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "                weighted_graphs_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                weighted_graphs_shuffled_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "                sig_edges_alltypes = dict.fromkeys(DBN_group_typenames, [])\n",
    "\n",
    "                # different session conditions (aka DBN groups)\n",
    "                for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "                    iDBN_group_typename = DBN_group_typenames[iDBN_group] \n",
    "                    iDBN_group_typeID =  DBN_group_typeIDs[iDBN_group] \n",
    "                    iDBN_group_cothres = DBN_group_coopthres[iDBN_group] \n",
    "\n",
    "                    try:\n",
    "                        bhv_df_all = DBN_input_data_alltypes[iDBN_group_typename]\n",
    "                        bhv_df_all = bhv_df_all.reset_index(drop=True)\n",
    "                        # bhv_df = bhv_df_all.sample(30*100,replace = True, random_state = round(time())) # take the subset for DBN training\n",
    "\n",
    "                        #Anirban(Alec) shuffle, slow\n",
    "                        # bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "\n",
    "\n",
    "                        # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                        colnames = list(bhv_df_all.columns)\n",
    "                        eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\"]\n",
    "                        nevents = np.size(eventnames)\n",
    "\n",
    "                        all_pops = list(bhv_df_all.columns)\n",
    "                        from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                        to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                        causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                        nFromNodes = np.shape(from_pops)[0]\n",
    "                        nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                        DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        score_randstart = np.zeros((num_starting_points))\n",
    "                        score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                        # step 1: randomize the starting point for num_starting_points times\n",
    "                        for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                            # try different down/re-sampling size\n",
    "                            bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                            aic = AicScore(bhv_df)\n",
    "\n",
    "                            #Anirban(Alec) shuffle, slow\n",
    "                            bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                            aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                            np.random.seed(istarting_points)\n",
    "                            random.seed(istarting_points)\n",
    "                            starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                            starting_graph = DAG()\n",
    "                            starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                            starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                            # step 2: add the shffled data results\n",
    "                            # shuffled bhv_df\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                        DAGs_alltypes[iDBN_group_typename] = DAGs_randstart \n",
    "                        DAGs_shuffle_alltypes[iDBN_group_typename] = DAGs_randstart_shuffle\n",
    "\n",
    "                        DAGs_scores_alltypes[iDBN_group_typename] = score_randstart\n",
    "                        DAGs_shuffle_scores_alltypes[iDBN_group_typename] = score_randstart_shuffle\n",
    "\n",
    "                        weighted_graphs = get_weighted_dags(DAGs_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                        weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[iDBN_group_typename],nbootstraps)\n",
    "                        sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                        weighted_graphs_alltypes[iDBN_group_typename] = weighted_graphs\n",
    "                        weighted_graphs_shuffled_alltypes[iDBN_group_typename] = weighted_graphs_shuffled\n",
    "                        sig_edges_alltypes[iDBN_group_typename] = sig_edges\n",
    "\n",
    "                    except:\n",
    "                        DAGs_alltypes[iDBN_group_typename] = [] \n",
    "                        DAGs_shuffle_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                        DAGs_scores_alltypes[iDBN_group_typename] = []\n",
    "                        DAGs_shuffle_scores_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                        weighted_graphs_alltypes[iDBN_group_typename] = []\n",
    "                        weighted_graphs_shuffled_alltypes[iDBN_group_typename] = []\n",
    "                        sig_edges_alltypes[iDBN_group_typename] = []\n",
    "\n",
    "                DAGscores_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "                DAGscores_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "                weighted_graphs_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "                sig_edges_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    savedata = 0\n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if moreSampSize:  \n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "            with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fb96410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "if not mergetempRos:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes = pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "        DBN_input_data_alltypes = pickle.load(f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3077623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for self condition, use the same for failed pull and succ pull\n",
    "# DAGscores_diffTempRo_diffSampSize[('failedpull', '1', 'max_row_number')]['self']=DAGscores_diffTempRo_diffSampSize[('succpull', '1', 'max_row_number')]['self']\n",
    "# DAGscores_shuffled_diffTempRo_diffSampSize[('failedpull', '1', 'max_row_number')]['self']=DAGscores_shuffled_diffTempRo_diffSampSize[('succpull', '1', 'max_row_number')]['self']\n",
    "# weighted_graphs_diffTempRo_diffSampSize[('failedpull', '1', 'max_row_number')]['self']=weighted_graphs_diffTempRo_diffSampSize[('succpull', '1', 'max_row_number')]['self']\n",
    "# weighted_graphs_shuffled_diffTempRo_diffSampSize[('failedpull', '1', 'max_row_number')]['self']=weighted_graphs_shuffled_diffTempRo_diffSampSize[('succpull', '1', 'max_row_number')]['self']\n",
    "# sig_edges_diffTempRo_diffSampSize[('failedpull', '1', 'max_row_number')]['self']=sig_edges_diffTempRo_diffSampSize[('succpull', '1', 'max_row_number')]['self']\n",
    "\n",
    "DAGscores_diffTempRo_diffSampSize[('failedpull', '1', 'min_row_number')]['self']=DAGscores_diffTempRo_diffSampSize[('succpull', '1', 'min_row_number')]['self']\n",
    "DAGscores_shuffled_diffTempRo_diffSampSize[('failedpull', '1', 'min_row_number')]['self']=DAGscores_shuffled_diffTempRo_diffSampSize[('succpull', '1', 'min_row_number')]['self']\n",
    "weighted_graphs_diffTempRo_diffSampSize[('failedpull', '1', 'min_row_number')]['self']=weighted_graphs_diffTempRo_diffSampSize[('succpull', '1', 'min_row_number')]['self']\n",
    "weighted_graphs_shuffled_diffTempRo_diffSampSize[('failedpull', '1', 'min_row_number')]['self']=weighted_graphs_shuffled_diffTempRo_diffSampSize[('succpull', '1', 'min_row_number')]['self']\n",
    "sig_edges_diffTempRo_diffSampSize[('failedpull', '1', 'min_row_number')]['self']=sig_edges_diffTempRo_diffSampSize[('succpull', '1', 'min_row_number')]['self']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "01d3f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull1_t0=np.array(DBN_input_data_alltypes['failedpull']['coop(1s)']['pull1_t0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cc4ac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull2_t0=np.array(DBN_input_data_alltypes['failedpull']['coop(1s)']['pull2_t0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "988f15ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pull1_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "63d54e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pull1_t0+pull2_t0==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab023b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d0963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7cf1eea",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge with arrows; show the best time bin and row number; show the three time lag separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,5]\n",
    "DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "pulltype_forplot = 'succpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "# eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "# eventnodes_color = ['b','r','y','g']\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00']\n",
    "eventnodes_shape = [\"o\",\"o\",\"^\",\"^\"]\n",
    "    \n",
    "savefigs = 0\n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,nDBN_groups)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3],[4,5,6,7],[8,9,10,11]]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "    for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "\n",
    "        try:\n",
    "\n",
    "            iDBN_group_typename = DBN_group_typenames[iDBN_group]\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "            #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "            \n",
    "            sig_avg_dags = weighted_graphs_tgt.mean(axis = 0) * sig_edges_tgt\n",
    "            sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,iDBN_group].set_title(pulltype_forplot+' '+iDBN_group_typename,fontsize=18)\n",
    "            axs[ilag*2+0,iDBN_group].set_xlim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_ylim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,iDBN_group].axis('equal')\n",
    "\n",
    "            \n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                              eventnodes_shape[ieventnode],markersize=60,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                              markeredgecolor='none')              \n",
    "                #axs[ilag*2+0,iDBN_group].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                #                       eventnames[ieventnode],fontsize=15)\n",
    "                \n",
    "                clmap = mpl.cm.get_cmap('Greens')\n",
    "                \n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt>0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        # width=0.04*abs(edge_weight_tgt),\n",
    "                                                        head_width=0.08,\n",
    "                                                        width=0.04,   \n",
    "                                                        color = clmap(edge_weight_tgt))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      # 0.04*abs(edge_weight_tgt),\n",
    "                                                      0.04,\n",
    "                                                      color = clmap(edge_weight_tgt))\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap(edge_weight_tgt), \n",
    "                                    edgecolor=clmap(edge_weight_tgt)\n",
    "                                )\n",
    "                                axs[ilag*2+0,iDBN_group].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap(edge_weight_tgt),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "                                else:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap(edge_weight_tgt),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = 0,1\n",
    "            import matplotlib as mpl\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,iDBN_group].pcolormesh(sig_avg_dags_df,cmap=\"Greens\",norm=norm)\n",
    "            #\n",
    "            if iDBN_group == nDBN_groups-1:\n",
    "                cax = axs[ilag*2+1,iDBN_group].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,iDBN_group], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,iDBN_group].axis('equal')\n",
    "            axs[ilag*2+1,iDBN_group].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,iDBN_group].set_xticks(np.arange(0.5,4.5,1))\n",
    "            axs[ilag*2+1,iDBN_group].set_xticklabels(eventnames)\n",
    "            if iDBN_group == 0:\n",
    "                axs[ilag*2+1,iDBN_group].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks(np.arange(0.5,4.5,1))\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,iDBN_group].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,iDBN_group].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks([])\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels([])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if moreSampSize:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows.pdf')\n",
    "    else:  \n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34546be",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge differences, use one condition as the base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR PLOT!! \n",
    "# define DBN related summarizing variables\n",
    "DBN_group_typenames = ['self','coop(1s)','no-vision']\n",
    "DBN_group_typeIDs  =  [1,3,5]\n",
    "DBN_group_coopthres = [0,1,0]\n",
    "nDBN_groups = np.shape(DBN_group_typenames)[0]\n",
    "\n",
    "pulltype_forplot = 'failedpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['min_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "basecondition = 'coop(1s)'\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "# eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\"]\n",
    "eventnames = [\"M1pull\",\"M2pull\",\"M1gaze\",\"M2gaze\"]\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0]]\n",
    "eventname_locations = [[-0.5,1.0],[1.2,1],[-0.6,0],[1.2,0]]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25]]]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45]]]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "# eventnodes_color = ['b','r','y','g']\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00']\n",
    "eventnodes_shape = [\"o\",\"o\",\"^\",\"^\"]\n",
    "\n",
    "nFromNodes = nevents\n",
    "nToNodes = nevents\n",
    "    \n",
    "savefigs = 0\n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,nDBN_groups)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*nDBN_groups)\n",
    "\n",
    "time_lags = ['t_-3','t_-2','t_-1']\n",
    "fromRowIDs =[[0,1,2,3],[4,5,6,7],[8,9,10,11]]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "    \n",
    "weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "#sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "# sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "\n",
    "weighted_graphs_base = weighted_graphs_tgt\n",
    "\n",
    "sig_edges_base = sig_edges_tgt\n",
    "\n",
    "sig_avg_dags_base =  weighted_graphs_base.mean(axis = 0) * sig_edges_base\n",
    "    \n",
    "    \n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "       \n",
    "    for iDBN_group in np.arange(0,nDBN_groups,1):\n",
    "\n",
    "        try:\n",
    "\n",
    "            iDBN_group_typename = DBN_group_typenames[iDBN_group]\n",
    "\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][iDBN_group_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "            #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "            \n",
    "            if 0:\n",
    "                weighted_graphs_delta = (weighted_graphs_tgt-weighted_graphs_base)\n",
    "                weighted_graphs_delta = weighted_graphs_delta.mean(axis=0)\n",
    "                #\n",
    "                sig_edges_delta = ((sig_edges_tgt+sig_edges_base)>0)*1\n",
    "            else:\n",
    "                weighted_graphs_delta,sig_edges_delta = Modulation_Index(weighted_graphs_base, weighted_graphs_tgt,\n",
    "                                                                         sig_edges_base, sig_edges_tgt, 8000)\n",
    "                weighted_graphs_delta = weighted_graphs_delta.mean(axis=0)\n",
    "                \n",
    "            sig_avg_dags = weighted_graphs_delta * sig_edges_delta\n",
    "            sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,iDBN_group].set_title(iDBN_group_typename,fontsize=18)\n",
    "            axs[ilag*2+0,iDBN_group].set_xlim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_ylim([-0.5,1.5])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_xticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticks([])\n",
    "            axs[ilag*2+0,iDBN_group].set_yticklabels([])\n",
    "            axs[ilag*2+0,iDBN_group].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,iDBN_group].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,iDBN_group].axis('equal')\n",
    "\n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                              eventnodes_shape[ieventnode],markersize=60,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                              markeredgecolor='none')              \n",
    "                \n",
    "                #axs[ilag*2+0,iDBN_group].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                #                       eventnames[ieventnode],fontsize=10)\n",
    "                \n",
    "                clmap = mpl.cm.get_cmap('bwr')\n",
    "                \n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt!=0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,iDBN_group].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        # width=0.04*abs(edge_weight_tgt),\n",
    "                                                        head_width=0.08,\n",
    "                                                        width=0.04,       \n",
    "                                                        color = clmap((1+edge_weight_tgt)/2))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      # 0.04*abs(edge_weight_tgt)\n",
    "                                                      0.04\n",
    "                                                     )\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap((1+edge_weight_tgt)/2), \n",
    "                                    edgecolor=clmap((1+edge_weight_tgt)/2)\n",
    "                                )\n",
    "                                axs[ilag*2+0,iDBN_group].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap((1+edge_weight_tgt)/2),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "                                else:\n",
    "                                    axs[ilag*2+0,iDBN_group].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap((1+edge_weight_tgt)/2),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = -1,1\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,iDBN_group].pcolormesh(sig_avg_dags_df,cmap=\"bwr\",norm=norm)\n",
    "            #-\n",
    "            if iDBN_group == nDBN_groups-1:\n",
    "                cax = axs[ilag*2+1,iDBN_group].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,iDBN_group], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,iDBN_group].axis('equal')\n",
    "            axs[ilag*2+1,iDBN_group].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,iDBN_group].set_xticks(np.arange(0.5,4.5,1))\n",
    "            axs[ilag*2+1,iDBN_group].set_xticklabels(eventnames)\n",
    "            if iDBN_group == 0:\n",
    "                axs[ilag*2+1,iDBN_group].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks(np.arange(0.5,4.5,1))\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,iDBN_group].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,iDBN_group].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,iDBN_group].set_yticks([])\n",
    "                axs[ilag*2+1,iDBN_group].set_yticklabels([])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if moreSampSize:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_EdgeFifferenceFrom_'+basecondition+'AsBase.pdf')\n",
    "    else:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_EdgeFifferenceFrom_'+basecondition+'AsBase.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed62ef",
   "metadata": {},
   "source": [
    "## Plots that include all pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15a992",
   "metadata": {},
   "source": [
    "### VERSION 4: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis shows the pairs. y axis averaged the MI among time lag and show CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "animalpairs_datashapes = ['o','v','^','*']\n",
    "#animal1_fixedorders = ['eddie']\n",
    "#animal2_fixedorders = ['sparkle']\n",
    "#animalpairs_datashapes = ['o']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "pulltype_forplot = 'failedpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(2,6)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(5*6)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    if 0:\n",
    "        MI_coop_self_all = weighted_graphs_coop-weighted_graphs_self\n",
    "        MI_nov_coop_all = weighted_graphs_nov-weighted_graphs_coop  \n",
    "        MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "        MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "        #\n",
    "        sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "        sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "        #\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    else:\n",
    "        MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_coop,\n",
    "                                          sig_edges_self, sig_edges_coop, 8000)\n",
    "        MI_coop_self = MI_coop_self_all.mean(axis = 0)\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop_all,sig_edges_nov_coop  = Modulation_Index(weighted_graphs_coop, weighted_graphs_nov,\n",
    "                                          sig_edges_coop, sig_edges_nov, 8000)\n",
    "        MI_nov_coop = MI_nov_coop_all.mean(axis = 0)\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    #\n",
    "    pull_pull_fromNodes_all = [0,1,4,5,8,9]\n",
    "    pull_pull_toNodes_all = [1,0,1,0,1,0]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [2,3,6,7,10,11]\n",
    "    gaze_gaze_toNodes_all = [3,2,3,2,3,2]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [0,1,4,5,8,9]\n",
    "    within_pullgaze_toNodes_all = [2,3,2,3,2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [0,1,4,5,8,9]\n",
    "    across_pullgaze_toNodes_all = [3,2,3,2,3,2]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [2,3,6,7,10,11]\n",
    "    within_gazepull_toNodes_all = [0,1,0,1,0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [2,3,6,7,10,11]\n",
    "    across_gazepull_toNodes_all = [1,0,1,0,1,0]\n",
    "\n",
    "    \n",
    "    # plot coop self modulation\n",
    "    \n",
    "    # pull-pull\n",
    "    xxx1 = np.mean(MI_coop_self_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all])\n",
    "    # err1 = np.std(MI_coop_self_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all])\n",
    "    err1 = st.sem(MI_coop_self_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all].flatten())\n",
    "    axs[0,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 5)\n",
    "    line1 = axs[0,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # gaze-gaze\n",
    "    xxx2 = np.mean(MI_coop_self_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all])\n",
    "    # err2 = np.std(MI_coop_self_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all])\n",
    "    err2 = st.sem(MI_coop_self_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all].flatten())\n",
    "    axs[0,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 5)\n",
    "    line2 = axs[0,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal gazepull\n",
    "    xxx3 = np.mean(MI_coop_self_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all])\n",
    "    # err3 = np.std(MI_coop_self_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all])\n",
    "    err3 = st.sem(MI_coop_self_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all].flatten())\n",
    "    axs[0,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 5)\n",
    "    line3 = axs[0,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal gazepull\n",
    "    xxx4 = np.mean(MI_coop_self_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all])\n",
    "    # err4 = np.std(MI_coop_self_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all])\n",
    "    err4 = st.sem(MI_coop_self_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all].flatten())\n",
    "    axs[0,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 5)\n",
    "    line4 = axs[0,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal pullgaze\n",
    "    xxx5 = np.mean(MI_coop_self_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all])\n",
    "    #err5 = np.std(MI_coop_self_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all])\n",
    "    err5 = st.sem(MI_coop_self_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all].flatten())\n",
    "    axs[0,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 5)\n",
    "    line5 = axs[0,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal pullgaze\n",
    "    xxx6 = np.mean(MI_coop_self_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all])\n",
    "    # err6 = np.std(MI_coop_self_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all])\n",
    "    err6 = st.sem(MI_coop_self_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all].flatten())\n",
    "    axs[0,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 5)\n",
    "    line6 = axs[0,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    \n",
    "    #\n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze',\n",
    "                 'within animal gaze->pull','across animal gaze->pull',\n",
    "                 'within animal pull->gaze','across animal pull->gaze',\n",
    "                 ]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[0,iplot].set_xlim([-0.1,nanimalpairs-0.9])\n",
    "        axs[0,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[0,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "        axs[0,iplot].set_xticklabels([])\n",
    "        axs[0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        #\n",
    "        if iplot == 0:\n",
    "            axs[0,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[0,iplot].set_yticklabels([])\n",
    "        axs[0,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[0,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "\n",
    "    # plot novision coop modulation\n",
    "    # pull-pull\n",
    "    xxx1 = np.mean(MI_nov_coop_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all])\n",
    "    # err1 = np.std(MI_nov_coop_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all])\n",
    "    err1 = st.sem(MI_nov_coop_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all].flatten())\n",
    "    axs[1,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 5)\n",
    "    line1 = axs[1,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # gaze-gaze\n",
    "    xxx2 = np.mean(MI_nov_coop_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all])\n",
    "    # err2 = np.std(MI_nov_coop_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all])\n",
    "    err2 = st.sem(MI_nov_coop_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all].flatten())\n",
    "    axs[1,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 5)\n",
    "    line2 = axs[1,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal gazepull\n",
    "    xxx3 = np.mean(MI_nov_coop_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all])\n",
    "    # err3 = np.std(MI_nov_coop_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all])\n",
    "    err3 = st.sem(MI_nov_coop_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all].flatten())\n",
    "    axs[1,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 5)\n",
    "    line3 = axs[1,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal gazepull\n",
    "    xxx4 = np.mean(MI_nov_coop_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all])\n",
    "    # err4 = np.std(MI_nov_coop_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all])\n",
    "    err4 = st.sem(MI_nov_coop_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all].flatten())\n",
    "    axs[1,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 5)\n",
    "    line4 = axs[1,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal pullgaze\n",
    "    xxx5 = np.mean(MI_nov_coop_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all])\n",
    "    #err5 = np.std(MI_nov_coop_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all])\n",
    "    err5 = st.sem(MI_nov_coop_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all].flatten())\n",
    "    axs[1,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 5)\n",
    "    line5 = axs[1,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal pullgaze\n",
    "    xxx6 = np.mean(MI_nov_coop_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all])\n",
    "    # err6 = np.std(MI_nov_coop_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all])\n",
    "    err6 = st.sem(MI_nov_coop_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all].flatten())\n",
    "    axs[1,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 5)\n",
    "    line6 = axs[1,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    \n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze',\n",
    "                 'within animal gaze->pull','across animal gaze->pull',\n",
    "                 'within animal pull->gaze','across animal pull->gaze',\n",
    "                 ]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[1,iplot].set_xlim([-0.1,nanimalpairs-0.9])\n",
    "        axs[1,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[1,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "        #axs[1,iplot].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "        axs[1,iplot].set_xticklabels(['ED/SP','DO/SC','GI/KA','DA/KA'],fontsize = 13)\n",
    "        #axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "        axs[1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        if iplot == 0:\n",
    "            axs[1,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[1,iplot].set_yticklabels([])\n",
    "        axs[1,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[1,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "        \n",
    "    #axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "    #                 'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "    #                 'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "    \n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_mergedLag_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_meanSEM.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_mergedLag_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_meanSEM.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346193d",
   "metadata": {},
   "source": [
    "### version 5: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis shows the pairs. y axis averaged the MI only for 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72961be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "animalpairs_datashapes = ['o','v','^','*']\n",
    "#animal1_fixedorders = ['eddie']\n",
    "#animal2_fixedorders = ['sparkle']\n",
    "#animalpairs_datashapes = ['o']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "pulltype_forplot = 'failedpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(2,6)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(5*6)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    #\n",
    "    if 0:\n",
    "        MI_coop_self_all = weighted_graphs_coop-weighted_graphs_self\n",
    "        MI_nov_coop_all = weighted_graphs_nov-weighted_graphs_coop  \n",
    "        MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "        MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "        #\n",
    "        sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "        sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "        #\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    else:\n",
    "        MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_coop,\n",
    "                                          sig_edges_self, sig_edges_coop, 8000)\n",
    "        MI_coop_self = MI_coop_self_all.mean(axis = 0)\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop_all,sig_edges_nov_coop  = Modulation_Index(weighted_graphs_coop, weighted_graphs_nov,\n",
    "                                          sig_edges_coop, sig_edges_nov, 8000)\n",
    "        MI_nov_coop = MI_nov_coop_all.mean(axis = 0)\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    #\n",
    "    pull_pull_fromNodes_all = [8,9]\n",
    "    pull_pull_toNodes_all = [1,0]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [10,11]\n",
    "    gaze_gaze_toNodes_all = [3,2]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [8,9]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [8,9]\n",
    "    across_pullgaze_toNodes_all = [3,2]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [10,11]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [10,11]\n",
    "    across_gazepull_toNodes_all = [1,0]\n",
    "\n",
    "    \n",
    "    # plot coop self modulation\n",
    "    \n",
    "    # pull-pull\n",
    "    a = MI_coop_self_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all].flatten()\n",
    "    xxx1 = np.mean(a)\n",
    "    err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[0,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "    line1 = axs[0,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # gaze-gaze\n",
    "    a = (MI_coop_self_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all]).flatten()\n",
    "    xxx2 = np.mean(a)\n",
    "    err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[0,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "    line2 = axs[0,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal gazepull\n",
    "    a = (MI_coop_self_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all]).flatten()\n",
    "    xxx3 = np.mean(a)\n",
    "    err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[0,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "    line3 = axs[0,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal gazepull\n",
    "    a = (MI_coop_self_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all]).flatten()\n",
    "    xxx4 = np.mean(a)\n",
    "    err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[0,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "    line4 = axs[0,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal pullgaze\n",
    "    a = (MI_coop_self_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all]).flatten()\n",
    "    xxx5 = np.mean(a)\n",
    "    err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[0,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "    line5 = axs[0,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal pullgaze\n",
    "    a = (MI_coop_self_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all]).flatten()\n",
    "    xxx6 = np.mean(a)\n",
    "    err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[0,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "    line6 = axs[0,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    \n",
    "    #\n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze',\n",
    "                 'within animal gaze->pull','across animal gaze->pull',\n",
    "                 'within animal pull->gaze','across animal pull->gaze',\n",
    "                 ]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[0,iplot].set_xlim([-0.3,nanimalpairs-0.7])\n",
    "        axs[0,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[0,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "        axs[0,iplot].set_xticklabels([])\n",
    "        axs[0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        #\n",
    "        if iplot == 0:\n",
    "            axs[0,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[0,iplot].set_yticklabels([])\n",
    "        axs[0,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[0,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "\n",
    "    # plot novision coop modulation\n",
    "    # pull-pull\n",
    "    a = MI_nov_coop_all[:,pull_pull_fromNodes_all,pull_pull_toNodes_all].flatten()\n",
    "    xxx1 = np.mean(a)\n",
    "    err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[1,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "    line1 = axs[1,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # gaze-gaze\n",
    "    a = (MI_nov_coop_all[:,gaze_gaze_fromNodes_all,gaze_gaze_toNodes_all]).flatten()\n",
    "    xxx2 = np.mean(a)\n",
    "    err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[1,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "    line2 = axs[1,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal gazepull\n",
    "    a = (MI_nov_coop_all[:,within_gazepull_fromNodes_all,within_gazepull_toNodes_all]).flatten()\n",
    "    xxx3 = np.mean(a)\n",
    "    err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[1,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "    line3 = axs[1,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal gazepull\n",
    "    a = (MI_nov_coop_all[:,across_gazepull_fromNodes_all,across_gazepull_toNodes_all]).flatten()\n",
    "    xxx4 = np.mean(a)\n",
    "    err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[1,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "    line4 = axs[1,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # within animal pullgaze\n",
    "    a = (MI_nov_coop_all[:,within_pullgaze_fromNodes_all,within_pullgaze_toNodes_all]).flatten()\n",
    "    xxx5 = np.mean(a)\n",
    "    err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[1,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "    line5 = axs[1,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    # across animal pullgaze\n",
    "    a = (MI_nov_coop_all[:,across_pullgaze_fromNodes_all,across_pullgaze_toNodes_all]).flatten()\n",
    "    xxx6 = np.mean(a)\n",
    "    err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "    axs[1,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "    line6 = axs[1,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "    \n",
    "    plottypes = ['across animal pull<->pull','across animal gaze<->gaze',\n",
    "                 'within animal gaze->pull','across animal gaze->pull',\n",
    "                 'within animal pull->gaze','across animal pull->gaze',\n",
    "                 ]\n",
    "    for iplot in np.arange(0,6,1):\n",
    "        axs[1,iplot].set_xlim([-0.3,nanimalpairs-0.7])\n",
    "        axs[1,iplot].set_ylim([-1.05,1.05])\n",
    "        axs[1,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "        #axs[1,iplot].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "        axs[1,iplot].set_xticklabels(['ED/SP','DO/SC','GI/KA','DA/KA'],fontsize = 13)\n",
    "        #axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "        axs[1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "        if iplot == 0:\n",
    "            axs[1,iplot].tick_params(axis='y', labelsize=13)\n",
    "            axs[1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "        else:\n",
    "            axs[1,iplot].set_yticklabels([])\n",
    "        axs[1,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "        #\n",
    "        if ianimalpair == nanimalpairs-1:\n",
    "            axs[1,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "        \n",
    "    #axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "    #                 'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "    #                 'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "    \n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_1secondLag_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_1secondLag_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e72e3",
   "metadata": {},
   "source": [
    "### version 6: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis shows the pairs. y axis showed the MI; separate animal 1 and 2; merge time lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "animalpairs_datashapes = ['o','v','^','*']\n",
    "#animal1_fixedorders = ['eddie']\n",
    "#animal2_fixedorders = ['sparkle']\n",
    "#animalpairs_datashapes = ['o']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "pulltype_forplot = 'succpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(4,6)\n",
    "fig.set_figheight(8*4)\n",
    "fig.set_figwidth(8*6)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    #\n",
    "    if 0:\n",
    "        MI_coop_self_all = weighted_graphs_coop-weighted_graphs_self\n",
    "        MI_nov_coop_all = weighted_graphs_nov-weighted_graphs_coop  \n",
    "        MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "        MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "        #\n",
    "        sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "        sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "        #\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    else:\n",
    "        MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_coop,\n",
    "                                          sig_edges_self, sig_edges_coop, 8000)\n",
    "        MI_coop_self = MI_coop_self_all.mean(axis = 0)\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop_all,sig_edges_nov_coop  = Modulation_Index(weighted_graphs_coop, weighted_graphs_nov,\n",
    "                                          sig_edges_coop, sig_edges_nov, 8000)\n",
    "        MI_nov_coop = MI_nov_coop_all.mean(axis = 0)\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    \n",
    "    #\n",
    "    pull_pull_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "    pull_pull_toNodes_all = [[1,1,1],[0,0,0]]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "    gaze_gaze_toNodes_all = [[3,3,3],[2,2,2]]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "    within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "    across_pullgaze_toNodes_all = [[3,3,3],[2,2,2]]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "    within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "    across_gazepull_toNodes_all = [[1,1,1],[0,0,0]]\n",
    "    \n",
    "    animalshortnames = ['A1','A2']\n",
    "\n",
    "    \n",
    "    for ianimal in np.arange(0,2,1):\n",
    "        \n",
    "        anishortname = animalshortnames[ianimal]\n",
    "        \n",
    "        \n",
    "        # plot coop self modulation\n",
    "\n",
    "        # pull-pull\n",
    "        a = MI_coop_self_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[2*ianimal+0,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (MI_coop_self_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[2*ianimal+0,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (MI_coop_self_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[2*ianimal+0,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (MI_coop_self_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[2*ianimal+0,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (MI_coop_self_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[2*ianimal+0,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (MI_coop_self_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[2*ianimal+0,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "\n",
    "        #\n",
    "        plottypes = ['from '+anishortname+' across animal pull<->pull','from '+anishortname+' across animal gaze<->gaze',\n",
    "                     'from '+anishortname+' within animal gaze->pull','from '+anishortname+' across animal gaze->pull',\n",
    "                     'from '+anishortname+' within animal pull->gaze','from '+anishortname+' across animal pull->gaze',\n",
    "                     ]\n",
    "        for iplot in np.arange(0,6,1):\n",
    "            axs[2*ianimal+0,iplot].set_xlim([-0.3,nanimalpairs-0.7])\n",
    "            axs[2*ianimal+0,iplot].set_ylim([-1.05,1.05])\n",
    "            axs[2*ianimal+0,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "            axs[2*ianimal+0,iplot].set_xticklabels([])\n",
    "            axs[2*ianimal+0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "            #\n",
    "            if iplot == 0:\n",
    "                axs[2*ianimal+0,iplot].tick_params(axis='y', labelsize=13)\n",
    "                axs[2*ianimal+0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "            else:\n",
    "                axs[2*ianimal+0,iplot].set_yticklabels([])\n",
    "            axs[2*ianimal+0,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "            #\n",
    "            if ianimalpair == nanimalpairs-1:\n",
    "                axs[2*ianimal+0,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "\n",
    "        # plot novision coop modulation\n",
    "        # pull-pull\n",
    "        a = MI_nov_coop_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[2*ianimal+1,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (MI_nov_coop_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[2*ianimal+1,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (MI_nov_coop_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[2*ianimal+1,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (MI_nov_coop_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[2*ianimal+1,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (MI_nov_coop_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[2*ianimal+1,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (MI_nov_coop_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[2*ianimal+1,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "\n",
    "        plottypes = ['from '+anishortname+' across animal pull<->pull','from '+anishortname+' across animal gaze<->gaze',\n",
    "                     'from '+anishortname+' within animal gaze->pull','from '+anishortname+' across animal gaze->pull',\n",
    "                     'from '+anishortname+' within animal pull->gaze','from '+anishortname+' across animal pull->gaze',\n",
    "                     ]\n",
    "        for iplot in np.arange(0,6,1):\n",
    "            axs[2*ianimal+1,iplot].set_xlim([-0.3,nanimalpairs-0.7])\n",
    "            axs[2*ianimal+1,iplot].set_ylim([-1.05,1.05])\n",
    "            axs[2*ianimal+1,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "            #axs[1,iplot].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "            axs[2*ianimal+1,iplot].set_xticklabels(['ED/SP','DO/SC','GI/KA','DA/KA'],fontsize = 13)\n",
    "            #axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "            axs[2*ianimal+1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "            if iplot == 0:\n",
    "                axs[2*ianimal+1,iplot].tick_params(axis='y', labelsize=13)\n",
    "                axs[2*ianimal+1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "            else:\n",
    "                axs[1,iplot].set_yticklabels([])\n",
    "            axs[2*ianimal+1,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "            #\n",
    "            if ianimalpair == nanimalpairs-1:\n",
    "                axs[2*ianimal+1,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "\n",
    "        #axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "        #                 'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "        #                 'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "\n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_mergedLag_IndiAnimal_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_mergedLag_IndiAnimal_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2cdfe",
   "metadata": {},
   "source": [
    "### version 7: plot the key edges' modulation; only show the modulation among coop1s, self, no-vision; x axis shows the pairs. y axis showed the MI; separate animal 1 and 2; only for 1s time lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7466a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','ginger','dannon']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "animalpairs_datashapes = ['o','v','^','*']\n",
    "#animal1_fixedorders = ['eddie']\n",
    "#animal2_fixedorders = ['sparkle']\n",
    "#animalpairs_datashapes = ['o']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "pulltype_forplot = 'succpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(4,6)\n",
    "fig.set_figheight(8*4)\n",
    "fig.set_figwidth(8*6)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    #\n",
    "    if 0:\n",
    "        MI_coop_self_all = weighted_graphs_coop-weighted_graphs_self\n",
    "        MI_nov_coop_all = weighted_graphs_nov-weighted_graphs_coop  \n",
    "        MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "        MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "        #\n",
    "        sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "        sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "        #\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    else:\n",
    "        MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_coop,\n",
    "                                          sig_edges_self, sig_edges_coop, 8000)\n",
    "        MI_coop_self = MI_coop_self_all.mean(axis = 0)\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop_all,sig_edges_nov_coop  = Modulation_Index(weighted_graphs_coop, weighted_graphs_nov,\n",
    "                                          sig_edges_coop, sig_edges_nov, 8000)\n",
    "        MI_nov_coop = MI_nov_coop_all.mean(axis = 0)\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    #\n",
    "    pull_pull_fromNodes_all = [8,9]\n",
    "    pull_pull_toNodes_all = [1,0]\n",
    "    #\n",
    "    gaze_gaze_fromNodes_all = [10,11]\n",
    "    gaze_gaze_toNodes_all = [3,2]\n",
    "    #\n",
    "    within_pullgaze_fromNodes_all = [8,9]\n",
    "    within_pullgaze_toNodes_all = [2,3]\n",
    "    #\n",
    "    across_pullgaze_fromNodes_all = [8,9]\n",
    "    across_pullgaze_toNodes_all = [3,2]\n",
    "    #\n",
    "    within_gazepull_fromNodes_all = [10,11]\n",
    "    within_gazepull_toNodes_all = [0,1]\n",
    "    #\n",
    "    across_gazepull_fromNodes_all = [10,11]\n",
    "    across_gazepull_toNodes_all = [1,0]\n",
    "\n",
    "    animalshortnames = ['A1','A2']\n",
    "    \n",
    "    for ianimal in np.arange(0,2,1):\n",
    "        \n",
    "        anishortname = animalshortnames[ianimal]\n",
    "        \n",
    "        # plot coop self modulation\n",
    "\n",
    "        # pull-pull\n",
    "        a = MI_coop_self_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[2*ianimal+0,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (MI_coop_self_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[2*ianimal+0,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (MI_coop_self_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[2*ianimal+0,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (MI_coop_self_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[2*ianimal+0,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (MI_coop_self_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[2*ianimal+0,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (MI_coop_self_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+0,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[2*ianimal+0,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "\n",
    "        #\n",
    "        plottypes = ['from '+anishortname+' across animal pull<->pull','from '+anishortname+' across animal gaze<->gaze',\n",
    "                     'from '+anishortname+' within animal gaze->pull','from '+anishortname+' across animal gaze->pull',\n",
    "                     'from '+anishortname+' within animal pull->gaze','from '+anishortname+' across animal pull->gaze',\n",
    "                     ]\n",
    "        for iplot in np.arange(0,6,1):\n",
    "            axs[2*ianimal+0,iplot].set_xlim([-0.3,nanimalpairs-0.7])\n",
    "            axs[2*ianimal+0,iplot].set_ylim([-1.05,1.05])\n",
    "            axs[2*ianimal+0,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "            axs[2*ianimal+0,iplot].set_xticklabels([])\n",
    "            axs[2*ianimal+0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "            #\n",
    "            if iplot == 0:\n",
    "                axs[2*ianimal+0,iplot].tick_params(axis='y', labelsize=13)\n",
    "                axs[2*ianimal+0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=15)\n",
    "            else:\n",
    "                axs[2*ianimal+0,iplot].set_yticklabels([])\n",
    "            axs[2*ianimal+0,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "            #\n",
    "            if ianimalpair == nanimalpairs-1:\n",
    "                axs[2*ianimal+0,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "\n",
    "        # plot novision coop modulation\n",
    "        # pull-pull\n",
    "        a = MI_nov_coop_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,0].errorbar(ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[2*ianimal+1,0].plot(ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (MI_nov_coop_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,1].errorbar(ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[2*ianimal+1,1].plot(ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (MI_nov_coop_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,2].errorbar(ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[2*ianimal+1,2].plot(ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (MI_nov_coop_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,3].errorbar(ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[2*ianimal+1,3].plot(ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (MI_nov_coop_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,4].errorbar(ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[2*ianimal+1,4].plot(ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (MI_nov_coop_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[2*ianimal+1,5].errorbar(ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[2*ianimal+1,5].plot(ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "\n",
    "        plottypes = ['from '+anishortname+' across animal pull<->pull','from '+anishortname+' across animal gaze<->gaze',\n",
    "                     'from '+anishortname+' within animal gaze->pull','from '+anishortname+' across animal gaze->pull',\n",
    "                     'from '+anishortname+' within animal pull->gaze','from '+anishortname+' across animal pull->gaze',\n",
    "                     ]\n",
    "        for iplot in np.arange(0,6,1):\n",
    "            axs[2*ianimal+1,iplot].set_xlim([-0.3,nanimalpairs-0.7])\n",
    "            axs[2*ianimal+1,iplot].set_ylim([-1.05,1.05])\n",
    "            axs[2*ianimal+1,iplot].set_xticks(np.arange(0,nanimalpairs,1))\n",
    "            #axs[1,iplot].set_xticklabels(['t-3','t-2','t-1'],fontsize = 13)\n",
    "            axs[2*ianimal+1,iplot].set_xticklabels(['ED/SP','DO/SC','GI/KA','DA/KA'],fontsize = 13)\n",
    "            #axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "            axs[2*ianimal+1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "            if iplot == 0:\n",
    "                axs[2*ianimal+1,iplot].tick_params(axis='y', labelsize=13)\n",
    "                axs[2*ianimal+1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=15)\n",
    "            else:\n",
    "                axs[2*ianimal+1,iplot].set_yticklabels([])\n",
    "            axs[2*ianimal+1,iplot].set_title(plottypes[iplot],fontsize = 16)\n",
    "            #\n",
    "            if ianimalpair == nanimalpairs-1:\n",
    "                axs[2*ianimal+1,iplot].plot([-1,nanimalpairs],[0,0],'k--')\n",
    "\n",
    "        #axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "        #                 'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "        #                 'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "\n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_1secondLag_IndiAnimal_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_1secondLag_IndiAnimal_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3371b3f",
   "metadata": {},
   "source": [
    "### version 7-2-2: \n",
    "#### plot the key edges' modulation; \n",
    "#### only show the modulation among coop1s, self, no-vision; \n",
    "#### separate animal 1 and 2, plot individual animal; \n",
    "#### put all animal in one plot - based on the \"to Node\"; for one time lag or merged all time lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ff85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga']\n",
    "animalpairs_datashapes = ['o','o','o','o']\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "\n",
    "timelag = 1 # 1 or 2 or 3 or 0(merged)\n",
    "timelagname = '1second' # '1/2/3second' or 'merged'\n",
    "# timelagname = 'merged'\n",
    "\n",
    "pulltype_forplot = 'succpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "#\n",
    "fig, axs = plt.subplots(2,6)\n",
    "fig.set_figheight(8*2)\n",
    "fig.set_figwidth(8*6)\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    #\n",
    "    if 0:\n",
    "        MI_coop_self_all = weighted_graphs_coop-weighted_graphs_self\n",
    "        MI_nov_coop_all = weighted_graphs_nov-weighted_graphs_coop  \n",
    "        MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "        MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "        #\n",
    "        sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "        sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "        #\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    else:\n",
    "        MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_coop,\n",
    "                                          sig_edges_self, sig_edges_coop, 8000)\n",
    "        MI_coop_self = MI_coop_self_all.mean(axis = 0)\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop_all,sig_edges_nov_coop  = Modulation_Index(weighted_graphs_coop, weighted_graphs_nov,\n",
    "                                          sig_edges_coop, sig_edges_nov, 8000)\n",
    "        MI_nov_coop = MI_nov_coop_all.mean(axis = 0)\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    #\n",
    "    if timelag == 1:\n",
    "        pull_pull_fromNodes_all = [9,8]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [11,10]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [8,9]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [9,8]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [10,11]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [11,10]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "    elif timelag == 2:\n",
    "        pull_pull_fromNodes_all = [5,4]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [7,6]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [4,5]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [5,4]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [6,7]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [7,6]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "    elif timelag == 3:\n",
    "        pull_pull_fromNodes_all = [1,0]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [3,2]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [0,1]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [1,0]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [2,3]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [3,2]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "    elif timelag == 0:\n",
    "        pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "    \n",
    "    \n",
    "    for ianimal in np.arange(0,2,1):\n",
    "                \n",
    "        # plot coop self modulation\n",
    "\n",
    "        # pull-pull\n",
    "        a = MI_coop_self_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[0,0].errorbar(ianimal*nanimalpairs+ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[0,0].plot(ianimal*nanimalpairs+ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (MI_coop_self_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[0,1].errorbar(ianimal*nanimalpairs+ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[0,1].plot(ianimal*nanimalpairs+ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (MI_coop_self_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[0,2].errorbar(ianimal*nanimalpairs+ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[0,2].plot(ianimal*nanimalpairs+ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (MI_coop_self_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[0,3].errorbar(ianimal*nanimalpairs+ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[0,3].plot(ianimal*nanimalpairs+ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (MI_coop_self_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[0,4].errorbar(ianimal*nanimalpairs+ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[0,4].plot(ianimal*nanimalpairs+ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (MI_coop_self_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[0,5].errorbar(ianimal*nanimalpairs+ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[0,5].plot(ianimal*nanimalpairs+ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "\n",
    "        #\n",
    "        plottypes = ['To Node; across animal pull<->pull','To Node; across animal gaze<->gaze',\n",
    "                     'To Node; within animal gaze->pull','To Node; across animal gaze->pull',\n",
    "                     'To Node; within animal pull->gaze','To Node; across animal pull->gaze',\n",
    "                     ]\n",
    "        for iplot in np.arange(0,6,1):\n",
    "            axs[0,iplot].set_xlim([-0.3,nanimalpairs*2-0.7])\n",
    "            axs[0,iplot].set_ylim([-1.05,1.05])\n",
    "            axs[0,iplot].set_xticks(np.arange(0,nanimalpairs*2,1))\n",
    "            axs[0,iplot].set_xticklabels(['E','Do','Da','G','Sp','Sc','K','K'],fontsize = 20)\n",
    "            axs[0,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "            #\n",
    "            if iplot == 0:\n",
    "                axs[0,iplot].tick_params(axis='y', labelsize=13)\n",
    "                axs[0,iplot].set_ylabel('Cooperative - Self Reward\\nmodulation index',fontsize=22)\n",
    "            else:\n",
    "                axs[0,iplot].set_yticklabels([])\n",
    "            axs[0,iplot].set_title(plottypes[iplot],fontsize = 21)\n",
    "            #\n",
    "            if ianimalpair == nanimalpairs-1:\n",
    "                axs[0,iplot].plot([-1,nanimalpairs*2],[0,0],'k--')\n",
    "\n",
    "        # plot novision coop modulation\n",
    "        # pull-pull\n",
    "        a = MI_nov_coop_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a)\n",
    "        err1 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[1,0].errorbar(ianimal*nanimalpairs+ianimalpair,xxx1,err1,color='k',capsize = 15)\n",
    "        line1 = axs[1,0].plot(ianimal*nanimalpairs+ianimalpair,xxx1,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # gaze-gaze\n",
    "        a = (MI_nov_coop_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a)\n",
    "        err2 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[1,1].errorbar(ianimal*nanimalpairs+ianimalpair,xxx2,err2,color='k',capsize = 15)\n",
    "        line2 = axs[1,1].plot(ianimal*nanimalpairs+ianimalpair,xxx2,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal gazepull\n",
    "        a = (MI_nov_coop_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a)\n",
    "        err3 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[1,2].errorbar(ianimal*nanimalpairs+ianimalpair,xxx3,err3,color='k',capsize = 15)\n",
    "        line3 = axs[1,2].plot(ianimal*nanimalpairs+ianimalpair,xxx3,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal gazepull\n",
    "        a = (MI_nov_coop_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a)\n",
    "        err4 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[1,3].errorbar(ianimal*nanimalpairs+ianimalpair,xxx4,err4,color='k',capsize = 15)\n",
    "        line4 = axs[1,3].plot(ianimal*nanimalpairs+ianimalpair,xxx4,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # within animal pullgaze\n",
    "        a = (MI_nov_coop_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a)\n",
    "        err5 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[1,4].errorbar(ianimal*nanimalpairs+ianimalpair,xxx5,err5,color='k',capsize = 15)\n",
    "        line5 = axs[1,4].plot(ianimal*nanimalpairs+ianimalpair,xxx5,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "        # across animal pullgaze\n",
    "        a = (MI_nov_coop_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a)\n",
    "        err6 = (st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))-a.mean())[1]\n",
    "        axs[1,5].errorbar(ianimal*nanimalpairs+ianimalpair,xxx6,err6,color='k',capsize = 15)\n",
    "        line6 = axs[1,5].plot(ianimal*nanimalpairs+ianimalpair,xxx6,animalpairs_datashapes[ianimalpair],markersize = 13,color='k')\n",
    "\n",
    "        plottypes = ['To Node; across animal pull<->pull','To Node; across animal gaze<->gaze',\n",
    "                     'To Node; within animal gaze->pull','To Node; across animal gaze->pull',\n",
    "                     'To Node; within animal pull->gaze','To Node; across animal pull->gaze',\n",
    "                     ]\n",
    "        for iplot in np.arange(0,6,1):\n",
    "            axs[1,iplot].set_xlim([-0.3,nanimalpairs*2-0.7])\n",
    "            axs[1,iplot].set_ylim([-1.05,1.05])\n",
    "            axs[1,iplot].set_xticks(np.arange(0,nanimalpairs*2,1))\n",
    "            #axs[1,iplot].set_xticklabels(['ED/SP','DO/SC','GI/KA'],fontsize = 13)\n",
    "            axs[1,iplot].set_xticklabels(['E','Do','Da','G','Sp','Sc','K','K'],fontsize = 20)\n",
    "            #axs[1,iplot].set_xlabel('time lag',fontsize=15)\n",
    "            axs[1,iplot].set_yticks([-1,-0.5,0,0.5,1])\n",
    "            if iplot == 0:\n",
    "                axs[1,iplot].tick_params(axis='y', labelsize=13)\n",
    "                axs[1,iplot].set_ylabel('No Vision - Cooperative\\nmodulation index',fontsize=22)\n",
    "            else:\n",
    "                axs[1,iplot].set_yticklabels([])\n",
    "            axs[1,iplot].set_title(plottypes[iplot],fontsize = 21)\n",
    "            #\n",
    "            if ianimalpair == nanimalpairs-1:\n",
    "                axs[1,iplot].plot([-1,nanimalpairs*2],[0,0],'k--')\n",
    "\n",
    "        #axs[0,0].legend(['pair:'+animal1_fixedorders[0][0:2]+'/'+animal2_fixedorders[0][0:2],\n",
    "        #                 'pair:'+animal1_fixedorders[1][0:2]+'/'+animal2_fixedorders[1][0:2],\n",
    "        #                 'pair:'+animal1_fixedorders[2][0:2]+'/'+animal2_fixedorders[2][0:2]],fontsize=17)\n",
    "\n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_'+timelagname+'Lag_IndiAnimal_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_mean95CI_version3.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_'+timelagname+'Lag_IndiAnimal_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_mean95CI_version3.pdf')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c0609",
   "metadata": {},
   "source": [
    "### version 7-2-3-2:\n",
    "#### plot the key edges' modulation; \n",
    "#### only show the modulation among coop1s, self, no-vision; \n",
    "#### separate animal 1 and 2, plot individual animal; \n",
    "#### pool 1) all the animals, 2) male and female, 3) subordinate and dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "animal_pooled_list = ['E','SP','DO','SC','DA','KwDA','G','KwG','K','V']\n",
    "\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "nanimalpooled = np.shape(animal_pooled_list)[0]\n",
    "\n",
    "timelag = 0 # 1 or 2 or 3 or 0(merged - merge all three lags) or 12 (merged lag 1 and 2)\n",
    "# timelagname = '1second' # '1/2/3second' or 'merged' or '12merged'\n",
    "timelagname = 'merged' # together with timelag = 0\n",
    "# timelagname = '12merged' # together with timelag = 12\n",
    "\n",
    "pulltype_forplot = 'succpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "nMIbootstraps = 150\n",
    "\n",
    "# \n",
    "MI_coop_self_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps,6])\n",
    "MI_coop_self_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "MI_nov_coop_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps,6])\n",
    "MI_nov_coop_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "ntimelags = 1\n",
    "if timelag == 0:\n",
    "    ntimelags = 3\n",
    "    MI_coop_self_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*3,6])\n",
    "    MI_coop_self_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "    MI_nov_coop_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*3,6])\n",
    "    MI_nov_coop_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "if timelag == 12:\n",
    "    ntimelags = 2\n",
    "    MI_coop_self_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*2,6])\n",
    "    MI_coop_self_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "    MI_nov_coop_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*2,6])\n",
    "    MI_nov_coop_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "#\n",
    "\n",
    "#\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    weighted_graphs_self = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    weighted_graphs_sf_coop = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    sig_edges_coop = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['coop(1s)']\n",
    "    #\n",
    "    weighted_graphs_nov = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "    # organize the key edge data\n",
    "    weighted_graphs_self_mean = weighted_graphs_self.mean(axis=0)\n",
    "    weighted_graphs_coop_mean = weighted_graphs_coop.mean(axis=0)\n",
    "    weighted_graphs_nov_mean = weighted_graphs_nov.mean(axis=0)\n",
    "    # MI_coop_self = (weighted_graphs_coop_mean-weighted_graphs_self_mean)/(weighted_graphs_coop_mean+weighted_graphs_self_mean)\n",
    "    # MI_nov_coop = (weighted_graphs_nov_mean-weighted_graphs_coop_mean)/(weighted_graphs_nov_mean+weighted_graphs_coop_mean)\n",
    "    # MI_coop_self = ((weighted_graphs_coop-weighted_graphs_self)/(weighted_graphs_coop+weighted_graphs_self)).mean(axis=0)\n",
    "    # MI_nov_coop = ((weighted_graphs_nov-weighted_graphs_coop)/(weighted_graphs_nov+weighted_graphs_coop)).mean(axis=0)\n",
    "    #\n",
    "    if 0:\n",
    "        MI_coop_self_all = weighted_graphs_coop-weighted_graphs_self\n",
    "        MI_nov_coop_all = weighted_graphs_nov-weighted_graphs_coop  \n",
    "        MI_coop_self = (weighted_graphs_coop-weighted_graphs_self).mean(axis=0)\n",
    "        MI_nov_coop = (weighted_graphs_nov-weighted_graphs_coop).mean(axis=0)\n",
    "        #\n",
    "        sig_edges_coop_self = ((sig_edges_coop+sig_edges_self)>0)*1\n",
    "        sig_edges_nov_coop = ((sig_edges_coop+sig_edges_nov)>0)*1\n",
    "        #\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "        #\n",
    "        nMIbootstraps = 1\n",
    "    else:\n",
    "        nMIbootstraps = 150\n",
    "        #\n",
    "        MI_coop_self_all,sig_edges_coop_self = Modulation_Index(weighted_graphs_self, weighted_graphs_coop,\n",
    "                                          sig_edges_self, sig_edges_coop, nMIbootstraps)\n",
    "        sig_edges_coop_self = sig_edges_coop_self.astype('float')\n",
    "        sig_edges_coop_self[sig_edges_coop_self==0]=np.nan\n",
    "\n",
    "        # MI_coop_self_all = MI_coop_self_all * sig_edges_coop_self\n",
    "        # # MI_coop_self_all[MI_coop_self_all==0] = np.nan\n",
    "        MI_coop_self = MI_coop_self_all.mean(axis = 0)\n",
    "        MI_coop_self = MI_coop_self * sig_edges_coop_self\n",
    "        \n",
    "        \n",
    "        MI_nov_coop_all,sig_edges_nov_coop  = Modulation_Index(weighted_graphs_coop, weighted_graphs_nov,\n",
    "                                          sig_edges_coop, sig_edges_nov, nMIbootstraps)\n",
    "        MI_nov_coop = MI_nov_coop_all.mean(axis = 0)\n",
    "        # MI_nov_coop = MI_nov_coop * sig_edges_nov_coop\n",
    "    \n",
    "    #\n",
    "    if timelag == 1:\n",
    "        pull_pull_fromNodes_all = [9,8]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [11,10]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [8,9]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [9,8]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [10,11]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [11,10]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        ntimelags = 1\n",
    "        #\n",
    "    elif timelag == 2:\n",
    "        pull_pull_fromNodes_all = [5,4]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [7,6]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [4,5]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [5,4]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [6,7]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [7,6]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        ntimelags = 1\n",
    "        #\n",
    "    elif timelag == 3:\n",
    "        pull_pull_fromNodes_all = [1,0]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [3,2]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [0,1]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [1,0]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [2,3]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [3,2]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        ntimelags = 1\n",
    "        #\n",
    "    elif timelag == 0:\n",
    "        pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        ntimelags = 3\n",
    "        #\n",
    "    elif timelag == 12:\n",
    "        pull_pull_fromNodes_all = [[5,9],[4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[7,11],[6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[4,8],[5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[5,9],[4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[6,10],[7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[7,11],[6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        ntimelags = 2\n",
    "        #\n",
    "    \n",
    "    \n",
    "    for ianimal in np.arange(0,2,1):\n",
    "                \n",
    "        # coop self modulation\n",
    "        # pull-pull\n",
    "        a1 = MI_coop_self_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a1)\n",
    "        MI_coop_self_all_IndiAni[2*ianimalpair+ianimal,:,0] = a1\n",
    "        MI_coop_self_mean_IndiAni[2*ianimalpair+ianimal,0] = xxx1\n",
    "        # gaze-gaze\n",
    "        a2 = (MI_coop_self_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a2)\n",
    "        MI_coop_self_all_IndiAni[2*ianimalpair+ianimal,:,1] = a2\n",
    "        MI_coop_self_mean_IndiAni[2*ianimalpair+ianimal,1] = xxx2\n",
    "        # within animal gazepull\n",
    "        a3 = (MI_coop_self_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a3)\n",
    "        MI_coop_self_all_IndiAni[2*ianimalpair+ianimal,:,2] = a3\n",
    "        MI_coop_self_mean_IndiAni[2*ianimalpair+ianimal,2] = xxx3\n",
    "        # across animal gazepull\n",
    "        a4 = (MI_coop_self_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a4)\n",
    "        MI_coop_self_all_IndiAni[2*ianimalpair+ianimal,:,3] = a4\n",
    "        MI_coop_self_mean_IndiAni[2*ianimalpair+ianimal,3] = xxx4\n",
    "        # within animal pullgaze\n",
    "        a5 = (MI_coop_self_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a5)\n",
    "        MI_coop_self_all_IndiAni[2*ianimalpair+ianimal,:,4] = a5\n",
    "        MI_coop_self_mean_IndiAni[2*ianimalpair+ianimal,4] = xxx5\n",
    "        # across animal pullgaze\n",
    "        a6 = (MI_coop_self_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a6)\n",
    "        MI_coop_self_all_IndiAni[2*ianimalpair+ianimal,:,5] = a6\n",
    "        MI_coop_self_mean_IndiAni[2*ianimalpair+ianimal,5] = xxx6\n",
    "        \n",
    "        \n",
    "        # novision coop modulation\n",
    "        # pull-pull\n",
    "        a1 = MI_nov_coop_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.mean(a1)\n",
    "        MI_nov_coop_all_IndiAni[2*ianimalpair+ianimal,:,0] = a1\n",
    "        MI_nov_coop_mean_IndiAni[2*ianimalpair+ianimal,0] = xxx1\n",
    "        # gaze-gaze\n",
    "        a2 = (MI_nov_coop_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.mean(a2)\n",
    "        MI_nov_coop_all_IndiAni[2*ianimalpair+ianimal,:,1] = a2\n",
    "        MI_nov_coop_mean_IndiAni[2*ianimalpair+ianimal,1] = xxx2\n",
    "        # within animal gazepull\n",
    "        a3 = (MI_nov_coop_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.mean(a3)\n",
    "        MI_nov_coop_all_IndiAni[2*ianimalpair+ianimal,:,2] = a3\n",
    "        MI_nov_coop_mean_IndiAni[2*ianimalpair+ianimal,2] = xxx3\n",
    "        # across animal gazepull\n",
    "        a4 = (MI_nov_coop_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.mean(a4)\n",
    "        MI_nov_coop_all_IndiAni[2*ianimalpair+ianimal,:,3] = a4\n",
    "        MI_nov_coop_mean_IndiAni[2*ianimalpair+ianimal,3] = xxx4\n",
    "        # within animal pullgaze\n",
    "        a5 = (MI_nov_coop_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.mean(a5)\n",
    "        MI_nov_coop_all_IndiAni[2*ianimalpair+ianimal,:,4] = a5\n",
    "        MI_nov_coop_mean_IndiAni[2*ianimalpair+ianimal,4] = xxx5\n",
    "        # across animal pullgaze\n",
    "        a6 = (MI_nov_coop_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.mean(a6)\n",
    "        MI_nov_coop_all_IndiAni[2*ianimalpair+ianimal,:,5] = a6\n",
    "        MI_nov_coop_mean_IndiAni[2*ianimalpair+ianimal,5] = xxx6\n",
    "        \n",
    "\n",
    "# prepare the data\n",
    "# average all animals for each dependency\n",
    "MI_coop_self_all_IndiAni_all = MI_coop_self_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)\n",
    "MI_nov_coop_all_IndiAni_all = MI_nov_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)\n",
    "MI_coop_self_all_IndiAni_allmean = np.nanmean(MI_coop_self_all_IndiAni_all,axis=0)\n",
    "MI_nov_coop_all_IndiAni_allmean = np.nanmean(MI_nov_coop_all_IndiAni_all,axis=0) \n",
    "MI_coop_self_all_IndiAni_allse = np.nanstd(MI_coop_self_all_IndiAni_all,axis=0)/np.sqrt(8*nMIbootstraps*ntimelags) \n",
    "MI_nov_coop_all_IndiAni_allse = np.nanstd(MI_nov_coop_all_IndiAni_all,axis=0)/np.sqrt(8*nMIbootstraps*ntimelags) \n",
    "\n",
    "# average all males and females for each dependency\n",
    "MI_coop_self_all_IndiAni_male = MI_coop_self_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1),np.arange(9*nMIbootstraps*ntimelags,10*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_coop_all_IndiAni_male = MI_nov_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1),np.arange(9*nMIbootstraps*ntimelags,10*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_self_all_IndiAni_malemean = np.nanmean(MI_coop_self_all_IndiAni_male,axis=0)\n",
    "MI_nov_coop_all_IndiAni_malemean = np.nanmean(MI_nov_coop_all_IndiAni_male,axis=0) \n",
    "MI_coop_self_all_IndiAni_malese = np.nanstd(MI_coop_self_all_IndiAni_male,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "MI_nov_coop_all_IndiAni_malese = np.nanstd(MI_nov_coop_all_IndiAni_male,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "# average all males and females for each dependency\n",
    "MI_coop_self_all_IndiAni_female = MI_coop_self_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,9*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_coop_all_IndiAni_female = MI_nov_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,9*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_self_all_IndiAni_femalemean = np.nanmean(MI_coop_self_all_IndiAni_female,axis=0)\n",
    "MI_nov_coop_all_IndiAni_femalemean = np.nanmean(MI_nov_coop_all_IndiAni_female,axis=0) \n",
    "MI_coop_self_all_IndiAni_femalese = np.nanstd(MI_coop_self_all_IndiAni_female,axis=0)/np.sqrt(6*nMIbootstraps*ntimelags) \n",
    "MI_nov_coop_all_IndiAni_femalese = np.nanstd(MI_nov_coop_all_IndiAni_female,axis=0)/np.sqrt(6*nMIbootstraps*ntimelags) \n",
    "\n",
    "# average all subordinate and dominant for each dependency\n",
    "MI_coop_self_all_IndiAni_sub = MI_coop_self_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1),np.arange(6*nMIbootstraps*ntimelags,7*nMIbootstraps*ntimelags,1),np.arange(8*nMIbootstraps*ntimelags,9*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_coop_all_IndiAni_sub = MI_nov_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1),np.arange(6*nMIbootstraps*ntimelags,7*nMIbootstraps*ntimelags,1),np.arange(8*nMIbootstraps*ntimelags,9*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_self_all_IndiAni_submean = np.nanmean(MI_coop_self_all_IndiAni_sub,axis=0)\n",
    "MI_nov_coop_all_IndiAni_submean = np.nanmean(MI_nov_coop_all_IndiAni_sub,axis=0) \n",
    "MI_coop_self_all_IndiAni_subse = np.nanstd(MI_coop_self_all_IndiAni_sub,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "MI_nov_coop_all_IndiAni_subse = np.nanstd(MI_nov_coop_all_IndiAni_sub,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "# average all subordinate and dominant for each dependency\n",
    "MI_coop_self_all_IndiAni_dom = MI_coop_self_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,6*nMIbootstraps*ntimelags,1),np.arange(7*nMIbootstraps*ntimelags,8*nMIbootstraps*ntimelags,1),np.arange(9*nMIbootstraps*ntimelags,10*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_coop_all_IndiAni_dom = MI_nov_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,6*nMIbootstraps*ntimelags,1),np.arange(7*nMIbootstraps*ntimelags,8*nMIbootstraps*ntimelags,1),np.arange(9*nMIbootstraps*ntimelags,10*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_self_all_IndiAni_dommean = np.nanmean(MI_coop_self_all_IndiAni_dom,axis=0)\n",
    "MI_nov_coop_all_IndiAni_dommean = np.nanmean(MI_nov_coop_all_IndiAni_dom,axis=0) \n",
    "MI_coop_self_all_IndiAni_domse = np.nanstd(MI_coop_self_all_IndiAni_dom,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "MI_nov_coop_all_IndiAni_domse = np.nanstd(MI_nov_coop_all_IndiAni_dom,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "\n",
    "# pool everything together\n",
    "MI_coop_self_all_IndiAni_pooled = {'all':MI_coop_self_all_IndiAni_all,\n",
    "                                   'male':MI_coop_self_all_IndiAni_male,\n",
    "                                   'female':MI_coop_self_all_IndiAni_female,\n",
    "                                   'subordinate':MI_coop_self_all_IndiAni_sub,\n",
    "                                   'dominant':MI_coop_self_all_IndiAni_dom}\n",
    "MI_nov_coop_all_IndiAni_pooled =  {'all':MI_nov_coop_all_IndiAni_all,\n",
    "                                   'male':MI_nov_coop_all_IndiAni_male,\n",
    "                                   'female':MI_nov_coop_all_IndiAni_female,\n",
    "                                   'subordinate':MI_nov_coop_all_IndiAni_sub,\n",
    "                                   'dominant':MI_nov_coop_all_IndiAni_dom}\n",
    "MI_coop_self_mean_IndiAni_pooled ={'all':MI_coop_self_mean_IndiAni,\n",
    "                                   'male':MI_coop_self_mean_IndiAni[[0,2,4,9],:],\n",
    "                                   'female':MI_coop_self_mean_IndiAni[[1,3,5,6,7,8],:],\n",
    "                                   'subordinate':MI_coop_self_mean_IndiAni[[0,2,4,6,8],:],\n",
    "                                   'dominant':MI_coop_self_mean_IndiAni[[1,3,5,7,9],:]}\n",
    "MI_nov_coop_mean_IndiAni_pooled = {'all':MI_nov_coop_mean_IndiAni,\n",
    "                                   'male':MI_nov_coop_mean_IndiAni[[0,2,4,9],:],\n",
    "                                   'female':MI_nov_coop_mean_IndiAni[[1,3,5,6,7,8],:],\n",
    "                                   'subordinate':MI_nov_coop_mean_IndiAni[[0,2,4,6,8],:],\n",
    "                                   'dominant':MI_nov_coop_mean_IndiAni[[1,3,5,7,9],:]}\n",
    "\n",
    "# for plot\n",
    "dependencynames = ['pull-pull','gaze-gaze','within_gazepull','across_gazepull','within_pullgaze','across_pullgaze']\n",
    "dependencytargets = ['pull-pull','within_gazepull','across_pullgaze']\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency\n",
    "# barplot\n",
    "MI_coop_self_all_IndiAni_all_df = pd.DataFrame(MI_coop_self_all_IndiAni_pooled['all'])\n",
    "MI_coop_self_all_IndiAni_all_df.columns = dependencynames\n",
    "MI_coop_self_all_IndiAni_all_df['MItype'] = 'coop-self'\n",
    "#\n",
    "MI_nov_coop_all_IndiAni_all_df = pd.DataFrame(MI_nov_coop_all_IndiAni_pooled['all'])\n",
    "MI_nov_coop_all_IndiAni_all_df.columns = dependencynames\n",
    "MI_nov_coop_all_IndiAni_all_df['MItype'] = 'nov-coop'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_self_all_IndiAni_all_df,MI_nov_coop_all_IndiAni_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on all bootstrap data point\n",
    "# seaborn.barplot(ax=ax1,data=df_long2,x='condition',y='value',hue='MItype',errorbar='ci',alpha=.5)\n",
    "#\n",
    "# swarmplot\n",
    "MI_coop_self_mean_IndiAni_df = pd.DataFrame(MI_coop_self_mean_IndiAni_pooled['all'])\n",
    "MI_coop_self_mean_IndiAni_df.columns = dependencynames\n",
    "MI_coop_self_mean_IndiAni_df['MItype'] = 'coop-self'\n",
    "#\n",
    "MI_nov_coop_mean_IndiAni_df = pd.DataFrame(MI_nov_coop_mean_IndiAni_pooled['all'])\n",
    "MI_nov_coop_mean_IndiAni_df.columns = dependencynames\n",
    "MI_nov_coop_mean_IndiAni_df['MItype'] = 'nov-coop'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_self_mean_IndiAni_df,MI_nov_coop_mean_IndiAni_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on mean value for each animal\n",
    "seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='MItype',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].set_ylabel('Modulation Index',fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals',fontsize=24)\n",
    "axs.ravel()[0].set_ylim([-1.35,1.35])\n",
    "\n",
    "# plot 2\n",
    "# average male and female animals for each dependency\n",
    "# barplot\n",
    "MI_coop_self_all_IndiAni_male_df = pd.DataFrame(MI_coop_self_all_IndiAni_pooled['male'])\n",
    "MI_coop_self_all_IndiAni_male_df.columns = dependencynames\n",
    "MI_coop_self_all_IndiAni_male_df['MItype'] = 'male coop-self'\n",
    "#\n",
    "MI_nov_coop_all_IndiAni_male_df = pd.DataFrame(MI_nov_coop_all_IndiAni_pooled['male'])\n",
    "MI_nov_coop_all_IndiAni_male_df.columns = dependencynames\n",
    "MI_nov_coop_all_IndiAni_male_df['MItype'] = 'male nov-coop'\n",
    "#\n",
    "MI_coop_self_all_IndiAni_female_df = pd.DataFrame(MI_coop_self_all_IndiAni_pooled['female'])\n",
    "MI_coop_self_all_IndiAni_female_df.columns = dependencynames\n",
    "MI_coop_self_all_IndiAni_female_df['MItype'] = 'female coop-self'\n",
    "#\n",
    "MI_nov_coop_all_IndiAni_female_df = pd.DataFrame(MI_nov_coop_all_IndiAni_pooled['female'])\n",
    "MI_nov_coop_all_IndiAni_female_df.columns = dependencynames\n",
    "MI_nov_coop_all_IndiAni_female_df['MItype'] = 'female nov-coop'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_self_all_IndiAni_male_df,MI_nov_coop_all_IndiAni_male_df,\n",
    "                   MI_coop_self_all_IndiAni_female_df,MI_nov_coop_all_IndiAni_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on all bootstrap data point\n",
    "# seaborn.barplot(ax=ax1,data=df_long2,x='condition',y='value',hue='MItype',errorbar='ci',alpha=.5)\n",
    "#\n",
    "# swarmplot\n",
    "MI_coop_self_mean_IndiAni_male_df = pd.DataFrame(MI_coop_self_mean_IndiAni_pooled['male'])\n",
    "MI_coop_self_mean_IndiAni_male_df.columns = dependencynames\n",
    "MI_coop_self_mean_IndiAni_male_df['MItype'] = 'male coop-self'\n",
    "#\n",
    "MI_nov_coop_mean_IndiAni_male_df = pd.DataFrame(MI_nov_coop_mean_IndiAni_pooled['male'])\n",
    "MI_nov_coop_mean_IndiAni_male_df.columns = dependencynames\n",
    "MI_nov_coop_mean_IndiAni_male_df['MItype'] = 'male nov-coop'\n",
    "#\n",
    "MI_coop_self_mean_IndiAni_female_df = pd.DataFrame(MI_coop_self_mean_IndiAni_pooled['female'])\n",
    "MI_coop_self_mean_IndiAni_female_df.columns = dependencynames\n",
    "MI_coop_self_mean_IndiAni_female_df['MItype'] = 'female coop-self'\n",
    "#\n",
    "MI_nov_coop_mean_IndiAni_female_df = pd.DataFrame(MI_nov_coop_mean_IndiAni_pooled['female'])\n",
    "MI_nov_coop_mean_IndiAni_female_df.columns = dependencynames\n",
    "MI_nov_coop_mean_IndiAni_female_df['MItype'] = 'female nov-coop'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_self_mean_IndiAni_male_df,MI_nov_coop_mean_IndiAni_male_df,\n",
    "                   MI_coop_self_mean_IndiAni_female_df,MI_nov_coop_mean_IndiAni_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on mean value for each animal\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='MItype',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].set_ylabel('Modulation Index',fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female',fontsize=24)\n",
    "axs.ravel()[1].set_ylim([-1.35,1.35])\n",
    "\n",
    "# plot 3\n",
    "# average sub and dom animals for each dependency\n",
    "# barplot\n",
    "MI_coop_self_all_IndiAni_sub_df = pd.DataFrame(MI_coop_self_all_IndiAni_pooled['subordinate'])\n",
    "MI_coop_self_all_IndiAni_sub_df.columns = dependencynames\n",
    "MI_coop_self_all_IndiAni_sub_df['MItype'] = 'sub coop-self'\n",
    "#\n",
    "MI_nov_coop_all_IndiAni_sub_df = pd.DataFrame(MI_nov_coop_all_IndiAni_pooled['subordinate'])\n",
    "MI_nov_coop_all_IndiAni_sub_df.columns = dependencynames\n",
    "MI_nov_coop_all_IndiAni_sub_df['MItype'] = 'sub nov-coop'\n",
    "#\n",
    "MI_coop_self_all_IndiAni_dom_df = pd.DataFrame(MI_coop_self_all_IndiAni_pooled['dominant'])\n",
    "MI_coop_self_all_IndiAni_dom_df.columns = dependencynames\n",
    "MI_coop_self_all_IndiAni_dom_df['MItype'] = 'dom coop-self'\n",
    "#\n",
    "MI_nov_coop_all_IndiAni_dom_df = pd.DataFrame(MI_nov_coop_all_IndiAni_pooled['dominant'])\n",
    "MI_nov_coop_all_IndiAni_dom_df.columns = dependencynames\n",
    "MI_nov_coop_all_IndiAni_dom_df['MItype'] = 'dom nov-coop'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_self_all_IndiAni_sub_df,MI_nov_coop_all_IndiAni_sub_df,\n",
    "                   MI_coop_self_all_IndiAni_dom_df,MI_nov_coop_all_IndiAni_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on all bootstrap data point\n",
    "# seaborn.barplot(ax=ax1,data=df_long2,x='condition',y='value',hue='MItype',errorbar='ci',alpha=.5)\n",
    "#\n",
    "# swarmplot\n",
    "MI_coop_self_mean_IndiAni_sub_df = pd.DataFrame(MI_coop_self_mean_IndiAni_pooled['subordinate'])\n",
    "MI_coop_self_mean_IndiAni_sub_df.columns = dependencynames\n",
    "MI_coop_self_mean_IndiAni_sub_df['MItype'] = 'sub coop-self'\n",
    "#\n",
    "MI_nov_coop_mean_IndiAni_sub_df = pd.DataFrame(MI_nov_coop_mean_IndiAni_pooled['subordinate'])\n",
    "MI_nov_coop_mean_IndiAni_sub_df.columns = dependencynames\n",
    "MI_nov_coop_mean_IndiAni_sub_df['MItype'] = 'sub nov-coop'\n",
    "#\n",
    "MI_coop_self_mean_IndiAni_dom_df = pd.DataFrame(MI_coop_self_mean_IndiAni_pooled['dominant'])\n",
    "MI_coop_self_mean_IndiAni_dom_df.columns = dependencynames\n",
    "MI_coop_self_mean_IndiAni_dom_df['MItype'] = 'dom coop-self'\n",
    "#\n",
    "MI_nov_coop_mean_IndiAni_dom_df = pd.DataFrame(MI_nov_coop_mean_IndiAni_pooled['dominant'])\n",
    "MI_nov_coop_mean_IndiAni_dom_df.columns = dependencynames\n",
    "MI_nov_coop_mean_IndiAni_dom_df['MItype'] = 'dom nov-coop'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_self_mean_IndiAni_sub_df,MI_nov_coop_mean_IndiAni_sub_df,\n",
    "                   MI_coop_self_mean_IndiAni_dom_df,MI_nov_coop_mean_IndiAni_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on mean value for each animal\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='MItype',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].set_ylabel('Modulation Index',fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom',fontsize=24)\n",
    "axs.ravel()[2].set_ylim([-1.35,1.35])\n",
    "\n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_'+timelagname+'Lag_IndiAnimal_summarized_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_subset_version2.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_'+timelagname+'Lag_IndiAnimal_summarized_'+pulltype_forplot+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_subset_version2.pdf')\n",
    "           \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "succdf=MI_coop_self_mean_IndiAni_df.copy()\n",
    "succdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "faildf=MI_coop_self_mean_IndiAni_df.copy()\n",
    "faildf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(succdf['pull-pull'],faildf['pull-pull'],'.')\n",
    "plt.plot([-1,1],[-1,1],'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a191a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "succdf['pull-pull']-faildf['pull-pull']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deab886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MI_coop_self_mean_IndiAni_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca897ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sts = scipy.stats.wilcoxon(MI_coop_self_mean_IndiAni_sub_df['pull-pull'])\n",
    "sts = scipy.stats.ttest_1samp(MI_coop_self_mean_IndiAni_sub_df['across_pullgaze'],0)\n",
    "pvalue = sts.pvalue\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sts = scipy.stats.wilcoxon(MI_nov_coop_mean_IndiAni_sub_df['within_gazepull'])\n",
    "sts = scipy.stats.ttest_1samp(MI_nov_coop_mean_IndiAni_sub_df['across_pullgaze'],0)\n",
    "pvalue = sts.pvalue\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = scipy.stats.ttest_ind(MI_coop_self_mean_IndiAni_sub_df['pull-pull'],MI_coop_self_mean_IndiAni_dom_df['pull-pull'])\n",
    "pvalue = sts.pvalue\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b697ba8",
   "metadata": {},
   "source": [
    "### version 7-2-3-3:\n",
    "#### plot the key edges' modulation; \n",
    "#### only show the modulation among coop1s, self, no-vision; \n",
    "#### separate animal 1 and 2, plot individual animal; \n",
    "#### pool 1) all the animals, 2) male and female, 3) subordinate and dominant\n",
    "#### compare successful vs failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT multiple pairs in one plot, so need to load data seperately\n",
    "moreSampSize = 0\n",
    "#\n",
    "animal1_fixedorders = ['eddie','dodson','dannon','ginger','koala']\n",
    "animal2_fixedorders = ['sparkle','scorch','kanga','kanga','vermelho']\n",
    "animal_pooled_list = ['E','SP','DO','SC','DA','KwDA','G','KwG','K','V']\n",
    "\n",
    "nanimalpairs = np.shape(animal1_fixedorders)[0]\n",
    "nanimalpooled = np.shape(animal_pooled_list)[0]\n",
    "\n",
    "timelag = 0 # 1 or 2 or 3 or 0(merged - merge all three lags) or 12 (merged lag 1 and 2)\n",
    "# timelagname = '1second' # '1/2/3second' or 'merged' or '12merged'\n",
    "timelagname = 'merged' # together with timelag = 0\n",
    "# timelagname = '12merged' # together with timelag = 12\n",
    "\n",
    "MI_cooptype = 'coop(1s)'  # 'coop(3s)','coop(2s)','coop(1.5s)','coop(1s)'\n",
    "\n",
    "nMIbootstraps = 150\n",
    "\n",
    "# \n",
    "MI_coop_coop_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps,6])\n",
    "MI_coop_coop_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "MI_nov_nov_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps,6])\n",
    "MI_nov_nov_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "ntimelags = 1\n",
    "if timelag == 0:\n",
    "    ntimelags = 3\n",
    "    MI_coop_coop_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*3,6])\n",
    "    MI_coop_coop_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "    MI_nov_nov_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*3,6])\n",
    "    MI_nov_nov_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "if timelag == 12:\n",
    "    ntimelags = 2\n",
    "    MI_coop_coop_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*2,6])\n",
    "    MI_coop_coop_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "    MI_nov_nov_all_IndiAni = np.zeros([nanimalpairs*2,nMIbootstraps*2,6])\n",
    "    MI_nov_nov_mean_IndiAni = np.zeros([nanimalpairs*2,6])\n",
    "#\n",
    "\n",
    "#\n",
    "#\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10*3)\n",
    "\n",
    "\n",
    "for ianimalpair in np.arange(0,nanimalpairs,1):\n",
    "    animal1_fixedorder = animal1_fixedorders[ianimalpair]\n",
    "    animal2_fixedorder = animal2_fixedorders[ianimalpair]\n",
    "    #\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_SuccAndFailedPull_newDefinition'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder+animal2_fixedorder+'/'\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder+animal2_fixedorder+'.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    # make sure these variables are the same as in the previous steps\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [1] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "    #\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        # samplingsizes = np.arange(1100,3000,100)\n",
    "        samplingsizes = [1100]\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "    else:\n",
    "        samplingsizes_name = ['min_row_number']   \n",
    "    nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "    #\n",
    "    temp_resolu = temp_resolus[0]\n",
    "    j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "    # load edge weight data    \n",
    "    # load successful pulls\n",
    "    weighted_graphs_self_succ = weighted_graphs_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self_succ = weighted_graphs_shuffled_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self_succ = sig_edges_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop_succ = weighted_graphs_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)][MI_cooptype]\n",
    "    weighted_graphs_sf_coop_succ = weighted_graphs_shuffled_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)][MI_cooptype]\n",
    "    sig_edges_coop_succ = sig_edges_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)][MI_cooptype]\n",
    "    #\n",
    "    weighted_graphs_nov_succ = weighted_graphs_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov_succ = weighted_graphs_shuffled_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov_succ = sig_edges_diffTempRo_diffSampSize[('succpull',str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    #\n",
    "    # load failed pulls\n",
    "    weighted_graphs_self_fail = weighted_graphs_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)]['self']\n",
    "    weighted_graphs_sf_self_fail = weighted_graphs_shuffled_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)]['self']\n",
    "    sig_edges_self_fail = sig_edges_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)]['self']\n",
    "    #\n",
    "    weighted_graphs_coop_fail = weighted_graphs_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)][MI_cooptype]\n",
    "    weighted_graphs_sf_coop_fail = weighted_graphs_shuffled_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)][MI_cooptype]\n",
    "    sig_edges_coop_fail = sig_edges_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)][MI_cooptype]\n",
    "    #\n",
    "    weighted_graphs_nov_fail = weighted_graphs_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    weighted_graphs_sf_nov_fail = weighted_graphs_shuffled_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "    sig_edges_nov_fail = sig_edges_diffTempRo_diffSampSize[('failedpull',str(temp_resolu),j_sampsize_name)]['no-vision']\n",
    "\n",
    "\n",
    "    # modulation index between MC and SR\n",
    "    #\n",
    "    # nMIbootstraps = 150\n",
    "    #\n",
    "    # MI_coop_self_succ_all,sig_edges_coop_self_succ = Modulation_Index(weighted_graphs_self_succ, \n",
    "    #                                                                   weighted_graphs_coop_succ,\n",
    "    #                                                                   sig_edges_self_succ, \n",
    "    #                                                                   sig_edges_coop_succ, nMIbootstraps)\n",
    "    # #\n",
    "    # MI_coop_self_fail_all,sig_edges_coop_self_fail = Modulation_Index(weighted_graphs_self_fail, \n",
    "    #                                                                   weighted_graphs_coop_fail,\n",
    "    #                                                                   sig_edges_self_fail, \n",
    "    #                                                                   sig_edges_coop_fail, nMIbootstraps)\n",
    "    # #\n",
    "    # MI_coop_coop_all,sig_edges_coop_coop = Modulation_Index(MI_coop_self_succ_all, \n",
    "    #                                                         MI_coop_self_fail_all,\n",
    "    #                                                         sig_edges_coop_self_succ, \n",
    "    #                                                         sig_edges_coop_self_fail, nMIbootstraps)\n",
    "    # #\n",
    "    # # sig_edges_coop_coop = sig_edges_coop_coop.astype('float')\n",
    "    # # sig_edges_coop_coop[sig_edges_coop_coop==0]=np.nan\n",
    "    \n",
    "    # modulation index between succ and failed only for MC (and NV)\n",
    "    #\n",
    "    nMIbootstraps = 150\n",
    "    #\n",
    "    MI_coop_coop_all,sig_edges_coop_coop = Modulation_Index(weighted_graphs_coop_succ, weighted_graphs_coop_fail,\n",
    "                                      sig_edges_coop_succ, sig_edges_coop_fail, nMIbootstraps)\n",
    "    \n",
    "    sig_edges_coop_coop = sig_edges_coop_coop.astype('float')\n",
    "    sig_edges_coop_coop[sig_edges_coop_coop==0]=np.nan\n",
    "    \n",
    "    # MI_coop_coop_all = MI_coop_coop_all * sig_edges_coop_coop\n",
    "    # # MI_coop_coop_all[MI_coop_coop_all==0] = np.nan\n",
    "    MI_coop_coop = MI_coop_coop_all.mean(axis = 0)\n",
    "    MI_coop_coop = MI_coop_coop * sig_edges_coop_coop\n",
    "   \n",
    "    MI_nov_nov_all,sig_edges_nov_nov  = Modulation_Index(weighted_graphs_nov_succ, weighted_graphs_nov_fail,\n",
    "                                      sig_edges_nov_succ, sig_edges_nov_fail, nMIbootstraps)\n",
    "    \n",
    "    sig_edges_nov_nov = sig_edges_nov_nov.astype('float')\n",
    "    sig_edges_nov_nov[sig_edges_nov_nov==0]=np.nan\n",
    "    \n",
    "    # MI_nov_nov_all = MI_nov_nov_all * sig_edges_nov_nov\n",
    "    # MI_nov_nov_all[MI_nov_nov_all==0] = np.nan\n",
    "    MI_nov_nov = MI_nov_nov_all.mean(axis = 0)\n",
    "    MI_nov_nov = MI_nov_nov * sig_edges_nov_nov\n",
    "    \n",
    "    #\n",
    "    if timelag == 1:\n",
    "        pull_pull_fromNodes_all = [9,8]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [11,10]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [8,9]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [9,8]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [10,11]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [11,10]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        ntimelags = 1\n",
    "        #\n",
    "    elif timelag == 2:\n",
    "        pull_pull_fromNodes_all = [5,4]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [7,6]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [4,5]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [5,4]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [6,7]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [7,6]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        ntimelags = 1\n",
    "        #\n",
    "    elif timelag == 3:\n",
    "        pull_pull_fromNodes_all = [1,0]\n",
    "        pull_pull_toNodes_all = [0,1]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [3,2]\n",
    "        gaze_gaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [0,1]\n",
    "        within_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [1,0]\n",
    "        across_pullgaze_toNodes_all = [2,3]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [2,3]\n",
    "        within_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [3,2]\n",
    "        across_gazepull_toNodes_all = [0,1]\n",
    "        #\n",
    "        ntimelags = 1\n",
    "        #\n",
    "    elif timelag == 0:\n",
    "        pull_pull_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[0,4,8],[1,5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[1,5,9],[0,4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2,2],[3,3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[2,6,10],[3,7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[3,7,11],[2,6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0,0],[1,1,1]]\n",
    "        #\n",
    "        ntimelags = 3\n",
    "        #\n",
    "    elif timelag == 12:\n",
    "        pull_pull_fromNodes_all = [[5,9],[4,8]]\n",
    "        pull_pull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        gaze_gaze_fromNodes_all = [[7,11],[6,10]]\n",
    "        gaze_gaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        within_pullgaze_fromNodes_all = [[4,8],[5,9]]\n",
    "        within_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        across_pullgaze_fromNodes_all = [[5,9],[4,8]]\n",
    "        across_pullgaze_toNodes_all = [[2,2],[3,3]]\n",
    "        #\n",
    "        within_gazepull_fromNodes_all = [[6,10],[7,11]]\n",
    "        within_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        across_gazepull_fromNodes_all = [[7,11],[6,10]]\n",
    "        across_gazepull_toNodes_all = [[0,0],[1,1]]\n",
    "        #\n",
    "        ntimelags = 2\n",
    "        #\n",
    "    \n",
    "    \n",
    "    for ianimal in np.arange(0,2,1):\n",
    "                \n",
    "        # coop self modulation\n",
    "        # pull-pull\n",
    "        a1 = MI_coop_coop_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.nanmean(a1)\n",
    "        MI_coop_coop_all_IndiAni[2*ianimalpair+ianimal,:,0] = a1\n",
    "        MI_coop_coop_mean_IndiAni[2*ianimalpair+ianimal,0] = xxx1\n",
    "        # gaze-gaze\n",
    "        a2 = (MI_coop_coop_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.nanmean(a2)\n",
    "        MI_coop_coop_all_IndiAni[2*ianimalpair+ianimal,:,1] = a2\n",
    "        MI_coop_coop_mean_IndiAni[2*ianimalpair+ianimal,1] = xxx2\n",
    "        # within animal gazepull\n",
    "        a3 = (MI_coop_coop_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.nanmean(a3)\n",
    "        MI_coop_coop_all_IndiAni[2*ianimalpair+ianimal,:,2] = a3\n",
    "        MI_coop_coop_mean_IndiAni[2*ianimalpair+ianimal,2] = xxx3\n",
    "        # across animal gazepull\n",
    "        a4 = (MI_coop_coop_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.nanmean(a4)\n",
    "        MI_coop_coop_all_IndiAni[2*ianimalpair+ianimal,:,3] = a4\n",
    "        MI_coop_coop_mean_IndiAni[2*ianimalpair+ianimal,3] = xxx4\n",
    "        # within animal pullgaze\n",
    "        a5 = (MI_coop_coop_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.nanmean(a5)\n",
    "        MI_coop_coop_all_IndiAni[2*ianimalpair+ianimal,:,4] = a5\n",
    "        MI_coop_coop_mean_IndiAni[2*ianimalpair+ianimal,4] = xxx5\n",
    "        # across animal pullgaze\n",
    "        a6 = (MI_coop_coop_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.nanmean(a6)\n",
    "        MI_coop_coop_all_IndiAni[2*ianimalpair+ianimal,:,5] = a6\n",
    "        MI_coop_coop_mean_IndiAni[2*ianimalpair+ianimal,5] = xxx6\n",
    "        \n",
    "        \n",
    "        # novision coop modulation\n",
    "        # pull-pull\n",
    "        a1 = MI_nov_nov_all[:,pull_pull_fromNodes_all[ianimal],pull_pull_toNodes_all[ianimal]].flatten()\n",
    "        xxx1 = np.nanmean(a1)\n",
    "        MI_nov_nov_all_IndiAni[2*ianimalpair+ianimal,:,0] = a1\n",
    "        MI_nov_nov_mean_IndiAni[2*ianimalpair+ianimal,0] = xxx1\n",
    "        # gaze-gaze\n",
    "        a2 = (MI_nov_nov_all[:,gaze_gaze_fromNodes_all[ianimal],gaze_gaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx2 = np.nanmean(a2)\n",
    "        MI_nov_nov_all_IndiAni[2*ianimalpair+ianimal,:,1] = a2\n",
    "        MI_nov_nov_mean_IndiAni[2*ianimalpair+ianimal,1] = xxx2\n",
    "        # within animal gazepull\n",
    "        a3 = (MI_nov_nov_all[:,within_gazepull_fromNodes_all[ianimal],within_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx3 = np.nanmean(a3)\n",
    "        MI_nov_nov_all_IndiAni[2*ianimalpair+ianimal,:,2] = a3\n",
    "        MI_nov_nov_mean_IndiAni[2*ianimalpair+ianimal,2] = xxx3\n",
    "        # across animal gazepull\n",
    "        a4 = (MI_nov_nov_all[:,across_gazepull_fromNodes_all[ianimal],across_gazepull_toNodes_all[ianimal]]).flatten()\n",
    "        xxx4 = np.nanmean(a4)\n",
    "        MI_nov_nov_all_IndiAni[2*ianimalpair+ianimal,:,3] = a4\n",
    "        MI_nov_nov_mean_IndiAni[2*ianimalpair+ianimal,3] = xxx4\n",
    "        # within animal pullgaze\n",
    "        a5 = (MI_nov_nov_all[:,within_pullgaze_fromNodes_all[ianimal],within_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx5 = np.nanmean(a5)\n",
    "        MI_nov_nov_all_IndiAni[2*ianimalpair+ianimal,:,4] = a5\n",
    "        MI_nov_nov_mean_IndiAni[2*ianimalpair+ianimal,4] = xxx5\n",
    "        # across animal pullgaze\n",
    "        a6 = (MI_nov_nov_all[:,across_pullgaze_fromNodes_all[ianimal],across_pullgaze_toNodes_all[ianimal]]).flatten()\n",
    "        xxx6 = np.nanmean(a6)\n",
    "        MI_nov_nov_all_IndiAni[2*ianimalpair+ianimal,:,5] = a6\n",
    "        MI_nov_nov_mean_IndiAni[2*ianimalpair+ianimal,5] = xxx6\n",
    "        \n",
    "\n",
    "# prepare the data\n",
    "# average all animals for each dependency\n",
    "MI_coop_coop_all_IndiAni_all = MI_coop_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)\n",
    "MI_nov_nov_all_IndiAni_all = MI_nov_nov_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)\n",
    "MI_coop_coop_all_IndiAni_allmean = np.nanmean(MI_coop_coop_all_IndiAni_all,axis=0)\n",
    "MI_nov_nov_all_IndiAni_allmean = np.nanmean(MI_nov_nov_all_IndiAni_all,axis=0) \n",
    "MI_coop_coop_all_IndiAni_allse = np.nanstd(MI_coop_coop_all_IndiAni_all,axis=0)/np.sqrt(8*nMIbootstraps*ntimelags) \n",
    "MI_nov_nov_all_IndiAni_allse = np.nanstd(MI_nov_nov_all_IndiAni_all,axis=0)/np.sqrt(8*nMIbootstraps*ntimelags) \n",
    "\n",
    "# average all males and females for each dependency\n",
    "MI_coop_coop_all_IndiAni_male = MI_coop_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_nov_all_IndiAni_male = MI_nov_nov_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_coop_all_IndiAni_malemean = np.nanmean(MI_coop_coop_all_IndiAni_male,axis=0)\n",
    "MI_nov_nov_all_IndiAni_malemean = np.nanmean(MI_nov_nov_all_IndiAni_male,axis=0) \n",
    "MI_coop_coop_all_IndiAni_malese = np.nanstd(MI_coop_coop_all_IndiAni_male,axis=0)/np.sqrt(3*nMIbootstraps*ntimelags) \n",
    "MI_nov_nov_all_IndiAni_malese = np.nanstd(MI_nov_nov_all_IndiAni_male,axis=0)/np.sqrt(3*nMIbootstraps*ntimelags) \n",
    "# average all males and females for each dependency\n",
    "MI_coop_coop_all_IndiAni_female = MI_coop_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,8*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_nov_all_IndiAni_female = MI_nov_nov_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,8*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_coop_all_IndiAni_femalemean = np.nanmean(MI_coop_coop_all_IndiAni_female,axis=0)\n",
    "MI_nov_nov_all_IndiAni_femalemean = np.nanmean(MI_nov_nov_all_IndiAni_female,axis=0) \n",
    "MI_coop_coop_all_IndiAni_femalese = np.nanstd(MI_coop_coop_all_IndiAni_female,axis=0)/np.sqrt(5*nMIbootstraps*ntimelags) \n",
    "MI_nov_nov_all_IndiAni_femalese = np.nanstd(MI_nov_nov_all_IndiAni_female,axis=0)/np.sqrt(5*nMIbootstraps*ntimelags) \n",
    "\n",
    "# average all subordinate and dominant for each dependency\n",
    "MI_coop_coop_all_IndiAni_sub = MI_coop_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1),np.arange(6*nMIbootstraps*ntimelags,7*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_nov_all_IndiAni_sub = MI_nov_nov_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(0,nMIbootstraps*ntimelags,1),np.arange(2*nMIbootstraps*ntimelags,3*nMIbootstraps*ntimelags,1),np.arange(4*nMIbootstraps*ntimelags,5*nMIbootstraps*ntimelags,1),np.arange(6*nMIbootstraps*ntimelags,7*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_coop_all_IndiAni_submean = np.nanmean(MI_coop_coop_all_IndiAni_sub,axis=0)\n",
    "MI_nov_nov_all_IndiAni_submean = np.nanmean(MI_nov_nov_all_IndiAni_sub,axis=0) \n",
    "MI_coop_coop_all_IndiAni_subse = np.nanstd(MI_coop_coop_all_IndiAni_sub,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "MI_nov_nov_all_IndiAni_subse = np.nanstd(MI_nov_nov_all_IndiAni_sub,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "# average all subordinate and dominant for each dependency\n",
    "MI_coop_coop_all_IndiAni_dom = MI_coop_coop_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,6*nMIbootstraps*ntimelags,1),np.arange(7*nMIbootstraps*ntimelags,8*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_nov_nov_all_IndiAni_dom = MI_nov_nov_all_IndiAni.reshape(nanimalpairs*2*nMIbootstraps*ntimelags,6)[np.concatenate((np.arange(1*nMIbootstraps*ntimelags,2*nMIbootstraps*ntimelags,1),np.arange(3*nMIbootstraps*ntimelags,4*nMIbootstraps*ntimelags,1),np.arange(5*nMIbootstraps*ntimelags,6*nMIbootstraps*ntimelags,1),np.arange(7*nMIbootstraps*ntimelags,8*nMIbootstraps*ntimelags,1))),:]\n",
    "MI_coop_coop_all_IndiAni_dommean = np.nanmean(MI_coop_coop_all_IndiAni_dom,axis=0)\n",
    "MI_nov_nov_all_IndiAni_dommean = np.nanmean(MI_nov_nov_all_IndiAni_dom,axis=0) \n",
    "MI_coop_coop_all_IndiAni_domse = np.nanstd(MI_coop_coop_all_IndiAni_dom,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "MI_nov_nov_all_IndiAni_domse = np.nanstd(MI_nov_nov_all_IndiAni_dom,axis=0)/np.sqrt(4*nMIbootstraps*ntimelags) \n",
    "\n",
    "# pool everything together\n",
    "MI_coop_coop_all_IndiAni_pooled = {'all':MI_coop_coop_all_IndiAni_all,\n",
    "                                   'male':MI_coop_coop_all_IndiAni_male,\n",
    "                                   'female':MI_coop_coop_all_IndiAni_female,\n",
    "                                   'subordinate':MI_coop_coop_all_IndiAni_sub,\n",
    "                                   'dominant':MI_coop_coop_all_IndiAni_dom}\n",
    "MI_nov_nov_all_IndiAni_pooled =  {'all':MI_nov_nov_all_IndiAni_all,\n",
    "                                   'male':MI_nov_nov_all_IndiAni_male,\n",
    "                                   'female':MI_nov_nov_all_IndiAni_female,\n",
    "                                   'subordinate':MI_nov_nov_all_IndiAni_sub,\n",
    "                                   'dominant':MI_nov_nov_all_IndiAni_dom}\n",
    "MI_coop_coop_mean_IndiAni_pooled ={'all':MI_coop_coop_mean_IndiAni,\n",
    "                                   'male':MI_coop_coop_mean_IndiAni[[0,2,4],:],\n",
    "                                   'female':MI_coop_coop_mean_IndiAni[[1,3,5,6,7],:],\n",
    "                                   'subordinate':MI_coop_coop_mean_IndiAni[[0,2,4,6],:],\n",
    "                                   'dominant':MI_coop_coop_mean_IndiAni[[1,3,5,7],:]}\n",
    "MI_nov_nov_mean_IndiAni_pooled = {'all':MI_nov_nov_mean_IndiAni,\n",
    "                                   'male':MI_nov_nov_mean_IndiAni[[0,2,4],:],\n",
    "                                   'female':MI_nov_nov_mean_IndiAni[[1,3,5,6,7],:],\n",
    "                                   'subordinate':MI_nov_nov_mean_IndiAni[[0,2,4,6],:],\n",
    "                                   'dominant':MI_nov_nov_mean_IndiAni[[1,3,5,7],:]}\n",
    "\n",
    "# for plot\n",
    "dependencynames = ['pull-pull','gaze-gaze','within_gazepull','across_gazepull','within_pullgaze','across_pullgaze']\n",
    "dependencytargets = ['pull-pull','within_gazepull','across_pullgaze',]\n",
    "# dependencytargets = dependencynames\n",
    "\n",
    "# plot 1\n",
    "# average all animals for each dependency\n",
    "# barplot\n",
    "MI_coop_coop_all_IndiAni_all_df = pd.DataFrame(MI_coop_coop_all_IndiAni_pooled['all'])\n",
    "MI_coop_coop_all_IndiAni_all_df.columns = dependencynames\n",
    "MI_coop_coop_all_IndiAni_all_df['MItype'] = 'coop:fail-succ'\n",
    "#\n",
    "MI_nov_nov_all_IndiAni_all_df = pd.DataFrame(MI_nov_nov_all_IndiAni_pooled['all'])\n",
    "MI_nov_nov_all_IndiAni_all_df.columns = dependencynames\n",
    "MI_nov_nov_all_IndiAni_all_df['MItype'] = 'nov:fail-succ'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_coop_all_IndiAni_all_df,MI_nov_nov_all_IndiAni_all_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on all bootstrap data point\n",
    "# seaborn.barplot(ax=ax1,data=df_long2,x='condition',y='value',hue='MItype',errorbar='ci',alpha=.5)\n",
    "#\n",
    "# swarmplot\n",
    "MI_coop_coop_mean_IndiAni_df = pd.DataFrame(MI_coop_coop_mean_IndiAni_pooled['all'])\n",
    "MI_coop_coop_mean_IndiAni_df.columns = dependencynames\n",
    "MI_coop_coop_mean_IndiAni_df['MItype'] = 'coop:fail-succ'\n",
    "#\n",
    "MI_nov_nov_mean_IndiAni_df = pd.DataFrame(MI_nov_nov_mean_IndiAni_pooled['all'])\n",
    "MI_nov_nov_mean_IndiAni_df.columns = dependencynames\n",
    "MI_nov_nov_mean_IndiAni_df['MItype'] = 'nov:fail-succ'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_coop_mean_IndiAni_df,MI_nov_nov_mean_IndiAni_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on mean value for each animal\n",
    "# seaborn.barplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.violinplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "# seaborn.swarmplot(ax=axs.ravel()[0],data=df_long2,x='condition',y='value',hue='MItype',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[0].set_xlabel('')\n",
    "axs.ravel()[0].set_ylabel('Modulation Index',fontsize=20)\n",
    "axs.ravel()[0].set_title('all animals',fontsize=24)\n",
    "axs.ravel()[0].set_ylim([-1.35,1.35])\n",
    "\n",
    "# plot 2\n",
    "# average male and female animals for each dependency\n",
    "# barplot\n",
    "MI_coop_coop_all_IndiAni_male_df = pd.DataFrame(MI_coop_coop_all_IndiAni_pooled['male'])\n",
    "MI_coop_coop_all_IndiAni_male_df.columns = dependencynames\n",
    "MI_coop_coop_all_IndiAni_male_df['MItype'] = 'male coop:fail-succ'\n",
    "#\n",
    "MI_nov_nov_all_IndiAni_male_df = pd.DataFrame(MI_nov_nov_all_IndiAni_pooled['male'])\n",
    "MI_nov_nov_all_IndiAni_male_df.columns = dependencynames\n",
    "MI_nov_nov_all_IndiAni_male_df['MItype'] = 'male nov:fail-succ'\n",
    "#\n",
    "MI_coop_coop_all_IndiAni_female_df = pd.DataFrame(MI_coop_coop_all_IndiAni_pooled['female'])\n",
    "MI_coop_coop_all_IndiAni_female_df.columns = dependencynames\n",
    "MI_coop_coop_all_IndiAni_female_df['MItype'] = 'female coop:fail-succ'\n",
    "#\n",
    "MI_nov_nov_all_IndiAni_female_df = pd.DataFrame(MI_nov_nov_all_IndiAni_pooled['female'])\n",
    "MI_nov_nov_all_IndiAni_female_df.columns = dependencynames\n",
    "MI_nov_nov_all_IndiAni_female_df['MItype'] = 'female nov:fail-succ'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_coop_all_IndiAni_male_df,MI_nov_nov_all_IndiAni_male_df,\n",
    "                   MI_coop_coop_all_IndiAni_female_df,MI_nov_nov_all_IndiAni_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on all bootstrap data point\n",
    "# seaborn.barplot(ax=ax1,data=df_long2,x='condition',y='value',hue='MItype',errorbar='ci',alpha=.5)\n",
    "#\n",
    "# swarmplot\n",
    "MI_coop_coop_mean_IndiAni_male_df = pd.DataFrame(MI_coop_coop_mean_IndiAni_pooled['male'])\n",
    "MI_coop_coop_mean_IndiAni_male_df.columns = dependencynames\n",
    "MI_coop_coop_mean_IndiAni_male_df['MItype'] = 'male coop:fail-succ'\n",
    "#\n",
    "MI_nov_nov_mean_IndiAni_male_df = pd.DataFrame(MI_nov_nov_mean_IndiAni_pooled['male'])\n",
    "MI_nov_nov_mean_IndiAni_male_df.columns = dependencynames\n",
    "MI_nov_nov_mean_IndiAni_male_df['MItype'] = 'male nov:fail-succ'\n",
    "#\n",
    "MI_coop_coop_mean_IndiAni_female_df = pd.DataFrame(MI_coop_coop_mean_IndiAni_pooled['female'])\n",
    "MI_coop_coop_mean_IndiAni_female_df.columns = dependencynames\n",
    "MI_coop_coop_mean_IndiAni_female_df['MItype'] = 'female coop:fail-succ'\n",
    "#\n",
    "MI_nov_nov_mean_IndiAni_female_df = pd.DataFrame(MI_nov_nov_mean_IndiAni_pooled['female'])\n",
    "MI_nov_nov_mean_IndiAni_female_df.columns = dependencynames\n",
    "MI_nov_nov_mean_IndiAni_female_df['MItype'] = 'female nov:fail-succ'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_coop_mean_IndiAni_male_df,MI_nov_nov_mean_IndiAni_male_df,\n",
    "                   MI_coop_coop_mean_IndiAni_female_df,MI_nov_nov_mean_IndiAni_female_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on mean value for each animal\n",
    "seaborn.barplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[1],data=df_long2,x='condition',y='value',hue='MItype',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[1].set_xlabel('')\n",
    "axs.ravel()[1].set_ylabel('Modulation Index',fontsize=20)\n",
    "axs.ravel()[1].set_title('male vs female',fontsize=24)\n",
    "axs.ravel()[1].set_ylim([-1.35,1.35])\n",
    "\n",
    "# plot 3\n",
    "# average sub and dom animals for each dependency\n",
    "# barplot\n",
    "MI_coop_coop_all_IndiAni_sub_df = pd.DataFrame(MI_coop_coop_all_IndiAni_pooled['subordinate'])\n",
    "MI_coop_coop_all_IndiAni_sub_df.columns = dependencynames\n",
    "MI_coop_coop_all_IndiAni_sub_df['MItype'] = 'sub coop-coop'\n",
    "#\n",
    "MI_nov_nov_all_IndiAni_sub_df = pd.DataFrame(MI_nov_nov_all_IndiAni_pooled['subordinate'])\n",
    "MI_nov_nov_all_IndiAni_sub_df.columns = dependencynames\n",
    "MI_nov_nov_all_IndiAni_sub_df['MItype'] = 'sub nov-nov'\n",
    "#\n",
    "MI_coop_coop_all_IndiAni_dom_df = pd.DataFrame(MI_coop_coop_all_IndiAni_pooled['dominant'])\n",
    "MI_coop_coop_all_IndiAni_dom_df.columns = dependencynames\n",
    "MI_coop_coop_all_IndiAni_dom_df['MItype'] = 'dom coop-coop'\n",
    "#\n",
    "MI_nov_nov_all_IndiAni_dom_df = pd.DataFrame(MI_nov_nov_all_IndiAni_pooled['dominant'])\n",
    "MI_nov_nov_all_IndiAni_dom_df.columns = dependencynames\n",
    "MI_nov_nov_all_IndiAni_dom_df['MItype'] = 'dom nov-nov'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_coop_all_IndiAni_sub_df,MI_nov_nov_all_IndiAni_sub_df,\n",
    "                   MI_coop_coop_all_IndiAni_dom_df,MI_nov_nov_all_IndiAni_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on all bootstrap data point\n",
    "# seaborn.barplot(ax=ax1,data=df_long2,x='condition',y='value',hue='MItype',errorbar='ci',alpha=.5)\n",
    "#\n",
    "# swarmplot\n",
    "MI_coop_coop_mean_IndiAni_sub_df = pd.DataFrame(MI_coop_coop_mean_IndiAni_pooled['subordinate'])\n",
    "MI_coop_coop_mean_IndiAni_sub_df.columns = dependencynames\n",
    "MI_coop_coop_mean_IndiAni_sub_df['MItype'] = 'sub coop-coop'\n",
    "#\n",
    "MI_nov_nov_mean_IndiAni_sub_df = pd.DataFrame(MI_nov_nov_mean_IndiAni_pooled['subordinate'])\n",
    "MI_nov_nov_mean_IndiAni_sub_df.columns = dependencynames\n",
    "MI_nov_nov_mean_IndiAni_sub_df['MItype'] = 'sub nov-nov'\n",
    "#\n",
    "MI_coop_coop_mean_IndiAni_dom_df = pd.DataFrame(MI_coop_coop_mean_IndiAni_pooled['dominant'])\n",
    "MI_coop_coop_mean_IndiAni_dom_df.columns = dependencynames\n",
    "MI_coop_coop_mean_IndiAni_dom_df['MItype'] = 'dom coop-coop'\n",
    "#\n",
    "MI_nov_nov_mean_IndiAni_dom_df = pd.DataFrame(MI_nov_nov_mean_IndiAni_pooled['dominant'])\n",
    "MI_nov_nov_mean_IndiAni_dom_df.columns = dependencynames\n",
    "MI_nov_nov_mean_IndiAni_dom_df['MItype'] = 'dom nov-nov'\n",
    "#\n",
    "df_long=pd.concat([MI_coop_coop_mean_IndiAni_sub_df,MI_nov_nov_mean_IndiAni_sub_df,\n",
    "                   MI_coop_coop_mean_IndiAni_dom_df,MI_nov_nov_mean_IndiAni_dom_df])\n",
    "df_long2 = df_long.melt(id_vars=['MItype'], value_vars=dependencytargets,var_name='condition', value_name='value')\n",
    "#\n",
    "# barplot based on mean value for each animal\n",
    "seaborn.barplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='MItype',errorbar='se',alpha=.5,capsize=0.1)\n",
    "seaborn.swarmplot(ax=axs.ravel()[2],data=df_long2,x='condition',y='value',hue='MItype',alpha=.9,size= 9,dodge=True,legend=False)\n",
    "axs.ravel()[2].set_xlabel('')\n",
    "axs.ravel()[2].set_ylabel('Modulation Index',fontsize=20)\n",
    "axs.ravel()[2].set_title('sub vs dom',fontsize=24)\n",
    "axs.ravel()[2].set_ylim([-1.35,1.35])\n",
    "\n",
    "savefig = 0\n",
    "if savefig:\n",
    "    if moreSampSize:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_'+timelagname+'Lag_IndiAnimal_summarized_failedvssucc_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_subset_version2.pdf')\n",
    "    else:\n",
    "        figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_SuccAndFailedPull_newDefinition_singlecam_wholebodylabels_combinesessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        plt.savefig(figsavefolder+'threeTimeLag_Edge_ModulationIndex_'+timelagname+'Lag_IndiAnimal_summarized_failedvssucc_'+str(temp_resolu)+'_'+j_sampsize_name+'_subset_version2.pdf')\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_coop_coop_mean_IndiAni_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72257eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.array(MI_coop_coop_mean_IndiAni_df['within_gazepull'])\n",
    "xxx = xxx[~np.isnan(xxx)]\n",
    "\n",
    "print(np.nanmean(xxx))\n",
    "print(np.nanstd(xxx))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c617dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = np.array(MI_nov_nov_mean_IndiAni_df['across_pullgaze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d99571",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = scipy.stats.ttest_1samp(xxx,0)\n",
    "pvalue = sts.pvalue\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ff61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_coop_coop[[1,5,9],[0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_coop_coop[[0,4,8],[1,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56027a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_coop_coop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_edges_coop_succ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77991d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_edges_self_succ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fa4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e946637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4369b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
