{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### This analysis is based on the pull aligned continuous bhv variables and neural activity analysis\n",
    "### The goal of this code is to define DHHM model and test the hypothesis that social gaze before pull serves as a evidence accumulation process, and test if the neural profile matches the accumulation hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7a900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # Added import for Pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import hddm\n",
    "import pymc as pm # Explicitly import pymc for summary function\n",
    "import arviz as az # Explicitly import arviz for summary function\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9485f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lifelines\n",
    "from lifelines import CoxPHFitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a418549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be able to use the functions in the ana_functions under /3d_recontruction_analysis_self_and_coop_task_neural_analysis/\n",
    "sys.path.append(os.path.abspath('../3d_recontruction_analysis_self_and_coop_task_neural_analysis/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam import plot_continuous_bhv_var_singlecam\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - HDDM related function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions._apply_gaussian_burst import _apply_gaussian_burst\n",
    "from functions.generate_marmoset_pull_data import generate_marmoset_pull_data # create simulated data\n",
    "from functions.compute_stats import compute_stats\n",
    "from functions.align_and_plot_data import align_and_plot_data\n",
    "from functions.get_aligned_segment import get_aligned_segment\n",
    "from functions.analyze_pull_aligned_data import analyze_pull_aligned_data\n",
    "from functions.analyze_pull_aligned_data_flexibleTW import analyze_pull_aligned_data_flexibleTW\n",
    "from functions.analyze_pull_aligned_data_flexibleTW_newRTdefinition import analyze_pull_aligned_data_flexibleTW_newRTdefinition\n",
    "\n",
    "from functions.run_hddm_modeling import run_hddm_modeling\n",
    "from functions.run_hddm_modeling_exaustModel import run_hddm_modeling_exaustModel\n",
    "# from functions.run_hddmnn_modeling import run_hddmnn_modeling # do not work\n",
    "from functions.do_hddm_model_fitted_plot import do_hddm_model_fitted_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b030e",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ec5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more information for each pull: number of preceding failed pull and time since last reward/successful pull\n",
    "from ana_functions.get_pull_infos import get_pull_infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     # '20231101_Dodson_withGinger_SR',\n",
    "                                     # '20231107_Dodson_withGinger_SR',\n",
    "                                     # '20231122_Dodson_withGinger_SR',\n",
    "                                     # '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      # \"20231101_SR\",\n",
    "                      # \"20231107_SR\",\n",
    "                      # \"20231122_SR\",\n",
    "                      # \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 # 0.00,   \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         # 2, \n",
    "                         # 2, \n",
    "                         # 4, \n",
    "                         # 4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorders = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorders = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define hddm data summarizing data set    \n",
    "hddm_datas_all_dates = dict.fromkeys(dates_list, [])\n",
    "hddm_datas_flexibleTW_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "# data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "# data_saved_folder = '/Users/weikangshi/Downloads/MishaCode_temp/'\n",
    "data_saved_folder = ''\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # dummy \n",
    "    \n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/hddm_datas_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        hddm_datas_all_dates = pickle.load(f)\n",
    "   \n",
    "    with open(data_saved_subfolder+'/hddm_datas_flexibleTW_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        hddm_datas_flexibleTW_all_dates = pickle.load(f)        \n",
    "        \n",
    "    with open(data_saved_subfolder+'/hddm_datas_flexibleTW_newRTdefinition_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        hddm_datas_flexibleTW_newRTdefinition_all_dates = pickle.load(f)   \n",
    "        \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "    \n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        # a new definition of successful and failed pulls\n",
    "        # separate successful and failed pulls\n",
    "        # step 1 all pull and juice\n",
    "        time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "        time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "        time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "        time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "        # step 2:\n",
    "        # pull 1\n",
    "        # Find the last pull before each juice\n",
    "        successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "        # Convert to Pandas Series\n",
    "        successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "        # Find failed pulls (pulls that are not successful)\n",
    "        failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "        # pull 2\n",
    "        # Find the last pull before each juice\n",
    "        successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "        # Convert to Pandas Series\n",
    "        successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "        # Find failed pulls (pulls that are not successful)\n",
    "        failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "        #\n",
    "        # step 3:\n",
    "        time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "        time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "        time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "        time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.ceil(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        \n",
    "        #\n",
    "        # organize variables that are required by the HDDM functions\n",
    "        # load the data first, if not process and then save the data \n",
    "        print('prepare the data for HDDM')\n",
    "        try:\n",
    "            # dummy\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_hddm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            with open(add_date_dir+'/hddm_data.pkl', 'rb') as f:\n",
    "                hddm_data = pickle.load(f)\n",
    "            with open(add_date_dir+'/hddm_data_flexibleTW.pkl', 'rb') as f:\n",
    "                hddm_data_flexibleTW = pickle.load(f)\n",
    "            with open(add_date_dir+'/hddm_data_flexibleTW_newRT.pkl', 'rb') as f:\n",
    "                hddm_data_flexibleTW_newRT = pickle.load(f)\n",
    "        except:\n",
    "            print('no HDDM data, analyze it and save it')\n",
    "            #\n",
    "            gausKernelsize = 4\n",
    "            #\n",
    "            # clean the data\n",
    "            time_point_pull1 = np.array(time_point_pull1)+session_start_time\n",
    "            time_point_pull1 = time_point_pull1[time_point_pull1<totalsess_time]\n",
    "            time_point_pull2 = np.array(time_point_pull2)+session_start_time\n",
    "            time_point_pull2 = time_point_pull2[time_point_pull2<totalsess_time]\n",
    "            #\n",
    "            time_point_juice1 = np.array(time_point_juice1)+session_start_time\n",
    "            time_point_juice1 = time_point_juice1[time_point_juice1<totalsess_time]\n",
    "            time_point_juice2 = np.array(time_point_juice2)+session_start_time\n",
    "            time_point_juice2 = time_point_juice2[time_point_juice2<totalsess_time]\n",
    "            #\n",
    "            oneway_gaze1 = np.sort(np.hstack((oneway_gaze1,mutual_gaze1)))\n",
    "            animal1_gaze = oneway_gaze1\n",
    "            animal1_gaze = np.sort(np.unique(animal1_gaze))+session_start_time\n",
    "            #\n",
    "            oneway_gaze2 = np.sort(np.hstack((oneway_gaze2,mutual_gaze2)))\n",
    "            animal2_gaze = oneway_gaze2\n",
    "            animal2_gaze = np.sort(np.unique(animal2_gaze))+session_start_time\n",
    "            #\n",
    "            # organize the data into a time series\n",
    "            pull1_data = np.zeros([int(totalsess_time*fps),])\n",
    "            pull1_data[np.round(time_point_pull1*fps).astype(int)]=1\n",
    "            #\n",
    "            pull2_data = np.zeros([int(totalsess_time*fps),])\n",
    "            pull2_data[np.round(time_point_pull2*fps).astype(int)]=1\n",
    "            #\n",
    "            juice1_data = np.zeros([int(totalsess_time*fps),])\n",
    "            juice1_data[np.round(time_point_juice1*fps).astype(int)]=1\n",
    "            #\n",
    "            juice2_data = np.zeros([int(totalsess_time*fps),])\n",
    "            juice2_data[np.round(time_point_juice2*fps).astype(int)]=1\n",
    "            #\n",
    "            gaze1_data = np.zeros([int(totalsess_time*fps),])\n",
    "            gaze1_data[np.round(animal1_gaze*fps).astype(int)]=1\n",
    "            gaze1_data = scipy.ndimage.gaussian_filter1d(gaze1_data,gausKernelsize)\n",
    "            #\n",
    "            gaze2_data = np.zeros([int(totalsess_time*fps),])\n",
    "            gaze2_data[np.round(animal2_gaze*fps).astype(int)]=1\n",
    "            gaze2_data = scipy.ndimage.gaussian_filter1d(gaze2_data,gausKernelsize)\n",
    "            #\n",
    "            facemass1 = output_key_locations['facemass_loc_all_merge']['dodson'].transpose()\n",
    "            facemass1 = np.hstack((facemass1,[[np.nan],[np.nan]]))\n",
    "            at1_min_at0 = (facemass1[:,1:]-facemass1[:,:-1])\n",
    "            speed1_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "            speed1_data = scipy.ndimage.gaussian_filter1d(speed1_data,gausKernelsize)\n",
    "            #\n",
    "            facemass2 = output_key_locations['facemass_loc_all_merge']['scorch'].transpose()\n",
    "            facemass2 = np.hstack((facemass2,[[np.nan],[np.nan]]))\n",
    "            at1_min_at0 = (facemass2[:,1:]-facemass2[:,:-1])\n",
    "            speed2_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "            speed2_data = scipy.ndimage.gaussian_filter1d(speed2_data,gausKernelsize)\n",
    "            #\n",
    "            # get the data that can be used for the HDDM pipeline\n",
    "            # also plot the correlation \n",
    "            time_window_start_s=-4 # same setting as in the other analysis pipeline\n",
    "            time_window_end_s=0\n",
    "            coop_window = 1\n",
    "            resolution = 1/fps\n",
    "            #\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                hddm_data = analyze_pull_aligned_data(pull1_data, pull2_data, gaze1_data, gaze2_data, \n",
    "                                                     speed1_data, speed2_data, resolution, time_window_start_s, \n",
    "                                                     time_window_end_s, coop_window)\n",
    "\n",
    "                hddm_data_flexibleTW = analyze_pull_aligned_data_flexibleTW(pull1_data, pull2_data, \n",
    "                                                                           gaze1_data, gaze2_data, \n",
    "                                                                           speed1_data, speed2_data, resolution, \n",
    "                                                                           time_window_start_s, time_window_end_s, \n",
    "                                                                           coop_window)\n",
    "                \n",
    "                hddm_data_flexibleTW_newRT = analyze_pull_aligned_data_flexibleTW_newRTdefinition(pull1_data, pull2_data, \n",
    "                                                                           juice1_data, juice2_data,\n",
    "                                                                           gaze1_data, gaze2_data, \n",
    "                                                                           speed1_data, speed2_data, resolution, \n",
    "                                                                           time_window_start_s, time_window_end_s, \n",
    "                                                                           coop_window)\n",
    "            else:\n",
    "                hddm_data = analyze_pull_aligned_data(pull2_data, pull1_data, gaze2_data, gaze1_data, \n",
    "                                                     speed2_data, speed1_data, resolution, time_window_start_s, \n",
    "                                                     time_window_end_s, coop_window)\n",
    "\n",
    "                hddm_data_flexibleTW = analyze_pull_aligned_data_flexibleTW(pull2_data, pull1_data, \n",
    "                                                                           gaze2_data, gaze1_data, \n",
    "                                                                           speed2_data, speed1_data, resolution, \n",
    "                                                                           time_window_start_s, time_window_end_s, \n",
    "                                                                           coop_window)\n",
    "                \n",
    "                hddm_data_flexibleTW_newRT = analyze_pull_aligned_data_flexibleTW_newRTdefinition(pull2_data, pull1_data, \n",
    "                                                                           juice2_data, juice1_data,\n",
    "                                                                           gaze2_data, gaze1_data, \n",
    "                                                                           speed2_data, speed1_data, resolution, \n",
    "                                                                           time_window_start_s, time_window_end_s, \n",
    "                                                                           coop_window)\n",
    "            \n",
    "            #\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_hddm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(add_date_dir+'/hddm_data.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_data, f)\n",
    "            with open(add_date_dir+'/hddm_data_flexibleTW.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_data_flexibleTW, f)\n",
    "            with open(add_date_dir+'/hddm_data_flexibleTW_newRT.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_data_flexibleTW_newRT, f)\n",
    "      \n",
    "        #    \n",
    "        hddm_datas_all_dates[date_tgt] = hddm_data\n",
    "        hddm_datas_flexibleTW_all_dates[date_tgt] = hddm_data_flexibleTW\n",
    "        hddm_datas_flexibleTW_newRTdefinition_all_dates[date_tgt] = hddm_data_flexibleTW_newRT\n",
    "        \n",
    "        \n",
    "            \n",
    "        # # #     \n",
    "        # load and organize neural related data   \n",
    "        # # # \n",
    "            \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)     \n",
    "        \n",
    "        # \n",
    "        # # load spike sorting results\n",
    "        if 0:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            # spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            #\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)                     \n",
    "            \n",
    "            #\n",
    "            # Run PCA analysis\n",
    "            FR_zscore_allch_np_merged = np.array(pd.DataFrame(FR_zscore_allch).T)\n",
    "            FR_zscore_allch_np_merged = FR_zscore_allch_np_merged[~np.isnan(np.sum(FR_zscore_allch_np_merged,axis=1)),:]\n",
    "            # # run PCA on the entire session\n",
    "            pca = PCA(n_components=3)\n",
    "            FR_zscore_allch_PCs = pca.fit_transform(FR_zscore_allch_np_merged.T)\n",
    "           \n",
    "            \n",
    " \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/hddm_datas_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(hddm_datas_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/hddm_datas_flexibleTW_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(hddm_datas_flexibleTW_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/hddm_datas_flexibleTW_newRTdefinition_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(hddm_datas_flexibleTW_newRTdefinition_all_dates, f) \n",
    "            \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/hddm_datas_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(hddm_datas_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/hddm_datas_flexibleTW_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(hddm_datas_flexibleTW_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/hddm_datas_flexibleTW_newRTdefinition_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(hddm_datas_flexibleTW_newRTdefinition_all_dates, f) \n",
    "     \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664566b1",
   "metadata": {},
   "source": [
    "### fit the HDDM model for all the data, copy this part and following to the Misha later when HDDM pipeline is installed successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# animal_tgt = 'kanga' # 'kanga','kanga_partner','dodson','dodson_partner'\n",
    "animal_tgt = 'dodson'\n",
    "\n",
    "#\n",
    "# conditions_to_ana = np.unique(task_conditions)\n",
    "###\n",
    "# For Kanga\n",
    "# conditions_to_ana = [ 'MC_withVermelho']\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVermelho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "\n",
    "# cond_toplot_type = 'allSR'\n",
    "cond_toplot_type = 'allMC'\n",
    "# cond_toplot_type =  'MC_withVermelho'\n",
    "\n",
    "# make sure to only look certain condition, and keep things consistent\n",
    "ind_tgt = list(np.isin(task_conditions,conditions_to_ana))\n",
    "\n",
    "animal1_fixedorders_toana = list(np.array(animal1_fixedorders)[ind_tgt])\n",
    "animal2_fixedorders_toana = list(np.array(animal2_fixedorders)[ind_tgt])\n",
    "task_conditions_toana = list(np.array(task_conditions)[ind_tgt])\n",
    "\n",
    "dates_to_ana = list(np.array(dates_list)[ind_tgt])\n",
    "ndates_to_ana = np.shape(dates_to_ana)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f69569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the hddm fitting for all sessions separately\n",
    "# not in use because the reaction time definition is outdated\n",
    "if 0:\n",
    "\n",
    "    dofixTW = 0 # 0: not do the fixed TW, instead use the flexible TW (from one pull to another); 1: do the fixed tw (-4s to 0s)\n",
    "    doflexTW_newRT = 1 # 1: do the flexible TW with new definition of RT\n",
    "    #\n",
    "    if dofixTW:\n",
    "        fixTW_sufix = '_fixedTW'\n",
    "    elif not dofixTW:\n",
    "        if not doflexTW_newRT:\n",
    "            fixTW_sufix = ''\n",
    "        elif doflexTW_newRT:\n",
    "            fixTW_sufix = '_flexTW_newRT'\n",
    "    \n",
    "    # HDDM settings - the basic setting\n",
    "    samples=250 # 2000\n",
    "    burn=100\n",
    "    thin=1\n",
    "    \n",
    "    # keep the regression r and p between gaze_accum and diffusion slope v\n",
    "    corr_gazeaccum_driftv = np.ones((ndates_to_ana,))*np.nan\n",
    "    p_gazeaccum_driftv = np.ones((ndates_to_ana,))*np.nan\n",
    "\n",
    "    for idate in np.arange(0,ndates_to_ana,1):\n",
    "    # for idate in np.arange(3,4,1):\n",
    "\n",
    "        date_toana = dates_to_ana[idate]\n",
    "        \n",
    "        task_condition_toana = task_conditions_toana[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders_toana[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders_toana[idate]]\n",
    "\n",
    "        try:\n",
    "            # try to load the data first\n",
    "            try:\n",
    "                current_dir = data_saved_folder+'bhv_events_singlecam_wholebody_with_hddm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_toana)\n",
    "                #\n",
    "                with open(add_date_dir+'/hddm_model_fitted_traces'+fixTW_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_traces = pickle.load(f)\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_traces'+fixTW_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "                \n",
    "                with open(add_date_dir+'/hddm_model_fitted_stats'+fixTW_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_stats = pickle.load(f)\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_stats'+fixTW_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "                    \n",
    "                with open(add_date_dir+'/hddm_model_fitted_dic'+fixTW_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_dic = pickle.load(f)\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_dic'+fixTW_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_nogaze_dic = pickle.load(f)\n",
    "                    \n",
    "                if dofixTW:\n",
    "                    with open(add_date_dir+'/hddm_data.pkl', 'rb') as f:\n",
    "                        hddm_data_tgt_idate = pickle.load(f)\n",
    "                elif not dofixTW:\n",
    "                    if not doflexTW_newRT:\n",
    "                        with open(add_date_dir+'/hddm_data_flexibleTW.pkl', 'rb') as f:\n",
    "                            hddm_data_tgt_idate = pickle.load(f)\n",
    "                    elif doflexTW_newRT:\n",
    "                        with open(add_date_dir+'/hddm_data_flexibleTW_newRT.pkl', 'rb') as f:\n",
    "                            hddm_data_tgt_idate = pickle.load(f)\n",
    "                #\n",
    "                hddm_data_tgt_idate = hddm_data_tgt_idate['hddm_data_'+animal_id_toana]\n",
    "                                                         \n",
    "                        \n",
    "                print(animal_tgt+' '+date_toana+' hddm fitting results loaded')\n",
    "\n",
    "            # do the hddm model fitting\n",
    "            except:\n",
    "\n",
    "                print(animal_tgt+' '+date_toana+' running hddm fitting')\n",
    "\n",
    "                if np.isin(animal_tgt,animal1_fixedorders[idate]):\n",
    "                    animal_id_toana = 'animal1'\n",
    "                elif np.isin(animal_tgt,animal2_fixedorders[idate]):\n",
    "                    animal_id_toana = 'animal2'\n",
    "\n",
    "                # # use the fixed -4s to 0s time window\n",
    "                if dofixTW:\n",
    "                    hddm_data = hddm_datas_all_dates[date_toana]\n",
    "                # use the flexible time window setting\n",
    "                elif not dofixTW:\n",
    "                    hddm_data = hddm_datas_flexibleTW_all_dates[date_toana]\n",
    "                    \n",
    "                hddm_data_tgt_idate = hddm_data['hddm_data_'+animal_id_toana]\n",
    "\n",
    "                    \n",
    "                # run the hddm nodel - to fix the problem, use a lazy methods\n",
    "                try:\n",
    "                    hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_idate, \n",
    "                                                                                animal_id_toana, samples, burn, thin)\n",
    "                except:\n",
    "                    samples=500 # 2000\n",
    "                    burn=100\n",
    "                    thin=2  \n",
    "                    hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_idate, \n",
    "                                                                                animal_id_toana, samples, burn, thin)\n",
    "\n",
    "                #\n",
    "                hddm_model_fitted_traces = hddm_model_fitted.get_traces()\n",
    "                hddm_model_fitted_stats  = hddm_model_fitted.gen_stats()\n",
    "                hddm_model_fitted_dic = hddm_model_fitted.dic_info\n",
    "                #\n",
    "                hddm_model_fitted_nogaze_traces = hddm_model_fitted_nogaze.get_traces()\n",
    "                hddm_model_fitted_nogaze_stats  = hddm_model_fitted_nogaze.gen_stats()\n",
    "                hddm_model_fitted_nogaze_dic = hddm_model_fitted_nogaze.dic_info\n",
    "\n",
    "                # save data\n",
    "                if 1:\n",
    "                    current_dir = data_saved_folder+'bhv_events_singlecam_wholebody_with_hddm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                    add_date_dir = os.path.join(current_dir,cameraID+'/'+date_toana)\n",
    "                    if not os.path.exists(add_date_dir):\n",
    "                        os.makedirs(add_date_dir)\n",
    "                    #\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_traces'+fixTW_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_traces, f)\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_nogaze_traces'+fixTW_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_nogaze_traces, f)\n",
    "                    \n",
    "                    with open(add_date_dir+'/hddm_model_fitted_stats'+fixTW_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_stats, f)\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_nogaze_stats'+fixTW_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_nogaze_stats, f)\n",
    "                        \n",
    "                    with open(add_date_dir+'/hddm_model_fitted_dic'+fixTW_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_dic, f)\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_nogaze_dic'+fixTW_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_nogaze_dic, f)\n",
    "                        \n",
    "                        \n",
    "            # do some plotting\n",
    "            if 0:\n",
    "                df_combined = hddm_data_tgt_idate\n",
    "                #\n",
    "                # traces = hddm_model_fitted_traces\n",
    "                traces = hddm_model_fitted_nogaze_traces\n",
    "                #\n",
    "                # Get the mean value for the intercept\n",
    "                v_intercept_mean = traces['v_Intercept'].mean()\n",
    "                #\n",
    "                # Get the mean value for the coefficient of each predictor\n",
    "                # v_gaze_coef_mean = traces['v_self_gaze_auc'].mean()\n",
    "                v_speed_coef_mean = traces['v_partner_mean_speed'].mean()\n",
    "                v_failed_pulls_before_reward_mean = traces['v_failed_pulls_before_reward'].mean()\n",
    "                v_time_since_last_reward_mean = traces['v_time_since_last_reward'].mean()\n",
    "                #\n",
    "                print(f\"Mean v_Intercept: {v_intercept_mean:.3f}\")\n",
    "                # print(f\"Mean v_self_gaze_auc Coef: {v_gaze_coef_mean:.3f}\")\n",
    "                print(f\"Mean v_partner_mean_speed Coef: {v_speed_coef_mean:.3f}\")\n",
    "                print(f\"Mean v_failed_pulls_before_reward Coef: {v_failed_pulls_before_reward_mean:.3f}\")\n",
    "                print(f\"Mean v_time_since_last_reward Coef: {v_time_since_last_reward_mean:.3f}\")\n",
    "                #\n",
    "                # --- STEP 2: Apply the regression equation to your dataframe ---\n",
    "                # This calculates the predicted 'v' for each trial based on its unique covariate values.\n",
    "                df_with_v = df_combined.copy() # Work with a copy\n",
    "                #\n",
    "                df_with_v['predicted_v'] = (v_intercept_mean +\n",
    "                                              # (v_gaze_coef_mean * df_with_v['self_gaze_auc']) +\n",
    "                                              (v_speed_coef_mean * df_with_v['partner_mean_speed'])+\n",
    "                                              (v_failed_pulls_before_reward_mean * df_with_v['failed_pulls_before_reward'])+\n",
    "                                              (v_time_since_last_reward_mean * df_with_v['time_since_last_reward']))\n",
    "\n",
    "                #!!! Add some criteria for plotting\n",
    "                # ind_good1 = (df_with_v['self_gaze_auc']>=0) & (df_with_v['self_gaze_auc']<=1)\n",
    "                # ind_good2 = df_with_v['predicted_v']<3\n",
    "                # df_with_v = df_with_v[ind_good1 & ind_good2]\n",
    "                #\n",
    "                fig, r_vals, p_vals = do_hddm_model_fitted_plot(df_with_v)\n",
    "                #\n",
    "                corr_gazeaccum_driftv[idate] = r_vals\n",
    "                p_gazeaccum_driftv[idate] = p_vals\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff994e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the target sessions together, then run the hddm fitting\n",
    "# not in use because the reaction time definition is outdated\n",
    "\n",
    "if 0:\n",
    "\n",
    "    dofixTW = 0 # 0: not do the fixed TW, instead use the flexible TW (from one pull to another); 1: do the fixed tw (-4s to 0s)\n",
    "    doflexTW_newRT = 1 # 1: do the flexible TW with new definition of RT\n",
    "    #\n",
    "    if dofixTW:\n",
    "        fixTW_sufix = '_fixedTW'\n",
    "    elif not dofixTW:\n",
    "        if not doflexTW_newRT:\n",
    "            fixTW_sufix = ''\n",
    "        elif doflexTW_newRT:\n",
    "            fixTW_sufix = '_flexTW_newRT'\n",
    "        \n",
    "    # only run the noselfgaze hddm fitting\n",
    "    doNogazeOnly = 0\n",
    "    \n",
    "    # use more variables for v a and z\n",
    "    doExaustModel = 0\n",
    "    if doExaustModel:\n",
    "        exaustModel_sufix = '_exaustModel'\n",
    "    elif not doExaustModel:\n",
    "        exaustModel_sufix = ''\n",
    "        \n",
    "        \n",
    "    # try to load the data first\n",
    "    try:\n",
    "        \n",
    "        # dummpy\n",
    "        \n",
    "        current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "        add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_combinedsession')\n",
    "        #\n",
    "        if not doNogazeOnly:\n",
    "            with open(add_date_dir+'/hddm_model_fitted_traces_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_traces = pickle.load(f)\n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_stats_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_stats = pickle.load(f)\n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_dic_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_dic = pickle.load(f)\n",
    "        #\n",
    "        with open(add_date_dir+'/hddm_model_fitted_nogaze_traces_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "        \n",
    "        with open(add_date_dir+'/hddm_model_fitted_nogaze_stats_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "        \n",
    "        with open(add_date_dir+'/hddm_model_fitted_nogaze_dic_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_model_fitted_nogaze_dic = pickle.load(f)\n",
    "            \n",
    "        with open(add_date_dir+'/hddm_data_tgt_alldates_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_data_tgt_alldates = pickle.load(f)\n",
    "\n",
    "        print(animal_tgt+' '+cond_toplot_type+' hddm fitting results loaded')\n",
    "\n",
    "    # do the hddm model fitting    \n",
    "    except:\n",
    "        print(animal_tgt+' '+cond_toplot_type+' running hddm fitting')\n",
    "        \n",
    "        for idate in np.arange(0,ndates_to_ana,1):\n",
    "        # for idate in np.arange(3,4,1):\n",
    "\n",
    "            date_toana = dates_to_ana[idate]\n",
    "            \n",
    "            task_condition_toana = task_conditions_toana[idate]\n",
    "\n",
    "            animal1_fixedorder = [animal1_fixedorders_toana[idate]]\n",
    "            animal2_fixedorder = [animal2_fixedorders_toana[idate]]\n",
    "    \n",
    "\n",
    "            if np.isin(animal_tgt,animal1_fixedorders[idate]):\n",
    "                animal_id_toana = 'animal1'\n",
    "            elif np.isin(animal_tgt,animal2_fixedorders[idate]):\n",
    "                animal_id_toana = 'animal2'\n",
    "\n",
    "            # # use the fixed -4s to 0s time window\n",
    "            if dofixTW:\n",
    "                hddm_data = hddm_datas_all_dates[date_toana]\n",
    "            # us the flexible time window setting\n",
    "            elif not dofixTW:\n",
    "                if not doflexTW_newRT:\n",
    "                    hddm_data = hddm_datas_flexibleTW_all_dates[date_toana]\n",
    "                # use the new RT definition\n",
    "                if doflexTW_newRT:\n",
    "                    hddm_data = hddm_datas_flexibleTW_newRTdefinition_all_dates[date_toana]\n",
    "\n",
    "            hddm_data_tgt_idate = hddm_data['hddm_data_'+animal_id_toana]\n",
    "            \n",
    "            # add a date column and task condition column\n",
    "            hddm_data_tgt_idate['date'] = date_toana\n",
    "            hddm_data_tgt_idate['condition'] = task_condition_toana       \n",
    "            \n",
    "            #\n",
    "            if idate == 0:\n",
    "                hddm_data_tgt_alldates = hddm_data_tgt_idate\n",
    "            else:\n",
    "                hddm_data_tgt_alldates = pd.concat([hddm_data_tgt_alldates, hddm_data_tgt_idate])\n",
    "              \n",
    "            \n",
    "        # # add some semi-artbitrary cretieras\n",
    "        # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['prev_trial_outcome']==1]\n",
    "        # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['partner_mean_speed']<=300]\n",
    "        #\n",
    "        lower_q = 0.05\n",
    "        upper_q = 0.95\n",
    "        rt_lower, rt_upper = hddm_data_tgt_alldates['rt'].quantile([lower_q, upper_q])\n",
    "        hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']>rt_lower]\n",
    "        hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']<rt_upper]\n",
    "        #\n",
    "        tslr_lower, tslr_upper = hddm_data_tgt_alldates['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "        hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']>tslr_lower]\n",
    "        hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']<tslr_upper]\n",
    "            \n",
    "        # \n",
    "        # remove nan row first\n",
    "        hddm_data_tgt_alldates = hddm_data_tgt_alldates.dropna()\n",
    "        \n",
    "        # \n",
    "        # normalize all the predictors\n",
    "        from scipy.stats import zscore\n",
    "        #\n",
    "        # List of predictor columns to normalize (exclude 'rt' and identifiers)\n",
    "        predictor_columns = [\n",
    "            'self_gaze_auc',\n",
    "            'partner_mean_speed',\n",
    "            'self_mean_speed',\n",
    "            'partner_speed_std',\n",
    "            'self_speed_std',\n",
    "            'failed_pulls_before_reward',\n",
    "            'time_since_last_reward'\n",
    "        ]\n",
    "        # Rename originals and replace with z-scored values\n",
    "        for col in predictor_columns:\n",
    "            hddm_data_tgt_alldates[f\"{col}_origin\"] = hddm_data_tgt_alldates[col]\n",
    "            hddm_data_tgt_alldates[col] = zscore(hddm_data_tgt_alldates[col])  \n",
    "            \n",
    "            \n",
    "        # run the hddm nodel\n",
    "        if not doExaustModel:\n",
    "            # HDDM settings - the basic setting\n",
    "            # samples=500 # 2000\n",
    "            # burn=100\n",
    "            # thin=2\n",
    "            samples=1000 # 2000\n",
    "            burn=100\n",
    "            thin=2\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_alldates, animal_id_toana,\n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "            elif doNogazeOnly:\n",
    "                _, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_alldates, animal_id_toana,\n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "                \n",
    "        elif doExaustModel:\n",
    "            # samples=2000 # 2000\n",
    "            # burn=1000\n",
    "            # thin=4\n",
    "            samples=200 # 2000\n",
    "            burn=100\n",
    "            thin=2\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling_exaustModel(hddm_data_tgt_alldates, animal_id_toana, \n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "            elif doNogazeOnly:\n",
    "                _, hddm_model_fitted_nogaze = run_hddm_modeling_exaustModel(hddm_data_tgt_alldates, animal_id_toana,\n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "\n",
    "        #\n",
    "        if not doNogazeOnly:\n",
    "            hddm_model_fitted_traces = hddm_model_fitted.get_traces()\n",
    "            hddm_model_fitted_stats  = hddm_model_fitted.gen_stats()\n",
    "            hddm_model_fitted_dic = hddm_model_fitted.dic_info\n",
    "        #\n",
    "        hddm_model_fitted_nogaze_traces = hddm_model_fitted_nogaze.get_traces()\n",
    "        hddm_model_fitted_nogaze_stats  = hddm_model_fitted_nogaze.gen_stats()\n",
    "        hddm_model_fitted_nogaze_dic = hddm_model_fitted_nogaze.dic_info\n",
    "\n",
    "        # save data\n",
    "        if 1:\n",
    "            current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "            add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_combinedsession')\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                with open(add_date_dir+'/hddm_model_fitted_traces_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_traces, f)\n",
    "            \n",
    "                with open(add_date_dir+'/hddm_model_fitted_stats_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_stats, f)\n",
    "               \n",
    "                with open(add_date_dir+'/hddm_model_fitted_dic_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_dic, f)\n",
    "            \n",
    "            #\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_traces_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_model_fitted_nogaze_traces, f)\n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_stats_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_model_fitted_nogaze_stats, f)  \n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_dic_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_model_fitted_nogaze_dic, f)\n",
    "\n",
    "            with open(add_date_dir+'/hddm_data_tgt_alldates_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_data_tgt_alldates, f)\n",
    "      \n",
    "    \n",
    "    #\n",
    "    # do some plotting\n",
    "    if 1:\n",
    "        df_combined = hddm_data_tgt_alldates  # your combined trial-level dataframe\n",
    "        traces = hddm_model_fitted_nogaze_traces  # or use `hddm_model_fitted_traces` if needed\n",
    "        stats = hddm_model_fitted_nogaze_stats\n",
    "\n",
    "        # --- STEP 1: Use all posterior samples to compute predicted v for each trial ---\n",
    "        n_samples = len(traces)\n",
    "        n_trials = len(df_combined)\n",
    "\n",
    "        # Allocate space to hold predicted v for each sample x trial\n",
    "        predicted_v_samples = np.zeros((n_samples, n_trials))\n",
    "\n",
    "        # Pre-fetch covariate values as numpy arrays\n",
    "        x1 = df_combined['partner_mean_speed'].values\n",
    "        x2 = df_combined['partner_speed_std'].values\n",
    "        x3 = df_combined['self_mean_speed'].values\n",
    "        x4 = df_combined['self_speed_std'].values\n",
    "        # x3 = df_combined['failed_pulls_before_reward'].values\n",
    "        # x3 = df_combined['time_since_last_reward'].values\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            intercept = traces['v_Intercept'].values[i]\n",
    "            coef_x1 = traces['v_partner_mean_speed'].values[i]\n",
    "            coef_x2 = traces['v_partner_speed_std'].values[i]\n",
    "            coef_x3 = traces['v_self_mean_speed'].values[i]\n",
    "            coef_x4 = traces['v_self_speed_std'].values[i]\n",
    "            # coef_x3 = traces['v_failed_pulls_before_reward'].values[i]\n",
    "            # coef_x3 = traces['v_time_since_last_reward'].values[i]\n",
    "\n",
    "            # Apply regression formula for all trials\n",
    "            predicted_v_samples[i, :] = (\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3\n",
    "                # intercept + coef_x1 * x1 \n",
    "                # intercept + coef_x1 * x1 + coef_x3 * x3\n",
    "                intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4\n",
    "                \n",
    "                \n",
    "            )\n",
    "\n",
    "        # --- STEP 2: Compute posterior mean (and optionally CI) of predicted v ---\n",
    "        df_with_v = df_combined.copy()\n",
    "        df_with_v['predicted_v'] = predicted_v_samples.mean(axis=0)\n",
    "\n",
    "        # Optional: also store credible intervals (e.g., 95% CI)\n",
    "        df_with_v['predicted_v_lower'] = np.percentile(predicted_v_samples, 2.5, axis=0)\n",
    "        df_with_v['predicted_v_upper'] = np.percentile(predicted_v_samples, 97.5, axis=0)        \n",
    "        \n",
    "        \n",
    "        # df_with_v['self_gaze_auc'] = df_with_v['self_gaze_auc'] * df_with_v['rt']\n",
    "        \n",
    "        #\n",
    "        #!!! Add some criteria for plotting\n",
    "        #\n",
    "        # remove dodson with gingerNew and Koala\n",
    "        # ind_bad = (df_with_v['condition']=='MC_withGingerNew') | (df_with_v['condition']=='MC_withKoala')\n",
    "        # df_with_v = df_with_v[~ind_bad]\n",
    "        \n",
    "        \n",
    "        # Define quantile thresholds\n",
    "        lower_q = 0.05\n",
    "        upper_q = 0.95\n",
    "        # Compute quantile-based limits\n",
    "        gaze_lower, gaze_upper = df_with_v['self_gaze_auc'].quantile([lower_q, upper_q])\n",
    "        v_lower, v_upper = df_with_v['predicted_v'].quantile([lower_q, upper_q])\n",
    "        # Filter out outliers\n",
    "        ind_good = (\n",
    "            # (df_with_v['self_gaze_auc'] >= gaze_lower) & (df_with_v['self_gaze_auc'] <= gaze_upper) &\n",
    "            (df_with_v['predicted_v'] >= v_lower) & (df_with_v['predicted_v'] <= v_upper)\n",
    "        )\n",
    "        #\n",
    "        # df_with_v = df_with_v[ind_good]\n",
    "        \n",
    "        fig, r_vals, p_vals = do_hddm_model_fitted_plot(df_with_v)\n",
    "        \n",
    "        # savefig\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            current_dir = data_saved_folder+'fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_hddm_model/'+animal_tgt\n",
    "            add_fig_dir = os.path.join(current_dir,cameraID+'/fig_for_hddm_model_fitted_combinedsession')\n",
    "            #\n",
    "            if not os.path.exists(add_fig_dir):\n",
    "                os.makedirs(add_fig_dir)\n",
    "            \n",
    "            fig.savefig(add_fig_dir+'/gaze_accum_vs_driftslopev_'+cond_toplot_type+fixTW_sufix+exaustModel_sufix+'.pdf', \n",
    "                        format='pdf', bbox_inches='tight')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        # do the plotting separating different conditions (different pairs)\n",
    "        conds_toplot = np.unique(df_with_v['condition'])\n",
    "        nconds_toplot = np.shape(conds_toplot)[0]\n",
    "        \n",
    "        for icond_toplot in np.arange(0,nconds_toplot,1):\n",
    "            \n",
    "            ind_toplot = df_with_v['condition']==conds_toplot[icond_toplot]\n",
    "            \n",
    "            fig_i, r_vals_i, p_vals_i = do_hddm_model_fitted_plot(df_with_v[ind_toplot])\n",
    "            \n",
    "            ax = fig_i.axes[0]\n",
    "            ax.set_title(conds_toplot[icond_toplot]+\n",
    "                        ' Deming Regression: Self-Gaze vs Predicted Drift Rate (v)', fontsize=14)\n",
    "            \n",
    "            # savefig\n",
    "            savefig = 1\n",
    "            if savefig:\n",
    "                current_dir = data_saved_folder+'fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_hddm_model'+savefile_sufix+'/'+animal_tgt\n",
    "                add_fig_dir = os.path.join(current_dir,cameraID+'/fig_for_hddm_model_fitted_combinedsession')\n",
    "                #\n",
    "                if not os.path.exists(add_fig_dir):\n",
    "                    os.makedirs(add_fig_dir)\n",
    "\n",
    "                fig_i.savefig(add_fig_dir+'/gaze_accum_vs_driftslopev_'+conds_toplot[icond_toplot]+fixTW_sufix+exaustModel_sufix+'.pdf', \n",
    "                            format='pdf', bbox_inches='tight')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b159e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the hddm fitting using the data set that contained average neuron activity (across neurons for each trial)\n",
    "# the data set is calculated in Misha - basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv\n",
    "\n",
    "if 0:\n",
    "    \n",
    "    # In this data set all the rt is calculated using the new definition, but the pull onset is defined slightly different\n",
    "    #\n",
    "    # if use onset of the first increase after min\n",
    "    doOnsetAfterMin = 1\n",
    "    if not doOnsetAfterMin:\n",
    "        doOnsetAfterMin_suffix = ''\n",
    "    elif doOnsetAfterMin:\n",
    "        doOnsetAfterMin_suffix = 'PullOnsetAfterMin_'\n",
    "\n",
    "    # if use a hmm based method to find the trial start\n",
    "    doHMMmethod = 0\n",
    "    if doHMMmethod:\n",
    "        doOnsetAfterMin_suffix = 'HMMmethods_'\n",
    "        \n",
    "        \n",
    "    # only run the noselfgaze hddm fitting\n",
    "    doNogazeOnly = 0\n",
    "    \n",
    "    # use more variables for v a and z\n",
    "    doExaustModel = 0\n",
    "    if doExaustModel:\n",
    "        exaustModel_sufix = '_exaustModel'\n",
    "    elif not doExaustModel:\n",
    "        exaustModel_sufix = ''\n",
    "        \n",
    "    # load the dataset first\n",
    "    current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "    add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_combinedsession_withNeurons')\n",
    "    \n",
    "    with open(add_date_dir+'/hddm_datas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\n",
    "              animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        hddm_data_tgt_alldates = pickle.load(f)\n",
    "\n",
    "    # make the hddm_data structure consistent with other code\n",
    "    ind_goodcond = np.isin(hddm_data_tgt_alldates['condition'],conditions_to_ana)\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[ind_goodcond]\n",
    "\n",
    "    rename_dict = {\n",
    "    'act_animal':'subj_idx',\n",
    "    'dates': 'date',\n",
    "    'pull_rt': 'rt',\n",
    "    'previous_pull_outcome': 'prev_trial_outcome',\n",
    "    'socialgaze_auc': 'self_gaze_auc',\n",
    "    'mass_move_speed_mean': 'self_mean_speed',\n",
    "    'mass_move_speed_std': 'self_speed_std',\n",
    "    'other_mass_move_speed_mean': 'partner_mean_speed',\n",
    "    'other_mass_move_speed_std': 'partner_speed_std',\n",
    "    'num_preceding_failpull': 'failed_pulls_before_reward',\n",
    "    'time_from_last_reward': 'time_since_last_reward',\n",
    "    }\n",
    "    # Apply the renaming\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates.rename(columns=rename_dict)\n",
    "    hddm_data_tgt_alldates['response'] = 1\n",
    "\n",
    "    # \n",
    "    # remove nan row first\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates.dropna()\n",
    "    \n",
    "    #\n",
    "    # # add some semi-artbitrary cretieras\n",
    "    # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['prev_trial_outcome']==1]\n",
    "    # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['partner_mean_speed']<=300]\n",
    "    #\n",
    "    lower_q = 0.05\n",
    "    upper_q = 0.95\n",
    "    rt_lower, rt_upper = hddm_data_tgt_alldates['rt'].quantile([lower_q, upper_q])\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']>rt_lower]\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']<rt_upper]\n",
    "    #\n",
    "    tslr_lower, tslr_upper = hddm_data_tgt_alldates['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']>tslr_lower]\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']<tslr_upper]\n",
    "\n",
    "    # \n",
    "    # normalize all the predictors\n",
    "    from scipy.stats import zscore\n",
    "    #\n",
    "    # List of predictor columns to normalize (exclude 'rt' and identifiers)\n",
    "    predictor_columns = [\n",
    "        'self_gaze_auc',\n",
    "        'partner_mean_speed',\n",
    "        'self_mean_speed',\n",
    "        'partner_speed_std',\n",
    "        'self_speed_std',\n",
    "        'failed_pulls_before_reward',\n",
    "        'time_since_last_reward'\n",
    "    ]\n",
    "    # Rename originals and replace with z-scored values\n",
    "    for col in predictor_columns:\n",
    "        hddm_data_tgt_alldates[f\"{col}_origin\"] = hddm_data_tgt_alldates[col]\n",
    "        hddm_data_tgt_alldates[col] = zscore(hddm_data_tgt_alldates[col])  \n",
    "        \n",
    "    # try to load the analyzed\n",
    "    try:\n",
    "        \n",
    "        # dummpy\n",
    "        #\n",
    "        if not doNogazeOnly:\n",
    "            with open(add_date_dir+'/hddm_model_fitted_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_traces = pickle.load(f)\n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_stats = pickle.load(f)\n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_dic = pickle.load(f)\n",
    "        #\n",
    "        with open(add_date_dir+'/hddm_model_fitted_nogaze_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "        \n",
    "        with open(add_date_dir+'/hddm_model_fitted_nogaze_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "        \n",
    "        with open(add_date_dir+'/hddm_model_fitted_nogaze_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "            hddm_model_fitted_nogaze_dic = pickle.load(f)\n",
    "            \n",
    "        print(animal_tgt+' '+cond_toplot_type+' hddm fitting results loaded')\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(animal_tgt+' '+cond_toplot_type+' running hddm fitting')\n",
    "            \n",
    "        # run the hddm nodel\n",
    "        if not doExaustModel:\n",
    "            # HDDM settings - the basic setting\n",
    "            # samples=500 # 2000\n",
    "            # burn=100\n",
    "            # thin=2\n",
    "            samples=200 # 2000\n",
    "            burn=100\n",
    "            thin=2\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_alldates, animal_tgt,\n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "            elif doNogazeOnly:\n",
    "                _, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_alldates, animal_tgt,\n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "                \n",
    "        elif doExaustModel:\n",
    "            # samples=2000 # 2000\n",
    "            # burn=1000\n",
    "            # thin=4\n",
    "            samples=200 # 2000\n",
    "            burn=100\n",
    "            thin=2\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling_exaustModel(hddm_data_tgt_alldates, animal_tgt, \n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "            elif doNogazeOnly:\n",
    "                _, hddm_model_fitted_nogaze = run_hddm_modeling_exaustModel(hddm_data_tgt_alldates, animal_tgt,\n",
    "                                                                            samples, burn, thin, doNogazeOnly)\n",
    "\n",
    "        #\n",
    "        if not doNogazeOnly:\n",
    "            hddm_model_fitted_traces = hddm_model_fitted.get_traces()\n",
    "            hddm_model_fitted_stats  = hddm_model_fitted.gen_stats()\n",
    "            hddm_model_fitted_dic = hddm_model_fitted.dic_info\n",
    "        #\n",
    "        hddm_model_fitted_nogaze_traces = hddm_model_fitted_nogaze.get_traces()\n",
    "        hddm_model_fitted_nogaze_stats  = hddm_model_fitted_nogaze.gen_stats()\n",
    "        hddm_model_fitted_nogaze_dic = hddm_model_fitted_nogaze.dic_info\n",
    "\n",
    "        # save data\n",
    "        if 1:\n",
    "            current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "            add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_combinedsession_withNeurons')\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                with open(add_date_dir+'/hddm_model_fitted_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_traces, f)\n",
    "            \n",
    "                with open(add_date_dir+'/hddm_model_fitted_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_stats, f)\n",
    "               \n",
    "                with open(add_date_dir+'/hddm_model_fitted_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_dic, f)\n",
    "            \n",
    "            #\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_model_fitted_nogaze_traces, f)\n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_model_fitted_nogaze_stats, f)  \n",
    "            \n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                pickle.dump(hddm_model_fitted_nogaze_dic, f)\n",
    "            \n",
    "          \n",
    "        #\n",
    "    # do some plotting\n",
    "    if 1:\n",
    "        df_combined = hddm_data_tgt_alldates  # your combined trial-level dataframe\n",
    "        # traces = hddm_model_fitted_nogaze_traces  # or use `hddm_model_fitted_traces` if needed\n",
    "        # stats = hddm_model_fitted_nogaze_stats\n",
    "        traces = hddm_model_fitted_traces  # or use `hddm_model_fitted_traces` if needed\n",
    "        stats = hddm_model_fitted_stats\n",
    "\n",
    "        # --- STEP 1: Use all posterior samples to compute predicted v for each trial ---\n",
    "        n_samples = len(traces)\n",
    "        n_trials = len(df_combined)\n",
    "\n",
    "        # Allocate space to hold predicted v for each sample x trial\n",
    "        predicted_v_samples = np.zeros((n_samples, n_trials))\n",
    "\n",
    "        # Pre-fetch covariate values as numpy arrays\n",
    "        x1 = df_combined['partner_mean_speed'].values\n",
    "        x2 = df_combined['partner_speed_std'].values\n",
    "        x3 = df_combined['self_mean_speed'].values\n",
    "        x4 = df_combined['self_speed_std'].values\n",
    "        # x3 = df_combined['failed_pulls_before_reward'].values\n",
    "        # x3 = df_combined['time_since_last_reward'].values\n",
    "        x7 = df_combined['self_gaze_auc'].values\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            intercept = traces['v_Intercept'].values[i]\n",
    "            coef_x1 = traces['v_partner_mean_speed'].values[i]\n",
    "            coef_x2 = traces['v_partner_speed_std'].values[i]\n",
    "            coef_x3 = traces['v_self_mean_speed'].values[i]\n",
    "            coef_x4 = traces['v_self_speed_std'].values[i]\n",
    "            # coef_x3 = traces['v_failed_pulls_before_reward'].values[i]\n",
    "            # coef_x3 = traces['v_time_since_last_reward'].values[i]\n",
    "            coef_x7 = traces['v_self_gaze_auc'].values[i]\n",
    "\n",
    "            # Apply regression formula for all trials\n",
    "            predicted_v_samples[i, :] = (\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3\n",
    "                # intercept + coef_x1 * x1 \n",
    "                # intercept + coef_x1 * x1 + coef_x3 * x3\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4\n",
    "                intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4 + coef_x7 * x7\n",
    "            )\n",
    "\n",
    "        # --- STEP 2: Compute posterior mean (and optionally CI) of predicted v ---\n",
    "        df_with_v = df_combined.copy()\n",
    "        df_with_v['predicted_v'] = predicted_v_samples.mean(axis=0)\n",
    "\n",
    "        # Optional: also store credible intervals (e.g., 95% CI)\n",
    "        df_with_v['predicted_v_lower'] = np.percentile(predicted_v_samples, 2.5, axis=0)\n",
    "        df_with_v['predicted_v_upper'] = np.percentile(predicted_v_samples, 97.5, axis=0)        \n",
    "        \n",
    "        \n",
    "        # df_with_v['self_gaze_auc'] = df_with_v['self_gaze_auc'] * df_with_v['rt']\n",
    "        \n",
    "        #\n",
    "        #!!! Add some criteria for plotting\n",
    "        #\n",
    "        # remove dodson with gingerNew and Koala\n",
    "        # ind_bad = (df_with_v['condition']=='MC_withGingerNew') | (df_with_v['condition']=='MC_withKoala')\n",
    "        # df_with_v = df_with_v[~ind_bad]\n",
    "        \n",
    "        \n",
    "        # Define quantile thresholds\n",
    "        lower_q = 0.05\n",
    "        upper_q = 0.95\n",
    "        # Compute quantile-based limits\n",
    "        gaze_lower, gaze_upper = df_with_v['self_gaze_auc'].quantile([lower_q, upper_q])\n",
    "        v_lower, v_upper = df_with_v['predicted_v'].quantile([lower_q, upper_q])\n",
    "        # Filter out outliers\n",
    "        ind_good = (\n",
    "            # (df_with_v['self_gaze_auc'] >= gaze_lower) & (df_with_v['self_gaze_auc'] <= gaze_upper) &\n",
    "            (df_with_v['predicted_v'] >= v_lower) & (df_with_v['predicted_v'] <= v_upper)\n",
    "        )\n",
    "        #\n",
    "        # df_with_v = df_with_v[ind_good]\n",
    "        \n",
    "        fig, r_vals, p_vals = do_hddm_model_fitted_plot(df_with_v)\n",
    "        \n",
    "        # savefig\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            current_dir = data_saved_folder+'fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_hddm_model/'+animal_tgt\n",
    "            add_fig_dir = os.path.join(current_dir,cameraID+'/fig_for_hddm_model_fitted_combinedsession_withNeurons')\n",
    "            #\n",
    "            if not os.path.exists(add_fig_dir):\n",
    "                os.makedirs(add_fig_dir)\n",
    "            \n",
    "            fig.savefig(add_fig_dir+'/gaze_accum_vs_driftslopev_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pdf', \n",
    "                        format='pdf', bbox_inches='tight')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        # do the plotting separating different conditions (different pairs)\n",
    "        conds_toplot = np.unique(df_with_v['condition'])\n",
    "        nconds_toplot = np.shape(conds_toplot)[0]\n",
    "        \n",
    "        for icond_toplot in np.arange(0,nconds_toplot,1):\n",
    "            \n",
    "            ind_toplot = df_with_v['condition']==conds_toplot[icond_toplot]\n",
    "            \n",
    "            fig_i, r_vals_i, p_vals_i = do_hddm_model_fitted_plot(df_with_v[ind_toplot])\n",
    "            \n",
    "            ax = fig_i.axes[0]\n",
    "            ax.set_title(conds_toplot[icond_toplot]+\n",
    "                        ' Deming Regression: Self-Gaze vs Predicted Drift Rate (v)', fontsize=14)\n",
    "            \n",
    "            # savefig\n",
    "            savefig = 1\n",
    "            if savefig:\n",
    "                current_dir = data_saved_folder+'fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_hddm_model'+savefile_sufix+'/'+animal_tgt\n",
    "                add_fig_dir = os.path.join(current_dir,cameraID+'/fig_for_hddm_model_fitted_combinedsession_withNeurons')\n",
    "                #\n",
    "                if not os.path.exists(add_fig_dir):\n",
    "                    os.makedirs(add_fig_dir)\n",
    "\n",
    "                fig_i.savefig(add_fig_dir+'/gaze_accum_vs_driftslopev_'+conds_toplot[icond_toplot]+doOnsetAfterMin_suffix+exaustModel_sufix+'.pdf', \n",
    "                            format='pdf', bbox_inches='tight')\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the hddm fitting using the data set that contained average neuron activity (across neurons for each trial)\n",
    "# the data set is calculated in Misha - basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv\n",
    "\n",
    "# this one is to fit the ddm in each sesssion separately\n",
    "\n",
    "if 0:\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from scipy.stats import ttest_1samp\n",
    "\n",
    "    # In this data set all the rt is calculated using the new definition, but the pull onset is defined slightly different\n",
    "    #\n",
    "    # if use onset of the first increase after min\n",
    "    doOnsetAfterMin = 1\n",
    "    if not doOnsetAfterMin:\n",
    "        doOnsetAfterMin_suffix = ''\n",
    "    elif doOnsetAfterMin:\n",
    "        doOnsetAfterMin_suffix = 'PullOnsetAfterMin_'\n",
    "\n",
    "    # if use a hmm based method to find the trial start\n",
    "    doHMMmethod = 0\n",
    "    if doHMMmethod:\n",
    "        doOnsetAfterMin_suffix = 'HMMmethods_'\n",
    "        \n",
    "        \n",
    "    # only run the noselfgaze hddm fitting\n",
    "    doNogazeOnly = 0\n",
    "        \n",
    "    \n",
    "    # use more variables for v a and z\n",
    "    doExaustModel = 0\n",
    "    if doExaustModel:\n",
    "        exaustModel_sufix = '_exaustModel'\n",
    "    elif not doExaustModel:\n",
    "        exaustModel_sufix = ''\n",
    "        \n",
    "    # load the dataset first\n",
    "    current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "    add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_combinedsession_withNeurons')\n",
    "    \n",
    "    with open(add_date_dir+'/hddm_datas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\n",
    "              animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        hddm_data_tgt_alldates = pickle.load(f)\n",
    "\n",
    "    # make the hddm_data structure consistent with other code\n",
    "    ind_goodcond = np.isin(hddm_data_tgt_alldates['condition'],conditions_to_ana)\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[ind_goodcond]\n",
    "\n",
    "    rename_dict = {\n",
    "    'act_animal':'subj_idx',\n",
    "    'dates': 'date',\n",
    "    'pull_rt': 'rt',\n",
    "    'previous_pull_outcome': 'prev_trial_outcome',\n",
    "    'socialgaze_auc': 'self_gaze_auc',\n",
    "    'mass_move_speed_mean': 'self_mean_speed',\n",
    "    'mass_move_speed_std': 'self_speed_std',\n",
    "    'other_mass_move_speed_mean': 'partner_mean_speed',\n",
    "    'other_mass_move_speed_std': 'partner_speed_std',\n",
    "    'num_preceding_failpull': 'failed_pulls_before_reward',\n",
    "    'time_from_last_reward': 'time_since_last_reward',\n",
    "    }\n",
    "    # Apply the renaming\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates.rename(columns=rename_dict)\n",
    "    hddm_data_tgt_alldates['response'] = 1\n",
    "\n",
    "    # \n",
    "    # remove nan row first\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates.dropna()\n",
    "    \n",
    "    #\n",
    "    # # add some semi-artbitrary cretieras\n",
    "    # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['prev_trial_outcome']==1]\n",
    "    # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['partner_mean_speed']<=300]\n",
    "    #\n",
    "    lower_q = 0.05\n",
    "    upper_q = 0.95\n",
    "    rt_lower, rt_upper = hddm_data_tgt_alldates['rt'].quantile([lower_q, upper_q])\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']>rt_lower]\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']<rt_upper]\n",
    "    #\n",
    "    tslr_lower, tslr_upper = hddm_data_tgt_alldates['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']>tslr_lower]\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']<tslr_upper]\n",
    "\n",
    "    # \n",
    "    # normalize all the predictors\n",
    "    from scipy.stats import zscore\n",
    "    #\n",
    "    # List of predictor columns to normalize (exclude 'rt' and identifiers)\n",
    "    predictor_columns = [\n",
    "        'self_gaze_auc',\n",
    "        'partner_mean_speed',\n",
    "        'self_mean_speed',\n",
    "        'partner_speed_std',\n",
    "        'self_speed_std',\n",
    "        'failed_pulls_before_reward',\n",
    "        'time_since_last_reward'\n",
    "    ]\n",
    "    # Rename originals and replace with z-scored values\n",
    "    for col in predictor_columns:\n",
    "        hddm_data_tgt_alldates[f\"{col}_origin\"] = hddm_data_tgt_alldates[col]\n",
    "        hddm_data_tgt_alldates[col] = zscore(hddm_data_tgt_alldates[col])  \n",
    "      \n",
    "    #\n",
    "    # initialize a dataframe for summarizing all sessions results\n",
    "    summary_list = []\n",
    "    \n",
    "    \n",
    "    # run the fitting for each session\n",
    "    for session_toana, hddm_data_tgt_idate in hddm_data_tgt_alldates.groupby('date'):\n",
    "    \n",
    "        # Inside the loop:\n",
    "        # 'session_toana' is the date for the current group\n",
    "        # 'hddm_data_tgt_idate' is a DataFrame containing only the rows for that date\n",
    "    \n",
    "        # try to load the analyzed\n",
    "        try:\n",
    "\n",
    "            # dummpy\n",
    "            current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "            add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_allsession_withNeurons/'+session_toana)\n",
    "\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                with open(add_date_dir+'/hddm_model_fitted_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_traces = pickle.load(f)\n",
    "\n",
    "                with open(add_date_dir+'/hddm_model_fitted_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_stats = pickle.load(f)\n",
    "\n",
    "                with open(add_date_dir+'/hddm_model_fitted_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                    hddm_model_fitted_dic = pickle.load(f)\n",
    "            #\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_nogaze_dic = pickle.load(f)\n",
    "                \n",
    "            \n",
    "\n",
    "            print(animal_tgt+' '+cond_toplot_type+' hddm fitting results loaded')\n",
    "\n",
    "        except:\n",
    "\n",
    "            print(animal_tgt+' '+cond_toplot_type+' running hddm fitting')\n",
    "\n",
    "            # run the hddm nodel\n",
    "            if not doExaustModel:\n",
    "                # HDDM settings - the basic setting\n",
    "                # samples=500 # 2000\n",
    "                # burn=100\n",
    "                # thin=2\n",
    "                samples=200 # 2000\n",
    "                burn=100\n",
    "                thin=2\n",
    "                #\n",
    "                if not doNogazeOnly:\n",
    "                    hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_idate, animal_tgt,\n",
    "                                                                                samples, burn, thin, doNogazeOnly)\n",
    "                elif doNogazeOnly:\n",
    "                    _, hddm_model_fitted_nogaze = run_hddm_modeling(hddm_data_tgt_idate, animal_tgt,\n",
    "                                                                    samples, burn, thin, doNogazeOnly)\n",
    "\n",
    "            elif doExaustModel:\n",
    "                # samples=2000 # 2000\n",
    "                # burn=1000\n",
    "                # thin=4\n",
    "                samples=200 # 2000\n",
    "                burn=100\n",
    "                thin=2\n",
    "                #\n",
    "                if not doNogazeOnly:\n",
    "                    hddm_model_fitted, hddm_model_fitted_nogaze = run_hddm_modeling_exaustModel(hddm_data_tgt_idate, animal_tgt, \n",
    "                                                                                samples, burn, thin, doNogazeOnly)\n",
    "                elif doNogazeOnly:\n",
    "                    _, hddm_model_fitted_nogaze = run_hddm_modeling_exaustModel(hddm_data_tgt_idate, animal_tgt,\n",
    "                                                                                samples, burn, thin, doNogazeOnly)\n",
    "\n",
    "            #\n",
    "            if not doNogazeOnly:\n",
    "                hddm_model_fitted_traces = hddm_model_fitted.get_traces()\n",
    "                hddm_model_fitted_stats  = hddm_model_fitted.gen_stats()\n",
    "                hddm_model_fitted_dic = hddm_model_fitted.dic_info\n",
    "            #\n",
    "            hddm_model_fitted_nogaze_traces = hddm_model_fitted_nogaze.get_traces()\n",
    "            hddm_model_fitted_nogaze_stats  = hddm_model_fitted_nogaze.gen_stats()\n",
    "            hddm_model_fitted_nogaze_dic = hddm_model_fitted_nogaze.dic_info\n",
    "\n",
    "            # save data\n",
    "            if 1:\n",
    "                current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "                add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_allsession_withNeurons/'+session_toana)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                if not doNogazeOnly:\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_traces, f)\n",
    "\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_stats, f)\n",
    "\n",
    "                    with open(add_date_dir+'/hddm_model_fitted_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(hddm_model_fitted_dic, f)\n",
    "\n",
    "                #\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_nogaze_traces, f)\n",
    "\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_nogaze_stats, f)  \n",
    "\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_nogaze_dic, f)\n",
    "            \n",
    "          \n",
    "        #\n",
    "        # reconstruct V\n",
    "        #\n",
    "        df_combined = hddm_data_tgt_idate # your combined trial-level dataframe\n",
    "        # traces = hddm_model_fitted_nogaze_traces  # or use `hddm_model_fitted_traces` if needed\n",
    "        # stats = hddm_model_fitted_nogaze_stats\n",
    "        traces = hddm_model_fitted_traces  # or use `hddm_model_fitted_traces` if needed\n",
    "        stats = hddm_model_fitted_stats\n",
    "\n",
    "        # --- STEP 1: Use all posterior samples to compute predicted v for each trial ---\n",
    "        n_samples = len(traces)\n",
    "        n_trials = len(df_combined)\n",
    "\n",
    "        # Allocate space to hold predicted v for each sample x trial\n",
    "        predicted_v_samples = np.zeros((n_samples, n_trials))\n",
    "\n",
    "        # Pre-fetch covariate values as numpy arrays\n",
    "        x1 = df_combined['partner_mean_speed'].values\n",
    "        x2 = df_combined['partner_speed_std'].values\n",
    "        x3 = df_combined['self_mean_speed'].values\n",
    "        x4 = df_combined['self_speed_std'].values\n",
    "        # x3 = df_combined['failed_pulls_before_reward'].values\n",
    "        # x3 = df_combined['time_since_last_reward'].values\n",
    "        x7 = df_combined['self_gaze_auc'].values\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            intercept = traces['v_Intercept'].values[i]\n",
    "            coef_x1 = traces['v_partner_mean_speed'].values[i]\n",
    "            coef_x2 = traces['v_partner_speed_std'].values[i]\n",
    "            coef_x3 = traces['v_self_mean_speed'].values[i]\n",
    "            coef_x4 = traces['v_self_speed_std'].values[i]\n",
    "            # coef_x3 = traces['v_failed_pulls_before_reward'].values[i]\n",
    "            # coef_x3 = traces['v_time_since_last_reward'].values[i]\n",
    "            coef_x7 = traces['v_self_gaze_auc'].values[i]\n",
    "\n",
    "            # Apply regression formula for all trials\n",
    "            predicted_v_samples[i, :] = (\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3\n",
    "                # intercept + coef_x1 * x1 \n",
    "                # intercept + coef_x1 * x1 + coef_x3 * x3\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4\n",
    "                intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4 + coef_x7 * x7\n",
    "            )\n",
    "\n",
    "        # --- STEP 2: Compute posterior mean (and optionally CI) of predicted v ---\n",
    "        df_with_v = df_combined.copy()\n",
    "        df_with_v['predicted_v'] = predicted_v_samples.mean(axis=0)\n",
    "\n",
    "        # Optional: also store credible intervals (e.g., 95% CI)\n",
    "        df_with_v['predicted_v_lower'] = np.percentile(predicted_v_samples, 2.5, axis=0)\n",
    "        df_with_v['predicted_v_upper'] = np.percentile(predicted_v_samples, 97.5, axis=0) \n",
    "            \n",
    "        \n",
    "            \n",
    "        #\n",
    "        # save the summaring data across date   \n",
    "        #\n",
    "        # --- Process the model data ---\n",
    "        # --- Task 1: Process the HDDM model coefficients (same as before) ---\n",
    "        hddm_model_fitted_stats['significance'] = (hddm_model_fitted_stats['2.5q'] * hddm_model_fitted_stats['97.5q'] > 0).astype(int)\n",
    "        session_summary = hddm_model_fitted_stats.reset_index().rename(columns={'index': 'coefficient'})\n",
    "        session_summary['date'] = session_toana\n",
    "        session_summary = session_summary[['date', 'coefficient', 'mean', 'std', 'significance']]\n",
    "        summary_list.append(session_summary)\n",
    "\n",
    "\n",
    "        # --- Task 2: Calculate and add the v-rt correlation ---\n",
    "        # Calculate the Pearson correlation coefficient and the p-value.\n",
    "        corr_coef, p_value = st.pearsonr(df_with_v['predicted_v'], df_with_v['rt'])\n",
    "        #\n",
    "        # Create a small DataFrame for the correlation result.\n",
    "        # We use the 'mean' column for the coefficient and 'significance' for the p-value\n",
    "        # to keep the data structure consistent.\n",
    "        corr_summary = pd.DataFrame([{\n",
    "            'date': session_toana,\n",
    "            'coefficient': 'v_rt_correlation',\n",
    "            'mean': corr_coef,\n",
    "            'std': np.nan,  # Standard deviation isn't applicable here\n",
    "            'significance': (p_value<0.05).astype(int)\n",
    "        }])\n",
    "        #\n",
    "        # Add the correlation result to the list.\n",
    "        summary_list.append(corr_summary)\n",
    "        \n",
    "        # with mean firing rate\n",
    "        corr_coef, p_value = st.pearsonr(df_with_v['predicted_v'], df_with_v['fr_mean'])\n",
    "        #\n",
    "        # Create a small DataFrame for the correlation result.\n",
    "        # We use the 'mean' column for the coefficient and 'significance' for the p-value\n",
    "        # to keep the data structure consistent.\n",
    "        corr_summary = pd.DataFrame([{\n",
    "            'date': session_toana,\n",
    "            'coefficient': 'v_fr_mean_correlation',\n",
    "            'mean': corr_coef,\n",
    "            'std': np.nan,  # Standard deviation isn't applicable here\n",
    "            'significance': (p_value<0.05).astype(int)\n",
    "        }])\n",
    "        #\n",
    "        # Add the correlation result to the list.\n",
    "        summary_list.append(corr_summary)\n",
    "        \n",
    "         # with slope firing rate\n",
    "        corr_coef, p_value = st.pearsonr(df_with_v['predicted_v'], df_with_v['fr_slope'])\n",
    "        #\n",
    "        # Create a small DataFrame for the correlation result.\n",
    "        # We use the 'mean' column for the coefficient and 'significance' for the p-value\n",
    "        # to keep the data structure consistent.\n",
    "        corr_summary = pd.DataFrame([{\n",
    "            'date': session_toana,\n",
    "            'coefficient': 'v_fr_slope_correlation',\n",
    "            'mean': corr_coef,\n",
    "            'std': np.nan,  # Standard deviation isn't applicable here\n",
    "            'significance': (p_value<0.05).astype(int)\n",
    "        }])\n",
    "        #\n",
    "        # Add the correlation result to the list.\n",
    "        summary_list.append(corr_summary)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Combine the list of DataFrames into the final summary DataFrame.\n",
    "    hddm_model_fitted_alldates_summary = pd.concat(summary_list, ignore_index=True)\n",
    "    \n",
    "    ## \n",
    "    # do some plotting\n",
    "    ##\n",
    "    plot_data = hddm_model_fitted_alldates_summary.copy()\n",
    "    \n",
    "    # ---- NEW: Filter out the coefficients you don't want to see ----\n",
    "    coeffs_to_exclude = ['t', 'z', 'v_Intercept', 'a_Intercept','a_time_since_last_reward']\n",
    "    #\n",
    "    do_v_and_FR = 1\n",
    "    if do_v_and_FR:\n",
    "        coeffs_to_exclude = ['t', 'z', 'v_Intercept', 'a_Intercept','a_time_since_last_reward',\n",
    "                             'v_self_gaze_auc', 'v_partner_mean_speed','v_partner_speed_std',\n",
    "                             'v_self_mean_speed','v_self_speed_std',]\n",
    "    #\n",
    "    plot_data = plot_data[~plot_data['coefficient'].isin(coeffs_to_exclude)]\n",
    "\n",
    "    plot_data['mean'] = pd.to_numeric(plot_data['mean'], errors='coerce')\n",
    "    \n",
    "    # ---- Remove outliers for each coefficient using the IQR method ----\n",
    "    # For each coefficient, calculate the bounds\n",
    "    Q1 = plot_data.groupby('coefficient')['mean'].transform(lambda x: x.quantile(0.25))\n",
    "    Q3 = plot_data.groupby('coefficient')['mean'].transform(lambda x: x.quantile(0.75))\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    #\n",
    "    # Filter out the outliers\n",
    "    plot_data_no_outliers = plot_data[(plot_data['mean'] >= lower_bound) & (plot_data['mean'] <= upper_bound)]\n",
    "    # plot_data_no_outliers = plot_data\n",
    "    \n",
    "    \n",
    "    # --- Plotting Code (using the outlier-free data) ---\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Create the swarm plot\n",
    "    # We use 'hue' to color points based on the 'significance' column for that date.\n",
    "    ax = sns.swarmplot(\n",
    "        x='coefficient',\n",
    "        y='mean',\n",
    "        data=plot_data_no_outliers,\n",
    "        hue='significance',\n",
    "        palette={0: 'gray', 1: 'red'},  # 0=Not Significant (gray), 1=Significant (red)\n",
    "        size=8\n",
    "    )\n",
    "    # Hide the legend as the colors are intuitive\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "\n",
    "    # --- Add Overall Significance Asterisks ---\n",
    "    unique_coeffs = plot_data_no_outliers['coefficient'].unique()\n",
    "    for i, coef in enumerate(unique_coeffs):\n",
    "        coef_data = plot_data_no_outliers[plot_data_no_outliers['coefficient'] == coef]['mean']\n",
    "\n",
    "        if len(coef_data) > 1:\n",
    "            t_stat, p_value = ttest_1samp(coef_data, 0, nan_policy='omit')\n",
    "            if p_value < 0.05:\n",
    "                y_pos = coef_data.max()\n",
    "                offset = plot_data_no_outliers['mean'].std() * 0.1\n",
    "                plt.text(i, y_pos + offset, '*', ha='center', va='bottom', fontsize=24, color='black')\n",
    "\n",
    "    plt.axhline(y=0, linestyle='--', color='black', linewidth=1.5)            \n",
    "    \n",
    "    # --- Finalize and Save Plot ---\n",
    "    plt.title('Model Coefficients Across All Dates', fontsize=18)\n",
    "    plt.ylabel('Coefficient Value (Mean)', fontsize=16)\n",
    "    plt.xlabel('Model Coefficient', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize = 16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "     # savefig\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        current_dir = data_saved_folder+'fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_hddm_model'+savefile_sufix+'/'+animal_tgt\n",
    "        add_fig_dir = os.path.join(current_dir,cameraID+'/fig_for_hddm_model_fitted_allsession_withNeurons')\n",
    "        #\n",
    "        if not os.path.exists(add_fig_dir):\n",
    "            os.makedirs(add_fig_dir)\n",
    "        if not do_v_and_FR:\n",
    "            plt.savefig(add_fig_dir+'/predicted_v_and_other_bhv_variables_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pdf', \n",
    "                        format='pdf', bbox_inches='tight')\n",
    "        elif do_v_and_FR:\n",
    "            plt.savefig(add_fig_dir+'/predicted_v_and_FR_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pdf', \n",
    "                        format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d334db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the hddm fitting using the data set that contained average neuron activity (across neurons for each trial)\n",
    "# the data set is calculated in Misha - basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv\n",
    "\n",
    "# this one is to fit the ddm in each sesssion separately\n",
    "# this one use gaze period to filter the partner's variables\n",
    "\n",
    "if 1:\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from scipy.stats import ttest_1samp\n",
    "\n",
    "    # In this data set all the rt is calculated using the new definition, but the pull onset is defined slightly different\n",
    "    #\n",
    "    # if use onset of the first increase after min\n",
    "    doOnsetAfterMin = 1\n",
    "    if not doOnsetAfterMin:\n",
    "        doOnsetAfterMin_suffix = ''\n",
    "    elif doOnsetAfterMin:\n",
    "        doOnsetAfterMin_suffix = 'PullOnsetAfterMin_'\n",
    "\n",
    "    # if use a hmm based method to find the trial start\n",
    "    doHMMmethod = 0\n",
    "    if doHMMmethod:\n",
    "        doOnsetAfterMin_suffix = 'HMMmethods_'\n",
    "        \n",
    "    exaustModel_sufix = ''\n",
    "        \n",
    "    # load the dataset first\n",
    "    current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "    add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_combinedsession_withNeurons')\n",
    "    \n",
    "    with open(add_date_dir+'/hddm_datas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\n",
    "              animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        hddm_data_tgt_alldates = pickle.load(f)\n",
    "\n",
    "    # make the hddm_data structure consistent with other code\n",
    "    ind_goodcond = np.isin(hddm_data_tgt_alldates['condition'],conditions_to_ana)\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[ind_goodcond]\n",
    "\n",
    "    rename_dict = {\n",
    "    'act_animal':'subj_idx',\n",
    "    'dates': 'date',\n",
    "    'pull_rt': 'rt',\n",
    "    'previous_pull_outcome': 'prev_trial_outcome',\n",
    "    'socialgaze_auc': 'self_gaze_auc',\n",
    "    'mass_move_speed_mean': 'self_mean_speed',\n",
    "    'mass_move_speed_std': 'self_speed_std',\n",
    "    'other_mass_move_speed_mean': 'partner_mean_speed',\n",
    "    'other_mass_move_speed_std': 'partner_speed_std',\n",
    "    'num_preceding_failpull': 'failed_pulls_before_reward',\n",
    "    'time_from_last_reward': 'time_since_last_reward',\n",
    "    }\n",
    "    # Apply the renaming\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates.rename(columns=rename_dict)\n",
    "    hddm_data_tgt_alldates['response'] = 1\n",
    "\n",
    "    \n",
    "    #\n",
    "    # calculated the new partner_mean_speed and partner_speed_std with self_gaze filter\n",
    "    #\n",
    "    hddm_data_tgt_alldates['partner_speed_std_nofilter'] = hddm_data_tgt_alldates['partner_speed_std']\n",
    "    hddm_data_tgt_alldates['partner_mean_speed_nofilter'] = hddm_data_tgt_alldates['partner_mean_speed']\n",
    "    \n",
    "    #\n",
    "    ntrials = np.shape(hddm_data_tgt_alldates)[0]\n",
    "    for itrial in np.arange(0,ntrials,1):\n",
    "        gaze_prob_itrial = hddm_data_tgt_alldates['socialgaze_prob'].iloc[itrial]\n",
    "        partner_speed_trial = hddm_data_tgt_alldates['other_mass_move_speed'].iloc[itrial]\n",
    "\n",
    "        partner_speed_filtered = partner_speed_trial[gaze_prob_itrial>0]\n",
    "\n",
    "        partner_mean_speed_filtered = np.nanmean(partner_speed_filtered)\n",
    "        partner_std_speed_filtered = np.nanstd(partner_speed_filtered)\n",
    "        \n",
    "        hddm_data_tgt_alldates['partner_mean_speed'].iloc[itrial] = partner_mean_speed_filtered\n",
    "        hddm_data_tgt_alldates['partner_speed_std'].iloc[itrial] = partner_std_speed_filtered\n",
    "    \n",
    "    \n",
    "    # \n",
    "    # remove nan row first\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates.dropna()\n",
    "    \n",
    "    #\n",
    "    # # add some semi-artbitrary cretieras\n",
    "    # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['prev_trial_outcome']==1]\n",
    "    # hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['partner_mean_speed']<=300]\n",
    "    #\n",
    "    lower_q = 0.05\n",
    "    upper_q = 0.95\n",
    "    rt_lower, rt_upper = hddm_data_tgt_alldates['rt'].quantile([lower_q, upper_q])\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']>rt_lower]\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']<rt_upper]\n",
    "    #\n",
    "    tslr_lower, tslr_upper = hddm_data_tgt_alldates['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']>tslr_lower]\n",
    "    hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']<tslr_upper]\n",
    "    \n",
    "    \n",
    "    # \n",
    "    # normalize all the predictors\n",
    "    from scipy.stats import zscore\n",
    "    #\n",
    "    # List of predictor columns to normalize (exclude 'rt' and identifiers)\n",
    "    predictor_columns = [\n",
    "        'self_gaze_auc',\n",
    "        'partner_mean_speed',\n",
    "        'self_mean_speed',\n",
    "        'partner_speed_std',\n",
    "        'self_speed_std',\n",
    "        'failed_pulls_before_reward',\n",
    "        'time_since_last_reward'\n",
    "    ]\n",
    "    # Rename originals and replace with z-scored values\n",
    "    for col in predictor_columns:\n",
    "        hddm_data_tgt_alldates[f\"{col}_origin\"] = hddm_data_tgt_alldates[col]\n",
    "        hddm_data_tgt_alldates[col] = zscore(hddm_data_tgt_alldates[col])  \n",
    "      \n",
    "    #\n",
    "    # initialize a dataframe for summarizing all sessions results\n",
    "    summary_list = []\n",
    "    \n",
    "    \n",
    "    # run the fitting for each session\n",
    "    for session_toana, hddm_data_tgt_idate in hddm_data_tgt_alldates.groupby('date'):\n",
    "    \n",
    "        # Inside the loop:\n",
    "        # 'session_toana' is the date for the current group\n",
    "        # 'hddm_data_tgt_idate' is a DataFrame containing only the rows for that date\n",
    "    \n",
    "        # try to load the analyzed\n",
    "        try:\n",
    "\n",
    "            # dummpy\n",
    "            current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "            add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_allsession_withNeurons/'+session_toana)\n",
    "\n",
    "            #\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_gazefilter_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_nogaze_gazefilter_traces = pickle.load(f)\n",
    "\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_gazefilter_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_nogaze_gazefilter_stats = pickle.load(f)\n",
    "\n",
    "            with open(add_date_dir+'/hddm_model_fitted_nogaze_gazefilter_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'rb') as f:\n",
    "                hddm_model_fitted_nogaze_gazefilter_dic = pickle.load(f)\n",
    "                \n",
    "            \n",
    "\n",
    "            print(animal_tgt+' '+cond_toplot_type+' hddm fitting results loaded')\n",
    "\n",
    "        except:\n",
    "\n",
    "            print(animal_tgt+' '+cond_toplot_type+' running hddm fitting')\n",
    "\n",
    "            # run the hddm nodel\n",
    "            # HDDM settings - the basic setting\n",
    "            # samples=500 # 2000\n",
    "            # burn=100\n",
    "            # thin=2\n",
    "            samples=200 # 2000\n",
    "            burn=100\n",
    "            thin=2\n",
    "            #\n",
    "            doNogazeOnly = 1\n",
    "            #\n",
    "            _, hddm_model_fitted_nogaze_gazefilter = run_hddm_modeling(hddm_data_tgt_idate, animal_tgt,\n",
    "                                                            samples, burn, thin, doNogazeOnly)\n",
    "\n",
    "            #\n",
    "            hddm_model_fitted_nogaze_gazefilter_traces = hddm_model_fitted_nogaze_gazefilter.get_traces()\n",
    "            hddm_model_fitted_nogaze_gazefilter_stats  = hddm_model_fitted_nogaze_gazefilter.gen_stats()\n",
    "            hddm_model_fitted_nogaze_gazefilter_dic = hddm_model_fitted_nogaze_gazefilter.dic_info\n",
    "\n",
    "            # save data\n",
    "            if 1:\n",
    "                current_dir = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+'/'+cameraID+'/'+animal_tgt\n",
    "                add_date_dir = os.path.join(current_dir+'/hddm_model_fitted_allsession_withNeurons/'+session_toana)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_gazefilter_traces_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_nogaze_gazefilter_traces, f)\n",
    "\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_gazefilter_stats_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_nogaze_gazefilter_stats, f)  \n",
    "\n",
    "                with open(add_date_dir+'/hddm_model_fitted_nogaze_gazefilter_dic_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(hddm_model_fitted_nogaze_gazefilter_dic, f)\n",
    "            \n",
    "          \n",
    "        #\n",
    "        # reconstruct V\n",
    "        #\n",
    "        df_combined = hddm_data_tgt_idate # your combined trial-level dataframe\n",
    "        traces = hddm_model_fitted_nogaze_gazefilter_traces  # or use `hddm_model_fitted_traces` if needed\n",
    "        stats = hddm_model_fitted_nogaze_gazefilter_stats\n",
    "\n",
    "\n",
    "        # --- STEP 1: Use all posterior samples to compute predicted v for each trial ---\n",
    "        n_samples = len(traces)\n",
    "        n_trials = len(df_combined)\n",
    "\n",
    "        # Allocate space to hold predicted v for each sample x trial\n",
    "        predicted_v_samples = np.zeros((n_samples, n_trials))\n",
    "\n",
    "        # Pre-fetch covariate values as numpy arrays\n",
    "        x1 = df_combined['partner_mean_speed'].values\n",
    "        x2 = df_combined['partner_speed_std'].values\n",
    "        x3 = df_combined['self_mean_speed'].values\n",
    "        x4 = df_combined['self_speed_std'].values\n",
    "        # x3 = df_combined['failed_pulls_before_reward'].values\n",
    "        # x3 = df_combined['time_since_last_reward'].values\n",
    "        # x7 = df_combined['self_gaze_auc'].values\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            intercept = traces['v_Intercept'].values[i]\n",
    "            coef_x1 = traces['v_partner_mean_speed'].values[i]\n",
    "            coef_x2 = traces['v_partner_speed_std'].values[i]\n",
    "            coef_x3 = traces['v_self_mean_speed'].values[i]\n",
    "            coef_x4 = traces['v_self_speed_std'].values[i]\n",
    "            # coef_x3 = traces['v_failed_pulls_before_reward'].values[i]\n",
    "            # coef_x3 = traces['v_time_since_last_reward'].values[i]\n",
    "            # coef_x7 = traces['v_self_gaze_auc'].values[i]\n",
    "\n",
    "            # Apply regression formula for all trials\n",
    "            predicted_v_samples[i, :] = (\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3\n",
    "                # intercept + coef_x1 * x1 \n",
    "                # intercept + coef_x1 * x1 + coef_x3 * x3\n",
    "                intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4\n",
    "                # intercept + coef_x1 * x1 + coef_x2 * x2 + coef_x3 * x3 + coef_x4 * x4 + coef_x7 * x7\n",
    "            )\n",
    "\n",
    "        # --- STEP 2: Compute posterior mean (and optionally CI) of predicted v ---\n",
    "        df_with_v = df_combined.copy()\n",
    "        df_with_v['predicted_v'] = predicted_v_samples.mean(axis=0)\n",
    "\n",
    "        # Optional: also store credible intervals (e.g., 95% CI)\n",
    "        df_with_v['predicted_v_lower'] = np.percentile(predicted_v_samples, 2.5, axis=0)\n",
    "        df_with_v['predicted_v_upper'] = np.percentile(predicted_v_samples, 97.5, axis=0) \n",
    "            \n",
    "        \n",
    "            \n",
    "        #\n",
    "        # save the summaring data across date   \n",
    "        #\n",
    "        # --- Process the model data ---\n",
    "        # --- Task 1: Process the HDDM model coefficients (same as before) ---\n",
    "        hddm_model_fitted_nogaze_gazefilter_stats['significance'] = (hddm_model_fitted_nogaze_gazefilter_stats['2.5q'] * \\\n",
    "                                                   hddm_model_fitted_nogaze_gazefilter_stats['97.5q'] > 0).astype(int)\n",
    "        session_summary = hddm_model_fitted_nogaze_gazefilter_stats.reset_index().rename(columns={'index': 'coefficient'})\n",
    "        session_summary['date'] = session_toana\n",
    "        session_summary = session_summary[['date', 'coefficient', 'mean', 'std', 'significance']]\n",
    "        summary_list.append(session_summary)\n",
    "\n",
    "\n",
    "        # --- Task 2: Calculate and add the v-rt correlation ---\n",
    "        # Calculate the Pearson correlation coefficient and the p-value.\n",
    "        corr_coef, p_value = st.pearsonr(df_with_v['predicted_v'], df_with_v['rt'])\n",
    "        #\n",
    "        # Create a small DataFrame for the correlation result.\n",
    "        # We use the 'mean' column for the coefficient and 'significance' for the p-value\n",
    "        # to keep the data structure consistent.\n",
    "        corr_summary = pd.DataFrame([{\n",
    "            'date': session_toana,\n",
    "            'coefficient': 'v_rt_correlation',\n",
    "            'mean': corr_coef,\n",
    "            'std': np.nan,  # Standard deviation isn't applicable here\n",
    "            'significance': (p_value<0.05).astype(int)\n",
    "        }])\n",
    "        #\n",
    "        # Add the correlation result to the list.\n",
    "        summary_list.append(corr_summary)\n",
    "        \n",
    "        # with mean firing rate\n",
    "        corr_coef, p_value = st.pearsonr(df_with_v['predicted_v'], df_with_v['fr_mean'])\n",
    "        #\n",
    "        # Create a small DataFrame for the correlation result.\n",
    "        # We use the 'mean' column for the coefficient and 'significance' for the p-value\n",
    "        # to keep the data structure consistent.\n",
    "        corr_summary = pd.DataFrame([{\n",
    "            'date': session_toana,\n",
    "            'coefficient': 'v_fr_mean_correlation',\n",
    "            'mean': corr_coef,\n",
    "            'std': np.nan,  # Standard deviation isn't applicable here\n",
    "            'significance': (p_value<0.05).astype(int)\n",
    "        }])\n",
    "        #\n",
    "        # Add the correlation result to the list.\n",
    "        summary_list.append(corr_summary)\n",
    "        \n",
    "         # with slope firing rate\n",
    "        corr_coef, p_value = st.pearsonr(df_with_v['predicted_v'], df_with_v['fr_slope'])\n",
    "        #\n",
    "        # Create a small DataFrame for the correlation result.\n",
    "        # We use the 'mean' column for the coefficient and 'significance' for the p-value\n",
    "        # to keep the data structure consistent.\n",
    "        corr_summary = pd.DataFrame([{\n",
    "            'date': session_toana,\n",
    "            'coefficient': 'v_fr_slope_correlation',\n",
    "            'mean': corr_coef,\n",
    "            'std': np.nan,  # Standard deviation isn't applicable here\n",
    "            'significance': (p_value<0.05).astype(int)\n",
    "        }])\n",
    "        #\n",
    "        # Add the correlation result to the list.\n",
    "        summary_list.append(corr_summary)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Combine the list of DataFrames into the final summary DataFrame.\n",
    "    hddm_model_fitted_alldates_summary = pd.concat(summary_list, ignore_index=True)\n",
    "    \n",
    "    ## \n",
    "    # do some plotting\n",
    "    ##\n",
    "    plot_data = hddm_model_fitted_alldates_summary.copy()\n",
    "    \n",
    "    # ---- NEW: Filter out the coefficients you don't want to see ----\n",
    "    coeffs_to_exclude = ['t', 'z', 'v_Intercept', 'a_Intercept','a_time_since_last_reward',\n",
    "                         'v_fr_mean_correlation','v_fr_slope_correlation']\n",
    "    #\n",
    "    do_v_and_FR = 0\n",
    "    if do_v_and_FR:\n",
    "        coeffs_to_exclude = ['t', 'z', 'v_Intercept', 'a_Intercept','a_time_since_last_reward',\n",
    "                             'v_self_gaze_auc', 'v_partner_mean_speed','v_partner_speed_std',\n",
    "                             'v_self_mean_speed','v_self_speed_std',]\n",
    "    #\n",
    "    plot_data = plot_data[~plot_data['coefficient'].isin(coeffs_to_exclude)]\n",
    "\n",
    "    plot_data['mean'] = pd.to_numeric(plot_data['mean'], errors='coerce')\n",
    "    \n",
    "    # ---- Remove outliers for each coefficient using the IQR method ----\n",
    "    # For each coefficient, calculate the bounds\n",
    "    Q1 = plot_data.groupby('coefficient')['mean'].transform(lambda x: x.quantile(0.25))\n",
    "    Q3 = plot_data.groupby('coefficient')['mean'].transform(lambda x: x.quantile(0.75))\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    #\n",
    "    # Filter out the outliers\n",
    "    plot_data_no_outliers = plot_data[(plot_data['mean'] >= lower_bound) & (plot_data['mean'] <= upper_bound)]\n",
    "    # plot_data_no_outliers = plot_data\n",
    "    \n",
    "    \n",
    "    # --- Plotting Code (using the outlier-free data) ---\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Create the swarm plot\n",
    "    # We use 'hue' to color points based on the 'significance' column for that date.\n",
    "    ax = sns.swarmplot(\n",
    "        x='coefficient',\n",
    "        y='mean',\n",
    "        data=plot_data_no_outliers,\n",
    "        hue='significance',\n",
    "        palette={0: 'gray', 1: 'red'},  # 0=Not Significant (gray), 1=Significant (red)\n",
    "        size=8\n",
    "    )\n",
    "    # Hide the legend as the colors are intuitive\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "\n",
    "    # --- Add Overall Significance Asterisks ---\n",
    "    unique_coeffs = plot_data_no_outliers['coefficient'].unique()\n",
    "    for i, coef in enumerate(unique_coeffs):\n",
    "        coef_data = plot_data_no_outliers[plot_data_no_outliers['coefficient'] == coef]['mean']\n",
    "\n",
    "        if len(coef_data) > 1:\n",
    "            t_stat, p_value = ttest_1samp(coef_data, 0, nan_policy='omit')\n",
    "            if p_value < 0.05:\n",
    "                y_pos = coef_data.max()\n",
    "                offset = plot_data_no_outliers['mean'].std() * 0.1\n",
    "                plt.text(i, y_pos + offset, '*', ha='center', va='bottom', fontsize=24, color='black')\n",
    "\n",
    "    plt.axhline(y=0, linestyle='--', color='black', linewidth=1.5)            \n",
    "    \n",
    "    # --- Finalize and Save Plot ---\n",
    "    plt.title('Model Coefficients Across All Dates', fontsize=18)\n",
    "    plt.ylabel('Coefficient Value (Mean)', fontsize=16)\n",
    "    plt.xlabel('Model Coefficient', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize = 16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "     # savefig\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        current_dir = data_saved_folder+'fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_hddm_model'+savefile_sufix+'/'+animal_tgt\n",
    "        add_fig_dir = os.path.join(current_dir,cameraID+'/fig_for_hddm_model_fitted_allsession_withNeurons')\n",
    "        #\n",
    "        if not os.path.exists(add_fig_dir):\n",
    "            os.makedirs(add_fig_dir)\n",
    "        if not do_v_and_FR:\n",
    "            plt.savefig(add_fig_dir+'/predicted_v_and_other_bhv_variables_nogaze_gazefilter_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pdf', \n",
    "                        format='pdf', bbox_inches='tight')\n",
    "        elif do_v_and_FR:\n",
    "            plt.savefig(add_fig_dir+'/predicted_v_and_FR_nogaze_gazefilter_'+cond_toplot_type+doOnsetAfterMin_suffix+exaustModel_sufix+'.pdf', \n",
    "                        format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d34f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm_model_fitted_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741aaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm_data_tgt_idate['partner_mean_speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8e8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b123a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deae0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1ddaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97982ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8866793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329604e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ef8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of z-scored predictor columns\n",
    "predictor_columns = [\n",
    "    'self_gaze_auc',\n",
    "    'partner_mean_speed',\n",
    "    'self_mean_speed',\n",
    "    'partner_speed_std',\n",
    "    'self_speed_std',\n",
    "    'failed_pulls_before_reward',\n",
    "    'time_since_last_reward'\n",
    "]\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = hddm_data_tgt_alldates[predictor_columns].corr()\n",
    "\n",
    "# Round for readability\n",
    "correlation_matrix_rounded = correlation_matrix.round(2)\n",
    "\n",
    "# Display the table\n",
    "# print(\"Correlation matrix among predictors:\")\n",
    "print(correlation_matrix_rounded)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Matrix of Predictors\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6902e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "\n",
    "# List of predictors (already z-scored)\n",
    "predictor_columns = [\n",
    "    'self_gaze_auc',\n",
    "    'partner_mean_speed',\n",
    "    'self_mean_speed',\n",
    "    'partner_speed_std',\n",
    "    'self_speed_std',\n",
    "    'failed_pulls_before_reward',\n",
    "    'time_since_last_reward'\n",
    "]\n",
    "\n",
    "# Extract predictor data\n",
    "X = hddm_data_tgt_alldates[predictor_columns].copy()\n",
    "\n",
    "# Add a constant term for intercept (required by VIF calc)\n",
    "X_const = add_constant(X)\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X_const.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
    "\n",
    "# Drop the constant from the results\n",
    "vif_data = vif_data[vif_data['Variable'] != 'const']\n",
    "\n",
    "# Display VIFs\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2722d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa75661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# List of variables\n",
    "predictor_columns = [\n",
    "    'rt',\n",
    "    'self_gaze_auc',\n",
    "    'partner_mean_speed',\n",
    "    'self_mean_speed',\n",
    "    'partner_speed_std',\n",
    "    'self_speed_std',\n",
    "    'predicted_v',\n",
    "    'fr_slope',\n",
    "    'fr_mean',\n",
    "]\n",
    "\n",
    "# Drop NaNs\n",
    "df_corr = df_with_v[predictor_columns].dropna()\n",
    "\n",
    "n = len(predictor_columns)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "# Compute correlations and p-values (upper triangle only)\n",
    "triu_indices = np.triu_indices(n, k=1)\n",
    "pvals = []\n",
    "\n",
    "for i, j in zip(*triu_indices):\n",
    "    r, p = pearsonr(df_corr[predictor_columns[i]], df_corr[predictor_columns[j]])\n",
    "    corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "    pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "    pvals.append(p)\n",
    "\n",
    "# Apply FDR correction\n",
    "pvals_corrected = multipletests(pvals, method='fdr_bh')[1]\n",
    "\n",
    "# Create corrected p-value matrix\n",
    "pval_corrected_matrix = np.ones((n, n))\n",
    "for idx, (i, j) in enumerate(zip(*triu_indices)):\n",
    "    pval_corrected_matrix[i, j] = pval_corrected_matrix[j, i] = pvals_corrected[idx]\n",
    "\n",
    "# Annotate with asterisks if corrected p < 0.01\n",
    "annot = np.empty_like(corr_matrix, dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        val = f\"{corr_matrix[i, j]:.2f}\"\n",
    "        if i != j and pval_corrected_matrix[i, j] < 0.01:\n",
    "            val += \"*\"\n",
    "        annot[i, j] = val\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, annot=annot, fmt='', cmap='coolwarm', square=True,\n",
    "    xticklabels=predictor_columns, yticklabels=predictor_columns,\n",
    "    cbar_kws={\"label\": \"Pearson r\"}\n",
    ")\n",
    "plt.title(\"Correlation Matrix of Predictors, predicted drift rate v and FR properties \\\n",
    "          \\n(* FDR-corrected p < 0.01)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_corrected_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb093929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_ = df_with_v['condition']=='MC'\n",
    "# ind_ = df_with_v['date']==np.unique(df_with_v['date'])[10]\n",
    "# ind_ = (df_with_v['condition']=='MC_withDodson')  & (df_with_v['rt'] > 5.5)\n",
    "# ind_ = df_with_v['prev_trial_outcome'] == 1\n",
    "# ind_ = df_with_v['rt']>5\n",
    "ind_ = df_with_v['rt']>=0\n",
    "print(np.sum(ind_))\n",
    "# df_with_v[df_with_v['prev_trial_outcome'] == 1]['rt'] = df_with_v[df_with_v['prev_trial_outcome'] == 1]['rt']-1\n",
    "\n",
    "# plt.plot(df_with_v[ind_]['rt'],df_with_v[ind_]['self_gaze_auc']/df_with_v[ind_]['rt'],'.')\n",
    "plt.plot(df_with_v[ind_]['partner_mean_speed'],df_with_v[ind_]['self_gaze_auc'],'.')\n",
    "\n",
    "# plt.plot(df_with_v[ind_]['rt'],df_with_v[ind_]['partner_mean_speed'],'.')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v[ind_]['partner_mean_speed']\n",
    "y = df_with_v[ind_]['self_gaze_auc']#/df_with_v[ind_]['rt']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_with_v['predicted_v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b308b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_with_v['prev_trial_outcome']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40372944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['rt'], df_with_v['time_since_last_reward'], '.')\n",
    "plt.xlabel('Reaction Time')\n",
    "plt.ylabel('time_since_last_reward')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['rt']\n",
    "y = df_with_v['time_since_last_reward']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['rt'], df_with_v['predicted_v'], '.')\n",
    "plt.xlabel('Reaction Time')\n",
    "plt.ylabel('predicted_v')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['rt']\n",
    "y = df_with_v['predicted_v']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50531919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['rt'], df_with_v['partner_mean_speed'], '.')\n",
    "plt.xlabel('rt')\n",
    "plt.ylabel('partner_mean_speed')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['rt']\n",
    "y = df_with_v['partner_mean_speed']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['rt'], df_with_v['self_gaze_auc'], '.')\n",
    "#plt.plot(df_with_v['rt'], df_with_v['self_gaze_auc_origin']/df_with_v['rt'], '.')\n",
    "plt.xlabel('rt')\n",
    "plt.ylabel('self_gaze_auc')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['rt']\n",
    "y = df_with_v['self_gaze_auc']\n",
    "# y = df_with_v['self_gaze_auc_origin']/df_with_v['rt']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb65cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['predicted_v'], df_with_v['partner_mean_speed'], '.')\n",
    "plt.xlabel('predicted_v')\n",
    "plt.ylabel('partner_mean_speed')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['predicted_v']\n",
    "y = df_with_v['partner_mean_speed']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['predicted_v'], df_with_v['self_speed_std'], '.')\n",
    "plt.xlabel('predicted_v')\n",
    "plt.ylabel('self_speed_std')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['predicted_v']\n",
    "y = df_with_v['self_speed_std']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_q = 0.025\n",
    "# upper_q = 0.975\n",
    "# tslr_lower, tslr_upper = df_with_v['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']>tslr_lower]\n",
    "# df_with_v = df_with_v[df_with_v['time_since_last_reward']<tslr_upper]\n",
    "# df_with_v = df_with_v[df_with_v['prev_trial_outcome']==1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_with_v['time_since_last_reward'], df_with_v['partner_mean_speed'], '.')\n",
    "plt.xlabel('time_since_last_reward')\n",
    "plt.ylabel('partner_mean_speed')\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "x = df_with_v['time_since_last_reward']\n",
    "y = df_with_v['partner_mean_speed']\n",
    "\n",
    "slope, intercept, _, _, _ = linregress(x, y)\n",
    "r_value, p_value = pearsonr(x, y)\n",
    "\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df_with_v[['partner_mean_speed', 'time_since_last_reward']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm_model_fitted_nogaze_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm_model_fitted_nogaze_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56964f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm_model_fitted_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06397edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm_model_fitted_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20371b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_with_v['prev_trial_outcome']==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(traces['v_Intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae33012",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_q = 0.025\n",
    "upper_q = 0.975\n",
    "rt_lower, rt_upper = hddm_data_tgt_alldates['rt'].quantile([lower_q, upper_q])\n",
    "hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']>rt_lower]\n",
    "hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['rt']<rt_upper]\n",
    "#\n",
    "tslr_lower, tslr_upper = hddm_data_tgt_alldates['time_since_last_reward'].quantile([lower_q, upper_q])\n",
    "hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']>tslr_lower]\n",
    "hddm_data_tgt_alldates = hddm_data_tgt_alldates[hddm_data_tgt_alldates['time_since_last_reward']<tslr_upper]\n",
    "#\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(hddm_data_tgt_alldates['rt'], hddm_data_tgt_alldates['time_since_last_reward'], '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62edce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a305963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a8600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c51db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1579ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6a2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60470649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2de93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a prediction model to predict the pull based on the hddm model fitting\n",
    "\n",
    "# --- Extract 'a' (new step) ---\n",
    "# my_model = hddm_model_fitted_nogaze\n",
    "# df_combined = hddm_data['hddm_data_'+animal_id_toana]\n",
    "df_combined = hddm_data_tgt_alldates\n",
    "\n",
    "# --- STEP 1: Get the mean posterior estimates for all 'v' coefficients ---\n",
    "# traces = my_model.get_traces()\n",
    "traces = hddm_model_fitted_nogaze_traces\n",
    "\n",
    "# Get the mean value for the intercept\n",
    "v_intercept_mean = traces['v_Intercept'].mean()\n",
    "\n",
    "# Get the mean value for the coefficient of each predictor\n",
    "# v_gaze_coef_mean = traces['v_self_gaze_auc'].mean()\n",
    "v_speed_coef_mean = traces['v_partner_mean_speed'].mean()\n",
    "v_failed_pulls_before_reward_mean = traces['v_failed_pulls_before_reward'].mean()\n",
    "v_time_since_last_reward_mean = traces['v_time_since_last_reward'].mean()\n",
    "\n",
    "# --- STEP 2: Apply the regression equation to your dataframe ---\n",
    "# This calculates the predicted 'v' for each trial based on its unique covariate values.\n",
    "df_lifelines = df_combined.copy() # Work with a copy\n",
    "\n",
    "df_lifelines['predicted_v']  = (v_intercept_mean +\n",
    "                              # (v_gaze_coef_mean * df_with_v['self_gaze_auc']) +\n",
    "                              (v_speed_coef_mean * df_with_v['partner_mean_speed'])+\n",
    "                              (v_failed_pulls_before_reward_mean * df_with_v['failed_pulls_before_reward'])+\n",
    "                              (v_time_since_last_reward_mean * df_with_v['time_since_last_reward']))\n",
    "\n",
    "\n",
    "\n",
    "a_intercept_mean = traces['a_Intercept'].mean()\n",
    "a_failed_pulls_coef = traces['a_failed_pulls_before_reward'].mean()\n",
    "# ... other a coefficients ...\n",
    "df_lifelines['predicted_a'] = (a_intercept_mean + (a_failed_pulls_coef * df_lifelines['failed_pulls_before_reward']))\n",
    "\n",
    "# --- Extract 'z' (new step) ---\n",
    "# Get the mean z values for each condition\n",
    "z_after_failure = traces['z_trans(0)'].mean()\n",
    "z_after_success = traces['z_trans(1)'].mean()\n",
    "# Create the predicted_z column based on the previous trial's outcome\n",
    "df_lifelines['predicted_z'] = np.where(df_lifelines['prev_trial_outcome'] == 0, z_after_failure, z_after_success)\n",
    "\n",
    "\n",
    "## --- Part 2: Fit the Cox Proportional Hazards Model ---\n",
    "print(\"\\n--- Step 2: Fitting the Cox Proportional Hazards Model ---\")\n",
    "\n",
    "df_for_fitting = df_lifelines[['rt', 'response', 'predicted_v', 'predicted_a', 'predicted_z']]\n",
    "# df_for_fitting = df_lifelines[['rt', 'response', 'predicted_v']]\n",
    "\n",
    "# cph = CoxPHFitter()\n",
    "cph = CoxPHFitter(penalizer=0.01)\n",
    "\n",
    "cph.fit(df_for_fitting, duration_col='rt', event_col='response')\n",
    "print(\"Model Fit Summary:\")\n",
    "cph.print_summary()\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- Part 3: Prepare Continuous Timeline for Prediction ---\n",
    "print(\"\\n--- Step 3: Preparing Continuous Timeline ---\")\n",
    "# Define the total length and resolution of your prediction timeline\n",
    "total_experiment_seconds = df_for_fitting['rt'].sum()\n",
    "prediction_timestep = 0.1 # Make a prediction every 100ms (10 Hz)\n",
    "# Create the continuous timeline dataframe\n",
    "timeline_df = pd.DataFrame({'time': np.arange(0, total_experiment_seconds, prediction_timestep)})\n",
    "# Get the absolute time when each pull (event) occurred\n",
    "event_times = df_for_fitting['rt'].cumsum().rename('time')\n",
    "\n",
    "# Create a dataframe of the covariate values at the time of each event\n",
    "covariates_at_events = df_for_fitting[['predicted_v', 'predicted_a', 'predicted_z']].set_index(event_times)\n",
    "# covariates_at_events = df_for_fitting[['predicted_v']].set_index(event_times)\n",
    "\n",
    "# Map the covariate values onto the continuous timeline\n",
    "timeline_with_covariates = pd.merge_asof(timeline_df, covariates_at_events, on='time')\n",
    "timeline_with_covariates = timeline_with_covariates.ffill().dropna()\n",
    "print(\"Timeline prepared for prediction.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- Part 4: Run Stochastic Simulation to Predict Pulls ---\n",
    "# This part replaces the simple thresholding method.\n",
    "print(\"\\n--- Step 4: Running Stochastic Simulation ---\")\n",
    "# 4a: Predict the partial hazard (the step function based on covariates)\n",
    "partial_hazard = cph.predict_partial_hazard(timeline_with_covariates)\n",
    "timeline_with_covariates['partial_hazard'] = partial_hazard.values\n",
    "\n",
    "# 4b: Get the baseline hazard from the fitted model\n",
    "baseline_hazard_df = cph.baseline_hazard_\n",
    "\n",
    "# 4c: Map the baseline hazard onto our continuous timeline\n",
    "# MODIFICATION 3: Add .bfill() to fix the NaN issue at the beginning of the timeline.\n",
    "timeline_with_full_hazard = pd.merge(timeline_with_covariates, baseline_hazard_df,\n",
    "                                     left_on='time', right_index=True, how='left').ffill().bfill()\n",
    "\n",
    "# 4d: Calculate the full, time-varying hazard\n",
    "# Full Hazard h(t) = Baseline Hazard h(t) * Partial Hazard (from covariates)\n",
    "timeline_with_full_hazard['full_hazard'] = (timeline_with_full_hazard['baseline hazard'] *\n",
    "                                             timeline_with_full_hazard['partial_hazard'])\n",
    "\n",
    "# 4e: Run the simulation loop\n",
    "binary_prediction_list = []\n",
    "pulls_predicted_count = 0\n",
    "for index, row in timeline_with_full_hazard.iterrows():\n",
    "    # Calculate the probability of a pull in this specific time step\n",
    "    prob_of_pull = row['full_hazard'] * prediction_timestep\n",
    "    # Ensure probability is not > 1\n",
    "    prob_of_pull = min(prob_of_pull, 1.0)\n",
    "    # Simulate a \"coin flip\" weighted by this probability\n",
    "    if np.random.rand() < prob_of_pull:\n",
    "        binary_prediction_list.append(1)\n",
    "        pulls_predicted_count += 1\n",
    "    else:\n",
    "        binary_prediction_list.append(0)\n",
    "\n",
    "# Add the final prediction to our dataframe\n",
    "timeline_with_full_hazard['stochastic_prediction'] = binary_prediction_list\n",
    "print(f\"Simulation complete. Predicted {pulls_predicted_count} pulls.\")\n",
    "print(\"Sample of timeline with final predictions:\")\n",
    "print(timeline_with_full_hazard[['time', 'full_hazard', 'stochastic_prediction']].head(20))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- Part 5: Visualize the Results of the Simulation ---\n",
    "print(\"\\n--- Step 5: Visualizing the Prediction ---\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "# Plot the full predicted hazard over time\n",
    "ax.plot(timeline_with_full_hazard['time'], timeline_with_full_hazard['full_hazard'],\n",
    "        label='Full Predicted Hazard h(t)', color='mediumseagreen', alpha=0.9)\n",
    "\n",
    "# Mark the actual pull times from the data\n",
    "ax.vlines(event_times, ymin=0, ymax=ax.get_ylim()[1], color='black', linestyle='-',\n",
    "          alpha=0.7, label='Actual Pulls', linewidth=1.5)\n",
    "\n",
    "# Mark the predicted pull times from the stochastic simulation\n",
    "predicted_pull_df = timeline_with_full_hazard[timeline_with_full_hazard['stochastic_prediction'] == 1]\n",
    "ax.scatter(predicted_pull_df['time'], [ax.get_ylim()[1] * 0.95] * len(predicted_pull_df),\n",
    "           color='red', marker='v', s=80, label='Predicted Pulls (from Simulation)', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Full Predicted Hazard Rate', fontsize=12)\n",
    "ax.set_title('Stochastic Simulation of Pull Events Based on Predicted Hazard', fontsize=16)\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 500) # Zoom in on the first 150 seconds for clarity\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prerequisite: You must have a fitted Cox model 'cph' ---\n",
    "# and your dataframe 'df_for_fitting' from the previous steps.\n",
    "\n",
    "print(\"\\n--- Evaluating Model with a Deterministic Method ---\")\n",
    "\n",
    "# --- Step 1: Predict the Median Survival Time for each Interval ---\n",
    "# This gives us the model's single best guess for the duration of each IPI.\n",
    "predicted_durations = cph.predict_median(df_for_fitting)\n",
    "df_for_fitting['predicted_rt'] = predicted_durations.values\n",
    "\n",
    "# --- Step 2: Calculate the Predicted Pull Times ---\n",
    "# We can see when the model predicted pulls would happen by taking the\n",
    "# cumulative sum of the predicted durations.\n",
    "actual_pull_times = df_for_fitting['rt'].cumsum()\n",
    "predicted_pull_times = df_for_fitting['predicted_rt'].cumsum()\n",
    "\n",
    "print(\"Comparison of first 10 actual vs. predicted pull times:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual Pull Time': actual_pull_times,\n",
    "    'Predicted Pull Time': predicted_pull_times\n",
    "})\n",
    "print(comparison_df.head(10))\n",
    "\n",
    "# --- Step 3: Visualize the Comparison ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot actual vs. predicted times in a scatter plot\n",
    "ax.scatter(actual_pull_times, predicted_pull_times, alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add a y=x line. Perfect predictions would fall on this line.\n",
    "perfect_line_max = max(actual_pull_times.max(), predicted_pull_times.max())\n",
    "ax.plot([0, perfect_line_max], [0, perfect_line_max], 'r--', label='Perfect Prediction (y=x)')\n",
    "\n",
    "ax.set_xlabel('Actual Pull Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Predicted Pull Time (seconds)', fontsize=12)\n",
    "ax.set_title('Model Predictive Accuracy: Actual vs. Predicted Pull Times', fontsize=16)\n",
    "ax.legend()\n",
    "ax.axis('equal') # Ensure the plot is square for easy comparison\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e511b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4c25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
