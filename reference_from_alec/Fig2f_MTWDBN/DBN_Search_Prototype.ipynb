{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40db7f79-0a71-449a-b397-f7aa52d221f2",
   "metadata": {},
   "source": [
    "# Alec Version (simplified, no custom search code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71fde1fd-62e5-4e4e-a384-1efc07efbf39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/jadi/ags72/conda_envs/DBN/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time, time_ns\n",
    "import os\n",
    "\n",
    "import pgmpy\n",
    "from AicScore import AicScore\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "from EfficientTimeShuffling import EfficientShuffle\n",
    "import random\n",
    "\n",
    "sys.path.append('/home/ags72/Documents/MTWDBN/Tools')\n",
    "from GraphFunctions import graph_to_matrix_6pop_3timeslice, generate_starting_graph\n",
    "\n",
    "# bootstrap = int(sys.argv[1])\n",
    "# iteration = int(sys.argv[2])\n",
    "# shuffle = sys.argv[3] in ['True', 'true']\n",
    "# drop = int(sys.argv[4])\n",
    "\n",
    "bootstrap = 0\n",
    "iteration = 1\n",
    "shuffle = True\n",
    "drop = 0\n",
    "\n",
    "#Check if this search has been performed previously. If so, abort.\n",
    "if shuffle:\n",
    "    if os.path.isfile('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration)):\n",
    "        sys.exit(0)\n",
    "else:\n",
    "    if os.path.isfile('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration)):\n",
    "        sys.exit(0)\n",
    "\n",
    "#Hard coded variables\n",
    "timelags = 3\n",
    "num_starting_points = 40\n",
    "\n",
    "#Load spike table\n",
    "spikes_df_all = pd.read_csv('Alec Dataframes/laminar2_spikes_drop_{drop}_iteration_{iteration}.csv'.format(drop=drop, iteration = iteration), index_col = 0)\n",
    "\n",
    "#Sample data for the bootstrap, set seed equal to bootstrap\n",
    "spikes_df = spikes_df_all.sample(8000,replace = True, random_state = bootstrap)\n",
    "\n",
    "#Shuffle data after already selecting bootstrap\n",
    "if shuffle:\n",
    "    test, df_shufflekeys = EfficientShuffle(spikes_df,seed = round(time_ns()))\n",
    "\n",
    "#Arrays for storing starting points, resulting DAGs, and scores\n",
    "DAGs = np.zeros((num_starting_points,18,6))\n",
    "starting_graphs = np.zeros((num_starting_points,18,6))\n",
    "scores = np.zeros((num_starting_points))\n",
    "\n",
    "all_pops = list(spikes_df_all.columns)\n",
    "from_pops = [pop for pop in all_pops if not pop.endswith('t{}'.format(timelags))]\n",
    "to_pops = [pop for pop in all_pops if pop.endswith('t{}'.format(timelags))]\n",
    "\n",
    "# causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "acausal_whitelist = [(from_pop, to_pop) for from_pop in all_pops for to_pop in to_pops] #Allow edges in the last time slice\n",
    "\n",
    "#Scoring criteria\n",
    "aic = AicScore(spikes_df)\n",
    "\n",
    "for starting_point in range(num_starting_points):\n",
    "\n",
    "    #Create Random Starting Point (seed not controlled to increase randomness, but starting graph is saved)\n",
    "    np.random.seed(round(time()))\n",
    "    random.seed(round(time()))\n",
    "    starting_graph = generate_starting_graph(nodes = all_pops, whitelist = acausal_whitelist, max_degree = 4, last_time_nodes = to_pops)\n",
    "    starting_graphs[starting_point,:,:] = graph_to_matrix_6pop_3timeslice(list(starting_graph.edges())) #Save the starting graph now, otherwise it appears that pgmpy modifies it in place to resemble the final graph\n",
    "\n",
    "    #Perform hill search for DAG\n",
    "    hc = HillClimbSearch(spikes_df)\n",
    "    model = hc.estimate(tabu_length= 7, max_indegree=None, white_list = acausal_whitelist, scoring_method = AicScore(spikes_df), start_dag = starting_graph)\n",
    "\n",
    "    #Save resulting DAG and score\n",
    "    DAGs[starting_point,:,:] = graph_to_matrix_6pop_3timeslice(list(model.edges()))\n",
    "    scores[starting_point] = aic.score(model)\n",
    "\n",
    "#Save all results\n",
    "if shuffle:\n",
    "    np.save('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), DAGs)\n",
    "    np.save('DBN Outputs/Starting Graphs/startGraphs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), starting_graphs)\n",
    "    np.save('DBN Outputs/Scores/scores_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), scores)\n",
    "\n",
    "else:\n",
    "    np.save('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), DAGs)\n",
    "    np.save('DBN Outputs/Starting Graphs/startGraphs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), starting_graphs)\n",
    "    np.save('DBN Outputs/Scores/scores_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), scores)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026cddcf-0669-4a15-929d-b2ec5a99bc96",
   "metadata": {},
   "source": [
    "# Anirban Search Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb296269-9ed4-41a2-877f-c2ab67261c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 unique starting points generated\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time, time_ns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pgmpy\n",
    "from AicScore import AicScore\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "from EfficientTimeShuffling import EfficientShuffle\n",
    "import random\n",
    "\n",
    "#Custom function\n",
    "sys.path.append('/home/ags72/Documents/MTWDBN/Tools')\n",
    "from GraphFunctions import graph_to_matrix_6pop_3timeslice, generate_starting_graph\n",
    "from Step2FittingBayesianNetworkToData import HillClimbSearch5\n",
    "\n",
    "#input parameters\n",
    "bootstrap = 1\n",
    "iteration = 1\n",
    "shuffle = True\n",
    "drop = 20\n",
    "\n",
    "# bootstrap = int(sys.argv[1])\n",
    "# iteration = int(sys.argv[2])\n",
    "# shuffle = sys.argv[3] in ['True', 'true']\n",
    "# drop = int(sys.argv[4])\n",
    "\n",
    "#Check if this search has been performed previously. If so, abort.\n",
    "if shuffle:\n",
    "    if os.path.isfile('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration)):\n",
    "        sys.exit(0)\n",
    "else:\n",
    "    if os.path.isfile('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration)):\n",
    "        sys.exit(0)\n",
    "\n",
    "#Hard coded variables\n",
    "timelags = 3\n",
    "num_starting_points = 10\n",
    "\n",
    "\n",
    "#Load spike table\n",
    "if shuffle:\n",
    "    spikes_df_all = pd.read_csv('Dataframes/laminar2_spikes_drop_{drop}_iteration_{iteration}_shuffle.csv'.format(drop=drop, iteration = iteration), index_col = 0)\n",
    "else:\n",
    "    spikes_df_all = pd.read_csv('Dataframes/laminar2_spikes_drop_{drop}_iteration_{iteration}.csv'.format(drop=drop, iteration = iteration), index_col = 0)\n",
    "\n",
    "#Sample data for the bootstrap, set seed equal to bootstrap\n",
    "spikes_df = spikes_df_all.sample(8000,replace = True, random_state = bootstrap)\n",
    "\n",
    "#Arrays for storing starting points, resulting DAGs, and scores\n",
    "DAGs = np.zeros((num_starting_points,18,6))\n",
    "starting_graphs = np.zeros((num_starting_points,18,6))\n",
    "scores = np.zeros((num_starting_points))\n",
    "\n",
    "all_pops = list(spikes_df_all.columns)\n",
    "from_pops = [pop for pop in all_pops if not pop.endswith('t{}'.format(timelags))]\n",
    "to_pops = [pop for pop in all_pops if pop.endswith('t{}'.format(timelags))]\n",
    "\n",
    "# causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "acausal_whitelist = [(from_pop, to_pop) for from_pop in all_pops for to_pop in to_pops] #Allow edges in the last time slice\n",
    "\n",
    "#Scoring criteria\n",
    "aic = AicScore(spikes_df)\n",
    "\n",
    "\n",
    "#Anirban HillClimbSearch Analysis\n",
    "hc = HillClimbSearch5(spikes_df, scoring_method= aic)\n",
    "\n",
    "\n",
    "#Create random starting points using Anirban's code\n",
    "starter_graphs = []\n",
    "while len(np.unique([seed[1] for seed in starter_graphs])) != num_starting_points: #This is to ensure that 120 unique graphs are generated by checking that the seed for each graph is unique\n",
    "    starter_graphs = []\n",
    "    for iSeed in range(num_starting_points):\n",
    "        random.seed(time_ns())\n",
    "        starter_graphs.append(hc.createRandLegalDag3(timelags, maxdeg = 4, seed = random.randint(1,2**32-1)))\n",
    "print('{} unique starting points generated'.format(num_starting_points))\n",
    "\n",
    "\n",
    "for idx,starting_point in enumerate(starter_graphs):\n",
    "\n",
    "    \n",
    "    starting_graphs[idx,:,:] = graph_to_matrix_6pop_3timeslice(list(starting_point[0].edges()))\n",
    "    model = hc.RestrictedEstimatetl(timelags,start=starting_point[0], tabu_length= 7 , max_indegree=None, N=5)\n",
    "    score = hc.scoring_method.score(model)\n",
    "\n",
    "    #Save resulting DAG and score\n",
    "    DAGs[idx,:,:] = graph_to_matrix_6pop_3timeslice(list(model.edges()))\n",
    "    scores[idx] = score\n",
    "\n",
    "\n",
    "# Save all results\n",
    "if shuffle:\n",
    "    np.save('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), DAGs)\n",
    "    np.save('DBN Outputs/Starting Graphs/startGraphs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), starting_graphs)\n",
    "    np.save('DBN Outputs/Scores/scores_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), scores)\n",
    "\n",
    "   \n",
    "else:\n",
    "    np.save('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), DAGs)\n",
    "    np.save('DBN Outputs/Starting Graphs/startGraphs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), starting_graphs)\n",
    "    np.save('DBN Outputs/Scores/scores_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), scores)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50deac-e8e1-417a-8fe4-3b6cdfd3cfb8",
   "metadata": {},
   "source": [
    "# Anirban Search Code on His Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12839192-f33e-41af-9c66-5e43cf97b31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 unique starting points generated\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time, time_ns\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import pgmpy\n",
    "from AicScore import AicScore\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "from EfficientTimeShuffling import EfficientShuffle\n",
    "import random\n",
    "\n",
    "#Custom function\n",
    "sys.path.append('/home/ags72/Documents/MTWDBN/Tools')\n",
    "from GraphFunctions import graph_to_matrix_6pop_3timeslice, generate_starting_graph\n",
    "from Step2FittingBayesianNetworkToData import HillClimbSearch5\n",
    "\n",
    "#input parameters\n",
    "bootstrap = 1\n",
    "shuffle = False\n",
    "drop = 20\n",
    "\n",
    "# bootstrap = int(sys.argv[1])\n",
    "# shuffle = sys.argv[2] in ['True', 'true']\n",
    "# drop = int(sys.argv[3])\n",
    "\n",
    "# Check if this search has been performed previously. If so, abort.\n",
    "if shuffle:\n",
    "    if os.path.isfile('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration)):\n",
    "        sys.exit(0)\n",
    "else:\n",
    "    if os.path.isfile('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration)):\n",
    "        sys.exit(0)\n",
    "\n",
    "#Hard coded variables\n",
    "timelags = 3\n",
    "num_starting_points = 10\n",
    "iteration = 1\n",
    "\n",
    "#Load spike table\n",
    "spikes_df_all = pd.read_csv('Anirban Dataframes/{drop}Laminar_Set2_bins_1667_timelayers_3_trials_1000_units_45_pooled__triallength_2000.csv'.format(drop=drop), index_col = 0)\n",
    "spikes_df_all.reset_index(inplace =True, drop = True)\n",
    "\n",
    "#Shuffle data if set\n",
    "if shuffle:\n",
    "    spikes_df_all, df_shufflekeys = EfficientShuffle(spikes_df_all,seed = round(time_ns()))\n",
    "\n",
    "\n",
    "#Sample data for the bootstrap, set seed equal to bootstrap\n",
    "spikes_df = spikes_df_all.sample(8000,replace = True, random_state = bootstrap)\n",
    "\n",
    "new_pop_names = {'V_lyr1_spk2_t1': 'E1_t1',\n",
    "                 'V_lyr1_spk1_t1': 'I1_t1',\n",
    "                 'V_lyr2_spk2_t1': 'E2_t1',\n",
    "                 'V_lyr2_spk1_t1': 'I2_t1',\n",
    "                 'V_lyr3_spk2_t1': 'E3_t1',\n",
    "                 'V_lyr3_spk1_t1': 'I3_t1',\n",
    "                 \n",
    "                 'V_lyr1_spk2_t2': 'E1_t2',\n",
    "                 'V_lyr1_spk1_t2': 'I1_t2',\n",
    "                 'V_lyr2_spk2_t2': 'E2_t2',\n",
    "                 'V_lyr2_spk1_t2': 'I2_t2',\n",
    "                 'V_lyr3_spk2_t2': 'E3_t2',\n",
    "                 'V_lyr3_spk1_t2': 'I3_t2',\n",
    "                 \n",
    "                 'V_lyr1_spk2_t3': 'E1_t3',\n",
    "                 'V_lyr1_spk1_t3': 'I1_t3',\n",
    "                 'V_lyr2_spk2_t3': 'E2_t3',\n",
    "                 'V_lyr2_spk1_t3': 'I2_t3',\n",
    "                 'V_lyr3_spk2_t3': 'E3_t3',\n",
    "                 'V_lyr3_spk1_t3': 'I3_t3'}\n",
    "\n",
    "spikes_df.rename(columns = new_pop_names,inplace = True)\n",
    "\n",
    "#Arrays for storing starting points, resulting DAGs, and scores\n",
    "DAGs = np.zeros((num_starting_points,18,6))\n",
    "starting_graphs = np.zeros((num_starting_points,18,6))\n",
    "scores = np.zeros((num_starting_points))\n",
    "\n",
    "all_pops = list(spikes_df.columns)\n",
    "from_pops = [pop for pop in all_pops if not pop.endswith('t{}'.format(timelags))]\n",
    "to_pops = [pop for pop in all_pops if pop.endswith('t{}'.format(timelags))]\n",
    "\n",
    "# causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "acausal_whitelist = [(from_pop, to_pop) for from_pop in all_pops for to_pop in to_pops] #Allow edges in the last time slice\n",
    "\n",
    "#Scoring criteria\n",
    "aic = AicScore(spikes_df)\n",
    "\n",
    "\n",
    "#Anirban HillClimbSearch Analysis\n",
    "hc = HillClimbSearch5(spikes_df, scoring_method= aic)\n",
    "\n",
    "\n",
    "#Create random starting points using Anirban's code\n",
    "starter_graphs = []\n",
    "while len(np.unique([seed[1] for seed in starter_graphs])) != num_starting_points: #This is to ensure that 120 unique graphs are generated by checking that the seed for each graph is unique\n",
    "    starter_graphs = []\n",
    "    for iSeed in range(num_starting_points):\n",
    "        random.seed(time_ns())\n",
    "        starter_graphs.append(hc.createRandLegalDag3(timelags, maxdeg = 4, seed = random.randint(1,2**32-1)))\n",
    "print('{} unique starting points generated'.format(num_starting_points))\n",
    "\n",
    "\n",
    "for idx,starting_point in enumerate(starter_graphs):\n",
    "\n",
    "    \n",
    "    starting_graphs[idx,:,:] = graph_to_matrix_6pop_3timeslice(list(starting_point[0].edges()))\n",
    "    model = hc.RestrictedEstimatetl(timelags,start=starting_point[0], tabu_length= 7 , max_indegree=None, N=5)\n",
    "    score = hc.scoring_method.score(model)\n",
    "\n",
    "    #Save resulting DAG and score\n",
    "    DAGs[idx,:,:] = graph_to_matrix_6pop_3timeslice(list(model.edges()))\n",
    "    scores[idx] = score\n",
    "\n",
    "\n",
    "# Save all results\n",
    "if shuffle:\n",
    "    np.save('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), DAGs)\n",
    "    np.save('DBN Outputs/Starting Graphs/startGraphs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), starting_graphs)\n",
    "    np.save('DBN Outputs/Scores/scores_drop{drop}_bootstrap{bootstrap}_iteration{iteration}_shuffle.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), scores)\n",
    "\n",
    "   \n",
    "else:\n",
    "    np.save('DBN Outputs/DAGs/DAGs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), DAGs)\n",
    "    np.save('DBN Outputs/Starting Graphs/startGraphs_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), starting_graphs)\n",
    "    np.save('DBN Outputs/Scores/scores_drop{drop}_bootstrap{bootstrap}_iteration{iteration}.npy'.format(drop = drop, bootstrap = bootstrap, iteration = iteration), scores)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9888f20-f89a-42f1-a8f7-b99d5e6ecbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
