{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8befe47-e21c-4360-8417-c78145e58722",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ed3be8-8eb4-43b6-bbb5-31f5a40e23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "\n",
    "from EfficientTimeShuffling import EfficientShuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd22e87a-c703-40a4-a82a-79790b9c0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_tables(binWidth = 1.2, shuffle = False, combine = False, seed = 42, timelags = 3, drop = 0):\n",
    "    #Set seed to be used for this iteration\n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    all_trials = []\n",
    "    all_spikes = loadmat('/home/ags72/Neural_Data/Synthetic/Single_Epoch/Laminar_Set_2a.mat')['SPIKES']    #Bin spikes\n",
    "    \n",
    "    for trial in tqdm(range(all_spikes.shape[-1])):\n",
    "       #Bin spikes\n",
    "        trial_duration = 2000 \n",
    "        hist_spikes = []\n",
    "        for iNeuron,neuron in enumerate(all_spikes):\n",
    "            counts, edges = np.histogram(neuron[trial], bins = np.arange(0,trial_duration+binWidth, binWidth))\n",
    "            hist_spikes.append(counts)\n",
    "            \n",
    "        #Combine spikes from same population by either layer/class or layer only\n",
    "        if combine:\n",
    "            population_indices = [np.arange(0,15),\n",
    "                                  np.arange(15,30),\n",
    "                                  np.arange(30,45)]\n",
    "            \n",
    "            pop_type = 'layeronly'\n",
    "            column_names = ['_'.join([layer,'t{}'.format(timelag)]) for timelag in np.arange(1,timelags+1) for layer in ['L1','L2','L3'] ]\n",
    "        else:\n",
    "            population_indices = [np.arange(0,10),\n",
    "                                  np.arange(10,15),\n",
    "                                  np.arange(15,25),\n",
    "                                  np.arange(25,30),\n",
    "                                  np.arange(30,40),\n",
    "                                  np.arange(40,45)]  \n",
    "            \n",
    "            pop_type = 'layerclass'\n",
    "            column_names = ['_'.join([pop,'t{}'.format(timelag)]) for timelag in np.arange(1,timelags+1) for pop in ['E1', 'I1', 'E2', 'I2', 'E3', 'I3']]\n",
    "        \n",
    "        \n",
    "        #Drop some percentage of neurons in each population\n",
    "        if drop is not 0:\n",
    "            for idx,indicies in enumerate(population_indices):\n",
    "                population_indices[idx] = np.random.choice(indicies, math.ceil(len(indicies) * (1-drop)), replace = False)\n",
    "        \n",
    "        #Combine spikes in each population\n",
    "        all_pop_spikes = []\n",
    "        for indicies in population_indices:\n",
    "            pop_hist_spikes = [hist_spikes[idx] for idx in indicies]\n",
    "            all_pop_spikes.append(np.sum(pop_hist_spikes,axis = 0))\n",
    "            \n",
    "        all_pop_spikes = np.array(all_pop_spikes, dtype = int)\n",
    "        all_pop_spikes[all_pop_spikes>=2] = 2\n",
    "        \n",
    "        #Creat lagged variables\n",
    "        lagged_spikes = []\n",
    "        for lag in range(timelags):\n",
    "            if lag == timelags - 1:\n",
    "                lagged_spikes.append(all_pop_spikes[:,lag:])\n",
    "            else:\n",
    "                lagged_spikes.append(all_pop_spikes[:,lag:lag-timelags+1])\n",
    "        lagged_spikes = np.vstack(lagged_spikes)\n",
    "                                  \n",
    "        trial_df = pd.DataFrame(lagged_spikes.T, columns = column_names)\n",
    "        all_trials.append(trial_df)\n",
    "\n",
    "    spikes_df_all = pd.concat(all_trials).reset_index(drop=True)\n",
    "    \n",
    "    #Anirban shuffle, slow\n",
    "    spikes_df_all_shuffle, df_shufflekeys = EfficientShuffle(spikes_df_all,seed = round(time()))\n",
    "    \n",
    "    #Faster shuffle algorithm, only shuffles columns\n",
    "    # spikes_df_all_shuffle = spikes_df_all.copy()\n",
    "    # for (columnName, columnData) in spikes_df_all.iteritems():\n",
    "    #     test[columnName] = np.random.permutation(columnData)\n",
    "    \n",
    "    #Save dataframes to csv\n",
    "    spikes_df_all.to_csv('Dataframes/laminar2_spikes_drop_{drop}_iteration_{iteration}.csv'.format(drop = int(100*drop), iteration = seed))\n",
    "    spikes_df_all_shuffle.to_csv('Dataframes/laminar2_spikes_drop_{drop}_iteration_{iteration}_shuffle.csv'.format(drop = int(100*drop), iteration = seed))\n",
    "\n",
    "\n",
    "    # return spikes_df_all, spikes_df_all_shuffle\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c048ef8-13c3-4fc8-8c9d-35ec7a2db4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 215.80it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 224.09it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 218.65it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 207.43it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 213.49it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 226.69it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 214.77it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 231.06it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 224.34it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 225.78it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 228.39it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 227.62it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 215.30it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 222.76it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 219.35it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 220.65it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 216.05it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 219.70it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 224.24it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 219.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for drop in [0.2, 0.4, 0.6, 0.8]:\n",
    "    for iteration in [1,2,3,4,5]:\n",
    "        prepare_data_tables(drop = drop, seed = iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39314f7-6989-4953-960f-fd9cfe6a5082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:04<00:00, 212.18it/s]\n"
     ]
    }
   ],
   "source": [
    "prepare_data_tables(drop = 0, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59bf13-417d-4835-bb19-6b10619ce246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
