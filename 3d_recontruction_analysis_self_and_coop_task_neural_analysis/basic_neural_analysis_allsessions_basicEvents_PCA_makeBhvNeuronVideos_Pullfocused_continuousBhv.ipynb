{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### analyze the spike triggered pull and gaze ditribution\n",
    "#### the following detailed analysis focused on pull related behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam import plot_continuous_bhv_var_singlecam\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545792d",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d40abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "from ana_functions.get_pull_infos import get_pull_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI_dual_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# if use onset of the first increase after min\n",
    "doOnsetAfterMin = 0\n",
    "if not doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = ''\n",
    "elif doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = 'PullOnsetAfterMin_'\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     # '20231101_Dodson_withGinger_SR', # ginger didnot pull or only pulled once\n",
    "                                     # '20231107_Dodson_withGinger_SR', # ginger didnot pull or only pulled once\n",
    "                                     # '20231122_Dodson_withGinger_SR', # ginger didnot pull or only pulled once\n",
    "                                     # '20231129_Dodson_withGinger_SR', # ginger didnot pull or only pulled once\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      # \"20231101_SR\",\n",
    "                      # \"20231107_SR\",\n",
    "                      # \"20231122_SR\",\n",
    "                      # \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 # 0.00,   \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         # 2, \n",
    "                         # 2, \n",
    "                         # 4, \n",
    "                         # 4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorders = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorders = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_infos_all_dates = dict.fromkeys(dates_list, []) # keep some useful information about pulls - time from last reward, number of preceding failed pull etc\n",
    "\n",
    "spike_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvevents_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "strategy_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "strategy_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/succpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpull_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/failpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpull_trig_events_all_dates = pickle.load(f)    \n",
    "    \n",
    "    with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_infos_all_dates  = pickle.load(f)      \n",
    "    \n",
    "    # this one should be calculated in the XX_PullStartfocused_xx code\n",
    "    with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_rts_all_dates  = pickle.load(f)\n",
    "    \n",
    "    with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        spike_trig_events_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos_all_dates = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                              time_point_juice1, time_point_juice2)\n",
    "\n",
    "    \n",
    "            \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, totalsess_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, totalsess_time, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "     \n",
    "        \n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 0:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+cameraID+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "\n",
    "            savefig = 1\n",
    "            \n",
    "            aligntwins = 4 # 5 second\n",
    "            \n",
    "            min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "\n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                    aligntwins, savefig, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                        aligntwins, savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpull_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                        aligntwins, savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpull_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 1:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+\\\n",
    "                            animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_average_all,bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_aligned_FR_all_dates[date_tgt] = bhvevents_aligned_FR_average_all\n",
    "                bhvevents_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "            \n",
    "            # the three strategy aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot strategy aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+\\\n",
    "                            animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                stg_twins = 1.5 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                strategy_aligned_FR_average_all,strategy_aligned_FR_allevents_all = plot_strategy_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,stg_twins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                strategy_aligned_FR_all_dates[date_tgt] = strategy_aligned_FR_average_all\n",
    "                strategy_aligned_FR_allevents_all_dates[date_tgt] = strategy_aligned_FR_allevents_all\n",
    "                \n",
    "            \n",
    "            #\n",
    "            # Run PCA analysis\n",
    "            FR_zscore_allch_np_merged = np.array(pd.DataFrame(FR_zscore_allch).T)\n",
    "            FR_zscore_allch_np_merged = FR_zscore_allch_np_merged[~np.isnan(np.sum(FR_zscore_allch_np_merged,axis=1)),:]\n",
    "            # # run PCA on the entire session\n",
    "            pca = PCA(n_components=3)\n",
    "            FR_zscore_allch_PCs = pca.fit_transform(FR_zscore_allch_np_merged.T)\n",
    "            #\n",
    "            # # run PCA around the -PCAtwins to PCAtwins for each behavioral events\n",
    "            PCAtwins = 4 # 5 second\n",
    "            gaze_thresold = 0.5 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "            savefigs = 0 \n",
    "            if 0:\n",
    "                PCA_around_bhv_events(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                              savefigs,data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            if 0:\n",
    "                if (np.isin(animal1, ['dodson'])) | (np.isin(animal2, ['kanga'])):\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                                      oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "                elif (np.isin(animal2, ['dodson'])) | (np.isin(animal1, ['kanga'])):\n",
    "                    time_point_pulls_succfail_rev = time_point_pulls_succfail.copy()\n",
    "                    time_point_pulls_succfail_rev['pull1_succ'] = time_point_pulls_succfail['pull2_succ']\n",
    "                    time_point_pulls_succfail_rev['pull1_fail'] = time_point_pulls_succfail['pull2_fail']\n",
    "                    time_point_pulls_succfail_rev['pull2_succ'] = time_point_pulls_succfail['pull1_succ']\n",
    "                    time_point_pulls_succfail_rev['pull2_fail'] = time_point_pulls_succfail['pull1_fail']\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull2,time_point_pull1,time_point_pulls_succfail_rev, \n",
    "                                      oneway_gaze2,oneway_gaze1,mutual_gaze2,mutual_gaze1,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # do the spike triggered average of different bhv variables, for the single camera tracking, look at the pulling and social gaze actions\n",
    "            # the goal is to get a sense for glm\n",
    "            if 1: \n",
    "                print('plot spike triggered bhv variables')\n",
    "\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+\\\n",
    "                            animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                do_shuffle = 0\n",
    "                #\n",
    "                min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "                #\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                \n",
    "                gaze_thresold = 0.2\n",
    "                \n",
    "                stg_twins = 3 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                #\n",
    "                spike_trig_average_all =  plot_spike_triggered_singlecam_bhvevent(date_tgt,savefig,save_path, animal1, animal2, session_start_time,min_length, trig_twins,\n",
    "                                                                              stg_twins, time_point_pull1, time_point_pull2, time_point_pulls_succfail,\n",
    "                                                                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,animalnames_videotrack,\n",
    "                                                                              spike_clusters_data, spike_time_data,spike_channels_data,do_shuffle)\n",
    "\n",
    "                spike_trig_events_all_dates[date_tgt] = spike_trig_average_all\n",
    "\n",
    "            \n",
    "        # load filtered lfp\n",
    "        if 0:\n",
    "            print('load LFP data for '+neural_record_condition)\n",
    "            lfp_filt_filename = neural_data_folder+neural_record_condition+'/lfp_filt_subsample.txt' # already downsample to 30Hz\n",
    "            lfp_filt_data_df = genfromtxt(lfp_filt_filename, delimiter=',')\n",
    "            # aligned to the session start\n",
    "            lfp_filt_sess_aligned=lfp_filt_data_df[:,int(-neural_start_time_session_start_offset*30):]\n",
    "            # normalize the activity to 0 - 1\n",
    "            lfp_filt_sess_aligned = (lfp_filt_sess_aligned-np.min(lfp_filt_sess_aligned))/(np.max(lfp_filt_sess_aligned)-np.min(lfp_filt_sess_aligned))\n",
    "\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            print('make the demo videos')\n",
    "            if 0:\n",
    "                # all the bhv traces in the same panel\n",
    "                tracking_video_singlecam_wholebody_withNeuron_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                  lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                  animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                  animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                                  video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body,\n",
    "                                                  spike_time_data,lfp_filt_sess_aligned,spike_channels_data,channel_to_depth)\n",
    "            if 1:\n",
    "                # all the bhv traces are in separate panels\n",
    "                tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                 lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                 animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                 animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                                 video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body,\n",
    "                                                 spike_time_data,lfp_filt_sess_aligned,spike_channels_data,channel_to_depth)\n",
    "        \n",
    "        # plot the example frame from the tracking demo video\n",
    "        if 0: \n",
    "            print('print the example frame from the demo videos')\n",
    "            if 1:\n",
    "                example_frame = 60*30+1\n",
    "                start_frame = 55*30\n",
    "                # all the bhv traces are in separate panels\n",
    "                tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                 lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                 animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                 animal1_filename,animal2_filename,session_start_time,fps,start_frame,example_frame,cameraID,\n",
    "                                                 video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body,\n",
    "                                                 spike_time_data,lfp_filt_sess_aligned,spike_channels_data,channel_to_depth)\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+\\\n",
    "                            animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt+\"/\"\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                if savefig:\n",
    "                    plt.savefig(save_path+'singlecam_wholebody_tracking_withNeuron_sepbhv_demo_oneframe.pdf')\n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpull_trig_events_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)             \n",
    "            \n",
    "        with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(spike_trig_events_all_dates, f)  \n",
    "    \n",
    "        with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_allevents_all_dates, f) \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_trig_events_all_dates['20240508'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a99422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_infos_all_dates['20240508'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_rts_all_dates['20240531']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "#### re-organized the data\n",
    "#### for the activity aligned at the different single behavioral events, mostly focus on self pull\n",
    "#### the ultamate goal is to analyze the difference in single trial and if gaze related variables related to any of them\n",
    "#### make some prilimary plot for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "# pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "pull_trig_events_tgtname = 'socialgaze_prob' # for testing if individual trial different was from gaze start time\n",
    "# pull_trig_events_tgtname = 'othergaze_prob' # if to test things aligned to partner's pull (in that case, the subject's gaze becomes othergaze) \n",
    "\n",
    "\n",
    "# Keep these as additional controls\n",
    "pull_trig_otherpull_name = 'otherpull_prob'\n",
    "pull_trig_selfpull_name = 'selfpull_prob'\n",
    "pull_time_pre_reward_name = 'time_from_last_reward'\n",
    "pull_num_pre_failpull_name = 'num_preceding_failpull'\n",
    "pull_rt_name = 'pull_rt'\n",
    "\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                                    'succrate','clusterID',\n",
    "                                                                    'channelID','FR_allevents'])\n",
    "bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                          'succrate','clusterID',\n",
    "                                                          'channelID','FR_average'])\n",
    "\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "\n",
    "    succrate = succ_rate_all_dates[idate]\n",
    "    \n",
    "    bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "\n",
    "        clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if np.shape(ibhv_type_split)[0]==3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "        # load the pull_trig_continuous_events\n",
    "        if ibhv_type_split[1] == 'pull':\n",
    "            try:\n",
    "                pull_trig_events_tgt = pull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_otherpull = pull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherpull_name)]\n",
    "                pull_trig_selfpull = pull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfpull_name)]\n",
    "                pull_trig_prerewardtime = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],pull_time_pre_reward_name)]))\n",
    "                pull_num_pre_failpull = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],pull_num_pre_failpull_name)]))\n",
    "                pull_rt = np.array(list(pull_rts_all_dates[date_tgt][ibhv_type_split[0]]))\n",
    "                succornot_pull = (np.hstack([pull_num_pre_failpull[1:],0])==0)*1\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_otherpull = np.nan\n",
    "                pull_trig_selfpull = np.nan\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                pull_num_pre_failpull = np.nan\n",
    "                pull_rt = np.nan\n",
    "                succornot_pull = np.nan\n",
    "        #\n",
    "        elif ibhv_type_split[1] == 'succpull':\n",
    "            try:\n",
    "                pull_trig_events_tgt = succpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_otherpull = succpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherpull_name)]\n",
    "                pull_trig_selfpull = succpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfpull_name)]\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                pull_num_pre_failpull = np.nan\n",
    "                pull_rt = np.nan\n",
    "                succornot_pull = np.nan\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_otherpull = np.nan\n",
    "                pull_trig_selfpull = np.nan\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                pull_num_pre_failpull = np.nan\n",
    "                pull_rt = np.nan\n",
    "                succornot_pull = np.nan\n",
    "        #\n",
    "        elif ibhv_type_split[1] == 'failpull':\n",
    "            try:\n",
    "                pull_trig_events_tgt = failpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_otherpull = failpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherpull_name)]\n",
    "                pull_trig_selfpull = failpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfpull_name)]\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                pull_num_pre_failpull = np.nan\n",
    "                pull_rt = np.nan\n",
    "                succornot_pull = np.nan\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_otherpull = np.nan\n",
    "                pull_trig_selfpull = np.nan\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                pull_num_pre_failpull = np.nan\n",
    "                pull_rt = np.nan\n",
    "                succornot_pull = np.nan\n",
    "        #\n",
    "        else:\n",
    "            pull_trig_events_tgt = np.nan\n",
    "            \n",
    "        for iclusterID in clusterIDs:   \n",
    "\n",
    "            ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'succrate':succrate,\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_allevents':iFR_average,\n",
    "                                                                                     pull_trig_events_tgtname:pull_trig_events_tgt,                          \n",
    "                                                                                     pull_trig_otherpull_name:pull_trig_otherpull,                          \n",
    "                                                                                     pull_trig_selfpull_name:pull_trig_selfpull,\n",
    "                                                                                     pull_time_pre_reward_name:pull_trig_prerewardtime,\n",
    "                                                                                     pull_num_pre_failpull_name:pull_num_pre_failpull,\n",
    "                                                                                     'succornot_pull':succornot_pull,\n",
    "                                                                                     pull_rt_name:pull_rt,\n",
    "                                                                                    }, ignore_index=True)\n",
    "\n",
    "            #\n",
    "            ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'succrate':succrate,\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_average':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc23192",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f524cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc01146",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf229da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "# act_animals_to_ana = ['kanga']\n",
    "act_animals_to_ana = ['dodson']\n",
    "# act_animals_to_ana = ['kanga_partner'] # align thing to partner, remember to pair this with pull_trig_events_tgtname = 'othergaze_prob'\n",
    "# act_animals_to_ana = ['kanga','kanga_partner']\n",
    "# act_animals_to_ana = ['dodson','dodson_partner']\n",
    "\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "# bhv_names_to_ana = ['failpull']\n",
    "bhv_names_to_ana = ['pull']\n",
    "# bhv_names_to_ana = ['gazestart']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "\n",
    "#\n",
    "conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'SR', 'SR_withDodson' ]\n",
    "# \n",
    "# for dodson only\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                       'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "#\n",
    "# conditions_to_ana = ['MC', \n",
    "#               'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', 'SR', 'SR_withGingerNew', 'SR_withKanga',\n",
    "#              'SR_withKoala',  ]\n",
    "\n",
    "\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25773d1a",
   "metadata": {},
   "source": [
    "### basic basic sanity check plot, bhv event aligned mean FR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:   \n",
    "        \n",
    "    # prepare the data\n",
    "    mean_FR_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                       'clusterID','meanFR'])\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            \n",
    "            if not 'partner' in act_animal_ana:\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            elif 'partner' in act_animal_ana:\n",
    "                animal_dontwant = act_animal_ana.split('_')[0]\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']!=animal_dontwant\n",
    "                \n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                        #\n",
    "                        # load and plot bhv event ('pull') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "                            \n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            time_trace = np.arange(-4,4,1/fps)  # Assuming time is just indices\n",
    "                        #\n",
    "                        else:\n",
    "                            FRsmoothed_allevents_ineuron = np.nan\n",
    "                            time_trace = np.arange(-4,4,1/fps)  # Assuming time is just indices\n",
    "                            mean_trace = np.ones(np.shape(time_trace))*np.nan\n",
    "                            \n",
    "                        #\n",
    "                        mean_FR_df = mean_FR_df.append({'dates': date_ana, \n",
    "                                                        'condition':cond_ana,\n",
    "                                                        'act_animal':act_animal_ana,\n",
    "                                                        'bhv_name': bhvname_ana,\n",
    "                                                        'clusterID':clusterID_ineuron,\n",
    "                                                        'meanFR':mean_trace,     \n",
    "                                                       }, ignore_index=True)\n",
    "                            \n",
    "    \n",
    "    # do the plotting\n",
    "    \n",
    "    #####\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    mean_FR_df[\"condition_general\"] = mean_FR_df[\"condition\"].apply(generalize_condition)\n",
    "    #####\n",
    "    \n",
    "    # Get unique animals and generalized conditions\n",
    "    animals = mean_FR_df['act_animal'].unique()\n",
    "    conditions_general_toplot = np.unique(mean_FR_df['condition_general'])\n",
    "\n",
    "    # Count subplots needed\n",
    "    ncols = len(conditions_general_toplot)\n",
    "    nrows = len(animals)\n",
    "\n",
    "    fig = plt.figure(figsize=(6 * ncols, 4 * nrows))\n",
    "    gs = gridspec.GridSpec(nrows, ncols + 1, width_ratios=[1] * ncols + [0.05], wspace=0.4, hspace=0.6)\n",
    "\n",
    "    # Determine global color limits across all data\n",
    "    all_vmax, all_vmin = [], []\n",
    "\n",
    "    for animal in animals:\n",
    "        for cond in conditions_general_toplot:\n",
    "            df_sub = mean_FR_df[(mean_FR_df['act_animal'] == animal) &\n",
    "                                (mean_FR_df['condition_general'] == cond)]\n",
    "            if not df_sub.empty:\n",
    "                meanFR_matrix = np.vstack(df_sub['meanFR'].values)\n",
    "                all_vmax.append(np.max(meanFR_matrix))\n",
    "                all_vmin.append(np.min(meanFR_matrix))\n",
    "\n",
    "    global_vmax = max(all_vmax)\n",
    "    global_vmin = min(all_vmin)\n",
    "    center_val = 0 if global_vmin < 0 else np.mean([global_vmin, global_vmax])\n",
    "    vmax = global_vmax\n",
    "    vmin = 2 * center_val - vmax\n",
    "\n",
    "    # Start plotting\n",
    "    for i_animal, animal in enumerate(animals):\n",
    "        for i_cond, cond in enumerate(conditions_general_toplot):\n",
    "            ax = fig.add_subplot(gs[i_animal, i_cond])\n",
    "            df_sub = mean_FR_df[(mean_FR_df['act_animal'] == animal) &\n",
    "                                (mean_FR_df['condition_general'] == cond)]\n",
    "\n",
    "            if df_sub.empty:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "\n",
    "            meanFR_matrix = np.vstack(df_sub['meanFR'].values)\n",
    "            peak_times = np.argmax(meanFR_matrix, axis=1)\n",
    "            sort_idx = np.argsort(peak_times)\n",
    "            meanFR_sorted = meanFR_matrix[sort_idx, :]\n",
    "\n",
    "            im = ax.imshow(meanFR_sorted,\n",
    "                           aspect='auto',\n",
    "                           cmap='seismic',\n",
    "                           extent=[time_trace[0], time_trace[-1], 0, meanFR_sorted.shape[0]],\n",
    "                           interpolation='nearest',\n",
    "                           vmin=vmin,\n",
    "                           vmax=vmax)\n",
    "\n",
    "            ax.axvline(0, color='k', linestyle='--', linewidth=1)\n",
    "            ax.set_title(f'{animal} - {cond}')\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            if i_cond == 0:\n",
    "                ax.set_ylabel('Neuron index')\n",
    "\n",
    "    # Add colorbar in the last column\n",
    "    cax = fig.add_subplot(gs[:, -1])\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.set_label('Mean Firing Rate (Hz)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #                        \n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"    \n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_across_allevents_'+bhvname_ana+\n",
    "                     savefile_sufix+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd094a",
   "metadata": {},
   "source": [
    "### sanity check plot; mean self pull aligned gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65660d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:   \n",
    "    \n",
    "    sanitycheck_bhv_toplot_name = 'socialgaze_prob'\n",
    "    \n",
    "    # prepare the data\n",
    "    sanitycheck_bhv_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name', 'bhvID', ])\n",
    "        \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            \n",
    "            if not 'partner' in act_animal_ana:\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            elif 'partner' in act_animal_ana:\n",
    "                animal_dontwant = act_animal_ana.split('_')[0]\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']!=animal_dontwant\n",
    "                \n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # \n",
    "                # only one neuron since looking at bhv only\n",
    "                # for ineuron in np.arange(0,nneurons,1):\n",
    "                for ineuron in np.arange(0,1,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                        \n",
    "                        sanitycheck_bhv = np.array(bhvevents_allevents_tgt[sanitycheck_bhv_toplot_name])[0]\n",
    "                        \n",
    "                        #\n",
    "                        nevents = np.shape(sanitycheck_bhv)[0]\n",
    "                        \n",
    "                        \n",
    "                        for ievent in np.arange(0,nevents,1):\n",
    "                            \n",
    "                            # calculate the social gaze accumulation\n",
    "                            if sanitycheck_bhv_toplot_name == 'socialgaze_prob':\n",
    "                                \n",
    "                                gaze_prob = sanitycheck_bhv[ievent]\n",
    "                                \n",
    "                                try:\n",
    "                                    timewins = np.arange(-4,4,1/fps)\n",
    "                                    dt = 1/fps\n",
    "                                    gaze_accum = np.trapz(gaze_prob[timewins<0], dx=dt)\n",
    "                                except:\n",
    "                                    gaze_accum = np.nan\n",
    "                            else:\n",
    "                                gaze_accum = np.nan\n",
    "                                \n",
    "                            # remove trials that has no gaze_accum\n",
    "                            if gaze_accum == 0:\n",
    "                                gaze_accum = np.nan\n",
    "                            \n",
    "                            \n",
    "                            sanitycheck_bhv_df = sanitycheck_bhv_df.append({'dates': date_ana, \n",
    "                                                                            'condition':cond_ana,\n",
    "                                                                            'act_animal':act_animal_ana,\n",
    "                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                            'bhvID':ievent,\n",
    "                                                                            sanitycheck_bhv_toplot_name:sanitycheck_bhv[ievent],     \n",
    "                                                                            'gaze_accum':gaze_accum,\n",
    "                                                                           }, ignore_index=True)\n",
    "            \n",
    "    # do the plotting\n",
    "    \n",
    "    #####\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    sanitycheck_bhv_df[\"condition_general\"] = sanitycheck_bhv_df[\"condition\"].apply(generalize_condition)\n",
    "    #####\n",
    "    \n",
    "    # \n",
    "    onlyMCSR = 1\n",
    "    if onlyMCSR:\n",
    "        ind_MCSR = np.isin(sanitycheck_bhv_df['condition_general'],['MC','SR'])\n",
    "        sanitycheck_bhv_df = sanitycheck_bhv_df[ind_MCSR]\n",
    "                        \n",
    "    # Group by condition_general and calculate mean  sem (standard error of mean)\n",
    "    grouped = sanitycheck_bhv_df.groupby('condition_general')[sanitycheck_bhv_toplot_name].apply(list)\n",
    "\n",
    "    # Prepare plot\n",
    "    figg = plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for cond, traces in grouped.items():\n",
    "        # Convert list of lists to a 2D numpy array: shape = (num_trials, trace_length)\n",
    "        traces_array = np.array(traces)\n",
    "        mean_trace = np.nanmean(traces_array,axis=0)\n",
    "        sem_trace = np.nanstd(traces_array,axis=0) / np.sqrt(traces_array.shape[0])\n",
    "\n",
    "        x = np.arange(-4,4,1/fps)\n",
    "        plt.plot(x, mean_trace, label=cond)\n",
    "        plt.fill_between(x, mean_trace - sem_trace, mean_trace + sem_trace, alpha=0.3)\n",
    "        \n",
    "    plt.plot([0,0],[-0.01,0.11],'--')\n",
    "    plt.xlim([-4.2,4.2])\n",
    "    # plt.ylim([-0.01,0.11])\n",
    "        \n",
    "    plt.xlabel(\"time (s)\")\n",
    "    plt.ylabel(sanitycheck_bhv_toplot_name)\n",
    "    plt.title(\"Mean Social Gaze Trace by Condition\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    #\n",
    "    ind_MC = np.isin(sanitycheck_bhv_df['condition_general'],['MC'])\n",
    "    sanitycheck_bhv_MC_df = sanitycheck_bhv_df[ind_MC].copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #                        \n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"    \n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        figg.savefig(figsavefolder+bhvname_ana+'_aligned_'+sanitycheck_bhv_toplot_name+\n",
    "                     savefile_sufix+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94652a",
   "metadata": {},
   "source": [
    "### sanity check plot; mean pull aligned firing rate and pull aligned gaze events (3 gaussian kernel smoothed)\n",
    "#### add the option to look at gaze accumulation over time\n",
    "#### also use this code to defined significant neurons - label neurons that significantly encode gaze accumulation before pull, this is for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:   \n",
    "    # gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    gaze_duration_type = 'around_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    \n",
    "    # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "    doGazeAccum = 0\n",
    "        \n",
    "    significant_neurons_data_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                        'clusterID','significance_or_not',\n",
    "                                                        'gaze_duration_type','gaze_variable_name'])\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            \n",
    "            if not 'partner' in act_animal_ana:\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            elif 'partner' in act_animal_ana:\n",
    "                animal_dontwant = act_animal_ana.split('_')[0]\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']!=animal_dontwant\n",
    "\n",
    "            \n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # Determine subplot grid (5 columns, dynamic rows)\n",
    "                ncols = 5\n",
    "                nrows = int(np.ceil(nneurons / ncols))\n",
    "\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "                axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "                \n",
    "                # === New heatmap plot per date for neuron correlation over time ===\n",
    "                fig_corr, ax_corr = plt.subplots(figsize=(10, max(6, 0.3 * nneurons)))\n",
    "\n",
    "                # Store r_trace and p_trace for each neuron\n",
    "                r_traces_all_neurons = []\n",
    "                p_traces_all_neurons = []\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                        #\n",
    "                        # load and plot bhv event ('pull') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot the results\n",
    "                            time_trace = np.arange(-4,4,1/fps)  # Assuming time is just indices\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            ax.plot(time_trace, mean_trace, label=bhvname_ana+'(n='+str(nevents)+')', \n",
    "                                    color=bhvname_clrs[ibhvname_ana])\n",
    "                            ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                            color=bhvname_clrs[ibhvname_ana], alpha=0.3)\n",
    "                        #\n",
    "                        else:\n",
    "                            FRsmoothed_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # load and plot the pull aligned continuous bhv variables\n",
    "                        conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                        \n",
    "                        #\n",
    "                        # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "                        if doGazeAccum:\n",
    "                            from sklearn.metrics import auc\n",
    "                            if (pull_trig_events_tgtname == 'socialgaze_prob') |\\\n",
    "                               (pull_trig_events_tgtname == 'othergaze_prob') :\n",
    "                                num_time_points = conBhv_allevents_ineuron.shape[0]\n",
    "                                num_conbhv = conBhv_allevents_ineuron.shape[1]\n",
    "                                accumulated_auc = np.zeros((num_time_points, num_conbhv))\n",
    "                                # Create a time axis (assuming equal spacing)\n",
    "                                time_ind = np.arange(num_time_points)\n",
    "                                #\n",
    "                                for i in range(num_conbhv):\n",
    "                                    for t in range(1, num_time_points):\n",
    "                                        # Calculate AUC up to the current time point for the i-th conbhv\n",
    "                                        y = conBhv_allevents_ineuron[:t+1, i]\n",
    "                                        accumulated_auc[t, i] = auc(time_ind[:t+1], y)\n",
    "                            #\n",
    "                            conBhv_allevents_ineuron = accumulated_auc\n",
    "                            \n",
    "                        # zscored the behavioral events\n",
    "                        # Flatten the data\n",
    "                        flattened = conBhv_allevents_ineuron.flatten()\n",
    "                        # Z-score the entire dataset as a single distribution\n",
    "                        flattened_z = np.full_like(flattened, np.nan)\n",
    "                        valid_mask = ~np.isnan(flattened)\n",
    "                        flattened_z[valid_mask] = st.zscore(flattened[valid_mask])\n",
    "                        # Reshape back to original shape\n",
    "                        conBhv_allevents_ineuron_z = flattened_z.reshape(conBhv_allevents_ineuron.shape)\n",
    "                        # \n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron_z\n",
    "    \n",
    "                        try:\n",
    "                            nevents = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents = 0\n",
    "                        \n",
    "                        try:\n",
    "                            FRconBhv_allevents_ineuron = gaussian_filter1d(conBhv_allevents_ineuron, sigma=6, axis=0)\n",
    "                        except:\n",
    "                            FRconBhv_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        # if the pull aligned FR and bhv have different number\n",
    "                        try:\n",
    "                            nevents_fr = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents_fr = 0\n",
    "                            \n",
    "                        if not  nevents_fr == nevents: \n",
    "                            print(date_ana+' mismatched number')\n",
    "                            if nevents_fr < nevents:\n",
    "                                FRconBhv_allevents_ineuron = FRconBhv_allevents_ineuron[:,0:nevents_fr]\n",
    "                            else:\n",
    "                                FRs_allevents_ineuron = FRs_allevents_ineuron[:,0:nevents]\n",
    "                            \n",
    "                        \n",
    "                        # Compute correlation coefficient between FR and behavior at each time point\n",
    "                        try:\n",
    "                            corrs = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "                            pvals = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "\n",
    "                            for t in range(FRsmoothed_allevents_ineuron.shape[0]):\n",
    "                                fr_t = FRsmoothed_allevents_ineuron[t, :]\n",
    "                                bhv_t = FRconBhv_allevents_ineuron[t, :]\n",
    "\n",
    "                                valid_mask = ~np.isnan(fr_t) & ~np.isnan(bhv_t)\n",
    "                                if np.sum(valid_mask) > 5:  # Only compute if enough data points\n",
    "                                    r, p = st.pearsonr(fr_t[valid_mask], bhv_t[valid_mask])\n",
    "                                    corrs[t] = r\n",
    "                                    pvals[t] = p    \n",
    "                        except:\n",
    "                            time_trace = np.arange(-4,4,1/fps)\n",
    "                            corrs = np.full(time_trace.shape[0], np.nan)\n",
    "                            pvals = np.full(time_trace.shape[0], np.nan)\n",
    "\n",
    "\n",
    "                        r_traces_all_neurons.append(corrs)\n",
    "                        p_traces_all_neurons.append(pvals)\n",
    "\n",
    "                        # decide if this neuron is significant or not\n",
    "                        if gaze_duration_type == 'around_pull':\n",
    "                            significant_neuron = np.sum(pvals<0.01)>0\n",
    "                        elif gaze_duration_type == 'before_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace<0]<0.01)>0\n",
    "                        elif gaze_duration_type == 'after_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace>0]<0.01)>0\n",
    "                                                \n",
    "                        #\n",
    "                        # put information about the significance \n",
    "                        if doGazeAccum:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':'gaze_accum',     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        else:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':'socialgaze_prob',     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            if doGazeAccum:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_events_tgtname+'_AUC(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "                            else:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_events_tgtname+'(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "\n",
    "\n",
    "                            # Create a twin axis for the correlation plot\n",
    "                            ax2 = ax.twinx()                   \n",
    "\n",
    "                            # Plot correlation coefficient trace on the right y-axis\n",
    "                            ax2.plot(time_trace, corrs, color='black', linestyle='--', label='FRBhv r')\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "\n",
    "                            # Highlight significant timepoints (p < 0.01) with red dots on the right y-axis\n",
    "                            significant_mask = (pvals < 0.01) & ~np.isnan(pvals)\n",
    "                            ax2.plot(time_trace[significant_mask], corrs[significant_mask], 'ro', label='p < 0.01')\n",
    "\n",
    "                            # Set the label for the right axis\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "                            ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                            # Optionally adjust limits or formatting if necessary\n",
    "                            ax2.set_ylim(-1, 1)  # Adjust this as necessary for your data range\n",
    "\n",
    "\n",
    "                    ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "                    ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                    # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                    ax.legend()\n",
    "\n",
    "                # Hide empty subplots if nneurons < total grid size\n",
    "                for i in range(nneurons, len(axes)):\n",
    "                    fig.delaxes(axes[i])\n",
    "\n",
    "                # Figure title\n",
    "                fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)\n",
    "\n",
    "                \n",
    "                # Convert to numpy array for heatmap\n",
    "                r_traces_all_neurons = np.array(r_traces_all_neurons)\n",
    "\n",
    "                # === Sort r_traces by the time of their first peak ===\n",
    "                peak_times = []\n",
    "                for trace in r_traces_all_neurons:\n",
    "                    if np.all(np.isnan(trace)):\n",
    "                        peak_times.append(np.inf)\n",
    "                    else:\n",
    "                        peak_idx = np.nanargmax(trace)\n",
    "                        peak_times.append(time_trace[peak_idx])\n",
    "\n",
    "                # Get sorting indices based on peak times\n",
    "                sorted_indices = np.argsort(peak_times)\n",
    "                r_traces_sorted = r_traces_all_neurons[sorted_indices, :]\n",
    "\n",
    "                # Plot heatmap of r values\n",
    "                im = ax_corr.imshow(r_traces_sorted, aspect='auto', cmap='gray_r', interpolation='none',\n",
    "                                    extent=[time_trace[0], time_trace[-1], 0, nneurons],\n",
    "                                    vmin=-0.7, vmax=0.7)\n",
    "\n",
    "                # Overlay significance as red dots\n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    sig_times = np.where(p_traces_all_neurons[idx] < 0.01)[0]\n",
    "                    for t in sig_times:\n",
    "                        ax_corr.plot(time_trace[t], i + 0.5, 'r.', markersize=3)  # i+0.5 to center in the row\n",
    "\n",
    "                # Add vertical dashed line at time = 0\n",
    "                ax_corr.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                # Add vertical dashed line at time zero\n",
    "                ax_corr.axvline(x=0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "                ax_corr.set_xlabel(\"Time (s)\")\n",
    "                ax_corr.set_ylabel(\"Neuron (sorted by peak time)\")\n",
    "                ax_corr.set_title(f\"Neuron-wise Corr(Gaze, FR) Heatmap (Sorted): {act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                cbar = fig_corr.colorbar(im, ax=ax_corr)\n",
    "                cbar.set_label('Pearson r')\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                savefig = 0\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    if doGazeAccum:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_events_tgtname+'_auc'+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_events_tgtname+'_auc'+savefile_sufix+'.pdf')\n",
    "                    else:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_events_tgtname+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_events_tgtname+savefile_sufix+'.pdf')\n",
    "\n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig)\n",
    "                plt.close(fig_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_neurons_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from scipy.integrate import cumtrapz  # Add this import\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    ind1 = bhvevents_aligned_FR_allevents_all_dates_df['condition']=='MC_withGinger'\n",
    "    ind2 = bhvevents_aligned_FR_allevents_all_dates_df['dates']=='20240808'\n",
    "    ind3 = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == 'pull'\n",
    "    ind4 = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']=='kanga'\n",
    "    conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_events_tgtname])[0]\n",
    "    \n",
    "    np.shape(conBhv_allevents_ineuron)\n",
    "    \n",
    "    # Setup\n",
    "    trace = conBhv_allevents_ineuron[6]\n",
    "    time_trace = np.arange(-4, 4, 1/30)\n",
    "    \n",
    "    # try to only use the meaningful gazes\n",
    "    # filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "    # trace = filtered_trace\n",
    "\n",
    "    # Compute accumulated AUC using trapezoidal integration\n",
    "    accum_auc = cumtrapz(trace, time_trace, initial=0)\n",
    "\n",
    "    # Create plot with dual y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Plot the gaze distribution\n",
    "    color1 = 'tab:blue'\n",
    "    line1, = ax1.plot(time_trace, trace, color=color1, label='Gaze Distribution')\n",
    "    fill = ax1.fill_between(time_trace, trace, alpha=0.3, color=color1)\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Gaze Distribution', color=color1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.axvline(0, color='k', linestyle='--', linewidth=1)\n",
    "    ax1.set_title('Social gaze aligned pull')\n",
    "\n",
    "    # Create a second y-axis for accumulated AUC\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:red'\n",
    "    line2, = ax2.plot(time_trace, accum_auc, color=color2, label='Accumulated AUC')\n",
    "    ax2.set_ylabel('Accumulated AUC', color=color2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Legend: lines and manual patch for shaded area\n",
    "    legend_elements = [\n",
    "        line1,\n",
    "        Patch(facecolor=color1, alpha=0.3, label='AUC Area'),\n",
    "        line2\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'example_event_pull_gaze_accum.pdf')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a58b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2b24c5b",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately for each condition\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### run PCA for all bhvevent together combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "        \n",
    "    # Step 1 - run PCA separately\n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','succrate','act_animal',\n",
    "                                                                'bhv_name','bhv_id','PCs',])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "               \n",
    "            if not 'partner' in act_animal_ana:\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            elif 'partner' in act_animal_ana:\n",
    "                animal_dontwant = act_animal_ana.split('_')[0]\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']!=animal_dontwant\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana         \n",
    "\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    succrate = np.array(bhvevents_aligned_FR_allevents_tgt['succrate'])[0][0]\n",
    "                    \n",
    "                    # to better combine different bhv events, choose the same amount\n",
    "                    nbhv_topick = 50\n",
    "\n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    valid_bhvs = ~np.any(np.isnan(data_array), axis=(0, 1))  # Shape (144,)\n",
    "                    data_array = data_array[:, :, valid_bhvs]\n",
    "\n",
    "                    nneurons = np.shape(data_array)[0]\n",
    "                    timepointnums = np.shape(data_array)[1]\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                    # Randomly select bhv events with replacement, once for all neurons\n",
    "                    selected_bhvs = np.random.choice(mbhv_total, nbhv_topick, replace=True)\n",
    "                    sampled_data = data_array[:, :, selected_bhvs]\n",
    "\n",
    "                    # Reshape by flattening the last two dimensions\n",
    "                    final_array = sampled_data.reshape(nneurons, -1)\n",
    "\n",
    "                    PCA_dataset_ibv = final_array\n",
    "\n",
    "                    # combine all bhv for running PCA in the same neural space\n",
    "                    if ibhvname_ana == 0:\n",
    "                        PCA_dataset = PCA_dataset_ibv\n",
    "                    else:\n",
    "                        PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # Run PCA on this concatenated data \n",
    "                pca = PCA(n_components=3)\n",
    "                pca.fit(PCA_dataset)\n",
    "\n",
    "                totalneuronNum = np.shape(PCA_dataset)[1]\n",
    "\n",
    "                # project on the individual events\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    # get the pull triggered continuous variable of target\n",
    "                    data_array_conBhv = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname]))\n",
    "                    data_array_conBhv = np.nanmean(data_array_conBhv,axis=0)\n",
    "                    data_array_conBhv = data_array_conBhv.transpose()\n",
    "                    \n",
    "                    # get the self and other pull variable\n",
    "                    data_array_otherpull = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_otherpull_name]))\n",
    "                    data_array_otherpull = np.nanmean(data_array_otherpull,axis=0)\n",
    "                    data_array_otherpull = data_array_otherpull.transpose()\n",
    "                    #\n",
    "                    data_array_selfpull = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_selfpull_name]))\n",
    "                    data_array_selfpull = np.nanmean(data_array_selfpull,axis=0)\n",
    "                    data_array_selfpull = data_array_selfpull.transpose()\n",
    "                    \n",
    "                    # get the pull triggered last reward time\n",
    "                    data_array_lastreward = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_time_pre_reward_name]))\n",
    "                    data_array_lastreward = data_array_lastreward[0]\n",
    "                    \n",
    "                    # get the pull reaction time\n",
    "                    data_array_rt = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_rt_name]))\n",
    "                    data_array_rt = data_array_rt[0]\n",
    "                    \n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                    for ibhv in np.arange(0,mbhv_total,1):\n",
    "\n",
    "                        data_ibhv = data_array[:,:,ibhv]\n",
    "\n",
    "                        # get the pull triggered continous variables of target for individual events\n",
    "                        try:\n",
    "                            data_array_conBhv_ibhv = data_array_conBhv[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_conBhv_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        \n",
    "                        # for the socialgaze_prob, only use the meaningful ones\n",
    "                        if 0:\n",
    "                            if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                                trace = data_array_conBhv_ibhv\n",
    "                                time_trace = np.arange(-4, 4, 1/30)\n",
    "                                filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                data_array_conBhv_ibhv = filtered_trace\n",
    "                                \n",
    "                        #\n",
    "                        # get the pull triggered self and pther pull for individual events\n",
    "                        try:\n",
    "                            data_array_otherpull_ibhv = data_array_otherpull[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_otherpull_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        #\n",
    "                        try:\n",
    "                            data_array_selfpull_ibhv = data_array_selfpull[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_selfpull_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                            \n",
    "                        #    \n",
    "                        # get the pull triggered last reward time\n",
    "                        try:\n",
    "                            data_array_lastreward_ibhv = data_array_lastreward[ibhv]\n",
    "                        except:\n",
    "                            data_array_lastreward_ibhv = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # get the pull reaction time\n",
    "                        try:\n",
    "                            data_array_rt_ibhv = data_array_rt[ibhv]\n",
    "                        except:\n",
    "                            data_array_rt_ibhv = np.nan\n",
    "              \n",
    "\n",
    "                        # for firing rate, project on the PC space    \n",
    "                        try:\n",
    "                            PCA_proj_ibhv = pca.transform(np.transpose(data_ibhv))\n",
    "                        except:\n",
    "                            PCA_proj_ibhv = np.full((timepointnums, 3), np.nan)\n",
    "\n",
    "                        FRPCA_all_sessions_allevents_sum_df = FRPCA_all_sessions_allevents_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':succrate,\n",
    "                                                                                'bhv_id':ibhv,\n",
    "                                                                                'PCs':PCA_proj_ibhv,\n",
    "                                                                                'neuronNumBeforePCA':totalneuronNum,\n",
    "                                                                                pull_trig_events_tgtname:data_array_conBhv_ibhv,                         \n",
    "                                                                                pull_trig_otherpull_name:data_array_otherpull_ibhv,\n",
    "                                                                                pull_trig_selfpull_name:data_array_selfpull_ibhv,\n",
    "                                                                                pull_time_pre_reward_name:data_array_lastreward_ibhv,\n",
    "                                                                                pull_rt_name: data_array_rt_ibhv,\n",
    "                                                                                 }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8234dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPCA_all_sessions_allevents_sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPCA_all_sessions_allevents_sum_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e601db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FRPCA_all_sessions_allevents_sum_df['time_from_last_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480a20f",
   "metadata": {},
   "source": [
    "#### first analysis to test hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 for each PCA trace, calculate the length, curvature, and/or tortusity for comparison later\n",
    "# test hypothesis: 1. for testing if individual trial different was from gaze start time/stop time/gaze duration\n",
    "    \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "FRPCAfeatures_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','act_animal','succrate',\n",
    "                                                                    'bhv_name','bhv_id',\n",
    "                                                                    'PClength','PCcurv','PCtort','PCspeed','PCsmoothness',\n",
    "                                                                    'PCspeed_trace','PCcurv_trace',\n",
    "                                                                    ])\n",
    "FRPCAfeatures_gazeduration_corr_all_sessions_df = pd.DataFrame(columns=['condition','session','succrate',\n",
    "                                                                        'act_animal','bhv_name',])\n",
    "\n",
    "\n",
    "# newly added control!!\n",
    "# only look at pull aligned events that has no preceding self pull \n",
    "doSingleSelfPulls = 1\n",
    "\n",
    "# only look at pull aligned events that do not have juice deliever effect from the previous pull\n",
    "doFarLastRewards = 0\n",
    "\n",
    "# only look at pulls that has large reaction time\n",
    "doLargeRTpulls = 1\n",
    "largeRTthreshold = 3 # the threshold for defining large threahold\n",
    "\n",
    "\n",
    "# add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "gaze_duration_type = 'before_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "\n",
    "#\n",
    "for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "    act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "    ind_animal = FRPCA_all_sessions_allevents_sum_df['act_animal']==act_animal_ana\n",
    "        \n",
    "    # get the dates\n",
    "    dates_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal]['session'])\n",
    "    ndates_toplot = np.shape(dates_toplot)[0]\n",
    "    \n",
    "    # figures \n",
    "    fig1, axs1 = plt.subplots(nconds_to_ana,ndates_toplot)\n",
    "    fig1.set_figheight(24*nconds_to_ana)\n",
    "    fig1.set_figwidth(8*ndates_toplot)\n",
    "    # Ensure axs1 is always 2D\n",
    "    axs1 = np.atleast_2d(axs1)\n",
    "    axs1_flat = axs1.flatten()  # Flatten for easier iteration\n",
    "    # Track used axes\n",
    "    used_axes = set()\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_sessions_allevents_sum_df['condition']==cond_ana\n",
    "\n",
    "        # get the dates\n",
    "        dates_ana = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal & ind_cond]['session'])\n",
    "        ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "        for idate_ana in np.arange(0,ndates_ana,1):\n",
    "            date_ana = dates_ana[idate_ana]\n",
    "            ind_date = FRPCA_all_sessions_allevents_sum_df['session']==date_ana         \n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_sessions_allevents_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                FRPCA_allevents_toana = FRPCA_all_sessions_allevents_sum_df[ind_ana]\n",
    "\n",
    "                bhv_ids = np.array(FRPCA_allevents_toana['bhv_id'])\n",
    "                nbhvevents = np.shape(bhv_ids)[0]\n",
    "\n",
    "                for ibhv_id in np.arange(0,nbhvevents,1):\n",
    "\n",
    "                    bhv_id = bhv_ids[ibhv_id]\n",
    "                    ind_bhvid = FRPCA_allevents_toana['bhv_id'] == bhv_id\n",
    "\n",
    "                    # \n",
    "                    # count self pull before self pull, the goal is to make sure the effect we will find later is not because of the preceding pulls\n",
    "                    from scipy.signal import find_peaks\n",
    "                    #\n",
    "                    pulltrig_selfpull = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_selfpull_name])[0]\n",
    "                    #\n",
    "                    x_full = np.arange(-4,4,1/fps)\n",
    "                    pre_mask = x_full<-0.15\n",
    "                    try:\n",
    "                        # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                        data = pulltrig_selfpull[pre_mask]\n",
    "                        peaks, _ = find_peaks(data)\n",
    "                        selfpull_num = len(peaks)\n",
    "                    except:\n",
    "                        selfpull_num = np.nan\n",
    "                    #\n",
    "                    if selfpull_num > 0:\n",
    "                        lastselfpull_time = x_full[peaks[-1]]\n",
    "                    else:\n",
    "                        lastselfpull_time =  np.nan\n",
    "\n",
    "                    # \n",
    "                    # add the info about the last reward time\n",
    "                    lastreward_time = -np.array(FRPCA_allevents_toana[ind_bhvid][pull_time_pre_reward_name])[0]\n",
    "                    \n",
    "                    # \n",
    "                    # try to define a relavant time window for each pull\n",
    "                    # this is almost the reaction time\n",
    "                    # last_relavant_time = np.nanmax([lastreward_time, lastselfpull_time, -4])\n",
    "                    last_relavant_time = np.nanmax([lastreward_time, lastselfpull_time])\n",
    "                    \n",
    "                    #\n",
    "                    # add information about the pull reaction time\n",
    "                    pull_rt = np.array(FRPCA_allevents_toana[ind_bhvid][pull_rt_name])[0]\n",
    "                    \n",
    "                    # \n",
    "                    # analyze the pull triggered behavioral events\n",
    "                    if (pull_trig_events_tgtname == 'socialgaze_prob') |\\\n",
    "                       (pull_trig_events_tgtname == 'othergaze_prob') :\n",
    "                        # calculate the gaze start and gaze stop time, and finally gaze duration\n",
    "                        try:\n",
    "                            pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "\n",
    "                            # Find the point of first increasing and last decrease to estimate gaze start and end\n",
    "                            #\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            #\n",
    "                            # if gaze_duration_type == 'before_pull':\n",
    "                            #     pulltrig_conBhv[timewins>0] = 0\n",
    "                            # elif gaze_duration_type == 'after_pull':\n",
    "                            #     pulltrig_conBhv[timewins<0] = 0                           \n",
    "                            \n",
    "                            if 1:\n",
    "                                first_increase_idx = np.where(np.diff(pulltrig_conBhv) > 0)[0][0] + 1\n",
    "                                #\n",
    "                                last_decrease_idx = np.where(np.diff(pulltrig_conBhv) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_increase_idx].copy()\n",
    "                                gazestop_time = timewins[last_decrease_idx].copy()\n",
    "                            if 0:\n",
    "                                # Find peaks\n",
    "                                peaks, _ = scipy.signal.find_peaks(pulltrig_conBhv)\n",
    "                                #\n",
    "                                # Get first and last peak\n",
    "                                first_peak = peaks[0] \n",
    "                                last_peak = peaks[-1]\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_peak].copy()\n",
    "                                gazestop_time = timewins[last_peak].copy()\n",
    "                            #\n",
    "                            # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                gazestart_time = gazestart_time\n",
    "                                gazestop_time = gazestop_time\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                if (gazestart_time > 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestop_time > 0):\n",
    "                                    gazestop_time = 0\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                if (gazestop_time < 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestart_time < 0):\n",
    "                                    gazestart_time = 0                                                \n",
    "                            #\n",
    "                            # if (gazestart_time == timewins[0]) | (gazestart_time == timewins[-1]):\n",
    "                            #     gazestart_time = np.nan\n",
    "                            # if (gazestop_time == timewins[0]) | (gazestop_time == timewins[-1]):\n",
    "                            #     gazestop_time = np.nan\n",
    "                            if (gazestop_time < gazestart_time):\n",
    "                                gazestart_time = np.nan\n",
    "                                gazestop_time = np.nan                           \n",
    "                        except:\n",
    "                            gazestart_time = np.nan\n",
    "                            gazestop_time = np.nan\n",
    "                            \n",
    "                        # calculate the gaze accumulation (use auc to estimate)\n",
    "                        try:\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            dt = 1 / fps  # sampling interval in seconds\n",
    "                            #\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv, dx=dt)\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins<0], dx=dt)\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins>0], dx=dt)\n",
    "                            #\n",
    "                            gaze_accum = auc\n",
    "                        #\n",
    "                        except:\n",
    "                            gaze_accum = np.nan\n",
    "                            \n",
    "                        # remove trial with zero gaze_accumulation\n",
    "                        if gaze_accum == 0:\n",
    "                            gaze_accum = np.nan\n",
    "                                \n",
    "                            \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the PCs \n",
    "                    FRPCA_ievent_toana = np.array(FRPCA_allevents_toana[ind_bhvid]['PCs'])[0]\n",
    "\n",
    "                    # smooth the pc trajectory\n",
    "                    if 0:\n",
    "                        FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                                 arr=FRPCA_ievent_toana, sigma=6)\n",
    "\n",
    "                    # calculate the length, curvature and tortuosity\n",
    "                    PC_traj = FRPCA_ievent_toana.copy()  # Shape (240, 3)\n",
    "                    \n",
    "                    #\n",
    "                    if gaze_duration_type == 'before_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[0:int(ntimepoints/2),:]\n",
    "                    elif gaze_duration_type == 'after_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[int(ntimepoints/2):,:]\n",
    "\n",
    "                    # Compute differences between consecutive points\n",
    "                    diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                    # Compute segment lengths\n",
    "                    segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                    total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                    # Compute curvature\n",
    "                    # First derivatives\n",
    "                    dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                    dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                    dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                    dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                    # Second derivatives\n",
    "                    d2X_dt2 = np.gradient(dX_dt)\n",
    "                    d2Y_dt2 = np.gradient(dY_dt)\n",
    "                    d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                    d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                    # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                    cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                    curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                    # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                    euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                    tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "                    \n",
    "                    # Compute speed \n",
    "                    dt = 1.0 / fps  # Time between frames\n",
    "                    # Velocity: first derivative of position\n",
    "                    velocity = np.gradient(PC_traj, axis=0) / dt\n",
    "                    # Speed: magnitude of velocity\n",
    "                    speed = np.linalg.norm(velocity, axis=1)\n",
    "                    \n",
    "                    # Compute Smoothness - A simple way to compute trajectory smoothness is to look at the jerk \n",
    "                    #  the third derivative of position (how quickly acceleration changes), \n",
    "                    # which reflects sudden directional/velocity shifts\n",
    "                    # Acceleration: second derivative\n",
    "                    acceleration = np.gradient(velocity, axis=0) / dt\n",
    "                    # Jerk: third derivative\n",
    "                    jerk = np.gradient(acceleration, axis=0) / dt\n",
    "                    # Smoothness metric: integrated squared jerk over time\n",
    "                    squared_jerk = np.linalg.norm(jerk, axis=1) ** 2\n",
    "                    smoothness = np.sum(squared_jerk) * dt\n",
    "\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df.append({\n",
    "                                                                                'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                                'bhv_id':ibhv_id,\n",
    "                                                                                'PClength':total_length,\n",
    "                                                                                'PCcurv':np.nanmean(curvature),\n",
    "                                                                                'PCtort':tortuosity,\n",
    "                                                                                'PCspeed':np.nanmean(speed),\n",
    "                                                                                'PCsmoothness':smoothness,\n",
    "                                                                                'PCspeed_trace':speed,\n",
    "                                                                                'PCcurv_trace':curvature,\n",
    "                                                                                'gazestart_time':gazestart_time,\n",
    "                                                                                'gazestop_time':gazestop_time,\n",
    "                                                                                'gaze_accum':gaze_accum,\n",
    "                                                                                'neuronNumBeforePCA':np.array(FRPCA_allevents_toana[ind_bhvid]['neuronNumBeforePCA'])[0],\n",
    "                                                                                'selfpull_num':selfpull_num,\n",
    "                                                                                'lastreward_time':lastreward_time,\n",
    "                                                                                'lastselfpull_time':lastselfpull_time,\n",
    "                                                                                'last_relavant_time':last_relavant_time,\n",
    "                                                                                'pull_rt':pull_rt,\n",
    "                                                                            }, ignore_index=True)\n",
    "                    \n",
    "                 \n",
    "                # \n",
    "                # remove events that has multiple self pulls before the aligned self pulls \n",
    "                if doSingleSelfPulls:\n",
    "                    ind_good = FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num']==0\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df[ind_good]\n",
    "                \n",
    "                #\n",
    "                # remove events that still has the effect from previous juice\n",
    "                if doFarLastRewards:\n",
    "                    # reward should be beyond the -4s time window we are looking at, and also consider the reward effect last ~1s\n",
    "                    ind_good = FRPCAfeatures_all_sessions_allevents_sum_df['lastreward_time']<=(-4-1)\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df[ind_good]\n",
    "                    \n",
    "                #\n",
    "                # remove events that still has small reaction time\n",
    "                if doLargeRTpulls:\n",
    "                    # reward should be beyond the -4s time window we are looking at, and also consider the reward effect last ~1s\n",
    "                    ind_good = FRPCAfeatures_all_sessions_allevents_sum_df['pull_rt']>=largeRTthreshold\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df[ind_good]\n",
    "                    \n",
    "                #\n",
    "                # remove the zero (nan) gaze accumulation level\n",
    "                ind_nan = np.isnan(FRPCAfeatures_all_sessions_allevents_sum_df['gaze_accum'])\n",
    "                FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df[~ind_nan]\n",
    "                \n",
    "                \n",
    "                # after pool all the events related data together do some plotting and calculate the correlation\n",
    "                ind_sess_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == date_ana\n",
    "                ind_ani_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['act_animal'] == act_animal_ana\n",
    "                ind_bhv_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['bhv_name'] == bhvname_ana\n",
    "                ind_cond_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['condition'] == cond_ana\n",
    "                \n",
    "                ind_toplot = ind_sess_toplot & ind_ani_toplot & ind_bhv_toplot & ind_cond_toplot\n",
    "                FRPCAfeatures_toplot = FRPCAfeatures_all_sessions_allevents_sum_df[ind_toplot]\n",
    "                \n",
    "                yyy_types = ['PCcurv','PClength','PCsmoothness','PCspeed']\n",
    "                nytypes = np.shape(yyy_types)[0]\n",
    "                \n",
    "                # xxx_type = 'gaze_duration'\n",
    "                # xxx_type = 'gaze_accumulation'\n",
    "                xxx_type = 'pull_rt'\n",
    "                \n",
    "                # Use gridspec to divide axs1[icond_ana, idate_ana] into nytypes rows\n",
    "                gs = gridspec.GridSpecFromSubplotSpec(nytypes, 1, subplot_spec=axs1[icond_ana, idate_ana], hspace=0.3)\n",
    "\n",
    "                for iytype in np.arange(0,nytypes,1):\n",
    "                    \n",
    "                    ax = fig1.add_subplot(gs[iytype])  # Create subplots within the existing grid cell\n",
    "                    used_axes.add(ax)  # Mark this axis as used\n",
    "                    \n",
    "                    if xxx_type == 'gaze_duration':\n",
    "                        xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                    elif xxx_type == 'gaze_accumulation':\n",
    "                        xxx = FRPCAfeatures_toplot['gaze_accum']\n",
    "                    else:\n",
    "                        xxx = FRPCAfeatures_toplot[xxx_type]\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "\n",
    "                    ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "                    xxx = xxx[~ind_nan]\n",
    "                    yyy = yyy[~ind_nan]\n",
    "\n",
    "                    # Compute correlation\n",
    "                    if len(xxx) > 1:\n",
    "                        r, p = st.pearsonr(xxx, yyy)\n",
    "                        # Fit regression line\n",
    "                        slope, intercept = np.polyfit(xxx, yyy, 1)\n",
    "                        x_vals = np.array([min(xxx), max(xxx)])\n",
    "                        y_vals = slope * x_vals + intercept\n",
    "                        ax.plot(x_vals, y_vals, color='red', linestyle='--', label='Regression line')\n",
    "                    else:\n",
    "                        r, p = np.nan, np.nan\n",
    "\n",
    "                    # Scatter plot\n",
    "                    ax.plot(xxx, yyy, 'o', label='gaze around pull')\n",
    "\n",
    "                    # Title and labels\n",
    "                    ax.set_title(bhvname_ana + ' of ' + act_animal_ana + ' in ' +\n",
    "                                 cond_ana + ' ' + date_ana + '\\n neuron #=' +\n",
    "                                 str(FRPCAfeatures_toplot['neuronNumBeforePCA'].iloc[0]), fontsize=12)\n",
    "                    ax.set_ylabel(yyy_type, fontsize=12)\n",
    "\n",
    "                    # Add correlation text\n",
    "                    ax.text(0.05, 0.9, f\"r = {r:.3f}\\np = {p:.3f}\", transform=ax.transAxes, fontsize=12,\n",
    "                            verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "                    # Optional: show legend\n",
    "                    ax.legend()\n",
    "\n",
    "                    #\n",
    "                    # plot the one without any gaze as a comparison\n",
    "                    xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "                    \n",
    "                    ind_nan = (np.isnan(xxx)) & (~np.isnan(yyy))\n",
    "                    yyy = yyy[ind_nan]\n",
    "                    xxx = np.zeros(np.shape(yyy))\n",
    "                    \n",
    "                    ax.plot(xxx, yyy, 'ro',label='no gaze around pull')\n",
    "                    \n",
    "                    ax.set_xlabel(xxx_type+' around pull (s)',fontsize=12)  # Set xlabel only for the last subplot in the stack\n",
    "                    ax.legend(loc='lower right')\n",
    "            \n",
    "                    # \n",
    "                    FRPCAfeatures_gazeduration_corr_all_sessions_df = FRPCAfeatures_gazeduration_corr_all_sessions_df.append({\n",
    "                                                                                'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                                'corr_'+yyy_type+'_vs_'+xxx_type:r,\n",
    "                                                                                'pcorr_'+yyy_type+'_vs_'+xxx_type:p,\n",
    "                                                                                'gazeduration_definition':gaze_duration_type,\n",
    "                                                                               }, ignore_index=True)\n",
    "                                                               \n",
    "                                                               \n",
    "    # fig1.tight_layout()\n",
    "    \n",
    "    #  Hide unused subplots\n",
    "    for ax in axs1_flat:\n",
    "        if ax not in used_axes:  # If an axis wasn't used, hide it\n",
    "            ax.set_visible(False)\n",
    "\n",
    "    \n",
    "    \n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'bhvevents_aligned_PCspace_trajectory_features_and_continuousBhv_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+xxx_type+'_'+gaze_duration_type+savefile_sufix+'.pdf')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_tgt = np.isin(FRPCAfeatures_all_sessions_allevents_sum_df['condition'],['MC'])\n",
    "plt.hist(FRPCAfeatures_all_sessions_allevents_sum_df[ind_tgt]['last_relavant_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_tgt = FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num']>=0\n",
    "np.unique(FRPCAfeatures_all_sessions_allevents_sum_df[ind_tgt]['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(FRPCAfeatures_all_sessions_allevents_sum_df['lastreward_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_tgt = np.isin(FRPCAfeatures_all_sessions_allevents_sum_df['condition'],['MC'])\n",
    "xxx = FRPCAfeatures_all_sessions_allevents_sum_df[ind_tgt]['pull_rt']\n",
    "yyy = FRPCAfeatures_all_sessions_allevents_sum_df[ind_tgt]['gaze_accum']\n",
    "plt.plot(xxx,yyy,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26090f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPCAfeatures_gazeduration_corr_all_sessions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the session-by-session correlation between PC trajectory features and gaze duration \n",
    "if 1:    \n",
    "    import itertools\n",
    "    \n",
    "    #####\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    FRPCAfeatures_gazeduration_corr_all_sessions_df[\"condition_general\"] = \\\n",
    "        FRPCAfeatures_gazeduration_corr_all_sessions_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    # corr_type = 'corr_PClength_vs_gaze_duration'\n",
    "    # corr_type = 'corr_PClength_vs_gaze_accumulation'\n",
    "    # corr_type = 'corr_PCcurv_vs_gaze_accumulation'\n",
    "    corr_type = 'corr_PClength_vs_pull_rt'\n",
    "\n",
    "\n",
    "    fig2, axs2 = plt.subplots(1, 1)\n",
    "    fig2.set_figheight(5)\n",
    "    fig2.set_figwidth(12)\n",
    "\n",
    "    # Define a consistent color\n",
    "    box_color = 'skyblue'\n",
    "\n",
    "    # Plot boxplot and swarmplot\n",
    "    seaborn.violinplot(ax=axs2, data=FRPCAfeatures_gazeduration_corr_all_sessions_df,\n",
    "                x='condition_general', y=corr_type,\n",
    "                color=box_color)\n",
    "\n",
    "    seaborn.swarmplot(ax=axs2, data=FRPCAfeatures_gazeduration_corr_all_sessions_df,\n",
    "                   x='condition_general', y=corr_type,\n",
    "                   color='b', size=6)\n",
    "\n",
    "    # Rotate x-axis ticks\n",
    "    axs2.set_xticklabels(axs2.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Significance from 0 (Wilcoxon signed-rank)\n",
    "    conditions = FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'].unique()\n",
    "    y_max = FRPCAfeatures_gazeduration_corr_all_sessions_df[corr_type].max()\n",
    "    y_min = FRPCAfeatures_gazeduration_corr_all_sessions_df[corr_type].min()\n",
    "\n",
    "    y_offset = (y_max - y_min) * 0.15  # vertical spacing for significance bars\n",
    "\n",
    "    for i, cond in enumerate(conditions):\n",
    "        data = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond][corr_type].dropna()\n",
    "        if len(data) > 0 and np.any(data != 0):  # Wilcoxon requires non-zero variation\n",
    "            try:\n",
    "                stat, p = st.wilcoxon(data)\n",
    "                if p < 0.01:\n",
    "                    axs2.text(i, data.max() + y_offset, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "            except ValueError:\n",
    "                pass  # skip if data not suitable for Wilcoxon\n",
    "\n",
    "    # Pairwise comparisons (MannWhitney U)\n",
    "    pairs = list(itertools.combinations(range(len(conditions)), 2))\n",
    "    for j, (i1, i2) in enumerate(pairs):\n",
    "        cond1 = conditions[i1]\n",
    "        cond2 = conditions[i2]\n",
    "        data1 = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond1][corr_type].dropna()\n",
    "        data2 = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond2][corr_type].dropna()\n",
    "\n",
    "        if len(data1) > 0 and len(data2) > 0:\n",
    "            stat, p = st.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "            if p < 0.01:\n",
    "                y = max(data1.max(), data2.max()) + y_offset * (j + 2)\n",
    "                axs2.plot([i1, i2], [y, y], lw=1.5, c='black')\n",
    "                axs2.text((i1 + i2) / 2, y + y_offset * 0.2, '*', ha='center', va='bottom', fontsize=16)\n",
    "     \n",
    "    # fig2.tight_layout()\n",
    "\n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_PCspace_trajectory_features_and_continuousBhv_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+corr_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "\n",
    "    # to see if there's correlation between correlation coefficient of PC features vs gaze duration, and succ rate \n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    # Drop rows where either column is NaN\n",
    "    subset_df = FRPCAfeatures_gazeduration_corr_all_sessions_df[['condition_general', 'succrate', corr_type]].dropna()\n",
    "\n",
    "    # Group by condition and calculate Pearson correlation and p-value\n",
    "    def compute_corr_pval(group):\n",
    "        if len(group) < 2:\n",
    "            return pd.Series({'r': None, 'p': None})  # Not enough data to correlate\n",
    "        r, p = pearsonr(group['succrate'], group[corr_type])\n",
    "        return pd.Series({'r': r, 'p': p})\n",
    "\n",
    "    grouped_corrs = subset_df.groupby('condition_general').apply(compute_corr_pval)\n",
    "\n",
    "    # Display the result\n",
    "    print(f'Correlation between {corr_type} and succrate:')\n",
    "    print(grouped_corrs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the single trial trajectories of an example session - compared the nogaze_pull and gaze_pull\n",
    "if 0:\n",
    "    # Create a 3D figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20240603_MC'\n",
    "    animal_exmaple = 'kanga'\n",
    "    session_example = '202408123\",\"20250403_SR'\n",
    "\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "    #\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session']==session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    # plot the nogazearound_pull tracjectory\n",
    "    ind_nogaze = (np.isnan(FRPCAfeatures_example['gazestart_time'])) & (np.isnan(FRPCAfeatures_example['gazestop_time']))\n",
    "    nogaze_pullIDs = np.array(FRPCAfeatures_example[ind_nogaze]['bhv_id'])\n",
    "    #\n",
    "    ind_nogazepullIDs = np.isin(FRPCA_example['bhv_id'],nogaze_pullIDs)\n",
    "    FRPCA_nogazepulls = np.array(FRPCA_example[ind_nogazepullIDs]['PCs'])\n",
    "    n_nogazepulls = np.shape(FRPCA_nogazepulls)[0]\n",
    "    #\n",
    "    for ii in np.arange(0,n_nogazepulls,1):\n",
    "    # for ii in np.arange(0,1,1):\n",
    "        xxx = gaussian_filter1d(FRPCA_nogazepulls[ii][:,0],6)\n",
    "        yyy = gaussian_filter1d(FRPCA_nogazepulls[ii][:,1],6)\n",
    "        zzz = gaussian_filter1d(FRPCA_nogazepulls[ii][:,2],6)\n",
    "\n",
    "        if ii == 0:\n",
    "            ax.plot3D(xxx, yyy, zzz, color='r', linewidth=2, label='no gaze around pull FR PC trajectory')\n",
    "        else:\n",
    "            ax.plot3D(xxx, yyy, zzz, color='r', linewidth=2)\n",
    "\n",
    "    # plot the gazearound pull trajectory\n",
    "    FRPCA_gazepulls = np.array(FRPCA_example[~ind_nogazepullIDs]['PCs'])\n",
    "    n_gazepulls = np.shape(FRPCA_gazepulls)[0]\n",
    "    ind_gazepulls_toplot = np.random.choice(np.arange(0, n_gazepulls, 1), size=n_nogazepulls, replace=False)\n",
    "    # \n",
    "    for ii in ind_gazepulls_toplot:\n",
    "        xxx = gaussian_filter1d(FRPCA_gazepulls[ii][:,0],6)\n",
    "        yyy = gaussian_filter1d(FRPCA_gazepulls[ii][:,1],6)\n",
    "        zzz = gaussian_filter1d(FRPCA_gazepulls[ii][:,2],6)\n",
    "\n",
    "        if ii == ind_gazepulls_toplot[0]:\n",
    "            ax.plot3D(xxx, yyy, zzz, color='b', linewidth=2, label='gaze around pull FR PC trajectory')\n",
    "        else:\n",
    "            ax.plot3D(xxx, yyy, zzz, color='b', linewidth=2)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73692e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the single trial trajectories of an example session - compared the gaze_pull separating three quantile based on the gaze duration\n",
    "if 0:\n",
    "    # Create a 3D figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250415'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    session_example = '20240808'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20240606'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20250415'\n",
    "\n",
    "\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "    #\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session']==session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "    \n",
    "    #\n",
    "    # gaze_durations = FRPCAfeatures_example['gazestop_time'] - FRPCAfeatures_example['gazestart_time']\n",
    "    # alternatively\n",
    "    xxx_type = 'pull_rt' # 'gaze_accum' or 'pull_rt', or etc\n",
    "    gaze_durations = FRPCAfeatures_example[xxx_type]\n",
    "    gaze_durations = np.array(gaze_durations)\n",
    "    #\n",
    "    # Compute tertile (33rd and 67th percentiles), ignoring NaNs\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    #\n",
    "    # Separate data into three groups\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "    \n",
    "    for ii in np.arange(0,3,1):\n",
    "        if ii == 0:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_low]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'b'\n",
    "            traj_lab = 'low '+xxx_type+' ' + str(np.sum(ind_low))+' trials'\n",
    "        elif ii == 1:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_mid]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'r'\n",
    "            traj_lab = 'mid '+xxx_type+' ' + str(np.sum(ind_mid))+' trials'\n",
    "        elif ii == 2:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_high]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'y'\n",
    "            traj_lab = 'high '+xxx_type+' ' + str(np.sum(ind_high))+' trials'\n",
    "           \n",
    "        ntimepoints = np.shape(meanPCA_traj)[0]\n",
    "        \n",
    "        xxx = gaussian_filter1d(meanPCA_traj[:,0],6)\n",
    "        yyy = gaussian_filter1d(meanPCA_traj[:,1],6)\n",
    "        zzz = gaussian_filter1d(meanPCA_traj[:,2],6)\n",
    "        if gaze_duration_type == 'before_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),2],3)   \n",
    "        elif gaze_duration_type == 'after_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,2],3)   \n",
    "            \n",
    "        ax.plot3D(xxx, yyy, zzz, color=traj_clr, linewidth=2, label=traj_lab)\n",
    "        ax.plot3D(xxx[0], yyy[0], zzz[0], color=traj_clr, marker ='o',markersize = 12)\n",
    "        ax.plot3D(xxx[-1], yyy[-1], zzz[-1], color=traj_clr, marker ='s',markersize = 12)\n",
    "\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+session_example+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'forExample_PCAtrajectory_in_'+xxx_type+'_quantiles.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the single trial  an example session - compared the gaze_pull separating three quantile based on the gaze duration\n",
    "# average and show the speed or curvature trace of each quantile\n",
    "if 0:\n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    trace_toplot = 'PCspeed_trace'\n",
    "    \n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250415'\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20240612'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    session_example = '20240808'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20240809'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20240523'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20250415'\n",
    "\n",
    "\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "    #\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session']==session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    #\n",
    "    # gaze_durations = FRPCAfeatures_example['gazestop_time'] - FRPCAfeatures_example['gazestart_time']\n",
    "    # alternatively\n",
    "    xxx_type = 'pull_rt' # 'gaze_accum' or 'pull_rt', or etc\n",
    "    gaze_durations = FRPCAfeatures_example[xxx_type]\n",
    "    gaze_durations = np.array(gaze_durations)\n",
    "    #\n",
    "    # Compute tertile (33rd and 67th percentiles), ignoring NaNs\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    #\n",
    "    # Separate data into three groups\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "    \n",
    "    for ii in np.arange(0,3,1):\n",
    "        if ii == 0:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCAfeatures_example[ind_low][trace_toplot], axis=0),axis=0)\n",
    "            traj_clr = 'b'\n",
    "            traj_lab = 'low '+xxx_type+' ' + str(np.sum(ind_low))+' trials'\n",
    "        elif ii == 1:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCAfeatures_example[ind_mid][trace_toplot], axis=0),axis=0)\n",
    "            traj_clr = 'r'\n",
    "            traj_lab = 'mid '+xxx_type+' ' + str(np.sum(ind_mid))+' trials'\n",
    "        elif ii == 2:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCAfeatures_example[ind_high][trace_toplot], axis=0),axis=0)\n",
    "            traj_clr = 'y'\n",
    "            traj_lab = 'high '+xxx_type+' ' + str(np.sum(ind_high))+' trials'\n",
    "           \n",
    "        ntimepoints = np.shape(meanPCA_traj)[0]\n",
    "        \n",
    "        if gaze_duration_type == 'around_pull':\n",
    "            xtimes = np.arange(-4,4,1/fps)\n",
    "            xxx = gaussian_filter1d(meanPCA_traj,6)\n",
    "        else:\n",
    "            xtimes = np.arange(-4,0,1/fps)\n",
    "            xxx = gaussian_filter1d(meanPCA_traj,6)\n",
    "            \n",
    "        ax.plot(xtimes, xxx, color=traj_clr, linewidth=2, label=traj_lab)\n",
    "        ax.plot(xtimes[0], xxx[0], color=traj_clr, marker ='o',markersize = 12)\n",
    "        ax.plot(xtimes[-1], xxx[-1], color=traj_clr, marker ='s',markersize = 12)\n",
    "        \n",
    "        ax.set_xlabel('time/s')\n",
    "        ax.set_ylabel('PC trajectory speed')\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d957e",
   "metadata": {},
   "source": [
    "#### sanity check plot, individual neurons' pull aligned FR and gaze related measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one needs to be run after the previous code becuase it need the definition of the \n",
    "# gaze_duration or gaze_accumulation from the previous code\n",
    "\n",
    "if 0:\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond_FRPCA = FRPCAfeatures_all_sessions_allevents_sum_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal_FRPCA = FRPCAfeatures_all_sessions_allevents_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "                ind_date_FRPCA = FRPCAfeatures_all_sessions_allevents_sum_df['session']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # get the gaze related features\n",
    "                features_ana = FRPCAfeatures_all_sessions_allevents_sum_df[ind_animal_FRPCA&ind_cond_FRPCA&ind_date_FRPCA]\n",
    "                \n",
    "                # Determine subplot grid (5 columns, dynamic rows)\n",
    "                ncols = 5\n",
    "                nrows = int(np.ceil(nneurons / ncols))\n",
    "\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "                axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "                \n",
    "                # === New heatmap plot per date for neuron correlation over time ===\n",
    "                fig_corr, ax_corr = plt.subplots(figsize=(10, max(6, 0.3 * nneurons)))\n",
    "\n",
    "                # Store r_trace and p_trace for each neuron\n",
    "                r_traces_all_neurons = []\n",
    "                p_traces_all_neurons = []\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                            \n",
    "                        #\n",
    "                        # load and plot bhv event ('gazestart'/'gazestop') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "\n",
    "                        FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "\n",
    "                        # separating FR based on the gaze related features (three quantiles)\n",
    "                        # xxx_type = 'gaze_duration'\n",
    "                        xxx_type = 'gaze_accumulation'\n",
    "                        if xxx_type == 'gaze_duration':\n",
    "                            gaze_durs_ineurons = np.array(features_ana['gazestop_time']-\\\n",
    "                                                          features_ana['gazestart_time'])\n",
    "                        elif xxx_type == 'gaze_accumulation':\n",
    "                            gaze_durs_ineurons = np.array(features_ana['gaze_accum'])\n",
    "                        \n",
    "                        ngazes = np.shape(gaze_durs_ineurons)[0]\n",
    "                        \n",
    "                        print(date_ana+' pull aligned FR # = '+str(nevents)+'; pull aligned gaze # = '+str(ngazes))\n",
    "                        \n",
    "                        # Compute quantiles\n",
    "                        q1, q2 = np.nanpercentile(gaze_durs_ineurons, [33, 66])\n",
    "\n",
    "                        # Get indices for each quantile group\n",
    "                        short_idx = np.where(gaze_durs_ineurons <= q1)[0]\n",
    "                        mid_idx   = np.where((gaze_durs_ineurons > q1) & (gaze_durs_ineurons <= q2))[0]\n",
    "                        long_idx  = np.where(gaze_durs_ineurons > q2)[0]\n",
    "\n",
    "                        quantile_groups = {\n",
    "                            'Short': short_idx,\n",
    "                            'Medium': mid_idx,\n",
    "                            'Long': long_idx\n",
    "                        }\n",
    "\n",
    "                        quantile_colors = {\n",
    "                            'Short': '#1f77b4',   # blue\n",
    "                            'Medium': '#ff7f0e',  # orange\n",
    "                            'Long': '#2ca02c'     # green\n",
    "                        }\n",
    "\n",
    "                        time_trace = np.arange(-4,4,1/fps)\n",
    "                        \n",
    "                        # Plot FRs by quantile\n",
    "                        for label, idx in quantile_groups.items():\n",
    "                            if len(idx) == 0:\n",
    "                                continue\n",
    "\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron[:, idx], axis=1)\n",
    "                            sem_trace = np.nanstd(FRsmoothed_allevents_ineuron[:, idx], axis=1) / np.sqrt(len(idx))\n",
    "\n",
    "                            ax.plot(time_trace, mean_trace, label=f\"{bhvname_ana} - {label} (n={len(idx)})\",\n",
    "                                    color=quantile_colors[label])\n",
    "                            ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace,\n",
    "                                            color=quantile_colors[label], alpha=0.3)\n",
    "                        \n",
    "                        \n",
    "                        # plot the correlation coefficient\n",
    "                        # === Compute time-varying correlation between FR and gaze duration ===\n",
    "                        n_timepoints = FRsmoothed_allevents_ineuron.shape[0]\n",
    "                        r_trace = np.full(n_timepoints, np.nan)\n",
    "                        p_trace = np.full(n_timepoints, np.nan)\n",
    "\n",
    "                        if xxx_type == 'gaze_duration':\n",
    "                            gaze_durs = np.array(features_ana['gazestop_time']-\\\n",
    "                                                          features_ana['gazestart_time'])\n",
    "                        elif xxx_type == 'gaze_accumulation':\n",
    "                            gaze_durs = np.array(features_ana['gaze_accum'])\n",
    "                            \n",
    "                        valid_gaze_mask = ~np.isnan(gaze_durs)\n",
    "\n",
    "                        for t in range(n_timepoints):\n",
    "                            fr_t = FRsmoothed_allevents_ineuron[t, :]\n",
    "                            valid_fr_mask = ~np.isnan(fr_t)\n",
    "                            valid_mask = valid_fr_mask & valid_gaze_mask\n",
    "\n",
    "                            if np.sum(valid_mask) > 2:\n",
    "                                r, p = st.pearsonr(fr_t[valid_mask], gaze_durs[valid_mask])\n",
    "                                r_trace[t] = r\n",
    "                                p_trace[t] = p\n",
    "                                \n",
    "                        r_traces_all_neurons.append(r_trace)\n",
    "                        p_traces_all_neurons.append(p_trace)\n",
    "\n",
    "                        # === Plot on right Y-axis ===\n",
    "                        ax2 = ax.twinx()\n",
    "                        ax2.plot(time_trace, r_trace, color='black', linestyle='-', linewidth=2, label='Gaze-FR Corr')\n",
    "                        ax2.set_ylabel(\"Corr(GazeDur, FR)\", color='black')\n",
    "                        ax2.tick_params(axis='y', labelcolor='black')\n",
    "                        ax2.set_ylim([-1, 1])\n",
    "                        ax2.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "                        # Plot significant time points (p < 0.01) as red dots\n",
    "                        sig_mask = (p_trace < 0.01) & (~np.isnan(p_trace)) & (~np.isnan(r_trace))\n",
    "                        ax2.plot(time_trace[sig_mask], r_trace[sig_mask], 'ro', markersize=4, label='p < 0.01')\n",
    "                        \n",
    "                    ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "                    ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                    # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                    ax.legend()\n",
    "                    #\n",
    "                    ax2.set_ylabel(\"Corr(GazeDur, FR)\", color='black')\n",
    "                    ax2.tick_params(axis='y', labelcolor='black')\n",
    "                    ax2.set_ylim([-1, 1])\n",
    "\n",
    "                # Hide empty subplots if nneurons < total grid size\n",
    "                for i in range(nneurons, len(axes)):\n",
    "                    fig.delaxes(axes[i])\n",
    "\n",
    "                # Figure title\n",
    "                fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)                \n",
    "                \n",
    "                \n",
    "                # Convert to numpy array for heatmap\n",
    "                r_traces_all_neurons = np.array(r_traces_all_neurons)\n",
    "\n",
    "                # === Sort r_traces by the time of their first peak ===\n",
    "                peak_times = []\n",
    "                for trace in r_traces_all_neurons:\n",
    "                    if np.all(np.isnan(trace)):\n",
    "                        peak_times.append(np.inf)\n",
    "                    else:\n",
    "                        peak_idx = np.nanargmax(trace)\n",
    "                        peak_times.append(time_trace[peak_idx])\n",
    "\n",
    "                # Get sorting indices based on peak times\n",
    "                sorted_indices = np.argsort(peak_times)\n",
    "                r_traces_sorted = r_traces_all_neurons[sorted_indices, :]\n",
    "\n",
    "                # Plot heatmap of r values\n",
    "                im = ax_corr.imshow(r_traces_sorted, aspect='auto', cmap='gray_r', interpolation='none',\n",
    "                                    extent=[time_trace[0], time_trace[-1], 0, nneurons],\n",
    "                                    vmin=-0.7, vmax=0.7)\n",
    "\n",
    "                # Overlay significance as red dots\n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    sig_times = np.where(p_traces_all_neurons[idx] < 0.01)[0]\n",
    "                    for t in sig_times:\n",
    "                        ax_corr.plot(time_trace[t], i + 0.5, 'r.', markersize=3)  # i+0.5 to center in the row\n",
    "\n",
    "                # Add vertical dashed line at time = 0\n",
    "                ax_corr.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                # Add vertical dashed line at time zero\n",
    "                ax_corr.axvline(x=0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "                ax_corr.set_xlabel(\"Time (s)\")\n",
    "                ax_corr.set_ylabel(\"Neuron (sorted by peak time)\")\n",
    "                ax_corr.set_title(f\"Neuron-wise Corr(Gaze, FR) Heatmap (Sorted): {act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                cbar = fig_corr.colorbar(im, ax=ax_corr)\n",
    "                cbar.set_label('Pearson r')\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                savefig = 0\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    fig.savefig(figsavefolder+'individualneurons_FR_in_quantiles_'+\n",
    "                                 pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+xxx_type+savefile_sufix+'.pdf')\n",
    "\n",
    "                    fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_' + \n",
    "                                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+xxx_type+savefile_sufix+'.pdf')\n",
    "\n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig)\n",
    "                plt.close(fig_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b973f7",
   "metadata": {},
   "source": [
    "## analysis with 'trial pooling' across sessions from the same condition\n",
    "### pool sessions for each task conditions together and then run PCA\n",
    "#### pool sessions based on quantiles of gaze-accumulation or gaze-length variables (e.g. 5 quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only analyze a subset of conditions\n",
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "# act_animals_to_ana = ['kanga']\n",
    "act_animals_to_ana = ['dodson']\n",
    "# act_animals_to_ana = ['kanga_partner'] # align thing to partner, remember to pair this with pull_trig_events_tgtname = 'othergaze_prob'\n",
    "\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['pull']\n",
    "# bhv_names_to_ana = ['failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "# # the following analysis can only do one conditions \n",
    "# # multiple condition will be considered into one conditions for quantile and FR averaging analysis\n",
    "# conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['MC','SR',]\n",
    "###\n",
    "# For Kanga\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVermelho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "cond_toplot_type = 'allMC'\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "doOnlySigniNeurons = 0 # define the significant neurons using the previous code\n",
    "\n",
    "# newly added control!!\n",
    "# only look at pull aligned events that has no preceding self pull \n",
    "doSingleSelfPulls = 0\n",
    "\n",
    "# only look at pull aligned events that do not have juice deliever effect from the previous pull\n",
    "doFarLastRewards = 0\n",
    "\n",
    "# only look at pulls that has large reaction time\n",
    "doLargeRTpulls = 0\n",
    "largeRTthreshold = 2 # the threshold for defining large threahold\n",
    "\n",
    "doSuccPull = 0\n",
    "doFailPull = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "if 1:\n",
    "    # load and prepare the data\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.DataFrame(columns=['condition', 'session', 'act_animal',\n",
    "                                                                                 'bhv_name', 'bhv_id', 'FR_ievent',\n",
    "                                                                                 'clusterID', 'channelID',\n",
    "                                                                                 'gaze_accum_ievent', 'gaze_start_ievent',\n",
    "                                                                                 'gaze_stop_ievent',\n",
    "                                                                                 ])\n",
    "    #\n",
    "    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.DataFrame()\n",
    "\n",
    "    # step 1 for the bhvevents_aligned_FR_allevents_all_dates_df, get and gaze-related variables and calculate features\n",
    "\n",
    "    # it's better to match this variable with the previous one\n",
    "    # add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "    gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    # xxx_type = 'gaze_accum'  # 'gaze_dur' or 'gaze_accum' or 'pull_rt' or other\n",
    "    xxx_type = 'pull_rt'\n",
    "\n",
    "    # special here, number of quantile to use for pooling across different days\n",
    "    num_quantiles = 3 # 10\n",
    "\n",
    "    #\n",
    "    for icond_ana in np.arange(0, nconds_to_ana, 1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition'] == cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0, nanimal_to_ana, 1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "\n",
    "            if not 'partner' in act_animal_ana:\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            elif 'partner' in act_animal_ana:\n",
    "                animal_dontwant = act_animal_ana.split('_')[0]\n",
    "                ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']!=animal_dontwant\n",
    "                \n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0, ndates_ana, 1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates'] == date_ana\n",
    "\n",
    "                # get the neurons\n",
    "                neurons_ana = np.unique(\n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0, nneurons, 1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID'] == clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0, nbhvnames_to_ana, 1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date\n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "                            ind_ana].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "                        if not bhvevents_aligned_FR_allevents_tgt.empty:\n",
    "                            channelID = bhvevents_aligned_FR_allevents_tgt['channelID'].iloc[0]\n",
    "\n",
    "                            #\n",
    "                            # load and plot bhv event ('pull') aligned FR\n",
    "                            FRs_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt['FR_allevents'].iloc[0])\n",
    "                            nevents_FR = np.shape(FRs_allevents_ineuron)[1]\n",
    "\n",
    "                            #\n",
    "                            # load and plot the pull aligned continuous bhv variables\n",
    "                            conBhv_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname].iloc[0])\n",
    "                            conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                            conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                            #\n",
    "                            nevents_bhv = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                            \n",
    "                            #\n",
    "                            # load the self pull variables\n",
    "                            selfpull_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_selfpull_name].iloc[0])\n",
    "                            selfpull_allevents_ineuron = np.array(selfpull_allevents_ineuron)\n",
    "                            selfpull_allevents_ineuron = selfpull_allevents_ineuron.transpose()\n",
    "                            \n",
    "                            # \n",
    "                            # load the pull reaction time information\n",
    "                            pull_rt_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_rt_name].iloc[0])\n",
    "                            \n",
    "                            # \n",
    "                            # load the last reward time information\n",
    "                            lastreward_time_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_time_pre_reward_name].iloc[0])\n",
    "                            \n",
    "                            # \n",
    "                            # load the successful pull or not information\n",
    "                            succornot_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt['succornot_pull'].iloc[0])\n",
    "                            \n",
    "                            \n",
    "                            # if the pull aligned FR and bhv have different number\n",
    "                            if not nevents_FR == nevents_bhv:\n",
    "                                # print(date_ana+' mismatched number')\n",
    "                                if nevents_FR < nevents_bhv:\n",
    "                                    conBhv_allevents_ineuron = conBhv_allevents_ineuron[:, 0:nevents_FR]\n",
    "                                else:\n",
    "                                    FRs_allevents_ineuron = FRs_allevents_ineuron[:, 0:nevents_bhv]\n",
    "\n",
    "                            #\n",
    "                            nevents = np.min([nevents_FR, nevents_bhv])\n",
    "\n",
    "                            # get each bhv events\n",
    "                            for bhv_id in np.arange(0, nevents, 1):\n",
    "                                FRs_ievent_ineuron = FRs_allevents_ineuron[:, bhv_id]\n",
    "                                conBhv_ievent_ineuron = conBhv_allevents_ineuron[:, bhv_id]\n",
    "                                selfpull_ievent_ineuron = selfpull_allevents_ineuron[:, bhv_id]\n",
    "                                pull_rt_ievent_ineuron = pull_rt_allevents_ineuron[bhv_id]\n",
    "                                try:\n",
    "                                    lastreward_time_ievent_ineuron = -lastreward_time_allevents_ineuron[bhv_id]\n",
    "                                except:\n",
    "                                    lastreward_time_ievent_ineuron = np.nan\n",
    "                                succornot_ievent_ineuron = succornot_allevents_ineuron[bhv_id]\n",
    "\n",
    "                                #\n",
    "                                # count self pull before self pull, \n",
    "                                # the goal is to make sure the effect we will find later is not because of the preceding pulls\n",
    "                                from scipy.signal import find_peaks\n",
    "                                #\n",
    "                                x_full = np.arange(-4,4,1/fps)\n",
    "                                pre_mask = x_full<-0.15\n",
    "                                try:\n",
    "                                    # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                                    data = selfpull_ievent_ineuron[pre_mask]\n",
    "                                    peaks, _ = find_peaks(data)\n",
    "                                    selfpull_num = len(peaks)\n",
    "                                except:\n",
    "                                    selfpull_num = np.nan\n",
    "                                                              \n",
    "                            \n",
    "                        \n",
    "                                #\n",
    "                                # for the socialgaze_prob, only use the meaningful ones\n",
    "                                if 0:\n",
    "                                    if (pull_trig_events_tgtname == 'socialgaze_prob') | \\\n",
    "                                       (pull_trig_events_tgtname == 'othergaze_prob') :\n",
    "                                        trace = conBhv_ievent_ineuron\n",
    "                                        time_trace = np.arange(-4, 4, 1/30)\n",
    "                                        filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                        conBhv_ievent_ineuron = filtered_trace\n",
    "                            \n",
    "                        \n",
    "                                #\n",
    "                                # analyze the pull triggered behavioral events\n",
    "                                if (pull_trig_events_tgtname == 'socialgaze_prob') | \\\n",
    "                                   (pull_trig_events_tgtname == 'othergaze_prob') :\n",
    "                                    # calculate the gaze start and gaze stop time, and finally gaze duration\n",
    "                                    try:\n",
    "                                        timewins = np.arange(-4, 4, 1 / fps)  # make sure it align with the setting in the previous section\n",
    "\n",
    "                                        if 1:\n",
    "                                            first_increase_idx = np.where(\n",
    "                                                np.diff(conBhv_ievent_ineuron) > 0)[0][0] + 1\n",
    "                                            #\n",
    "                                            last_decrease_idx = np.where(\n",
    "                                                np.diff(conBhv_ievent_ineuron) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                            #\n",
    "                                            gazestart_time = timewins[first_increase_idx].copy()\n",
    "                                            gazestop_time = timewins[last_decrease_idx].copy()\n",
    "                                        if 0:\n",
    "                                            # Find peaks\n",
    "                                            peaks, _ = scipy.signal.find_peaks(conBhv_ievent_ineuron)\n",
    "                                            #\n",
    "                                            # Get first and last peak\n",
    "                                            first_peak = peaks[0]\n",
    "                                            last_peak = peaks[-1]\n",
    "                                            #\n",
    "                                            gazestart_time = timewins[first_peak].copy()\n",
    "                                            gazestop_time = timewins[last_peak].copy()\n",
    "                                        #\n",
    "                                        # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                                        if gaze_duration_type == 'around_pull':\n",
    "                                            gazestart_time = gazestart_time\n",
    "                                            gazestop_time = gazestop_time\n",
    "                                        if gaze_duration_type == 'before_pull':\n",
    "                                            if (gazestart_time > 0):\n",
    "                                                gazestart_time = np.nan\n",
    "                                                gazestop_time = np.nan\n",
    "                                            elif (gazestop_time > 0):\n",
    "                                                gazestop_time = 0\n",
    "                                        if gaze_duration_type == 'after_pull':\n",
    "                                            if (gazestop_time < 0):\n",
    "                                                gazestart_time = np.nan\n",
    "                                                gazestop_time = np.nan\n",
    "                                            elif (gazestart_time < 0):\n",
    "                                                gazestart_time = 0\n",
    "\n",
    "                                        #\n",
    "                                        if (gazestop_time < gazestart_time):\n",
    "                                            gazestart_time = np.nan\n",
    "                                            gazestop_time = np.nan\n",
    "                                    except:\n",
    "                                        gazestart_time = np.nan\n",
    "                                        gazestop_time = np.nan\n",
    "\n",
    "                                    # calculate the gaze accumulation (use auc to estimate)\n",
    "                                    try:\n",
    "                                        timewins = np.arange(-4, 4, 1 / fps)  # make sure it align with the setting in the previous section\n",
    "                                        dt = 1 / fps  # sampling interval in seconds\n",
    "                                        #\n",
    "                                        if gaze_duration_type == 'around_pull':\n",
    "                                            auc = np.trapz(conBhv_ievent_ineuron, dx=dt)\n",
    "                                        if gaze_duration_type == 'before_pull':\n",
    "                                            auc = np.trapz(conBhv_ievent_ineuron[timewins < 0], dx=dt)\n",
    "                                            # auc = np.trapz(conBhv_ievent_ineuron[(timewins<0)&(timewins>-3)], dx=dt)\n",
    "                                        if gaze_duration_type == 'after_pull':\n",
    "                                            auc = np.trapz(conBhv_ievent_ineuron[timewins > 0], dx=dt)\n",
    "                                        #\n",
    "                                        gaze_accum = auc\n",
    "                                    #\n",
    "                                    except:\n",
    "                                        gaze_accum = np.nan\n",
    "                                        \n",
    "                                    # remove trial with zero gaze_accumulation\n",
    "                                    if gaze_accum == 0:\n",
    "                                        gaze_accum = np.nan\n",
    "\n",
    "                                bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(\n",
    "                                    [bhvevents_aligned_FR_and_eventFeatures_all_dates_df, pd.DataFrame({\n",
    "                                        'condition': [cond_ana],\n",
    "                                        'act_animal': [act_animal_ana],\n",
    "                                        'bhv_name': [bhvname_ana],\n",
    "                                        'session': [date_ana],\n",
    "                                        'bhv_id': [bhv_id],\n",
    "                                        'clusterID': [clusterID_ineuron],\n",
    "                                        'channelID': [channelID],\n",
    "                                        'gaze_accum_ievent': [gaze_accum],\n",
    "                                        'gaze_start_ievent': [gazestart_time],\n",
    "                                        'gaze_stop_ievent': [gazestop_time],\n",
    "                                        'FR_ievent': [FRs_ievent_ineuron],\n",
    "                                        'selfpull_num_ievent': [selfpull_num],\n",
    "                                        'pull_rt_ievent':[pull_rt_ievent_ineuron],\n",
    "                                        'lastreward_time_ievent':[lastreward_time_ievent_ineuron],\n",
    "                                        'succornot_pull_ievent':[succornot_ievent_ineuron],\n",
    "                                         pull_trig_events_tgtname:[conBhv_ievent_ineuron],\n",
    "                                    })], ignore_index=True)\n",
    "\n",
    "    # only look at successful pull\n",
    "    if doSuccPull:\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['succornot_pull_ievent']==1\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "    \n",
    "    # only look at failed pull\n",
    "    if doFailPull:\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['succornot_pull_ievent']==0\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "    \n",
    "                                \n",
    "    # only look at the pulls that has no pull preceeding\n",
    "    if doSingleSelfPulls:\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['selfpull_num_ievent']==0\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "    \n",
    "    #\n",
    "    # remove events that still has the effect from previous juice\n",
    "    if doFarLastRewards:\n",
    "        # reward should be beyond the -4s time window we are looking at, and also consider the reward effect last ~1s\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['lastreward_time_ievent']<=(-4-1)\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "                    \n",
    "    #\n",
    "    # only consider pulls that has large reaction time\n",
    "    if doLargeRTpulls:\n",
    "        # reward should be beyond the -4s time window we are looking at, and also consider the reward effect last ~1s\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['pull_rt_ievent']>=largeRTthreshold\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "                    \n",
    "    #\n",
    "    # remove trial with zero (nan) in the gaze accumulation level\n",
    "    ind_nan = np.isnan(bhvevents_aligned_FR_and_eventFeatures_all_dates_df['gaze_accum_ievent'])\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[~ind_nan]\n",
    "\n",
    "            \n",
    "    # add the quantile information using the all sessions' data\n",
    "    # consider each date separately\n",
    "\n",
    "    # Create a list to store the DataFrames for each date\n",
    "    all_dates_dfs = []\n",
    "\n",
    "    # Get unique dates\n",
    "    unique_dates = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'].unique()\n",
    "\n",
    "    for date_ana in unique_dates:\n",
    "        date_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == date_ana].copy()\n",
    "\n",
    "        if xxx_type == 'gaze_accum':\n",
    "            quantile_tgt = date_df['gaze_accum_ievent'].dropna()  # Handle potential NaNs\n",
    "            n_unique = len(quantile_tgt.unique())\n",
    "            n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "            if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "                try:\n",
    "                    quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                    date_df['gaze_accum_quantile'] = pd.cut(\n",
    "                        date_df['gaze_accum_ievent'],\n",
    "                        bins=quantile_bins,\n",
    "                        labels=False,\n",
    "                        include_lowest=True,\n",
    "                        duplicates='drop'  # Drop duplicate bin edges\n",
    "                    )\n",
    "                    quantile_col = 'gaze_accum_quantile'\n",
    "                    title_prefix = 'Gaze Accumulation'\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                    date_df['gaze_accum_quantile'] = np.nan\n",
    "                    quantile_col = 'gaze_accum_quantile'\n",
    "                    title_prefix = 'Gaze Accumulation'\n",
    "            else:\n",
    "                date_df['gaze_accum_quantile'] = np.nan\n",
    "                quantile_col = 'gaze_accum_quantile'\n",
    "                title_prefix = 'Gaze Accumulation'\n",
    "                print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "\n",
    "        #\n",
    "        elif xxx_type == 'gaze_dur':\n",
    "            date_df['gaze_duration_ievent'] = \\\n",
    "                date_df['gaze_stop_ievent'] - \\\n",
    "                date_df['gaze_start_ievent']\n",
    "            quantile_tgt = date_df['gaze_duration_ievent'].dropna()  # Handle potential NaNs\n",
    "            n_unique = len(quantile_tgt.unique())\n",
    "            n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "            if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "                try:\n",
    "                    quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                    date_df['gaze_duration_quantile'] = pd.cut(\n",
    "                        date_df['gaze_duration_ievent'],\n",
    "                        bins=quantile_bins,\n",
    "                        labels=False,\n",
    "                        include_lowest=True,\n",
    "                        duplicates='drop'  # Drop duplicate bin edges\n",
    "                    )\n",
    "                    quantile_col = 'gaze_duration_quantile'\n",
    "                    title_prefix = 'Gaze Duration'\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                    date_df['gaze_duration_quantile'] = np.nan\n",
    "                    quantile_col = 'gaze_duration_quantile'\n",
    "                    title_prefix = 'Gaze Duration'\n",
    "            else:\n",
    "                date_df['gaze_duration_quantile'] = np.nan\n",
    "                quantile_col = 'gaze_duration_quantile'\n",
    "                title_prefix = 'Gaze Duration'\n",
    "                print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "        \n",
    "        #\n",
    "        else:\n",
    "            quantile_tgt = date_df[xxx_type+'_ievent'].dropna()  # Handle potential NaNs\n",
    "            n_unique = len(quantile_tgt.unique())\n",
    "            n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "            if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "                try:\n",
    "                    quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                    date_df[xxx_type+'_quantile'] = pd.cut(\n",
    "                        date_df[xxx_type+'_ievent'],\n",
    "                        bins=quantile_bins,\n",
    "                        labels=False,\n",
    "                        include_lowest=True,\n",
    "                        duplicates='drop'  # Drop duplicate bin edges\n",
    "                    )\n",
    "                    quantile_col = xxx_type+'_quantile'\n",
    "                    title_prefix = xxx_type\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                    date_df[xxx_type+'_quantile'] = np.nan\n",
    "                    quantile_col = xxx_type+'_quantile'\n",
    "                    title_prefix = xxx_type\n",
    "            else:\n",
    "                date_df[xxx_type+'_quantile'] = np.nan\n",
    "                quantile_col = xxx_type+'_quantile'\n",
    "                title_prefix = xxx_type\n",
    "                print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "\n",
    "        all_dates_dfs.append(date_df)\n",
    "\n",
    "    # Concatenate the DataFrames back together\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(all_dates_dfs, ignore_index=True)\n",
    "\n",
    "    # average for each neuron the firing rate of each quantile\n",
    "    if 'quantile_col' in locals():\n",
    "        # Iterate through unique (clusterID, session) pairs\n",
    "        for (cluster_id, session_id) in bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            ['clusterID', 'session']].drop_duplicates().itertuples(index=False):\n",
    "            neuron_session_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == session_id)\n",
    "            ].copy()\n",
    "\n",
    "            for q_val in neuron_session_df[quantile_col].dropna().unique():\n",
    "                quantile_df = neuron_session_df[neuron_session_df[quantile_col] == q_val]\n",
    "                if not quantile_df.empty:\n",
    "                    all_fr_traces = np.vstack(quantile_df['FR_ievent'].tolist())\n",
    "                    mean_fr_trace = np.nanmean(all_fr_traces, axis=0)\n",
    "\n",
    "                    # Get representative metadata\n",
    "                    condition = quantile_df['condition'].iloc[0]\n",
    "                    act_animal = quantile_df['act_animal'].iloc[0]\n",
    "                    bhv_name = quantile_df['bhv_name'].iloc[0]\n",
    "                    channelID = quantile_df['channelID'].iloc[0]\n",
    "\n",
    "                    new_row = {\n",
    "                        'condition': condition,\n",
    "                        'session': session_id,\n",
    "                        'act_animal': act_animal,\n",
    "                        'bhv_name': bhv_name,\n",
    "                        'clusterID': cluster_id,\n",
    "                        'channelID': channelID,\n",
    "                        quantile_col: int(q_val),\n",
    "                        'mean_FR_trace': mean_fr_trace\n",
    "                    }\n",
    "                    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.concat(\n",
    "                        [bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df,\n",
    "                         pd.DataFrame([new_row])],\n",
    "                        ignore_index=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(bhvevents_aligned_FR_and_eventFeatures_all_dates_df['selfpull_num_ievent']))\n",
    "print(np.max(bhvevents_aligned_FR_and_eventFeatures_all_dates_df['lastreward_time_ievent']))\n",
    "print(np.max(bhvevents_aligned_FR_and_eventFeatures_all_dates_df['succornot_pull_ievent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_and_eventFeatures_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5411c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's for defining slope ramping feature for each neurons, and regress over the gaze accumulation or other variables\n",
    "if 1:    \n",
    "    #\n",
    "    bhveventsFeatures_and_FRslopes_all_dates_df = pd.DataFrame(columns=['condition', 'session', 'act_animal',\n",
    "                                                                         'bhv_name', 'clusterID', 'channelID',                                                                   \n",
    "                                                                                 ])\n",
    "    \n",
    "    # xvar_name = 'gaze_accum' # or 'gaze_accum', 'gaze_start', 'gaze_stop', 'selfpull_num','pull_rt'\n",
    "    xvar_name = 'pull_rt' # or 'gaze_accum', 'gaze_start', 'gaze_stop', 'selfpull_num','pull_rt'','pull_rt'\n",
    "    \n",
    "    for icond_ana in np.arange(0, nconds_to_ana, 1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['condition'] == cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0, nanimal_to_ana, 1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['act_animal'] == act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_animal & ind_cond]['session'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0, ndates_ana, 1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == date_ana\n",
    "\n",
    "                # get the neurons\n",
    "                neurons_ana = np.unique(\n",
    "                    bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0, nneurons, 1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID'] == clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0, nbhvnames_to_ana, 1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['bhv_name'] == bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date\n",
    "\n",
    "                        bhvevents_aligned_FR_and_eventFeatures_allevents_tgt = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "                            ind_ana].copy() \n",
    "\n",
    "                        channelID_ineuron = bhvevents_aligned_FR_and_eventFeatures_allevents_tgt['channelID'].iloc[0]\n",
    "                        \n",
    "                        #\n",
    "                        # calculate the ramping slope and other FR related features\n",
    "                        # get the firing rate\n",
    "                        xxx = np.array(list(bhvevents_aligned_FR_and_eventFeatures_allevents_tgt['FR_ievent']))\n",
    "\n",
    "                        # Time vector: from -4s to 4s (e.g., 240 points for 30Hz)\n",
    "                        time_vector = np.arange(-4,4,1/fps)\n",
    "\n",
    "                        # Step 1: Compute average firing rate and find time of max abs deviation before 0s\n",
    "                        mean_fr = np.nanmean(xxx, axis=0)\n",
    "                        pre_mean_fr = mean_fr[time_vector<0]\n",
    "\n",
    "                        # Find index of max absolute deviation from baseline (i.e., strongest ramp up or down)\n",
    "                        baseline = np.nanmean(mean_fr)  # optional: use entire -4 to 0s as baseline\n",
    "                        abs_dev = np.abs(mean_fr - baseline)\n",
    "                        peak_idx = np.argmax(abs_dev)\n",
    "                        #\n",
    "                        abs_dev = np.abs(pre_mean_fr - baseline)\n",
    "                        prepull_peak_idx = np.argmax(abs_dev)\n",
    "\n",
    "                        # Get corresponding index in the full time vector\n",
    "                        peak_time = time_vector[peak_idx]\n",
    "                        #\n",
    "                        prepull_peak_time = time_vector[prepull_peak_idx]\n",
    "\n",
    "                        \n",
    "                        # Step 2: Compute slope from -4s to peak_time or 0 for each trial\n",
    "                        #         in addition, get the firing rate at the peak time\n",
    "                        slopes = []\n",
    "                        # flexible end time or fixed end time?\n",
    "                        dofixedendtime = 0\n",
    "                        #\n",
    "                        for trial_fr in xxx:\n",
    "                            # \n",
    "                            if not dofixedendtime:\n",
    "                                if peak_time < 0:\n",
    "                                    x = time_vector[:peak_idx + 1]  # from -4s to peak_time\n",
    "                                    y = trial_fr[:peak_idx + 1]\n",
    "                                else:\n",
    "                                    # zero_idx = np.where(time_vector>=0)[0][0]\n",
    "                                    # x = time_vector[:zero_idx]  # from -4s to 0\n",
    "                                    # y = trial_fr[:zero_idx]\n",
    "                                    x = time_vector[:prepull_peak_idx + 1]  # from -4s to peak_time\n",
    "                                    y = trial_fr[:prepull_peak_idx + 1]\n",
    "                            #\n",
    "                            elif dofixedendtime:\n",
    "                                endtime_idx = np.where(time_vector>=-0.85)[0][0]\n",
    "                                starttime_idx = np.where(time_vector>=-3.25)[0][0]\n",
    "                                x = time_vector[starttime_idx:endtime_idx]  # from -4s to the peak time (-0.8s) based on the average FR\n",
    "                                y = trial_fr[starttime_idx:endtime_idx]\n",
    "                            #\n",
    "                            slope, intercept, r_value, p_value, std_err = st.linregress(x, y)\n",
    "                            slopes.append(slope)\n",
    "                        #\n",
    "                        slopes = np.array(slopes)  \n",
    "                        #\n",
    "                        # if the neuron fr is ramping down overall, flip the slopes\n",
    "                        if np.nanmean(slopes)<0:\n",
    "                            slopes = -slopes\n",
    "                            \n",
    "                        #\n",
    "                        if not dofixedendtime:\n",
    "                            if peak_time < 0:\n",
    "                                peakFRs = xxx[:,peak_idx]\n",
    "                            else:\n",
    "                                # zero_idx = np.where(time_vector>=0)[0][0]\n",
    "                                # peakFRs = xxx[:,zero_idx]\n",
    "                                peakFRs = xxx[:,prepull_peak_idx]\n",
    "                        # \n",
    "                        elif dofixedendtime:\n",
    "                            endtime_idx = np.where(time_vector>=-0.85)[0][0]\n",
    "                            peakFRs = xxx[:,endtime_idx]\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # \n",
    "                        # get the gaze_accumulation or other variables \n",
    "                        xvar = np.array(list(bhvevents_aligned_FR_and_eventFeatures_allevents_tgt[xvar_name+'_ievent']))\n",
    "                        \n",
    "                        # \n",
    "                        # regression between FR slopes and xvar (e.g. gaze accumulation)\n",
    "                        slope2, intercept2, r_value2, p_value2, std_err2 = st.linregress(xvar, slopes)\n",
    "                        #\n",
    "                        if (r_value2 > 0.99) | (r_value2 < -0.99):\n",
    "                            r_value2 = np.nan\n",
    "                            p_value2 = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # correlation between the peak FR and xvar (e.g. gaze accumulation)\n",
    "                        slope3, intercept3, r_value3, p_value3, std_err3 = st.linregress(xvar, peakFRs)\n",
    "                        #\n",
    "                        if (r_value3 > 0.99) | (r_value3 < -0.99):\n",
    "                            r_value3 = np.nan\n",
    "                            p_value3 = np.nan\n",
    "                        \n",
    "                        # put the data together\n",
    "                        bhveventsFeatures_and_FRslopes_all_dates_df = pd.concat(\n",
    "                                    [bhveventsFeatures_and_FRslopes_all_dates_df, pd.DataFrame({\n",
    "                                        'condition': [cond_ana],\n",
    "                                        'act_animal': [act_animal_ana],\n",
    "                                        'bhv_name': [bhvname_ana],\n",
    "                                        'session': [date_ana],\n",
    "                                        'clusterID': [clusterID_ineuron],\n",
    "                                        'channelID': [channelID_ineuron],\n",
    "                                        'FRpeaktime': [peak_time],\n",
    "                                        'FRpeaktime_prepull': [prepull_peak_time],\n",
    "                                        'xvar_name': [xvar_name],\n",
    "                                        'corr_prepull_FRslope_and_xvar':[r_value2],\n",
    "                                        'pcorr_prepull_FRslope_and_xvar':[p_value2],\n",
    "                                        'corr_prepull_peakFR_and_xvar':[r_value3],\n",
    "                                        'pcorr_prepull_peakFR_and_xvar':[p_value3],\n",
    "                                    })], ignore_index=True)\n",
    "                        \n",
    "\n",
    "    # do some plotting\n",
    "    yvar_toplot_name = 'FRslope' # FRslope, peakFR\n",
    "    \n",
    "    #\n",
    "    # Assuming df is already your DataFrame:\n",
    "    df = bhveventsFeatures_and_FRslopes_all_dates_df.copy()\n",
    "\n",
    "    # Define significance\n",
    "    df['significance'] = df['pcorr_prepull_'+yvar_toplot_name+'_and_xvar'] < 0.05\n",
    "\n",
    "    # Drop NaNs in correlation column before analysis\n",
    "    df_clean = df.dropna(subset=['corr_prepull_'+yvar_toplot_name+'_and_xvar'])\n",
    "\n",
    "    # Define significance\n",
    "    df_clean['significance'] = df_clean['pcorr_prepull_'+yvar_toplot_name+'_and_xvar'] < 0.05\n",
    "\n",
    "    # Count neurons\n",
    "    n_sig = df_clean['significance'].sum()\n",
    "    n_nonsig = len(df_clean) - n_sig\n",
    "\n",
    "    # T-test against 0 (only on non-NaN correlations)\n",
    "    t_stat, t_pval = st.ttest_1samp(df_clean['corr_prepull_'+yvar_toplot_name+'_and_xvar'], 0)\n",
    "    meanvalue = np.nanmean(df_clean['corr_prepull_'+yvar_toplot_name+'_and_xvar'])\n",
    "\n",
    "    # Plot\n",
    "    figgg = plt.figure(figsize=(8, 5))\n",
    "    seaborn.swarmplot(\n",
    "        x='xvar_name',\n",
    "        y='corr_prepull_'+yvar_toplot_name+'_and_xvar',\n",
    "        data=df_clean,\n",
    "        hue='significance',\n",
    "        palette={True: 'red', False: 'gray'},\n",
    "        size=6\n",
    "    )\n",
    "\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.ylabel(\"Correlation with Pre-pull \"+yvar_toplot_name)\n",
    "    plt.xlabel(\"Behavioral Variable\")\n",
    "    plt.title(\"Swarm Plot of Correlations per Variable in \"+cond_toplot_type)\n",
    "\n",
    "    # Custom legend with counts\n",
    "    plt.legend(\n",
    "        title=f'Significant (p < 0.05)\\nRed: {n_sig}, Gray: {n_nonsig}',\n",
    "        # labels=['False', 'True'],  # must match unique values in significance\n",
    "        loc='best'\n",
    "    )\n",
    "\n",
    "    # Show t-test result\n",
    "    plt.text(-0.3, np.nanmax(df['corr_prepull_'+yvar_toplot_name+'_and_xvar']) * 0.9,\n",
    "             f'T-test vs 0: p = {t_pval:.3g};\\n mean = {meanvalue:.2g}', ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    #\n",
    "    # save the dataframe that have significant encoding information\n",
    "    savedata = 1\n",
    "    if savedata:\n",
    "        datasavefolder = data_saved_folder+\"data_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/significant_neuron_info/\"\n",
    "        \n",
    "        if not os.path.exists(datasavefolder):\n",
    "            os.makedirs(datasavefolder)\n",
    "            \n",
    "        with open(datasavefolder+'/'+recordedanimals[0]+'_PullFocus_significant_neuron_of_encoding_'+xxx_type+'.pkl', 'wb') as f:\n",
    "            pickle.dump(df_clean, f)  \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        if doSuccPull:\n",
    "            figgg.savefig(figsavefolder+cond_toplot_type+'_bhvevents_aligned_'+yvar_toplot_name+'_and_'+xvar_name+'_correlation_'+\n",
    "                          'succpull'+'_'+pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+savefile_sufix+'.pdf')\n",
    "        elif doFailPull:\n",
    "            figgg.savefig(figsavefolder+cond_toplot_type+'_bhvevents_aligned_'+yvar_toplot_name+'_and_'+xvar_name+'_correlation_'+\n",
    "                          'failpull'+'_'+pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+savefile_sufix+'.pdf')\n",
    "        else:\n",
    "            figgg.savefig(figsavefolder+cond_toplot_type+'_bhvevents_aligned_'+yvar_toplot_name+'_and_'+xvar_name+'_correlation_'+\n",
    "                          bhvname_ana+'_'+pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+savefile_sufix+'.pdf')\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_vector[endtime_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bhveventsFeatures_and_FRslopes_all_dates_df['FRpeaktime_prepull'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2440c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "timepointx = np.arange(-4,4,1/fps)\n",
    "xxx = np.array(list(bhvevents_aligned_FR_and_eventFeatures_allevents_tgt['FR_ievent']))\n",
    "\n",
    "plt.plot(timepointx,xxx[2,:])\n",
    "plt.plot([0,0],[-1,4],'--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80706fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['pull_rt_ievent']\n",
    "yyy = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['gaze_accum_ievent']\n",
    "\n",
    "ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "\n",
    "plt.plot(xxx[~ind_nan],yyy[~ind_nan],'.')\n",
    "\n",
    "rr,pp = st.pearsonr(xxx[~ind_nan],yyy[~ind_nan])\n",
    "print('corr:'+str(rr))\n",
    "print('pvalue:'+str(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca3c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27188011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d804b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01691733",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# do some big picture plot; here is to have a sense of the gaze-accumulation distribution of succ and failed pull\n",
    "# ...if the setting is set for that\n",
    "## \n",
    "if 0:\n",
    "    from scipy.stats import ks_2samp\n",
    "\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition_general\"] = \\\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    \n",
    "    # \n",
    "    plt.figure(figsize=(20,6))\n",
    "    data_toplot = bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "    # seaborn.kdeplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "    seaborn.kdeplot(data=data_toplot,x='gaze_accum_ievent',hue='condition_general',\n",
    "                   common_norm=False)  # don't normalize across groups\n",
    "    # seaborn.histplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "\n",
    "    # Compute group-wise quantiles (deciles from 10% to 90%)\n",
    "    # quantiles_df = data_toplot.groupby('bhv_name')['gaze_accum_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df = data_toplot.groupby('condition_general')['gaze_accum_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df.rename(columns={'gaze_accum_ievent': 'quantile_value'}, inplace=True)\n",
    "\n",
    "    # Draw vertical lines for each quantile\n",
    "    if 0:\n",
    "        # palette = dict(zip(data_toplot['bhv_name'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "        palette = dict(zip(data_toplot['condition_general'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "\n",
    "        for _, row in quantiles_df.iterrows():\n",
    "            plt.axvline(\n",
    "                row['quantile_value'],\n",
    "                # color=palette[row['bhv_name']],\n",
    "                color=palette[row['condition_general']],\n",
    "                linestyle='--',\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    plt.title('KDE Plot with Quantile Lines')\n",
    "    plt.show()\n",
    "    \n",
    "    # KS test: compare MC vs SR\n",
    "    if 0:\n",
    "        group_MC = data_toplot[data_toplot['condition_general'] == 'MC']['gaze_accum_ievent'].dropna()\n",
    "        group_SR = data_toplot[data_toplot['condition_general'] == 'SR']['gaze_accum_ievent'].dropna()\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(group_MC, group_SR)\n",
    "        print(f\"KS test: statistic={ks_stat:.4f}, p-value={ks_pval:.4g}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider the significant neurons based on the previous analysis\n",
    "if 1:\n",
    "    if doOnlySigniNeurons:\n",
    "        \n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = significant_bhv_df\n",
    "\n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = significant_bhv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98703bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "# bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4184afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - 2:\n",
    "# do a basic plot for sanity check - mean FR across all units in the pool condition for each quantile\n",
    "if 1:\n",
    "    from scipy.integrate import cumtrapz\n",
    "\n",
    "    doQuantMeanFRs = 1\n",
    "    # only do one example session, all neurons in that session, average\n",
    "    doExampleSession = 0\n",
    "    # only do one example neuron in one example cell \n",
    "    doExampleNeuron = 0\n",
    "    \n",
    "    timewins = np.arange(-4, 4, 1/30)\n",
    "    n_timepoints = len(timewins)\n",
    "        \n",
    "    #\n",
    "    if doExampleSession:\n",
    "        # examplesess = '20240606'\n",
    "        examplesess =  '20240808'\n",
    "        #\n",
    "        if doQuantMeanFRs:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col].unique())\n",
    "        else:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col].unique())      \n",
    "    # \n",
    "    elif doExampleNeuron:\n",
    "        examplesess =  '20240808'\n",
    "        exampleneuron = '129'\n",
    "        # \n",
    "        # has to do not QuantMeanFRs\n",
    "        ind_1 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "        ind_2 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID']==exampleneuron\n",
    "        quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col].unique()) \n",
    "    #\n",
    "    else:\n",
    "        if doQuantMeanFRs:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "        else:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col].unique())\n",
    "    \n",
    "    quantile_values = quantile_values[~np.isnan(quantile_values)]\n",
    "    \n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2 = ax.twinx()  # Secondary y-axis for AUC\n",
    "\n",
    "    for i_quantile, q_val in enumerate(quantile_values):\n",
    "    \n",
    "        #\n",
    "        if doExampleSession:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][ind_quantile]['FR_ievent'].tolist())\n",
    "        #\n",
    "        elif doExampleNeuron:\n",
    "            ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col] == q_val\n",
    "            fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile]['FR_ievent'].tolist())\n",
    "            gaze_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile][pull_trig_events_tgtname].tolist())\n",
    "     \n",
    "        #\n",
    "        else:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_quantile]['FR_ievent'].tolist())\n",
    "\n",
    "            \n",
    "        #     \n",
    "        mean_fr = np.nanmean(fr_traces_quantile, axis=0)\n",
    "        sem_fr = np.nanstd(fr_traces_quantile, axis=0) / np.sqrt(np.sum(~np.isnan(fr_traces_quantile[:, 0]))) # Standard Error of the Mean\n",
    "        \n",
    "        ax.plot(timewins, mean_fr, label=f'Quantile {int(q_val)}')\n",
    "        ax.fill_between(timewins, mean_fr - sem_fr, mean_fr + sem_fr, alpha=0.3)\n",
    "\n",
    "        \n",
    "        #\n",
    "        if doExampleNeuron:\n",
    "            # Accumulated AUC for each gaze trace, then average\n",
    "            auc_gaze_all = np.array([cumtrapz(trace, timewins, initial=0) for trace in gaze_traces_quantile])\n",
    "            mean_auc_gaze = np.nanmean(auc_gaze_all, axis=0)\n",
    "            sem_auc_gaze = np.nanstd(auc_gaze_all, axis=0) / np.sqrt(np.sum(~np.isnan(auc_gaze_all[:, 0])))\n",
    "            \n",
    "            # Plot AUC Gaze\n",
    "            ax2.plot(timewins, mean_auc_gaze, linestyle='--', color='lightblue', label=f'Gaze AUC {int(q_val)}')\n",
    "            ax2.fill_between(timewins, mean_auc_gaze - sem_auc_gaze, mean_auc_gaze + sem_auc_gaze, alpha=0.2, color='lightblue')\n",
    "\n",
    "        \n",
    "        \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'Mean Firing Rate by {title_prefix} Quantile '+gaze_duration_type)\n",
    "    ax.axvline(0, color='k', linestyle='--', linewidth=0.8, label='Pull Onset')\n",
    "    ax.legend()\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2.set_ylabel('Accumulated Gaze AUC')\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"\n",
    "    \n",
    "        if doExampleSession:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "        \n",
    "        if doExampleNeuron:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        if not xxx_type == 'pull_rt':\n",
    "            if not doExampleNeuron:\n",
    "                fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_acrossNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                         pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "            elif doExampleNeuron:\n",
    "                fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_exampleNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                         pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        #\n",
    "        else:\n",
    "            if not doExampleNeuron:\n",
    "                fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_acrossNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                         quantile_col+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "            elif doExampleNeuron:\n",
    "                fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_exampleNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                         quantile_col+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2889130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - calculate the PCA with the pooled data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import splprep, splev\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "if 1:\n",
    "    timewins = np.arange(-4, 4, 1 / 30)\n",
    "    n_timepoints = len(timewins)\n",
    "    quantile_col = 'gaze_accum_quantile'  # Or 'gaze_duration_quantile' or 'pull_rt_quantile'\n",
    "    # quantile_col = 'pull_rt_quantile'  # Or 'gaze_duration_quantile' or 'pull_rt_quantile'\n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    title_prefix = 'gaze_accum_quantile'  # Or 'Gaze Duration' or 'pull_rt_quantile'\n",
    "    # title_prefix = 'pull_rt_quantile'  # Or 'Gaze Duration' or 'pull_rt_quantile'\n",
    "    smooth_kernel_size = 6\n",
    "    n_bootstrap_iterations = 100\n",
    "    n_neurons_to_sample = 200\n",
    "    \n",
    "    # Get unique neurons\n",
    "    unique_neurons = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[['clusterID', 'session']].drop_duplicates()\n",
    "    n_neurons = len(unique_neurons)\n",
    "\n",
    "    # Get unique quantiles\n",
    "    unique_quantiles = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "    n_quantiles = len(unique_quantiles)\n",
    "\n",
    "    # Initialize the data matrix\n",
    "    data_matrix = np.empty((n_neurons, n_timepoints * n_quantiles))\n",
    "    neuron_index_lookup = {}\n",
    "\n",
    "    # Populate the data matrix\n",
    "    neuron_counter = 0\n",
    "    for neuron_row in unique_neurons.itertuples(index=False):\n",
    "        cluster_id = neuron_row.clusterID\n",
    "        session = neuron_row.session\n",
    "        neuron_index_lookup[(cluster_id, session)] = neuron_counter\n",
    "        neuron_counter += 1\n",
    "\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            # Get the mean FR trace for the current neuron and quantile\n",
    "            mean_fr_trace_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session'] == session) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val)\n",
    "            ]\n",
    "\n",
    "            if not mean_fr_trace_df.empty:\n",
    "                mean_fr_trace = mean_fr_trace_df['mean_FR_trace'].values[0]  # Take the first element\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = mean_fr_trace\n",
    "            else:\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = np.nan  # Handle missing data\n",
    "\n",
    "    # Prepare to store results\n",
    "    all_quantile_lengths = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_quantile_curvatures = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_boot_pca_data = np.zeros((n_bootstrap_iterations, 10, n_timepoints * n_quantiles))  # Store all PCA results\n",
    "\n",
    "    # Perform Bootstrapping\n",
    "    for boot_iter in range(n_bootstrap_iterations):\n",
    "        # 1. Randomly sample neurons\n",
    "        sampled_neuron_indices = np.random.choice(n_neurons, n_neurons_to_sample, replace=True)\n",
    "        sampled_data_matrix = data_matrix[sampled_neuron_indices, :].transpose()  # Neuron dimension becomes the columns\n",
    "\n",
    "        # 2. Run PCA\n",
    "        pca = PCA(n_components=10)  # Project to 10 PCs\n",
    "        pca.fit(np.nan_to_num(sampled_data_matrix))\n",
    "        pca_data = pca.transform(np.nan_to_num(sampled_data_matrix)).transpose()  # Project and transpose\n",
    "        all_boot_pca_data[boot_iter, :, :] = pca_data\n",
    "\n",
    "        # Calculate length and curvature for each quantile\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            start_col = i_quantile * n_timepoints\n",
    "            end_col = (i_quantile + 1) * n_timepoints\n",
    "            quantile_data = pca_data[:, start_col:end_col]  # (10, n_timepoints)\n",
    "\n",
    "            # \n",
    "            if gaze_duration_type == 'around_pull':\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, :], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, :], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'before_pull':\n",
    "                ind_tgt = timewins<0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'after_pull':\n",
    "                ind_tgt = timewins>0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "\n",
    "                    \n",
    "    # ---plotting---\n",
    "    # --- plotting number 1 ---\n",
    "    # Plot Length and Curvature (as before)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_lengths, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[0], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    #axes[0].boxplot(all_quantile_lengths)\n",
    "    # axes[0].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('PC Trajectory Length')\n",
    "    axes[0].set_title('Bootstrapped PC Trajectory Lengths')\n",
    "\n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_curvatures, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[1], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    # axes[1].boxplot(all_quantile_curvatures)\n",
    "    # axes[1].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('PC Trajectory Curvature')\n",
    "    axes[1].set_title('Bootstrapped PC Trajectory Curvatures')\n",
    "\n",
    "    # do the regression for each bootstrap iteration and then plot the average regression line\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Prepare x-axis (quantile indices)\n",
    "    quantiles = np.arange(0, n_quantiles).reshape(-1, 1)\n",
    "\n",
    "    # Store predicted regression lines from each bootstrap\n",
    "    predicted_lengths = []\n",
    "    predicted_curvatures = []\n",
    "    reg_coeff_lengths = []\n",
    "    reg_coeff_curvs = []\n",
    "\n",
    "    for i in range(n_bootstrap_iterations):\n",
    "        # Regression for length\n",
    "        y_len = all_quantile_lengths[i]\n",
    "        model_len = LinearRegression().fit(quantiles, y_len)\n",
    "        slope = model_len.coef_\n",
    "        reg_coeff_lengths.append(slope[0])\n",
    "        pred_len = model_len.predict(quantiles)\n",
    "        predicted_lengths.append(pred_len)\n",
    "\n",
    "        # Regression for curvature\n",
    "        y_curv = all_quantile_curvatures[i]\n",
    "        model_curv = LinearRegression().fit(quantiles, y_curv)\n",
    "        slope = model_curv.coef_\n",
    "        reg_coeff_curvs.append(slope[0])\n",
    "        pred_curv = model_curv.predict(quantiles)\n",
    "        predicted_curvatures.append(pred_curv)\n",
    "\n",
    "    # Convert to arrays\n",
    "    predicted_lengths = np.array(predicted_lengths)\n",
    "    predicted_curvatures = np.array(predicted_curvatures)\n",
    "\n",
    "    # Mean and 95% CI across bootstraps\n",
    "    mean_len = np.mean(predicted_lengths, axis=0)\n",
    "    ci_len_low = np.percentile(predicted_lengths, 2.5, axis=0)\n",
    "    ci_len_high = np.percentile(predicted_lengths, 97.5, axis=0)\n",
    "\n",
    "    mean_curv = np.mean(predicted_curvatures, axis=0)\n",
    "    ci_curv_low = np.percentile(predicted_curvatures, 2.5, axis=0)\n",
    "    ci_curv_high = np.percentile(predicted_curvatures, 97.5, axis=0)\n",
    "\n",
    "    # Length\n",
    "    axes[0].plot(quantiles, mean_len, color='red', label='Mean Regression')\n",
    "    axes[0].fill_between(quantiles.flatten(), ci_len_low, ci_len_high, color='red', alpha=0.3, label='95% CI')\n",
    "    axes[0].set_title('PC Trajectory Length')\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('Length')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Curvature\n",
    "    axes[1].plot(quantiles, mean_curv, color='blue', label='Mean Regression')\n",
    "    axes[1].fill_between(quantiles.flatten(), ci_curv_low, ci_curv_high, color='blue', alpha=0.3, label='95% CI')\n",
    "    axes[1].set_title('PC Trajectory Curvature')\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('Curvature')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_lengths, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the length regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "    #\n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_curvs, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the curvature regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "\n",
    "    # --- plotting number 1 end ---\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Average 3D Traces\n",
    "    fig2 = plt.figure(figsize=(10, 8))\n",
    "    ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "    mean_traces_3d = np.mean(all_boot_pca_data[:, :3, :], axis=0)  # Average across bootstrap iterations, use only first 3 PCs\n",
    "\n",
    "    for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "        start_col = i_quantile * n_timepoints\n",
    "        end_col = (i_quantile + 1) * n_timepoints\n",
    "        mean_quantile_data = mean_traces_3d[:, start_col:end_col]\n",
    "\n",
    "        # Smooth the trajectory\n",
    "        smooth_x = gaussian_filter1d(mean_quantile_data[0, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_y = gaussian_filter1d(mean_quantile_data[1, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_z = gaussian_filter1d(mean_quantile_data[2, ind_tgt], sigma=smooth_kernel_size)\n",
    "\n",
    "        # Plot the smoothed trajectory\n",
    "        ax.plot(smooth_x, smooth_y, smooth_z, label=f'Quantile {int(q_val)}')\n",
    "\n",
    "        # Mark start and end points\n",
    "        ax.scatter(smooth_x[0], smooth_y[0], smooth_z[0], marker='o', color='k')  # Start\n",
    "        ax.scatter(smooth_x[-1], smooth_y[-1], smooth_z[-1], marker='x', color='k')  # End\n",
    "\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_zlabel('PC 3')\n",
    "    ax.set_title(f'Average 3D Trajectories in Neuron-Reduced PC Space by {title_prefix} Quantile')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\n",
    "    print(\"\\n--- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\")\n",
    "\n",
    "    from scipy import stats\n",
    "    from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "    def perform_pairwise_paired_ttests(data, group_labels, alpha=0.01):\n",
    "        unique_groups = np.unique(group_labels)\n",
    "        n_groups = len(unique_groups)\n",
    "        p_values = []\n",
    "        comparisons = []\n",
    "\n",
    "        for i in range(n_groups):\n",
    "            for j in range(i + 1, n_groups):\n",
    "                group1_data = data[:, i]\n",
    "                group2_data = data[:, j]\n",
    "                t_stat, p_val = stats.ttest_rel(group1_data, group2_data)\n",
    "                p_values.append(p_val)\n",
    "                comparisons.append((unique_groups[i], unique_groups[j]))\n",
    "\n",
    "        reject, p_corrected, _, _ = multipletests(p_values, method='holm', alpha=alpha)\n",
    "\n",
    "        results_df = pd.DataFrame({'Comparison': comparisons,\n",
    "                                   'p_value': p_values,\n",
    "                                   'p_corrected': p_corrected,\n",
    "                                   'reject_null': reject})\n",
    "        return results_df\n",
    "\n",
    "    # Perform pairwise paired t-tests for Length\n",
    "    length_pairwise_results = perform_pairwise_paired_ttests(all_quantile_lengths, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Length:\")\n",
    "    print(length_pairwise_results)\n",
    "\n",
    "    # Perform pairwise paired t-tests for Curvature\n",
    "    curvature_pairwise_results = perform_pairwise_paired_ttests(all_quantile_curvatures, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Curvature:\")\n",
    "    print(curvature_pairwise_results)\n",
    "    \n",
    "    \n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'bhvevents_aligned_PCfeatures_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_PCtrajectory_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sampled_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f380d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_duration_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e511b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
