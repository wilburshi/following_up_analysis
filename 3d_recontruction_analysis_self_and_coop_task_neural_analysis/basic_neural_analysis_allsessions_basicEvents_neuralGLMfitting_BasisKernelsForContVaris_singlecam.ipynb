{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### use GLM model to analyze spike count trains, the GLM use continuous variables and use basis kernel to simplify the fitting\n",
    "#### also add the function to reduce the continuous variables into smaller dimensions to reduce the correlation and be more task related "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - GLM fitting for spike trains based on the discrete variables from single camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.singlecam_bhv_var_neuralGLM_fitting_BasisKernelsForContVaris import get_singlecam_bhv_var_for_neuralGLM_fitting_BasisKernelsForContVaris\n",
    "from ana_functions.singlecam_bhv_var_neuralGLM_fitting_BasisKernelsForContVaris import neuralGLM_fitting_BasisKernelsForContVaris\n",
    "from ana_functions.singlecam_bhv_var_neuralGLM_fitting_BasisKernelsForContVaris_PullGazeVectorProjection import neuralGLM_fitting_BasisKernelsForContVaris_PullGazeVectorProjection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e93bc7",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f009d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "from ana_functions.get_pull_infos import get_pull_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483723a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_based_correction_with_timing(real_mean, shuffled_coefs, alpha=0.05, time_axis=None):\n",
    "    n_boot, n_vars, n_basis = shuffled_coefs.shape\n",
    "    cluster_significance = np.zeros(n_vars, dtype=bool)\n",
    "    cluster_timing = ['None'] * n_vars\n",
    "\n",
    "    for var in range(n_vars):\n",
    "        real_coef = real_mean[var, :]\n",
    "        shuf_coef = shuffled_coefs[:, var, :]\n",
    "\n",
    "        # Empirical p-values\n",
    "        p_vals = (np.sum(np.abs(shuf_coef) >= np.abs(real_coef), axis=0) + 1) / (n_boot + 1)\n",
    "        sig_mask = p_vals < alpha\n",
    "\n",
    "        # Cluster label\n",
    "        labeled_array, n_clusters = label(sig_mask)\n",
    "        real_cluster_sizes = [\n",
    "            np.sum(labeled_array == cluster_idx + 1)\n",
    "            for cluster_idx in range(n_clusters)\n",
    "        ]\n",
    "        max_real_cluster = np.max(real_cluster_sizes) if real_cluster_sizes else 0\n",
    "\n",
    "        # Compute max cluster size in each shuffled iteration\n",
    "        shuf_max_clusters = []\n",
    "        for b in range(n_boot):\n",
    "            others = np.delete(shuf_coef, b, axis=0)\n",
    "            p_vals_shuf = np.mean(np.abs(others) >= np.abs(shuf_coef[b, :]), axis=0)\n",
    "            sig_shuf = p_vals_shuf < alpha\n",
    "            lbl_shuf, n_lbl = label(sig_shuf)\n",
    "            max_cluster = max([np.sum(lbl_shuf == i + 1) for i in range(n_lbl)], default=0)\n",
    "            shuf_max_clusters.append(max_cluster)\n",
    "\n",
    "        cluster_thresh = np.percentile(shuf_max_clusters, 100 * (1 - alpha))\n",
    "        cluster_significance[var] = max_real_cluster > cluster_thresh\n",
    "\n",
    "        # Dominant timing classification\n",
    "        if cluster_significance[var] and time_axis is not None:\n",
    "            sig_times = time_axis[sig_mask]\n",
    "            n_before = np.sum(sig_times < 0)\n",
    "            n_after = np.sum(sig_times > 0)\n",
    "\n",
    "            if n_before > n_after:\n",
    "                cluster_timing[var] = 'Reactive'\n",
    "            elif n_after > n_before:\n",
    "                cluster_timing[var] = 'Predictive'\n",
    "            else:\n",
    "                cluster_timing[var] = 'Both' if (n_before > 0) else 'None'\n",
    "\n",
    "    return cluster_significance, cluster_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the orthogonality among the three behavioral vectors\n",
    "def check_orthogonality(*vectors, tol=1e-6):\n",
    "    n = len(vectors)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dot = np.dot(vectors[i], vectors[j])\n",
    "            print(f\"Dot product between vector {i} and {j}: {dot:.6f}\")\n",
    "            if abs(dot) > tol:\n",
    "                print(\"⚠️ Not orthogonal!\")\n",
    "            else:\n",
    "                print(\"✅ Orthogonal\")\n",
    "                \n",
    "#  Gram-Schmidt Orthogonalization\n",
    "def gram_schmidt(vectors):\n",
    "    \"\"\"Orthogonalize a list of vectors using the Gram-Schmidt process.\"\"\"\n",
    "    orthogonal_vectors = []\n",
    "    for v in vectors:\n",
    "        # Subtract projection onto all previous orthogonal vectors\n",
    "        for u in orthogonal_vectors:\n",
    "            v = v - np.dot(v, u) * u\n",
    "        # Normalize the vector\n",
    "        norm = np.linalg.norm(v)\n",
    "        if norm > 1e-10:\n",
    "            orthogonal_vectors.append(v / norm)\n",
    "    return orthogonal_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "\n",
    "# GLM related variables\n",
    "# all these are not in use...\n",
    "Kernel_coefs_all_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_spikehist_all_dates = dict.fromkeys(dates_list, [])\n",
    "#\n",
    "Kernel_coefs_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "Kernel_spikehist_all_shuffled_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# for summarizing spike related traces\n",
    "spike_trig_trace_summary = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                        ])\n",
    "bhv_aligned_FR_trace_summary = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                        ])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neuralGLM_new'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "\n",
    "    with open(data_saved_subfolder+'/spike_trig_trace_summary_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        spike_trig_trace_summary = pickle.load(f)         \n",
    "    with open(data_saved_subfolder+'/bhv_aligned_FR_trace_summary_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_aligned_FR_trace_summary = pickle.load(f) \n",
    "    \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "    \n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "\n",
    "            # folder and file path\n",
    "            camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "            camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "            \n",
    "            # \n",
    "            try: \n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                    bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                    bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "                video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                    bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                    bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "            \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        lever_gaze1 = output_time_points_levertube['time_point_lookatlever1']\n",
    "        lever_gaze2 = output_time_points_levertube['time_point_lookatlever2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        lever_gaze1 = lever_gaze1[lever_gaze1>(firstpulltime-15)]\n",
    "        lever_gaze2 = lever_gaze2[lever_gaze2>(firstpulltime-15)]\n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        lever_gaze1 = lever_gaze1[lever_gaze1<(lastpulltime+15)] \n",
    "        lever_gaze2 = lever_gaze2[lever_gaze2<(lastpulltime+15)] \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        # a new definition of successful and failed pulls\n",
    "        # separate successful and failed pulls\n",
    "        # step 1 all pull and juice\n",
    "        time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "        time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "        time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "        time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "        # step 2:\n",
    "        # pull 1\n",
    "        # Find the last pull before each juice\n",
    "        successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "        # Convert to Pandas Series\n",
    "        successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "        # Find failed pulls (pulls that are not successful)\n",
    "        failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "        # pull 2\n",
    "        # Find the last pull before each juice\n",
    "        successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "        # Convert to Pandas Series\n",
    "        successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "        # Find failed pulls (pulls that are not successful)\n",
    "        failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "        #\n",
    "        # step 3:\n",
    "        time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "        time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "        time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "        time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        \n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.ceil(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        #\n",
    "        # remove task irrelavant period\n",
    "        if totalsess_time > (lastpulltime+session_start_time+15):\n",
    "            totalsess_time = np.ceil(lastpulltime+session_start_time+15)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 1:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # get the dataset for GLM and run GLM\n",
    "        if 1:\n",
    "            # get the organized data for GLM\n",
    "            print('get '+neural_record_condition+' data for single camera GLM fitting')\n",
    "            #\n",
    "            gausKernelsize = 4 # 4; 15\n",
    "            \n",
    "            data_summary, data_summary_names, spiketrain_summary = get_singlecam_bhv_var_for_neuralGLM_fitting_BasisKernelsForContVaris(gausKernelsize,fps, \n",
    "                                                                                        animal1, animal2, recordedanimal, animalnames_videotrack, \n",
    "                                                                                        session_start_time, time_point_pull1, time_point_pull2, time_point_juice1, time_point_juice2, \n",
    "                                                                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, \n",
    "                                                                                        output_look_ornot, output_allvectors, output_allangles, output_key_locations, \n",
    "                                                                                        spike_clusters_data, spike_time_data, spike_channels_data)\n",
    "                \n",
    "            # MODIFICATION: Define kernel parameters here for easy adjustment\n",
    "            KERNEL_DURATION_S = 8.0  # total span: -4s to +4s\n",
    "            KERNEL_OFFSET_S = -4.0   # shift so it starts at -4s\n",
    "            N_BASIS_FUNCS = 26     # The number of basis functions to represent the kernel\n",
    "                \n",
    "            var_toglm_names = ['gaze_other_angle', 'gaze_lever_angle', # 'gaze_tube_angle',\n",
    "                               'animal_animal_dist', 'animal_lever_dist', # 'animal_tube_dist',\n",
    "                               'mass_move_speed', 'gaze_angle_speed',\n",
    "                               'otherani_otherlever_dist', # 'otherani_othertube_dist', # 'othergaze_self_angle',\n",
    "                               'other_mass_move_speed',\n",
    "                               'selfpull_prob',\n",
    "                               'socialgaze_prob',\n",
    "                               'otherpull_prob',\n",
    "                              ]\n",
    "            nvars_toglm = np.shape(var_toglm_names)[0]\n",
    "            \n",
    "            #\n",
    "            # neuralGLM with all variables\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load the session wised data for neural GLM fitting')\n",
    "\n",
    "                current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_neuralglm_model'+savefile_sufix+'/'+\\\n",
    "                              animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+recordedanimal+'Recorded'\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "\n",
    "                with open(add_date_dir+'/neuralGLM_kernels_coef.pkl', 'rb') as f:\n",
    "                    neuralGLM_kernels_coef = pickle.load(f)\n",
    "                with open(add_date_dir+'/neuralGLM_kernels_tempFilter.pkl', 'rb') as f:\n",
    "                    neuralGLM_kernels_tempFilter = pickle.load(f)\n",
    "                with open(add_date_dir+'/neuralGLM_kernels_coef_shf.pkl', 'rb') as f:\n",
    "                    neuralGLM_kernels_coef_shf = pickle.load(f)\n",
    "                with open(add_date_dir+'/neuralGLM_kernels_tempFilter_shf.pkl', 'rb') as f:\n",
    "                    neuralGLM_kernels_tempFilter_shf = pickle.load(f)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                print('do GLM fitting for spike trains with continuous variables')\n",
    "                \n",
    "                # dp the glm for n bootstraps, each bootstrap do 80/20 training/testing\n",
    "                N_BOOTSTRAPS = 100\n",
    "                test_size = 0.4\n",
    "                #\n",
    "                dospikehist = 0\n",
    "                spikehist_twin = 2\n",
    "                #\n",
    "                try:\n",
    "                    neuralGLM_kernels_coef, neuralGLM_kernels_tempFilter, \\\n",
    "                    neuralGLM_kernels_coef_shf, neuralGLM_kernels_tempFilter_shf, _ \\\n",
    "                            = neuralGLM_fitting_BasisKernelsForContVaris(KERNEL_DURATION_S, KERNEL_OFFSET_S,\n",
    "                                                    N_BASIS_FUNCS, fps, \n",
    "                                                    animal1, animal2, recordedanimal,\n",
    "                                                    var_toglm_names, data_summary_names, data_summary, \n",
    "                                                    spiketrain_summary, dospikehist, spikehist_twin, \n",
    "                                                    N_BOOTSTRAPS,test_size )\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                # save data\n",
    "                if 1:\n",
    "                    current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_neuralglm_model'+savefile_sufix+'/'+\\\n",
    "                                  animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+recordedanimal+'Recorded'\n",
    "                    add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "                    if not os.path.exists(add_date_dir):\n",
    "                        os.makedirs(add_date_dir)\n",
    "                    #\n",
    "                    with open(add_date_dir+'/neuralGLM_kernels_coef.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_kernels_coef, f)\n",
    "                    with open(add_date_dir+'/neuralGLM_kernels_tempFilter.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_kernels_tempFilter, f)\n",
    "                    with open(add_date_dir+'/neuralGLM_kernels_coef_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_kernels_coef_shf, f)\n",
    "                    with open(add_date_dir+'/neuralGLM_kernels_tempFilter_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_kernels_tempFilter_shf, f)\n",
    "                        \n",
    "                \n",
    "            #\n",
    "            # neuralGLM with all projected variables to the three axies\n",
    "            try:\n",
    "                var_toglm_names_mainAxesProjected = ['pull_PC', 'gaze_PC', 'juice_PC']\n",
    "                # dummy\n",
    "                print('load the session wised data for neural GLM fitting - pull gaze juice projected axes neural-glm')\n",
    "\n",
    "                current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_neuralglm_model'+savefile_sufix+'/'+\\\n",
    "                              animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+recordedanimal+'Recorded'\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "\n",
    "                with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_coef.pkl', 'rb') as f:\n",
    "                    neuralGLM_mainAxesProjected_kernels_coef = pickle.load(f)\n",
    "                with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_tempFilter.pkl', 'rb') as f:\n",
    "                    neuralGLM_mainAxesProjected_kernels_tempFilter = pickle.load(f)\n",
    "                with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_coef_shf.pkl', 'rb') as f:\n",
    "                    neuralGLM_mainAxesProjected_kernels_coef_shf = pickle.load(f)\n",
    "                with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_tempFilter_shf.pkl', 'rb') as f:\n",
    "                    neuralGLM_mainAxesProjected_kernels_tempFilter_shf = pickle.load(f)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                print('do GLM fitting for spike trains with continuous variables - pull gaze juice projected axes neural-glm')\n",
    "                \n",
    "                # dp the glm for n bootstraps, each bootstrap do 80/20 training/testing\n",
    "                N_BOOTSTRAPS = 100\n",
    "                test_size = 0.4\n",
    "                #\n",
    "                dospikehist = 0\n",
    "                spikehist_twin = 2\n",
    "                #\n",
    "                try:\n",
    "                    neuralGLM_mainAxesProjected_kernels_coef, neuralGLM_mainAxesProjected_kernels_tempFilter, \\\n",
    "                    neuralGLM_mainAxesProjected_kernels_coef_shf, neuralGLM_mainAxesProjected_kernels_tempFilter_shf, \\\n",
    "                    var_toglm_names_mainAxesProjected = neuralGLM_fitting_BasisKernelsForContVaris_PullGazeVectorProjection(\n",
    "                                                    KERNEL_DURATION_S, KERNEL_OFFSET_S,\n",
    "                                                    N_BASIS_FUNCS, fps, \n",
    "                                                    animal1, animal2, recordedanimal,\n",
    "                                                    var_toglm_names, data_summary_names, data_summary, \n",
    "                                                    spiketrain_summary, dospikehist, spikehist_twin, \n",
    "                                                    N_BOOTSTRAPS,test_size )\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "                # save data\n",
    "                if 1:\n",
    "                    current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_neuralglm_model'+savefile_sufix+'/'+\\\n",
    "                                  animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+recordedanimal+'Recorded'\n",
    "                    add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "                    if not os.path.exists(add_date_dir):\n",
    "                        os.makedirs(add_date_dir)\n",
    "                    #\n",
    "                    with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_coef.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_mainAxesProjected_kernels_coef, f)\n",
    "                    with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_tempFilter.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_mainAxesProjected_kernels_tempFilter, f)\n",
    "                    with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_coef_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_mainAxesProjected_kernels_coef_shf, f)\n",
    "                    with open(add_date_dir+'/neuralGLM_mainAxesProjected_kernels_tempFilter_shf.pkl', 'wb') as f:\n",
    "                        pickle.dump(neuralGLM_mainAxesProjected_kernels_tempFilter_shf, f)\n",
    "                        \n",
    "\n",
    "                        \n",
    "            #\n",
    "            # determine if each neuron is encoding each variables and are they predicting or reacting based on the time\n",
    "            # make the summarying plot for each neuron\n",
    "            \n",
    "            doplot = 0\n",
    "                \n",
    "            # load spike triggered average and bhv triggered fr\n",
    "            if recordedanimal == 'kanga':\n",
    "                data_saved_subfolder_2 = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+\\\n",
    "                                   cameraID+'/dannonkanga/'\n",
    "                with open(data_saved_subfolder_2+'/spike_trig_events_all_dates_dannonkanga.pkl', 'rb') as f:\n",
    "                    spike_trig_events_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder_2+'/bhvevents_aligned_FR_allevents_all_dates_dannonkanga.pkl', 'rb') as f:\n",
    "                    bhvevents_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "            elif recordedanimal == 'dodson':\n",
    "                data_saved_subfolder_2 = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+\\\n",
    "                                   cameraID+'/dodsonginger/'\n",
    "                with open(data_saved_subfolder_2+'/spike_trig_events_all_dates_dodsonginger.pkl', 'rb') as f:\n",
    "                    spike_trig_events_all_dates = pickle.load(f)\n",
    "                with open(data_saved_subfolder_2+'/bhvevents_aligned_FR_allevents_all_dates_dodsonginger.pkl', 'rb') as f:\n",
    "                    bhvevents_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "\n",
    "            #\n",
    "            neuronIDs = np.array(list(neuralGLM_kernels_coef.keys()))\n",
    "            nneurons = np.shape(neuronIDs)[0]\n",
    "\n",
    "            # var_toglm_names_toplot = var_toglm_names\n",
    "            var_toglm_names_toplot = var_toglm_names_mainAxesProjected\n",
    "            nvars = np.shape(var_toglm_names_toplot)[0]\n",
    "\n",
    "            #\n",
    "            if doplot:\n",
    "                fig, axes = plt.subplots(nneurons, nvars + 4, figsize=(3 * (nvars + 4), 2.5 * nneurons), sharex=True)\n",
    "\n",
    "            #\n",
    "            for ineuron in np.arange(0,nneurons,1):\n",
    "                neuronID = neuronIDs[ineuron]\n",
    "\n",
    "                try:\n",
    "                    # real_coefs = np.array(neuralGLM_kernels_coef[neuronID])       # shape: (N_BOOTSTRAPS, nvars_toglm, N_BASIS_FUNCS)\n",
    "                    # shuffled_coefs = np.array(neuralGLM_kernels_coef_shf[neuronID])  # shape: (N_BOOTSTRAPS, nvars_toglm, N_BASIS_FUNCS)\n",
    "                    #\n",
    "                    real_coefs = np.array(neuralGLM_mainAxesProjected_kernels_coef[neuronID])       # shape: (N_BOOTSTRAPS, nvars_toglm, N_BASIS_FUNCS)\n",
    "                    shuffled_coefs = np.array(neuralGLM_mainAxesProjected_kernels_coef_shf[neuronID])  # shape: (N_BOOTSTRAPS, nvars_toglm, N_BASIS_FUNCS)\n",
    "\n",
    "                    N_BASIS_FUNCS = np.shape(real_coefs)[2]\n",
    "\n",
    "                    # Compute the mean beta over bootstraps for real data\n",
    "                    real_mean = np.mean(real_coefs, axis=0)\n",
    "\n",
    "                    time_axis = np.linspace(KERNEL_OFFSET_S, KERNEL_DURATION_S+KERNEL_OFFSET_S, N_BASIS_FUNCS)  # same time window used in kernel\n",
    "\n",
    "                    sig_vars, sig_timing = cluster_based_correction_with_timing(real_mean, shuffled_coefs, \n",
    "                                                                alpha=0.01, time_axis=time_axis)\n",
    "\n",
    "                    # get the kernel traces\n",
    "                    # real_kernels = np.array(neuralGLM_kernels_tempFilter[neuronID])\n",
    "                    # shuffled_kernels = np.array(neuralGLM_kernels_tempFilter_shf[neuronID])\n",
    "                    #\n",
    "                    real_kernels = np.array(neuralGLM_mainAxesProjected_kernels_tempFilter[neuronID])\n",
    "                    shuffled_kernels = np.array(neuralGLM_mainAxesProjected_kernels_tempFilter_shf[neuronID])\n",
    "\n",
    "                    ######\n",
    "                    # plot a figure that has the mean kernels for each variables and the neurons' spike triggered averaged\n",
    "                    # make a big plot for each session with all neurons in it\n",
    "                    ######\n",
    "\n",
    "                    # get spike triggered average and bhv triggered fr for each neuron\n",
    "\n",
    "                    spike_trig_selfpull = spike_trig_events_all_dates[date_tgt][recordedanimal]\\\n",
    "                                            ['leverpull_prob'][str(neuronID)]['st_average']\n",
    "                    spike_trig_selfgaze = spike_trig_events_all_dates[date_tgt][recordedanimal]\\\n",
    "                                            ['socialgaze_prob'][str(neuronID)]['st_average']\n",
    "                    pull_trig_fr = bhvevents_aligned_FR_allevents_all_dates[date_tgt][recordedanimal+' pull']\\\n",
    "                                            [str(neuronID)]['FR_allevents']\n",
    "                    gaze_trig_fr = bhvevents_aligned_FR_allevents_all_dates[date_tgt][recordedanimal+' gazestart']\\\n",
    "                                            [str(neuronID)]['FR_allevents']\n",
    "\n",
    "                    \n",
    "                    # put the result in the summarizing dataframe\n",
    "                    spike_trig_trace_summary = spike_trig_trace_summary.append({\n",
    "                                'dates': date_tgt, \n",
    "                                'condition':task_conditions[idate],\n",
    "                                'neuronID':neuronID,\n",
    "                                'act_animal':recordedanimal,\n",
    "                                'bhv_name': 'pull',\n",
    "                                'signiEncodedOrNot':sig_vars[0],\n",
    "                                'signiTemporalType':sig_timing[0],\n",
    "                                'SpikeTrigAverage': spike_trig_selfpull,\n",
    "                               }, ignore_index=True)\n",
    "                    #\n",
    "                    spike_trig_trace_summary = spike_trig_trace_summary.append({\n",
    "                                'dates': date_tgt, \n",
    "                                'condition':task_conditions[idate],\n",
    "                                'neuronID':neuronID,\n",
    "                                'act_animal':recordedanimal,\n",
    "                                'bhv_name': 'gaze',\n",
    "                                'signiEncodedOrNot':sig_vars[1],\n",
    "                                'signiTemporalType':sig_timing[1],\n",
    "                                'SpikeTrigAverage': spike_trig_selfgaze,\n",
    "                               }, ignore_index=True)\n",
    "                                \n",
    "                    #\n",
    "                    #\n",
    "                    bhv_aligned_FR_trace_summary = bhv_aligned_FR_trace_summary.append({\n",
    "                                'dates': date_tgt, \n",
    "                                'condition':task_conditions[idate],\n",
    "                                'neuronID':neuronID,\n",
    "                                'act_animal':recordedanimal,\n",
    "                                'bhv_name': 'pull',\n",
    "                                'signiEncodedOrNot':sig_vars[0],\n",
    "                                'signiTemporalType':sig_timing[0],\n",
    "                                'BhvTrigAverage': np.nanmean(pull_trig_fr,axis=1),\n",
    "                               }, ignore_index=True)\n",
    "                    #\n",
    "                    bhv_aligned_FR_trace_summary = bhv_aligned_FR_trace_summary.append({\n",
    "                                'dates': date_tgt, \n",
    "                                'condition':task_conditions[idate],\n",
    "                                'neuronID':neuronID,\n",
    "                                'act_animal':recordedanimal,\n",
    "                                'bhv_name': 'gaze',\n",
    "                                'signiEncodedOrNot':sig_vars[1],\n",
    "                                'signiTemporalType':sig_timing[1],\n",
    "                                'BhvTrigAverage': np.nanmean(gaze_trig_fr,axis=1),\n",
    "                               }, ignore_index=True)\n",
    "                    \n",
    "\n",
    "                    #####\n",
    "                    # do the plot with kernels, and spike_triggered_bhv and bhv_trigged_fr\n",
    "                    #####\n",
    "                    if doplot:\n",
    "\n",
    "                        time_axis = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "                        for jvar in range(nvars):\n",
    "\n",
    "                            sig_var = sig_vars[jvar]\n",
    "                            sig_timing_jvar = sig_timing[jvar]\n",
    "\n",
    "                            ax = axes[ineuron, jvar]\n",
    "                            real_mean = np.nanmean(real_kernels[jvar], axis=0)\n",
    "                            real_std = np.nanstd(real_kernels[jvar], axis=0)\n",
    "                            shuf_mean = np.nanmean(shuffled_kernels[jvar], axis=0)\n",
    "                            shuf_std = np.nanstd(shuffled_kernels[jvar], axis=0)\n",
    "\n",
    "                            ax.plot(time_axis, real_mean, label='Real', color='blue')\n",
    "                            ax.fill_between(time_axis, real_mean - real_std, real_mean + real_std, alpha=0.3, color='blue')\n",
    "                            ax.plot(time_axis, shuf_mean, label='Shuffled', color='gray')\n",
    "                            ax.fill_between(time_axis, shuf_mean - shuf_std, shuf_mean + shuf_std, alpha=0.3, color='gray')\n",
    "\n",
    "                            if ineuron == 0:\n",
    "                                ax.set_title(var_toglm_names_toplot[jvar])\n",
    "                            if jvar == 0:\n",
    "                                ax.set_ylabel('neuron# '+str(neuronID))\n",
    "                            if ineuron == nneurons - 1:\n",
    "                                ax.set_xlabel('Time (s)')\n",
    "\n",
    "                            if sig_var:\n",
    "                                 ax.text(0.95, 0.9, sig_timing_jvar, transform=ax.transAxes,\n",
    "                                        ha='right', va='top', fontsize=8, color='blue')\n",
    "\n",
    "                        # Spike→Pull\n",
    "                        ax_spkpull = axes[ineuron, nvars]\n",
    "                        ax_spkpull.plot(time_axis, spike_trig_selfpull, color='green')\n",
    "                        if ineuron == 0:\n",
    "                            ax_spkpull.set_title(\"Spike→Pull\")\n",
    "                        if ineuron == nneurons - 1:\n",
    "                            ax_spkpull.set_xlabel('Time (s)')\n",
    "\n",
    "                        # Spike→Gaze\n",
    "                        ax_spkgaze = axes[ineuron, nvars + 1]\n",
    "                        ax_spkgaze.plot(time_axis, spike_trig_selfgaze, color='purple')\n",
    "                        if ineuron == 0:\n",
    "                            ax_spkgaze.set_title(\"Spike→Gaze\")\n",
    "                        if ineuron == nneurons - 1:\n",
    "                            ax_spkgaze.set_xlabel('Time (s)')\n",
    "\n",
    "                        # Pull→FR\n",
    "                        ax_pullfr = axes[ineuron, nvars + 2]\n",
    "                        pull_mean = np.nanmean(pull_trig_fr, axis=1)\n",
    "                        pull_sem = np.nanstd(pull_trig_fr, axis=1) / np.sqrt(pull_trig_fr.shape[1])\n",
    "                        ax_pullfr.plot(time_axis, pull_mean, color='red')\n",
    "                        ax_pullfr.fill_between(time_axis, pull_mean - pull_sem, pull_mean + pull_sem, alpha=0.3, color='red')\n",
    "                        if ineuron == 0:\n",
    "                            ax_pullfr.set_title(\"Pull→FR\")\n",
    "                        if ineuron == nneurons - 1:\n",
    "                            ax_pullfr.set_xlabel('Time (s)')\n",
    "\n",
    "                        # Gaze→FR\n",
    "                        ax_gazefr = axes[ineuron, nvars + 3]\n",
    "                        gaze_mean = np.nanmean(gaze_trig_fr, axis=1)\n",
    "                        gaze_sem = np.nanstd(gaze_trig_fr,axis=1) / np.sqrt(gaze_trig_fr.shape[1])\n",
    "                        ax_gazefr.plot(time_axis, gaze_mean, color='orange')\n",
    "                        ax_gazefr.fill_between(time_axis, gaze_mean - gaze_sem, gaze_mean + gaze_sem, alpha=0.3, color='orange')\n",
    "                        if ineuron == 0:\n",
    "                            ax_gazefr.set_title(\"Gaze→FR\")\n",
    "                        if ineuron == nneurons - 1:\n",
    "                            ax_gazefr.set_xlabel('Time (s)')\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if doplot:\n",
    "                plt.tight_layout()\n",
    "                # plt.show()\n",
    "\n",
    "                savefig = 0\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_BasisKernelsForContVaris_singlecam/\"+\\\n",
    "                            cameraID+\"/\"+savefile_sufix+\"/\"+\\\n",
    "                            animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt+\"/\"\n",
    "                #\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                if savefig:\n",
    "                    plt.savefig(save_path+'allneurons_ContBhvKerneledNeuroGLM_and_spikeTrigBhv_and_bhvTrigFr.pdf')\n",
    "                #\n",
    "                # plt.close('all')\n",
    "\n",
    "                \n",
    "            \n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neuralGLM_new'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "          \n",
    "        with open(data_saved_subfolder+'/spike_trig_trace_summary_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(spike_trig_trace_summary, f)    \n",
    "        with open(data_saved_subfolder+'/bhv_aligned_FR_trace_summary_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_aligned_FR_trace_summary, f)    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e782ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a summarizing plot based on spike_trig_trace_summary and bhv_trig_trace_summary\n",
    "###\n",
    "# For Kanga\n",
    "conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVermelho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "cond_toplot_type = 'allMC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    bhv_aligned_FR_trace_tgt = bhv_aligned_FR_trace_summary[np.isin(bhv_aligned_FR_trace_summary['condition'],conditions_to_ana)]\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # The 'SpikeTrigAverage' column is a string representation of a list.\n",
    "    # We need to convert it into an actual list of numbers (or a numpy array).\n",
    "    # We use pd.eval to safely evaluate the string as a Python expression.\n",
    "    # spike_trig_trace_tgt['SpikeTrigAverage'] = spike_trig_trace_tgt['SpikeTrigAverage'].apply(pd.eval).apply(np.array)\n",
    "\n",
    "\n",
    "    # --- Define Plotting Categories and Behaviors ---\n",
    "    # We directly use the values from the 'signiTemporalType' column.\n",
    "    categories = ['Reactive', 'Predictive', 'Both', 'None']\n",
    "    behaviors = ['pull', 'gaze']\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey='row')\n",
    "    fig.suptitle('Mean Bhv Aligned Average Traces by Behavior and Temporal Type', fontsize=20)\n",
    "\n",
    "    for i, bhv in enumerate(behaviors):\n",
    "        for j, cat_name in enumerate(categories):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            # Filter the data for the current behavior and temporal type category\n",
    "            df_subset = bhv_aligned_FR_trace_tgt[\n",
    "                (bhv_aligned_FR_trace_tgt['bhv_name'] == bhv) &\n",
    "                (bhv_aligned_FR_trace_tgt['signiTemporalType'] == cat_name)\n",
    "            ]\n",
    "\n",
    "            # Get the number of unique neurons in this subset\n",
    "            num_neurons = df_subset['neuronID'].nunique()\n",
    "\n",
    "            if not df_subset.empty:\n",
    "                # Stack the traces into a 2D numpy array\n",
    "                all_traces = np.vstack(df_subset['BhvTrigAverage'].values)\n",
    "\n",
    "                # Calculate mean trace\n",
    "                mean_trace = np.mean(all_traces, axis=0)\n",
    "\n",
    "                # Create the time axis from -4s to 4s\n",
    "                num_points = len(mean_trace)\n",
    "                time_axis = np.linspace(-4, 4, num=num_points)\n",
    "\n",
    "                # Plot the mean trace\n",
    "                ax.plot(time_axis, mean_trace, lw=2, label=f'{bhv} - {cat_name}')\n",
    "\n",
    "                # Calculate and plot the error area (SEM) if there's more than one neuron\n",
    "                if num_neurons > 1:\n",
    "                    sem_trace = np.std(all_traces, axis=0) / np.sqrt(num_neurons)\n",
    "                    ax.fill_between(time_axis, \n",
    "                                    mean_trace - sem_trace, \n",
    "                                    mean_trace + sem_trace, \n",
    "                                    alpha=0.3)\n",
    "\n",
    "            # Add the neuron count as text in the top right corner\n",
    "            ax.text(0.95, 0.95, f'n = {num_neurons}',\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=12,\n",
    "                    verticalalignment='top',\n",
    "                    horizontalalignment='right')\n",
    "\n",
    "            # Set titles for the columns on the first row\n",
    "            if i == 0:\n",
    "                ax.set_title(cat_name, fontsize=16)\n",
    "\n",
    "            # Set y-axis labels for the rows on the first column\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(bhv.capitalize(), fontsize=16)\n",
    "\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            # Add a vertical line for event onset at t=0\n",
    "            ax.axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "    # Set common labels\n",
    "    fig.text(0.5, 0.04, 'Time (s)', ha='center', va='center', fontsize=14)\n",
    "    fig.text(0.08, 0.5, 'Mean normalized Firing Rate (Spikes/s)', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "    plt.tight_layout(rect=[0.1, 0.05, 0.95, 0.95]) # Adjust layout to make room for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    spike_trig_trace_tgt = spike_trig_trace_summary[np.isin(spike_trig_trace_summary['condition'],conditions_to_ana)]\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # The 'SpikeTrigAverage' column is a string representation of a list.\n",
    "    # We need to convert it into an actual list of numbers (or a numpy array).\n",
    "    # We use pd.eval to safely evaluate the string as a Python expression.\n",
    "    # spike_trig_trace_tgt['SpikeTrigAverage'] = spike_trig_trace_tgt['SpikeTrigAverage'].apply(pd.eval).apply(np.array)\n",
    "\n",
    "\n",
    "    # --- Define Plotting Categories and Behaviors ---\n",
    "    # We directly use the values from the 'signiTemporalType' column.\n",
    "    categories = ['Reactive', 'Predictive', 'Both', 'None']\n",
    "    behaviors = ['pull', 'gaze']\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey='row')\n",
    "    fig.suptitle('Mean Spike-Triggered Average Traces by Behavior and Temporal Type', fontsize=20)\n",
    "\n",
    "    for i, bhv in enumerate(behaviors):\n",
    "        for j, cat_name in enumerate(categories):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            # Filter the data for the current behavior and temporal type category\n",
    "            df_subset = spike_trig_trace_tgt[\n",
    "                (spike_trig_trace_tgt['bhv_name'] == bhv) &\n",
    "                (spike_trig_trace_tgt['signiTemporalType'] == cat_name)\n",
    "            ]\n",
    "\n",
    "            # Get the number of unique neurons in this subset\n",
    "            num_neurons = df_subset['neuronID'].nunique()\n",
    "\n",
    "            if not df_subset.empty:\n",
    "                # Stack the traces into a 2D numpy array\n",
    "                all_traces = np.vstack(df_subset['SpikeTrigAverage'].values)\n",
    "\n",
    "                # Calculate mean trace\n",
    "                mean_trace = np.mean(all_traces, axis=0)\n",
    "\n",
    "                # Create the time axis from -4s to 4s\n",
    "                num_points = len(mean_trace)\n",
    "                time_axis = np.linspace(-4, 4, num=num_points)\n",
    "\n",
    "                # Plot the mean trace\n",
    "                ax.plot(time_axis, mean_trace, lw=2, label=f'{bhv} - {cat_name}')\n",
    "\n",
    "                # Calculate and plot the error area (SEM) if there's more than one neuron\n",
    "                if num_neurons > 1:\n",
    "                    sem_trace = np.std(all_traces, axis=0) / np.sqrt(num_neurons)\n",
    "                    ax.fill_between(time_axis, \n",
    "                                    mean_trace - sem_trace, \n",
    "                                    mean_trace + sem_trace, \n",
    "                                    alpha=0.3)\n",
    "\n",
    "            # Add the neuron count as text in the top right corner\n",
    "            ax.text(0.95, 0.95, f'n = {num_neurons}',\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=12,\n",
    "                    verticalalignment='top',\n",
    "                    horizontalalignment='right')\n",
    "\n",
    "            # Set titles for the columns on the first row\n",
    "            if i == 0:\n",
    "                ax.set_title(cat_name, fontsize=16)\n",
    "\n",
    "            # Set y-axis labels for the rows on the first column\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(bhv.capitalize(), fontsize=16)\n",
    "\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            # Add a vertical line for event onset at t=0\n",
    "            ax.axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "    # Set common labels\n",
    "    fig.text(0.5, 0.04, 'Time (s)', ha='center', va='center', fontsize=14)\n",
    "    fig.text(0.08, 0.5, 'Mean probability (a.u)', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "    plt.tight_layout(rect=[0.1, 0.05, 0.95, 0.95]) # Adjust layout to make room for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_trig_events_all_dates['20250129']['dodson']['leverpull_prob'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_trig_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pull_trig_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0955b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_trig_selfpull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04839a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(real_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53853b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_trig_events_all_dates[date_tgt][recordedanimal]['socialgaze_prob']['2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.floor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_summary[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69024629",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_names[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(spiketrain_summary[95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spiketrain_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d248307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neuralGLM_mainAxesProjected_kernels_coef.keys())\n",
    "print(neuralGLM_kernels_coef_shf.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41833ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(neuralGLM_mainAxesProjected_kernels_coef[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5535acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for now\n",
    "# project the 8D continuous variables to smaller dimension that are more task relavant\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from scipy.linalg import orth\n",
    "\n",
    "# 1. Load and extract behavioral data\n",
    "var_toPCA_names = ['gaze_other_angle', 'gaze_lever_angle', 'gaze_tube_angle',\n",
    "                   'animal_animal_dist', 'animal_lever_dist', 'animal_tube_dist',\n",
    "                   'mass_move_speed', 'gaze_angle_speed']\n",
    "pull_axis_name = ['selfpull_prob']\n",
    "gaze_axis_name = ['socialgaze_prob']\n",
    "juice_axis_name = ['selfjuice_prob']\n",
    "\n",
    "# Indices\n",
    "PCAindices_in_summary = [data_summary_names.index(var) for var in var_toPCA_names]\n",
    "Pullindices_in_summary = [data_summary_names.index(var) for var in pull_axis_name]\n",
    "Gazeindices_in_summary = [data_summary_names.index(var) for var in gaze_axis_name]\n",
    "Juiceindices_in_summary = [data_summary_names.index(var) for var in juice_axis_name]\n",
    "\n",
    "# Data extraction\n",
    "data_summary = np.array(data_summary)\n",
    "vars_toPCA = data_summary[PCAindices_in_summary]        # shape (8, T)\n",
    "var_pull = data_summary[Pullindices_in_summary][0]      # shape (T,)\n",
    "var_gaze = data_summary[Gazeindices_in_summary][0]\n",
    "var_juice = data_summary[Juiceindices_in_summary][0]\n",
    "\n",
    "# 2. Z-score behavioral variables\n",
    "scaler = StandardScaler()\n",
    "vars_z = scaler.fit_transform(vars_toPCA.T).T            # (8, T)\n",
    "\n",
    "# 3. Get raw projection vectors (not yet orthogonal)\n",
    "gaze_weights = vars_z @ var_gaze        # shape (8,)\n",
    "pull_weights = vars_z @ var_pull\n",
    "juice_weights = vars_z @ var_juice\n",
    "\n",
    "# Normalize to get direction vectors\n",
    "gaze_dir = gaze_weights / np.linalg.norm(gaze_weights)\n",
    "\n",
    "# Orthogonalize pull_dir to gaze_dir\n",
    "pull_dir = pull_weights - (gaze_dir @ pull_weights) * gaze_dir\n",
    "pull_dir /= np.linalg.norm(pull_dir)\n",
    "\n",
    "# Orthogonalize juice_dir to both gaze and pull\n",
    "Q = orth(np.stack([gaze_dir, pull_dir], axis=1))  # (8,2)\n",
    "proj_mat = Q @ Q.T\n",
    "juice_resid = juice_weights - proj_mat @ juice_weights\n",
    "juice_dir = juice_resid / np.linalg.norm(juice_resid)\n",
    "\n",
    "# 4. Project onto mode directions\n",
    "gaze_PC = gaze_dir @ vars_z       # (T,)\n",
    "pull_PC = pull_dir @ vars_z\n",
    "juice_PC = juice_dir @ vars_z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05dad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation (pull vs gaze):\", np.corrcoef(pull_PC, gaze_PC)[0, 1])\n",
    "print(\"Correlation (pull vs juice):\", np.corrcoef(pull_PC, juice_PC)[0, 1])\n",
    "print(\"Correlation (gaze vs juice):\", np.corrcoef(gaze_PC, juice_PC)[0, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume these are unit vectors of shape (n_dims,)\n",
    "vectors = [pull_dir, gaze_dir, residual_dir]\n",
    "\n",
    "# Enforce orthogonality\n",
    "orthogonal_dirs = gram_schmidt(vectors)\n",
    "\n",
    "# Unpack them\n",
    "pull_orth, gaze_orth, residual_orth = orthogonal_dirs\n",
    "\n",
    "# Check orthogonality again\n",
    "check_orthogonality(pull_orth, gaze_orth, residual_orth)\n",
    "\n",
    "# Project behavioral data into 3D mode traces:\n",
    "# pull_PC = pull_orth @ vars_z  # shape: (T,)\n",
    "# gaze_PC = gaze_orth @ vars_z\n",
    "# residual_PC = residual_orth @ vars_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = [10000,12000]\n",
    "plt.plot(pull_PC[ind_[0]:ind_[1]],label='pull_PC')\n",
    "plt.plot(gaze_PC[ind_[0]:ind_[1]],label='gaze_PC')\n",
    "plt.plot(juice_PC[ind_[0]:ind_[1]],label='juice_PC')\n",
    "# plt.plot(residual_PC[ind_[0]:ind_[1]],label='residual_PC')\n",
    "plt.plot(var_pull[ind_[0]:ind_[1]]/4,label='var_pull')\n",
    "# plt.plot(var_juice[ind_[0]:ind_[1]]/4,label='var_juice')\n",
    "# plt.plot(var_gaze[ind_[0]:ind_[1]],label='var_gaze')\n",
    "# plt.plot(vars_toPCA[6,ind_[0]:ind_[1]],label='mass_speed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e46f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ae1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ef67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_DURATION_S = 8.0  # total span: -4s to +4s\n",
    "KERNEL_OFFSET_S = -4.0   # shift so it starts at -4s\n",
    "N_BASIS_FUNCS = 20     # The number of basis functions to represent the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BOOTSTRAPS =10\n",
    "test_size = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "import string\n",
    "import warnings\n",
    "import pickle    \n",
    "import random as random\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#\n",
    "def make_raised_cosine_basis(duration_s, n_basis, dt, offset_s=0.0):\n",
    "    t = np.arange(offset_s, offset_s + duration_s, dt)  # e.g., from -4 to +4 seconds\n",
    "    centers = np.linspace(offset_s, offset_s + duration_s, n_basis)\n",
    "    width = (centers[1] - centers[0]) * 1.5  # spread of each cosine\n",
    "\n",
    "    basis = []\n",
    "    for ci in centers:\n",
    "        phi = (t - ci) * np.pi / width\n",
    "        b = np.cos(np.clip(phi, -np.pi, np.pi))\n",
    "        b = (b + 1) / 2\n",
    "        b[(t < ci - width/2) | (t > ci + width/2)] = 0  # zero out beyond the support\n",
    "        basis.append(b)\n",
    "\n",
    "    basis = np.stack(basis, axis=1)  # shape: [time, n_basis]\n",
    "    return basis, t\n",
    "\n",
    "\n",
    "#\n",
    "def make_gaussian_basis(duration_s, n_basis, dt, offset_s=0.0, sigma_scale=1.5):\n",
    "    t = np.arange(offset_s, offset_s + duration_s, dt)  # e.g., -4 to 4s\n",
    "    centers = np.linspace(offset_s, offset_s + duration_s, n_basis)\n",
    "    sigma = (centers[1] - centers[0]) * sigma_scale\n",
    "\n",
    "    basis = []\n",
    "    for c in centers:\n",
    "        b = np.exp(-0.5 * ((t - c) / sigma) ** 2)\n",
    "        basis.append(b)\n",
    "\n",
    "    basis = np.stack(basis, axis=1)  # shape: [time, n_basis]\n",
    "    return basis, t\n",
    "\n",
    "#\n",
    "def make_square_basis(duration_s, n_basis, dt):\n",
    "    \"\"\"\n",
    "    Create square (boxcar) basis functions evenly tiling [-duration_s/2, duration_s/2]\n",
    "    \"\"\"\n",
    "    t = np.arange(-duration_s / 2, duration_s / 2, dt)\n",
    "    n_timepoints = len(t)\n",
    "    basis = np.zeros((n_timepoints, n_basis))\n",
    "\n",
    "    # Get bin edges using np.array_split for even division\n",
    "    indices = np.array_split(np.arange(n_timepoints), n_basis)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        basis[idx, i] = 1\n",
    "\n",
    "    return basis, t\n",
    "\n",
    "#\n",
    "def convolve_with_basis(var, basis_funcs):\n",
    "    return np.stack([\n",
    "        convolve(var, basis, mode='full')[:len(var)]\n",
    "        for basis in basis_funcs.T\n",
    "    ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1 / fps\n",
    "\n",
    "# basis_funcs, time_vector = make_raised_cosine_basis(KERNEL_DURATION_S, N_BASIS_FUNCS, dt, offset_s=KERNEL_OFFSET_S)\n",
    "basis_funcs, time_vector = make_gaussian_basis(KERNEL_DURATION_S, N_BASIS_FUNCS, dt, offset_s=KERNEL_OFFSET_S)\n",
    "# basis_funcs, t_basis = make_square_basis(KERNEL_DURATION_S, N_BASIS_FUNCS, dt)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i in range(basis_funcs.shape[1]):\n",
    "    plt.plot(basis_funcs[:, i], label=f'Basis {i+1}')\n",
    "plt.title(\"Gaussian Temporal Basis Functions\")\n",
    "plt.xlabel(\"Time bins\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cc2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = np.array(data_summary)\n",
    "np.shape(data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa57609",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pull_PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.vstack([pull_PC, gaze_PC, juice_PC]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the glm fitting with the projected vectors\n",
    "# \n",
    "# def neuralGLM_fitting_BasisKernelsForContVaris(KERNEL_DURATION_S, N_BASIS_FUNCS, fps, animal1, animal2, \n",
    "#                                                recordedanimal, data_summary_names, data_summary, \n",
    "#                                                spiketrain_summary, nbootstraps, dospikehist, spikehist_twin, \n",
    "#                                                N_BOOTSTRAPS,test_size ):\n",
    "\n",
    "dt = 1 / fps\n",
    "\n",
    "# basis_funcs, time_vector = make_raised_cosine_basis(KERNEL_DURATION_S, N_BASIS_FUNCS, dt, offset_s=KERNEL_OFFSET_S)\n",
    "basis_funcs, time_vector = make_gaussian_basis(KERNEL_DURATION_S, N_BASIS_FUNCS, dt, offset_s=KERNEL_OFFSET_S)\n",
    "# basis_funcs, t_basis = make_square_basis(KERNEL_DURATION_S, N_BASIS_FUNCS, dt)\n",
    "\n",
    "\n",
    "####\n",
    "# do the glm fitting\n",
    "####\n",
    "\n",
    "var_toglm_names = ['pull_PC', 'gaze_PC', 'juice_PC']\n",
    "\n",
    "# projected pull, gaze, juice action vector\n",
    "predictors = np.vstack([pull_PC, gaze_PC, juice_PC])\n",
    "\n",
    "# Design matrix from continuous variables\n",
    "X_continuous = np.hstack([convolve_with_basis(v, basis_funcs) for v in predictors])\n",
    "#\n",
    "# zscore again\n",
    "scaler = StandardScaler()\n",
    "X_continuous_z = scaler.fit_transform(X_continuous)\n",
    "\n",
    "\n",
    "# do the glm for each neuron\n",
    "neuron_clusters = list(spiketrain_summary.keys())\n",
    "nclusters = np.shape(neuron_clusters)[0]\n",
    "\n",
    "\n",
    "# Track kernel for each var × basis\n",
    "n_vars = len(var_toglm_names)\n",
    "n_basis = basis_funcs.shape[1]\n",
    "T_kernel = basis_funcs.shape[0]  # length of time kernel\n",
    "\n",
    "# storage\n",
    "Kernel_coefs_allboots_allcells = {}\n",
    "Kernel_coefs_spikehist_allboots_allcells = {}\n",
    "Kernel_coefs_allboots_allcells_shf = {}\n",
    "Kernel_coefs_spikehist_allboots_allcells_shf = {}\n",
    "#\n",
    "Temporal_filters_allcells = dict.fromkeys(neuron_clusters, None)\n",
    "Temporal_filters_spikehist_allcells = dict.fromkeys(neuron_clusters, None)\n",
    "Temporal_filters_allcells_shf = dict.fromkeys(neuron_clusters, None)\n",
    "Temporal_filters_spikehist_allcells_shf = dict.fromkeys(neuron_clusters, None)\n",
    "\n",
    "#    \n",
    "# for icluster in np.arange(0,nclusters,1):\n",
    "for icluster in np.arange(0,1,1):\n",
    "    iclusterID = neuron_clusters[icluster]\n",
    "\n",
    "    # Binary spike train\n",
    "    # Y = (spiketrain_summary[iclusterID] > 0).astype(int)\n",
    "    Y = spiketrain_summary[iclusterID]\n",
    "    #\n",
    "    Y_shuffled = np.random.permutation(Y)\n",
    "\n",
    "    #\n",
    "    Kernel_coefs_boots = []\n",
    "    filters_boot = []\n",
    "    #\n",
    "    Kernel_coefs_boots_shf = []\n",
    "    filters_boot_shf = []\n",
    "\n",
    "    for i in range(N_BOOTSTRAPS):\n",
    "\n",
    "        # Train/test split\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X_continuous, Y, test_size=0.2, random_state=random.randint(0, 10000)\n",
    "            )\n",
    "\n",
    "        # Fit Poisson GLM with L2 penalty\n",
    "        clf_full = PoissonRegressor(alpha=10, max_iter=500)  # alpha controls regularization strength\n",
    "        clf_full.fit(X_tr, y_tr)\n",
    "\n",
    "        # Extract coefficients\n",
    "        full_beta = clf_full.coef_.flatten()\n",
    "        kernel_matrix = full_beta.reshape(n_vars, n_basis)\n",
    "        Kernel_coefs_boots.append(kernel_matrix)\n",
    "\n",
    "        # Reconstruct temporal filter\n",
    "        temporal_filter = np.dot(kernel_matrix, basis_funcs.T)  # (n_vars, T_kernel)\n",
    "        filters_boot.append(temporal_filter)\n",
    "\n",
    "        # SHUFFLED CONTROL\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X_continuous, Y_shuffled, test_size=0.2, random_state=random.randint(0, 10000)\n",
    "        )\n",
    "\n",
    "        clf_shuffled = PoissonRegressor(alpha=10, max_iter=500)\n",
    "        clf_shuffled.fit(X_tr, y_tr)\n",
    "\n",
    "        full_beta_shf = clf_shuffled.coef_.flatten()\n",
    "        kernel_matrix_shf = full_beta_shf.reshape(n_vars, n_basis)\n",
    "        Kernel_coefs_boots_shf.append(kernel_matrix_shf)\n",
    "\n",
    "        temporal_filter_shf = np.dot(kernel_matrix_shf, basis_funcs.T)\n",
    "        filters_boot_shf.append(temporal_filter_shf)\n",
    "\n",
    "\n",
    "    # Save as array (n_boots, n_vars, T_kernel)\n",
    "    Kernel_coefs_allboots_allcells[iclusterID] = np.array(Kernel_coefs_boots)  # shape: (n_boots, n_vars, n_basis)\n",
    "    Temporal_filters_allcells[iclusterID] = np.array(filters_boot) # (n_boots, n_vars, T_kernel)\n",
    "\n",
    "    Kernel_coefs_allboots_allcells_shf[iclusterID] = np.array(Kernel_coefs_boots_shf)  # shape: (n_boots, n_vars, n_basis)\n",
    "    Temporal_filters_allcells_shf[iclusterID] = np.array(filters_boot_shf) # (n_boots, n_vars, T_kernel)\n",
    "\n",
    "\n",
    "\n",
    "neuralGLM_kernels_coef = Kernel_coefs_allboots_allcells\n",
    "neuralGLM_kernels_tempFilter = Temporal_filters_allcells\n",
    "neuralGLM_kernels_coef_shf = Kernel_coefs_allboots_allcells_shf\n",
    "neuralGLM_kernels_tempFilter_shf = Temporal_filters_allcells_shf\n",
    "\n",
    "# return neuralGLM_kernels_coef, neuralGLM_kernels_tempFilter, neuralGLM_kernels_coef_shf, neuralGLM_kernels_tempFilter_shf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec390924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var_toglm_names[0])\n",
    "plt.plot(np.nanmean(neuralGLM_kernels_tempFilter[2][:,0,:],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neuralGLM_kernels_tempFilter_shf[2][:,1,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = np.stack(neuralGLM_kernels_coef[2], axis=0)\n",
    "print(\"Filter shape:\", all_kernels.shape)  # (n_bootstraps, n_predictors * n_basis)\n",
    "\n",
    "# Compute variance across bootstraps\n",
    "filter_std = np.std(all_kernels, axis=0)\n",
    "plt.plot(filter_std)\n",
    "plt.title(\"Standard deviation across bootstraps\")\n",
    "plt.xlabel(\"Kernel index\")\n",
    "plt.ylabel(\"STD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96448562",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.stack(neuralGLM_kernels_tempFilter_shf[2], axis=0)  # shape: (n_bootstraps, n_vars, T)\n",
    "mean_filter = filters.mean(axis=0)\n",
    "std_filter = filters.std(axis=0)\n",
    "\n",
    "# Plot mean ± std for each variable\n",
    "for i, name in enumerate(var_toglm_names):\n",
    "    plt.figure()\n",
    "    plt.fill_between(np.arange(mean_filter.shape[1]), \n",
    "                     mean_filter[i] - std_filter[i], \n",
    "                     mean_filter[i] + std_filter[i], \n",
    "                     alpha=0.3, label='±1 STD')\n",
    "    plt.plot(mean_filter[i], label='Mean filter')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948dd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e897f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example shape: 16 signals × 11315 timepoints\n",
    "# data_summary = np.random.randn(16, 11315)  # <-- Replace with your actual data\n",
    "\n",
    "print(var_toglm_names)\n",
    "\n",
    "# Compute Pearson correlation across rows (pairwise)\n",
    "# corr_matrix = np.corrcoef(data_summary)\n",
    "corr_matrix = np.corrcoef(predictors)\n",
    "\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='vlag', square=True, \n",
    "            xticklabels=np.arange(1, np.shape(var_toglm_names)[0]), \n",
    "            yticklabels=np.arange(1, np.shape(var_toglm_names)[0]))\n",
    "plt.title(\"Cross-Correlation Heatmap (16x16)\")\n",
    "plt.xlabel(\"Signal Index\")\n",
    "plt.ylabel(\"Signal Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_names[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_names[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c38719",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa4040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(spiketrain_summary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsess_time*30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284a691",
   "metadata": {},
   "source": [
    "## plot - for individual animal\n",
    "### prepare the summarizing data set and run population level analysis such as PCA\n",
    "### plot the kernel defined based on the stretagy (pair of action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d16671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "Kernel_coefs_stretagy_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                          'Kernel_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    # make sure to be the same as the bhvvaris_toGLM\n",
    "    bhv_types = ['self sync_pull_prob', 'self gaze_lead_pull_prob', 'self social_attention_prob',]\n",
    "    nbhv_types = np.shape(bhv_types)[0]\n",
    "\n",
    "    for ibhv_type in np.arange(0,nbhv_types,1):\n",
    "        \n",
    "        bhv_type = bhv_types[ibhv_type]\n",
    "\n",
    "        clusterIDs = Kernel_coefs_stretagy_all_dates[date_tgt].keys()\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            kernel_ibhv = Kernel_coefs_stretagy_all_dates[date_tgt][iclusterID][:,ibhv_type,:]\n",
    "            \n",
    "            kernel_ibhv_average = np.nanmean(kernel_ibhv,axis = 0)\n",
    "\n",
    "            Kernel_coefs_stretagy_all_dates_df = Kernel_coefs_stretagy_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':bhv_type.split()[0],\n",
    "                                                                                    'bhv_name': bhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'Kernel_average':kernel_ibhv_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            \n",
    "            \n",
    "# only focus on the certain act animal and certain bhv_name\n",
    "# act_animals_all = np.unique(Kernel_coefs_stretagy_all_dates_df['act_animal'])\n",
    "act_animals_all = ['self']\n",
    "bhv_names_all = np.unique(Kernel_coefs_stretagy_all_dates_df['bhv_name'])\n",
    "# bhv_names_all = ['sync_pull_prob']\n",
    "conditions_all = np.unique(Kernel_coefs_stretagy_all_dates_df['condition'])\n",
    "\n",
    "nact_animals = np.shape(act_animals_all)[0]\n",
    "nbhv_names = np.shape(bhv_names_all)[0]\n",
    "nconditions = np.shape(conditions_all)[0]\n",
    "\n",
    "\n",
    "# run PCA and plot\n",
    "for ianimal in np.arange(0,nact_animals,1):\n",
    "    \n",
    "    act_animal = act_animals_all[ianimal]\n",
    "    \n",
    "    for icondition in np.arange(0,nconditions,1):\n",
    "        \n",
    "        task_condition = conditions_all[icondition]\n",
    "        \n",
    "        # set up for plotting\n",
    "        nPC_toplot = 4\n",
    "\n",
    "        # Create a figure with GridSpec, specifying height_ratios\n",
    "        fig = plt.figure(figsize=(nPC_toplot*2,6*nbhv_names))\n",
    "\n",
    "        # Define a grid with 4*nbhv_names rows in the left and nbhv_names rows in the right, \n",
    "        # but scale the right column's height by 3\n",
    "        gs = GridSpec(nPC_toplot*nbhv_names, 2, height_ratios=[1] * nPC_toplot*nbhv_names)\n",
    "\n",
    "        # Left column (4*nbhv_names rows, 1 column) for PC 1 to 4 traces\n",
    "        ax_left = [fig.add_subplot(gs[i, 0]) for i in range(nPC_toplot*nbhv_names)]  # Access all 4*nbhv_names rows in the left column\n",
    "\n",
    "        # Right column (nbhv_names rows, 1 column, scaling the height by using multiple rows for each plot)\n",
    "        # for the variance explanation\n",
    "        ax_right = [fig.add_subplot(gs[nPC_toplot * i:nPC_toplot * i + nPC_toplot, 1]) for i in range(nbhv_names)]  # Group 4 rows for each of the 3 subplots on the right\n",
    "\n",
    "\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (Kernel_coefs_stretagy_all_dates_df['act_animal']==act_animal)&(Kernel_coefs_stretagy_all_dates_df['bhv_name']==bhv_name)&(Kernel_coefs_stretagy_all_dates_df['condition']==task_condition)\n",
    "\n",
    "            Kernel_coefs_stretagy_tgt = np.vstack(list(Kernel_coefs_stretagy_all_dates_df[ind]['Kernel_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(Kernel_coefs_stretagy_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            Kernel_coefs_stretagy_tgt = Kernel_coefs_stretagy_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            Kernel_coefs_stretagy_pca = pca.fit_transform(Kernel_coefs_stretagy_tgt.transpose())\n",
    "\n",
    "            # Get the explained variance ratio\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "            # Calculate the cumulative explained variance\n",
    "            cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "            # Plot the cumulative explained variance\n",
    "            ax_right[ibhvname].plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', linestyle='--', color='blue', alpha=0.7)\n",
    "            ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "            ax_right[ibhvname].set_ylabel('Cumulative Percentage of Variance Explained')\n",
    "            ax_right[ibhvname].set_title(bhv_name)\n",
    "            ax_right[ibhvname].set_xticks(np.arange(1, len(cumulative_variance) + 1, 1))\n",
    "            ax_right[ibhvname].grid(True)\n",
    "            # \n",
    "            # if ibhvname == nbhv_names - 1:\n",
    "            #     ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "\n",
    "            # plot the PCs\n",
    "            for iPC_toplot in np.arange(0,nPC_toplot,1):\n",
    "\n",
    "                PCtoplot = Kernel_coefs_stretagy_pca[:,iPC_toplot]\n",
    "\n",
    "                trig_twin = [-4,4] # in the unit of second\n",
    "                xxx = np.arange(trig_twin[0]*fps,trig_twin[1]*fps,1)\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot(xxx, PCtoplot, 'k')\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot([0,0],[np.nanmin(PCtoplot)*1.1,np.nanmax(PCtoplot)*1.1],'k--')\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].set_title(bhv_name+' kernel PC'+str(iPC_toplot+1))\n",
    "\n",
    "                if iPC_toplot == nPC_toplot - 1:\n",
    "                    ax_left[iPC_toplot+ibhvname*nPC_toplot].set_xlabel('time (s)')\n",
    "\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()        \n",
    "        \n",
    "        if (animal1_filenames[0] == 'Kanga') | (animal2_filenames[0] == 'Kanga'):\n",
    "            recordedAnimal = 'Kanga'\n",
    "        elif (animal1_filenames[0] == 'Dodson') | (animal2_filenames[0] == 'Dodson'):\n",
    "            recordedAnimal = 'Dodson'\n",
    "\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_singlecam/\"+cameraID+\"/\"+recordedAnimal+\"_neuralGLM/\"\n",
    "\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            fig.savefig(figsavefolder+'stretagy_kernel_coefs_pca_patterns_all_dates'+savefile_sufix+'_'+act_animal+'action_in'+task_condition+'.pdf')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b8788",
   "metadata": {},
   "source": [
    "### prepare the summarizing data set and run population level analysis such as PCA\n",
    "### plot the kernel defined based on the single actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "Kernel_coefs_action_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                          'Kernel_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    # make sure to be the same as the bhvvaris_toGLM\n",
    "    bhv_types = ['self leverpull_prob', 'self socialgaze_prob', 'self juice_prob', \n",
    "                 'other leverpull_prob', 'other socialgaze_prob', 'other juice_prob', ]\n",
    "    nbhv_types = np.shape(bhv_types)[0]\n",
    "\n",
    "    for ibhv_type in np.arange(0,nbhv_types,1):\n",
    "        \n",
    "        bhv_type = bhv_types[ibhv_type]\n",
    "\n",
    "        clusterIDs = Kernel_coefs_all_dates[date_tgt].keys()\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            kernel_ibhv = Kernel_coefs_all_dates[date_tgt][iclusterID][:,ibhv_type,:]\n",
    "            \n",
    "            kernel_ibhv_average = np.nanmean(kernel_ibhv,axis = 0)\n",
    "\n",
    "            Kernel_coefs_action_all_dates_df = Kernel_coefs_action_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':bhv_type.split()[0],\n",
    "                                                                                    'bhv_name': bhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'Kernel_average':kernel_ibhv_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            \n",
    "            \n",
    "# only focus on the certain act animal and certain bhv_name\n",
    "act_animals_all = np.unique(Kernel_coefs_action_all_dates_df['act_animal'])\n",
    "# act_animals_all = ['self']\n",
    "bhv_names_all = np.unique(Kernel_coefs_action_all_dates_df['bhv_name'])\n",
    "# bhv_names_all = ['leverpull_prob']\n",
    "conditions_all = np.unique(Kernel_coefs_action_all_dates_df['condition'])\n",
    "\n",
    "nact_animals = np.shape(act_animals_all)[0]\n",
    "nbhv_names = np.shape(bhv_names_all)[0]\n",
    "nconditions = np.shape(conditions_all)[0]\n",
    "\n",
    "\n",
    "# run PCA and plot\n",
    "for ianimal in np.arange(0,nact_animals,1):\n",
    "    \n",
    "    act_animal = act_animals_all[ianimal]\n",
    "    \n",
    "    for icondition in np.arange(0,nconditions,1):\n",
    "        \n",
    "        task_condition = conditions_all[icondition]\n",
    "        \n",
    "        # set up for plotting\n",
    "        nPC_toplot = 4\n",
    "\n",
    "        # Create a figure with GridSpec, specifying height_ratios\n",
    "        fig = plt.figure(figsize=(nPC_toplot*2,6*nbhv_names))\n",
    "\n",
    "        # Define a grid with 4*nbhv_names rows in the left and nbhv_names rows in the right, \n",
    "        # but scale the right column's height by 3\n",
    "        gs = GridSpec(nPC_toplot*nbhv_names, 2, height_ratios=[1] * nPC_toplot*nbhv_names)\n",
    "\n",
    "        # Left column (4*nbhv_names rows, 1 column) for PC 1 to 4 traces\n",
    "        ax_left = [fig.add_subplot(gs[i, 0]) for i in range(nPC_toplot*nbhv_names)]  # Access all 4*nbhv_names rows in the left column\n",
    "\n",
    "        # Right column (nbhv_names rows, 1 column, scaling the height by using multiple rows for each plot)\n",
    "        # for the variance explanation\n",
    "        ax_right = [fig.add_subplot(gs[nPC_toplot * i:nPC_toplot * i + nPC_toplot, 1]) for i in range(nbhv_names)]  # Group 4 rows for each of the 3 subplots on the right\n",
    "\n",
    "\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (Kernel_coefs_action_all_dates_df['act_animal']==act_animal)&(Kernel_coefs_action_all_dates_df['bhv_name']==bhv_name)&(Kernel_coefs_action_all_dates_df['condition']==task_condition)\n",
    "\n",
    "            Kernel_coefs_action_tgt = np.vstack(list(Kernel_coefs_action_all_dates_df[ind]['Kernel_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(Kernel_coefs_action_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            Kernel_coefs_action_tgt = Kernel_coefs_action_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            Kernel_coefs_action_pca = pca.fit_transform(Kernel_coefs_action_tgt.transpose())\n",
    "\n",
    "            # Get the explained variance ratio\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "            # Calculate the cumulative explained variance\n",
    "            cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "            # Plot the cumulative explained variance\n",
    "            ax_right[ibhvname].plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', linestyle='--', color='blue', alpha=0.7)\n",
    "            ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "            ax_right[ibhvname].set_ylabel('Cumulative Percentage of Variance Explained')\n",
    "            ax_right[ibhvname].set_title(bhv_name)\n",
    "            ax_right[ibhvname].set_xticks(np.arange(1, len(cumulative_variance) + 1, 1))\n",
    "            ax_right[ibhvname].grid(True)\n",
    "            # \n",
    "            # if ibhvname == nbhv_names - 1:\n",
    "            #     ax_right[ibhvname].set_xlabel('Number of the principle component')\n",
    "\n",
    "            # plot the PCs\n",
    "            for iPC_toplot in np.arange(0,nPC_toplot,1):\n",
    "\n",
    "                PCtoplot = Kernel_coefs_action_pca[:,iPC_toplot]\n",
    "\n",
    "                trig_twin = [-4,4] # in the unit of second\n",
    "                xxx = np.arange(trig_twin[0]*fps,trig_twin[1]*fps,1)\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot(xxx, PCtoplot, 'k')\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].plot([0,0],[np.nanmin(PCtoplot)*1.1,np.nanmax(PCtoplot)*1.1],'k--')\n",
    "\n",
    "                ax_left[iPC_toplot+ibhvname*nPC_toplot].set_title(bhv_name+' kernel PC'+str(iPC_toplot+1))\n",
    "\n",
    "                if iPC_toplot == nPC_toplot - 1:\n",
    "                    ax_left[iPC_toplot+ibhvname*nPC_toplot].set_xlabel('time (s)')\n",
    "\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()        \n",
    "        \n",
    "        if (animal1_filenames[0] == 'Kanga') | (animal2_filenames[0] == 'Kanga'):\n",
    "            recordedAnimal = 'Kanga'\n",
    "        elif (animal1_filenames[0] == 'Dodson') | (animal2_filenames[0] == 'Dodson'):\n",
    "            recordedAnimal = 'Dodson'\n",
    "\n",
    "        savefig = 1\n",
    "        if savefig:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_GLMfitting_singlecam/\"+cameraID+\"/\"+recordedAnimal+\"_neuralGLM/\"\n",
    "\n",
    "            if not os.path.exists(figsavefolder):\n",
    "                os.makedirs(figsavefolder)\n",
    "\n",
    "            fig.savefig(figsavefolder+'action_kernel_coefs_pca_patterns_all_dates'+savefile_sufix+'_'+act_animal+'action_in'+task_condition+'.pdf')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3ab50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09f2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ea6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a5e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fc4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
