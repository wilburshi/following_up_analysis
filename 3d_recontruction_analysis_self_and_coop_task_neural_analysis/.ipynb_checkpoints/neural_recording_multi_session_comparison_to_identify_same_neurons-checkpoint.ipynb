{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea1560b",
   "metadata": {},
   "source": [
    "### This script examine for different days' recording, if the neurons are the same or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d0681",
   "metadata": {},
   "source": [
    "#### Especially test Kanga, since her probe is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea348f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecce77",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse\n",
    "\n",
    "from ana_functions.plot_continuous_bhv_var_and_neuralFR import plot_continuous_bhv_var_and_neuralFR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 1\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 0:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     # # '20231204_Dodson_withGinger_SR', \n",
    "                                     # '20231204_Dodson_withGinger_MC',\n",
    "                                    # '20240610_Dodson_MC',\n",
    "                                    '20240531_Dodson_MC_and_SR',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      # # \"20231204_SR\",\"20231204_MC\",\n",
    "                      # '20240610_MC',\n",
    "                      '20240531',\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                # # 0.00,  107.50, \n",
    "                                # 0.00, \n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         # # 2, 2,\n",
    "                         # 4,\n",
    "                         4,\n",
    "                       ]\n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     # '20231107_Dodson_withGinger_MC',\n",
    "                                     # '20231122_Dodson_withGinger_MC',\n",
    "                                     # '20231129_Dodson_withGinger_MC',\n",
    "                                     # '20231101_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      # \"20231107_MC\",\n",
    "                      # \"20231122_MC\",\n",
    "                      # \"20231129_MC\",\n",
    "                      # \"20231101_SR\",\n",
    "            \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                #  0.00,  \n",
    "                                #  0.00,  \n",
    "                                #  0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ 2, # 2, 2, 2,\n",
    "                       ]\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['ginger']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Ginger\"\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20240918_Kanga_EffortBasedMC',\n",
    "                                     '20241010_Kanga_EffortBasedMC',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20240918\",\n",
    "                      \"20241010\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 43.5,\n",
    "                                 66.0,\n",
    "                                 \n",
    "                              ] # in second\n",
    "        kilosortvers = [\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "            \n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "            \n",
    "                         4,\n",
    "                         4,\n",
    "                        ]\n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [\n",
    "     \n",
    "                        ] # 2 or 4\n",
    "    \n",
    "    animal1_fixedorder = ['dannon']\n",
    "    animal2_fixedorder = ['kanga']\n",
    "\n",
    "    animal1_filename = \"Dannon\"\n",
    "    animal2_filename = \"Kanga\"\n",
    "    \n",
    "\n",
    "\n",
    "# a test case\n",
    "if 0:\n",
    "    dates_list = [\"20230324\"]\n",
    "    session_start_times = [5.50] # in second\n",
    "    animal1_fixedorder = ['eddie']\n",
    "    animal2_fixedorder = ['sparkle']\n",
    "    animal1_filename = \"Eddie\"\n",
    "    animal2_filename = \"Sparkle\"\n",
    "\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "# switch animals to make animal 1 and 2 consistent\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_succtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_errtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,ndates)\n",
    "fig.set_figheight(5*1)\n",
    "fig.set_figwidth(5*ndates)\n",
    "\n",
    "nchannel = 64\n",
    "fig2, axs2 = plt.subplots(nchannel,ndates)\n",
    "fig2.set_figheight(2*nchannel)\n",
    "fig2.set_figwidth(2*ndates)\n",
    "#\n",
    "for ichan in np.arange(0,nchannel,1):\n",
    "    for idt in np.arange(0,ndates,1):\n",
    "        axs2[ichan,idt].set_xticks([])\n",
    "        axs2[ichan,idt].set_xticklabels([])\n",
    "        axs2[ichan,idt].set_yticks([])\n",
    "        axs2[ichan,idt].set_yticklabels([])\n",
    "        axs2[ichan,idt].spines['top'].set_visible(False)\n",
    "        axs2[ichan,idt].spines['right'].set_visible(False)\n",
    "        axs2[ichan,idt].spines['bottom'].set_visible(False)\n",
    "        axs2[ichan,idt].spines['left'].set_visible(False)\n",
    "        axs2[ichan,idt].set_xlim([0,61])\n",
    "        axs2[ichan,idt].set_ylim([-210,430])\n",
    "        #\n",
    "        if idt == 0:\n",
    "            axs2[ichan,idt].set_ylabel('fixed depth ch '+str(ichan))\n",
    "        if ichan == 0:\n",
    "            axs2[ichan,idt].set_title(dates_list[idt])\n",
    "\n",
    "for idate in np.arange(0,ndates,1):\n",
    "\n",
    "    date_tgt = dates_list[idate]\n",
    "    session_start_time = session_start_times[idate]\n",
    "    neural_record_condition = neural_record_conditions[idate]\n",
    "    kilosortver = kilosortvers[idate]\n",
    "\n",
    "\n",
    "    # get the neural data            \n",
    "\n",
    "    # load channel maps\n",
    "    channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "    # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "    channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "\n",
    "    # # load spike sorting results\n",
    "    print('load spike data for '+neural_record_condition)\n",
    "    if kilosortver == 2:\n",
    "        spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "        spike_time_data = np.load(spike_time_file)\n",
    "    elif kilosortver == 4:\n",
    "        spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "        spike_time_data = np.load(spike_time_file)\n",
    "    # \n",
    "    # down-sample the spike recording resolution to 30Hz\n",
    "    # spike_time_data = spike_time_data/fs_spikes*fps\n",
    "    # spike_time_data = np.round(spike_time_data)\n",
    "\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "        spike_clusters_data = np.load(spike_clusters_file)\n",
    "        spike_channels_data = np.copy(spike_clusters_data)\n",
    "    elif kilosortver == 4:\n",
    "        spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "        spike_clusters_data = np.load(spike_clusters_file)\n",
    "        spike_channels_data = np.copy(spike_clusters_data)\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "        channel_maps_data = np.load(channel_maps_file)\n",
    "    elif kilosortver == 4:\n",
    "        channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "        channel_maps_data = np.load(channel_maps_file)\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        spike_templates_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_templates.npy'\n",
    "        spike_templates_data = np.load(spike_templates_file)\n",
    "    elif kilosortver == 4:\n",
    "        spike_templates_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_templates.npy'\n",
    "        # spike_templates_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_detection_templates.npy'           \n",
    "        spike_templates_data = np.load(spike_templates_file)\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        templates_file = neural_data_folder+neural_record_condition+'/Kilosort/templates.npy'\n",
    "        templates_data = np.load(templates_file)\n",
    "    elif kilosortver == 4:\n",
    "        templates_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/templates.npy'\n",
    "        templates_data = np.load(templates_file)\n",
    "    templates_data = templates_data[:,:,channel_maps_data]\n",
    "     #\n",
    "    if kilosortver == 2:\n",
    "        amplitudes_file = neural_data_folder+neural_record_condition+'/Kilosort/amplitudes.npy'\n",
    "        spike_amp_data = np.load(amplitudes_file)\n",
    "    elif kilosortver == 4:\n",
    "        amplitudes_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/amplitudes.npy'\n",
    "        spike_amp_data = np.load(amplitudes_file)    \n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "        clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "    elif kilosortver == 4:\n",
    "        clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "        clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "        channel_pos_data = np.load(channel_pos_file)\n",
    "    elif kilosortver == 4:\n",
    "        channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "        channel_pos_data = np.load(channel_pos_file)\n",
    "    #\n",
    "    # only get the spikes that are manually checked\n",
    "    try:\n",
    "        good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "        clusters_info_data = clusters_info_data[np.isin(clusters_info_data.cluster_id,good_clusters)]\n",
    "    except:\n",
    "        good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "        clusters_info_data = clusters_info_data[np.isin(clusters_info_data.id,good_clusters)]\n",
    "    #\n",
    "    # clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "    clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "    #\n",
    "    spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    spike_templates_data = spike_templates_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    spike_amp_data = spike_amp_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "\n",
    "\n",
    "    # get the spike channel information\n",
    "    nclusters = np.shape(clusters_info_data)[0]\n",
    "    #\n",
    "    for icluster in np.arange(0,nclusters,1):\n",
    "        try:\n",
    "            cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "        except:\n",
    "            cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "        spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "    # \n",
    "    # get the channel to depth information, change 2 shanks to 1 shank \n",
    "    try:\n",
    "        channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "        # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "        # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "        channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "    except:\n",
    "        channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "        # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "        channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "        channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "\n",
    "\n",
    "    # get the mean AP templates (template * mean amplitude)\n",
    "    nclusters = np.shape(clusters_info_data)[0]\n",
    "    template_length = np.shape(templates_data)[1]\n",
    "    #\n",
    "    try:\n",
    "        meanAPshape_all = dict.fromkeys(list(map(str,list(clusters_info_data['id']))),[])            \n",
    "    except:\n",
    "        meanAPshape_all = dict.fromkeys(list(map(str,list(clusters_info_data['cluster_id']))),[])\n",
    "    # \n",
    "    # use the depth_history to see if one channel has multiple cluster\n",
    "    depth_iclu_hist = np.array([])\n",
    "    #\n",
    "    for icluster in np.arange(0,nclusters,1):\n",
    "        try:\n",
    "            cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "            channel_id = clusters_info_data['ch'].iloc[icluster]\n",
    "            depth_iclu = clusters_info_data['depth'].iloc[icluster]/30 # change to 0 - 63\n",
    "        except:\n",
    "            cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "            channel_id = clusters_info_data['ch'].iloc[icluster]\n",
    "            depth_iclu = clusters_info_data['depth'].iloc[icluster]/30 # change to 0 - 63\n",
    "        tempID = spike_templates_data[np.isin(spike_clusters_data,cluster_id)][0]\n",
    "        #\n",
    "        # APshape_template = templates_data[tempID,:,channel_id]\n",
    "        #\n",
    "        # find the chanel with the highest amplitude\n",
    "        a = (abs(templates_data[tempID,:,:]))\n",
    "        peak_channel = np.unravel_index(a.argmax(), a.shape)[1]\n",
    "        APshape_template = templates_data[tempID,:,peak_channel]\n",
    "\n",
    "        APmean_amp = np.nanmean(spike_amp_data[np.isin(spike_clusters_data,cluster_id)])\n",
    "        APmean_template = APmean_amp * APshape_template\n",
    "        # normalized \n",
    "        # APmean_template = (APmean_template-np.nanmin(APmean_template))/(np.nanmax(APmean_template)-np.nanmin(APmean_template))\n",
    "        #\n",
    "        meanAPshape_all[str(cluster_id)] = APmean_template\n",
    "\n",
    "        # summarizing plot \n",
    "        axs[idate].plot(APmean_template)\n",
    "        axs[idate].set_title(date_tgt)\n",
    "\n",
    "        # plot separating channel\n",
    "        FR_iclu = clusters_info_data['fr'].iloc[icluster]\n",
    "        #\n",
    "        axs2[int(depth_iclu),idate].plot(APmean_template)\n",
    "        #\n",
    "        if np.isin(depth_iclu,depth_iclu_hist):\n",
    "            axs2[int(depth_iclu),idate].text(40,220,\"FR \"+\"{:.1f}\".format(FR_iclu)+\"Hz\")\n",
    "        else:\n",
    "            axs2[int(depth_iclu),idate].text(40,300,\"FR \"+\"{:.1f}\".format(FR_iclu)+\"Hz\")\n",
    "        #\n",
    "        depth_iclu_hist = np.hstack((depth_iclu_hist,depth_iclu))\n",
    "\n",
    "        \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    fig2.savefig('meanAPtemplate_differentChannel_differentDays.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aaf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e995eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
