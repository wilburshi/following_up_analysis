{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with 3D Anipose tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### analyze the spike triggered pull and gaze ditribution\n",
    "#### the following detailed analysis focused on pull related behavioral events; with the specific focus on the partner Distance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_continuous_bhv import plot_spike_triggered_continuous_bhv\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c628f6",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b136aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "def keep_closest_cluster_single_trial(trace, time_trace):\n",
    "    \"\"\"\n",
    "    Keep only the contiguous region of non-zero gaze activity that's closest to time = 0.\n",
    "\n",
    "    Args:\n",
    "        trace (np.ndarray): 1D array of gaze values over time for one trial.\n",
    "        time_trace (np.ndarray): 1D array of time values corresponding to trace.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Same shape as trace, with only the closest cluster kept.\n",
    "    \"\"\"\n",
    "    # Step 1: Binary mask of non-zero gaze activity\n",
    "    binary = trace > 0\n",
    "\n",
    "    # Step 2: Label contiguous clusters of gaze\n",
    "    labeled, num_features = label(binary)\n",
    "\n",
    "    if num_features == 0:\n",
    "        return np.zeros_like(trace)\n",
    "\n",
    "    # Step 3: Identify cluster whose center is closest to time 0\n",
    "    closest_id = None\n",
    "    min_dist = np.inf\n",
    "\n",
    "    for region_id in range(1, num_features + 1):\n",
    "        inds = np.where(labeled == region_id)[0]\n",
    "        region_center_time = np.mean(time_trace[inds])\n",
    "        dist_to_zero = abs(region_center_time)\n",
    "        if dist_to_zero < min_dist:\n",
    "            min_dist = dist_to_zero\n",
    "            closest_id = region_id\n",
    "\n",
    "    # Step 4: Keep only the closest cluster\n",
    "    keep_mask = (labeled == closest_id)\n",
    "    return trace * keep_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250424_Kanga_NV_withDodson']\n",
    "    dates_list = [\"20250424\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['NV_withDodson']\n",
    "    session_start_times = [0] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250424_Dodson_NV_withKanga']\n",
    "    dates_list = [\"20250424\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['NV_withKanga']\n",
    "    session_start_times = [0] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "spike_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvevents_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "strategy_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "strategy_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  \n",
    "print(ndates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/succpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpull_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/failpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpull_trig_events_all_dates = pickle.load(f)    \n",
    "    \n",
    "    with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        spike_trig_events_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        # should be only one merge type - \"Anipose\"\n",
    "        merge_campair = merge_campairs[0]\n",
    "        \n",
    "        # load camera tracking results\n",
    "        try:\n",
    "            # dummy\n",
    "            if reanalyze_video:\n",
    "                print(\"re-analyze the data \",videodate_tgt)\n",
    "                dummy\n",
    "            ## read\n",
    "            with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                body_part_locs_Anipose = pickle.load(f)                 \n",
    "        except:\n",
    "            print(\"did not save data for Anipose - body part tracking \"+videodate_tgt)\n",
    "            # analyze and save\n",
    "            Anipose_h5_file = Anipose_analyzed_path +videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "            Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "            body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "            with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                pickle.dump(body_part_locs_Anipose, f)            \n",
    "\n",
    "        min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "            \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:\n",
    "            print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "            output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "        look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "        look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "        look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "        look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "        # change the unit to second, and aligned to session start\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                           \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                           \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "        timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "        timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "        timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)]\n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "            \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_face_or_not_Anipose']['dodson'])[0]/30) \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, totalsess_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, totalsess_time, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/'+date_tgt+\"_Anipose.pdf\")\n",
    "  \n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "     \n",
    "        \n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 0:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "\n",
    "            savefig = 1\n",
    "            \n",
    "            # aligntwins = 4 # 5 second # built in the plot_continuous_bhv_var code\n",
    "            \n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary, _, _ = plot_continuous_bhv_var(filepath_cont_var+date_tgt+merge_campair,\n",
    "                                    savefig, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var(filepath_cont_var+date_tgt+merge_campair,\n",
    "                                        savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpull_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var(filepath_cont_var+date_tgt+merge_campair,\n",
    "                                        savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpull_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 1:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_face_or_not_Anipose']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_neural_and_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/\"+\\\n",
    "                            merge_campair+\"/\"+animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_average_all,bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_aligned_FR_all_dates[date_tgt] = bhvevents_aligned_FR_average_all\n",
    "                bhvevents_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "            \n",
    "            # the three strategy aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot strategy aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_neural_and_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/\"+\\\n",
    "                            merge_campair+\"/\"+animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                stg_twins = 1.5 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                strategy_aligned_FR_average_all,strategy_aligned_FR_allevents_all = plot_strategy_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,stg_twins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                strategy_aligned_FR_all_dates[date_tgt] = strategy_aligned_FR_average_all\n",
    "                strategy_aligned_FR_allevents_all_dates[date_tgt] = strategy_aligned_FR_allevents_all\n",
    "                \n",
    "            \n",
    "            #\n",
    "            # Run PCA analysis\n",
    "            FR_zscore_allch_np_merged = np.array(pd.DataFrame(FR_zscore_allch).T)\n",
    "            FR_zscore_allch_np_merged = FR_zscore_allch_np_merged[~np.isnan(np.sum(FR_zscore_allch_np_merged,axis=1)),:]\n",
    "            # # run PCA on the entire session\n",
    "            pca = PCA(n_components=3)\n",
    "            FR_zscore_allch_PCs = pca.fit_transform(FR_zscore_allch_np_merged.T)\n",
    "            #\n",
    "            # # run PCA around the -PCAtwins to PCAtwins for each behavioral events\n",
    "            PCAtwins = 4 # 5 second\n",
    "            gaze_thresold = 0.5 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "            savefigs = 0 \n",
    "            if 0:\n",
    "                PCA_around_bhv_events(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                              savefigs,data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            if 0:\n",
    "                if (np.isin(animal1, ['dodson'])) | (np.isin(animal2, ['kanga'])):\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                                      oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "                elif (np.isin(animal2, ['dodson'])) | (np.isin(animal1, ['kanga'])):\n",
    "                    time_point_pulls_succfail_rev = time_point_pulls_succfail.copy()\n",
    "                    time_point_pulls_succfail_rev['pull1_succ'] = time_point_pulls_succfail['pull2_succ']\n",
    "                    time_point_pulls_succfail_rev['pull1_fail'] = time_point_pulls_succfail['pull2_fail']\n",
    "                    time_point_pulls_succfail_rev['pull2_succ'] = time_point_pulls_succfail['pull1_succ']\n",
    "                    time_point_pulls_succfail_rev['pull2_fail'] = time_point_pulls_succfail['pull1_fail']\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull2,time_point_pull1,time_point_pulls_succfail_rev, \n",
    "                                      oneway_gaze2,oneway_gaze1,mutual_gaze2,mutual_gaze1,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # do the spike triggered average of different bhv variables, both continuous tracking result, or the pulling and social gaze actions\n",
    "            # the goal is to get a sense for glm\n",
    "            if 1: \n",
    "                print('plot spike triggered bhv variables')\n",
    "\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_neural_and_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/\"+\\\n",
    "                            merge_campair+\"/\"+animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                do_shuffle = 0\n",
    "                #\n",
    "                min_length = np.shape(look_at_face_or_not_Anipose['dodson'])[0] # frame numbers of the video recording\n",
    "                #\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                \n",
    "                gaze_thresold = 0.2\n",
    "                \n",
    "                stg_twins = 3 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                #\n",
    "                spike_trig_average_all =  plot_spike_triggered_continuous_bhv(date_tgt,savefig,save_path, animal1, animal2, session_start_time, min_length, \n",
    "                                                                              time_point_pull1, time_point_pull2, time_point_pulls_succfail,\n",
    "                                                                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,animalnames_videotrack, \n",
    "                                                                              output_look_ornot, output_allvectors, output_allangles, output_key_locations, \n",
    "                                                                              spike_clusters_data, spike_time_data,spike_channels_data,do_shuffle)\n",
    "                \n",
    "                spike_trig_events_all_dates[date_tgt] = spike_trig_average_all\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpull_trig_events_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(spike_trig_events_all_dates, f)  \n",
    "    \n",
    "        with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_allevents_all_dates, f) \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/succpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpull_trig_events_all_dates, f) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd10bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57153890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51948f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc5918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "#### re-organized the data\n",
    "#### for the activity aligned at the different single behavioral events, mostly focus on self pull\n",
    "#### the ultamate goal is to analyze the difference in single trial and if gaze related variables related to any of them\n",
    "#### make some prilimary plot for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "\n",
    "pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "# pull_trig_events_tgtname = 'animal_lever_dist' \n",
    "# pull_trig_events_tgtname = 'otherani_othertube_dist' \n",
    "# pull_trig_events_tgtname = 'animal_tube_dist' \n",
    "\n",
    "# keep the following as well \n",
    "pull_trig_gazeprob_name = 'socialgaze_prob' # for testing if individual trial different was from gaze start time\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                                    'succrate','clusterID',\n",
    "                                                                    'channelID','FR_allevents'])\n",
    "bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                          'succrate','clusterID',\n",
    "                                                          'channelID','FR_average'])\n",
    "\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "\n",
    "    succrate = succ_rate_all_dates[idate]\n",
    "    \n",
    "    bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "\n",
    "        clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if np.shape(ibhv_type_split)[0]==3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "        # load the pull_trig_continuous_events\n",
    "        if ibhv_type_split[1] == 'pull':\n",
    "            try:\n",
    "                pull_trig_events_tgt = pull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_gazeprob = pull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_gazeprob_name)]\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_gazeprob = np.nan\n",
    "        #\n",
    "        elif ibhv_type_split[1] == 'succpull':\n",
    "            try:\n",
    "                pull_trig_events_tgt = succpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_gazeprob = succpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_gazeprob_name)]\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_gazeprob = np.nan\n",
    "        #\n",
    "        elif ibhv_type_split[1] == 'failpull':\n",
    "            try:\n",
    "                pull_trig_events_tgt = failpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_gazeprob = failpull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_gazeprob_name)]\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_gazeprob = np.nan\n",
    "        #\n",
    "        else:\n",
    "            pull_trig_events_tgt = np.nan\n",
    "            pull_trig_gazeprob = np.nan\n",
    "            \n",
    "        for iclusterID in clusterIDs:   \n",
    "\n",
    "            ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'succrate':succrate,\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_allevents':iFR_average,\n",
    "                                                                                     pull_trig_events_tgtname: pull_trig_events_tgt,                          \n",
    "                                                                                     pull_trig_gazeprob_name: pull_trig_gazeprob,                          \n",
    "                                                                                    }, ignore_index=True)\n",
    "\n",
    "            #\n",
    "            ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'succrate':succrate,\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_average':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc01146",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf229da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "bhv_names_to_ana = ['pull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# \n",
    "# for dodson only\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "# 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "\n",
    "\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94652a",
   "metadata": {},
   "source": [
    "### sanity check plot; mean pull aligned firing rate and pull aligned gaze events (3 gaussian kernel smoothed)\n",
    "#### add the option to look at gaze accumulation over time\n",
    "#### also use this code to defined significant neurons - label neurons that significantly encode gaze accumulation before pull, this is for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:   \n",
    "    # make sure that the significance is defined based on 'pull_trig_gazeprob_name', not 'pull_trig_events_tgtname'\n",
    "    \n",
    "    # gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    gaze_duration_type = 'around_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    \n",
    "    # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "    doGazeAccum = 0\n",
    "        \n",
    "    significant_neurons_data_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                        'clusterID','significance_or_not',\n",
    "                                                        'gaze_duration_type','gaze_variable_name'])\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # Determine subplot grid (5 columns, dynamic rows)\n",
    "                ncols = 5\n",
    "                nrows = int(np.ceil(nneurons / ncols))\n",
    "\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "                axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "                \n",
    "                # === New heatmap plot per date for neuron correlation over time ===\n",
    "                fig_corr, ax_corr = plt.subplots(figsize=(10, max(6, 0.3 * nneurons)))\n",
    "\n",
    "                # Store r_trace and p_trace for each neuron\n",
    "                r_traces_all_neurons = []\n",
    "                p_traces_all_neurons = []\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                        #\n",
    "                        # load and plot bhv event ('pull') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot the results\n",
    "                            time_trace = np.arange(-4,4,1/fps)  # Assuming time is just indices\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            ax.plot(time_trace, mean_trace, label=bhvname_ana+'(n='+str(nevents)+')', \n",
    "                                    color=bhvname_clrs[ibhvname_ana])\n",
    "                            ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                            color=bhvname_clrs[ibhvname_ana], alpha=0.3)\n",
    "                        #\n",
    "                        else:\n",
    "                            FRsmoothed_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # load and plot the pull aligned continuous bhv variables\n",
    "                        # conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_gazeprob_name])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                        \n",
    "                        #\n",
    "                        # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "                        if doGazeAccum:\n",
    "                            from sklearn.metrics import auc\n",
    "                            # if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                            if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                                num_time_points = conBhv_allevents_ineuron.shape[0]\n",
    "                                num_conbhv = conBhv_allevents_ineuron.shape[1]\n",
    "                                accumulated_auc = np.zeros((num_time_points, num_conbhv))\n",
    "                                # Create a time axis (assuming equal spacing)\n",
    "                                time_ind = np.arange(num_time_points)\n",
    "                                #\n",
    "                                for i in range(num_conbhv):\n",
    "                                    for t in range(1, num_time_points):\n",
    "                                        # Calculate AUC up to the current time point for the i-th conbhv\n",
    "                                        y = conBhv_allevents_ineuron[:t+1, i]\n",
    "                                        accumulated_auc[t, i] = auc(time_ind[:t+1], y)\n",
    "                            #\n",
    "                            conBhv_allevents_ineuron = accumulated_auc\n",
    "                            \n",
    "                        # zscored the behavioral events\n",
    "                        # Flatten the data\n",
    "                        flattened = conBhv_allevents_ineuron.flatten()\n",
    "                        # Z-score the entire dataset as a single distribution\n",
    "                        flattened_z = np.full_like(flattened, np.nan)\n",
    "                        valid_mask = ~np.isnan(flattened)\n",
    "                        flattened_z[valid_mask] = st.zscore(flattened[valid_mask])\n",
    "                        # Reshape back to original shape\n",
    "                        conBhv_allevents_ineuron_z = flattened_z.reshape(conBhv_allevents_ineuron.shape)\n",
    "                        # \n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron_z\n",
    "    \n",
    "                        try:\n",
    "                            nevents = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents = 0\n",
    "                        \n",
    "                        try:\n",
    "                            FRconBhv_allevents_ineuron = gaussian_filter1d(conBhv_allevents_ineuron, sigma=6, axis=0)\n",
    "                        except:\n",
    "                            FRconBhv_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        # if the pull aligned FR and bhv have different number\n",
    "                        try:\n",
    "                            nevents_fr = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents_fr = 0\n",
    "                            \n",
    "                        if not  nevents_fr == nevents: \n",
    "                            print(date_ana+' mismatched number')\n",
    "                            if nevents_fr < nevents:\n",
    "                                FRconBhv_allevents_ineuron = FRconBhv_allevents_ineuron[:,0:nevents_fr]\n",
    "                            else:\n",
    "                                FRs_allevents_ineuron = FRs_allevents_ineuron[:,0:nevents]\n",
    "                            \n",
    "                        \n",
    "                        # Compute correlation coefficient between FR and behavior at each time point\n",
    "                        try:\n",
    "                            corrs = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "                            pvals = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "\n",
    "                            for t in range(FRsmoothed_allevents_ineuron.shape[0]):\n",
    "                                fr_t = FRsmoothed_allevents_ineuron[t, :]\n",
    "                                bhv_t = FRconBhv_allevents_ineuron[t, :]\n",
    "\n",
    "                                valid_mask = ~np.isnan(fr_t) & ~np.isnan(bhv_t)\n",
    "                                if np.sum(valid_mask) > 5:  # Only compute if enough data points\n",
    "                                    r, p = st.pearsonr(fr_t[valid_mask], bhv_t[valid_mask])\n",
    "                                    corrs[t] = r\n",
    "                                    pvals[t] = p    \n",
    "                        except:\n",
    "                            time_trace = np.arange(-4,4,1/fps)\n",
    "                            corrs = np.full(time_trace.shape[0], np.nan)\n",
    "                            pvals = np.full(time_trace.shape[0], np.nan)\n",
    "\n",
    "\n",
    "                        r_traces_all_neurons.append(corrs)\n",
    "                        p_traces_all_neurons.append(pvals)\n",
    "\n",
    "                        # decide if this neuron is significant or not\n",
    "                        if gaze_duration_type == 'around_pull':\n",
    "                            significant_neuron = np.sum(pvals<0.01)>0\n",
    "                        elif gaze_duration_type == 'before_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace<0]<0.01)>0\n",
    "                        elif gaze_duration_type == 'after_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace>0]<0.01)>0\n",
    "                                                \n",
    "                        #\n",
    "                        # put information about the significance \n",
    "                        if doGazeAccum:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':'gaze_accum',     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        else:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':pull_trig_gazeprob_name,     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            if doGazeAccum:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_gazeprob_name+'_AUC(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "                            else:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_gazeprob_name+'(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "\n",
    "\n",
    "                            # Create a twin axis for the correlation plot\n",
    "                            ax2 = ax.twinx()                   \n",
    "\n",
    "                            # Plot correlation coefficient trace on the right y-axis\n",
    "                            ax2.plot(time_trace, corrs, color='black', linestyle='--', label='FRBhv r')\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "\n",
    "                            # Highlight significant timepoints (p < 0.01) with red dots on the right y-axis\n",
    "                            significant_mask = (pvals < 0.01) & ~np.isnan(pvals)\n",
    "                            ax2.plot(time_trace[significant_mask], corrs[significant_mask], 'ro', label='p < 0.01')\n",
    "\n",
    "                            # Set the label for the right axis\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "                            ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                            # Optionally adjust limits or formatting if necessary\n",
    "                            ax2.set_ylim(-1, 1)  # Adjust this as necessary for your data range\n",
    "\n",
    "\n",
    "                    ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "                    ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                    # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                    ax.legend()\n",
    "\n",
    "                # Hide empty subplots if nneurons < total grid size\n",
    "                for i in range(nneurons, len(axes)):\n",
    "                    fig.delaxes(axes[i])\n",
    "\n",
    "                # Figure title\n",
    "                fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)\n",
    "\n",
    "                \n",
    "                # Convert to numpy array for heatmap\n",
    "                r_traces_all_neurons = np.array(r_traces_all_neurons)\n",
    "\n",
    "                # === Sort r_traces by the time of their first peak ===\n",
    "                peak_times = []\n",
    "                for trace in r_traces_all_neurons:\n",
    "                    if np.all(np.isnan(trace)):\n",
    "                        peak_times.append(np.inf)\n",
    "                    else:\n",
    "                        peak_idx = np.nanargmax(trace)\n",
    "                        peak_times.append(time_trace[peak_idx])\n",
    "\n",
    "                # Get sorting indices based on peak times\n",
    "                sorted_indices = np.argsort(peak_times)\n",
    "                r_traces_sorted = r_traces_all_neurons[sorted_indices, :]\n",
    "\n",
    "                # Plot heatmap of r values\n",
    "                im = ax_corr.imshow(r_traces_sorted, aspect='auto', cmap='gray_r', interpolation='none',\n",
    "                                    extent=[time_trace[0], time_trace[-1], 0, nneurons],\n",
    "                                    vmin=-0.7, vmax=0.7)\n",
    "\n",
    "                # Overlay significance as red dots\n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    sig_times = np.where(p_traces_all_neurons[idx] < 0.01)[0]\n",
    "                    for t in sig_times:\n",
    "                        ax_corr.plot(time_trace[t], i + 0.5, 'r.', markersize=3)  # i+0.5 to center in the row\n",
    "\n",
    "                # Add vertical dashed line at time = 0\n",
    "                ax_corr.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                # Add vertical dashed line at time zero\n",
    "                ax_corr.axvline(x=0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "                ax_corr.set_xlabel(\"Time (s)\")\n",
    "                ax_corr.set_ylabel(\"Neuron (sorted by peak time)\")\n",
    "                ax_corr.set_title(f\"Neuron-wise Corr(Gaze, FR) Heatmap (Sorted): {act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                cbar = fig_corr.colorbar(im, ax=ax_corr)\n",
    "                cbar.set_label('Pearson r')\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                savefig = 1\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Anipose3d_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    merge_campairs[0]+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+date_ana+'/'\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    if doGazeAccum:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_gazeprob_name+'_auc'+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_gazeprob_name+'_auc'+savefile_sufix+'.pdf')\n",
    "                    else:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_gazeprob_name+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_gazeprob_name+savefile_sufix+'.pdf')\n",
    "\n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig)\n",
    "                plt.close(fig_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_neurons_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    from scipy.integrate import cumtrapz  # Add this import\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    ind1 = bhvevents_aligned_FR_allevents_all_dates_df['condition']=='MC_withGinger'\n",
    "    ind2 = bhvevents_aligned_FR_allevents_all_dates_df['dates']=='20240808'\n",
    "    ind3 = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == 'pull'\n",
    "    ind4 = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']=='kanga'\n",
    "    conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_events_tgtname])[0]\n",
    "    conBhv_gazeprob_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_gazeprob_name])[0]\n",
    "\n",
    "    np.shape(conBhv_gazeprob_ineuron)\n",
    "    \n",
    "    # Setup\n",
    "    plotID = 17\n",
    "    trace = conBhv_gazeprob_ineuron[plotID]\n",
    "    trace2 = conBhv_allevents_ineuron[plotID]\n",
    "    time_trace = np.arange(-4, 4, 1/30)\n",
    "\n",
    "    # Compute accumulated AUC using trapezoidal integration\n",
    "    accum_auc = cumtrapz(trace, time_trace, initial=0)\n",
    "\n",
    "    # Create plot with dual y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Plot the gaze distribution\n",
    "    color1 = 'tab:blue'\n",
    "    line1, = ax1.plot(time_trace, trace, color=color1, label='Gaze Distribution')\n",
    "    fill = ax1.fill_between(time_trace, trace, alpha=0.3, color=color1)\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Gaze Distribution', color=color1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.axvline(0, color='k', linestyle='--', linewidth=1)\n",
    "    ax1.set_title('Social gaze aligned pull')\n",
    "\n",
    "    # Create a second y-axis for accumulated AUC\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:red'\n",
    "    line2, = ax2.plot(time_trace, trace2, color=color2, label='Patner distance to lever')\n",
    "    # line2, = ax2.plot(time_trace, trace2, color=color2, label='Self distance to lever')\n",
    "    # line2, = ax2.plot(time_trace, trace2, color=color2, label='Partner distance to tube')\n",
    "    # line2, = ax2.plot(time_trace, trace2, color=color2, label='Self distance to tube')\n",
    "    ax2.set_ylabel('distance (a.u.)', color=color2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Legend: lines and manual patch for shaded area\n",
    "    legend_elements = [\n",
    "        line1,\n",
    "        Patch(facecolor=color1, alpha=0.3, label='AUC Area'),\n",
    "        line2\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Anipose3d_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        merge_campairs[0]+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'example_event_SelfGazeAccum_PartnerLeverDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_SelfLeverDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_PartnerTubeDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_SelfTubeDist.pdf')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b24c5b",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately for each condition\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### run PCA for all bhvevent together combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "        \n",
    "    # Step 1 - run PCA separately\n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','succrate','act_animal',\n",
    "                                                                'bhv_name','bhv_id','PCs',])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana         \n",
    "\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    succrate = np.array(bhvevents_aligned_FR_allevents_tgt['succrate'])[0][0]\n",
    "                    \n",
    "                    # to better combine different bhv events, choose the same amount\n",
    "                    nbhv_topick = 50\n",
    "\n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    valid_bhvs = ~np.any(np.isnan(data_array), axis=(0, 1))  # Shape (144,)\n",
    "                    data_array = data_array[:, :, valid_bhvs]\n",
    "\n",
    "                    nneurons = np.shape(data_array)[0]\n",
    "                    timepointnums = np.shape(data_array)[1]\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                    # Randomly select bhv events with replacement, once for all neurons\n",
    "                    selected_bhvs = np.random.choice(mbhv_total, nbhv_topick, replace=True)\n",
    "                    sampled_data = data_array[:, :, selected_bhvs]\n",
    "\n",
    "                    # Reshape by flattening the last two dimensions\n",
    "                    final_array = sampled_data.reshape(nneurons, -1)\n",
    "\n",
    "                    PCA_dataset_ibv = final_array\n",
    "\n",
    "                    # combine all bhv for running PCA in the same neural space\n",
    "                    if ibhvname_ana == 0:\n",
    "                        PCA_dataset = PCA_dataset_ibv\n",
    "                    else:\n",
    "                        PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # Run PCA on this concatenated data \n",
    "                pca = PCA(n_components=3)\n",
    "                pca.fit(PCA_dataset)\n",
    "\n",
    "                totalneuronNum = np.shape(PCA_dataset)[1]\n",
    "\n",
    "                # project on the individual events\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    # get the pull triggered continuous variable of target\n",
    "                    data_array_conBhv = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname]))\n",
    "                    data_array_conBhv = np.nanmean(data_array_conBhv,axis=0)\n",
    "                    data_array_conBhv = data_array_conBhv.transpose()\n",
    "\n",
    "                    # get the pull triggered gaze distribution \n",
    "                    data_array_gazeprob = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_gazeprob_name]))\n",
    "                    data_array_gazeprob = np.nanmean(data_array_gazeprob,axis=0)\n",
    "                    data_array_gazeprob = data_array_gazeprob.transpose()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                    for ibhv in np.arange(0,mbhv_total,1):\n",
    "\n",
    "                        data_ibhv = data_array[:,:,ibhv]\n",
    "\n",
    "                        # get the pull triggered continous variables of target for individual events\n",
    "                        try:\n",
    "                            data_array_conBhv_ibhv = data_array_conBhv[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_conBhv_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        \n",
    "                        # get the pull triggered gaze distribution\n",
    "                        try:\n",
    "                            data_array_gazeprob_ibhv = data_array_gazeprob[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_gazeprob_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                            \n",
    "                        # for the socialgaze_prob, only use the meaningful ones\n",
    "                        if 0:\n",
    "                            if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                                trace = data_array_gazeprob_ibhv\n",
    "                                time_trace = np.arange(-4, 4, 1/30)\n",
    "                                filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                data_array_gazeprob_ibhv = filtered_trace\n",
    "                                \n",
    "\n",
    "                        # for firing rate, project on the PC space    \n",
    "                        try:\n",
    "                            PCA_proj_ibhv = pca.transform(np.transpose(data_ibhv))\n",
    "                        except:\n",
    "                            PCA_proj_ibhv = np.full((timepointnums, 3), np.nan)\n",
    "\n",
    "                        FRPCA_all_sessions_allevents_sum_df = FRPCA_all_sessions_allevents_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':succrate,\n",
    "                                                                                'bhv_id':ibhv,\n",
    "                                                                                'PCs':PCA_proj_ibhv,\n",
    "                                                                                'neuronNumBeforePCA':totalneuronNum,\n",
    "                                                                                pull_trig_events_tgtname: data_array_conBhv_ibhv,\n",
    "                                                                                pull_trig_gazeprob_name: data_array_gazeprob_ibhv,\n",
    "                                                                               }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480a20f",
   "metadata": {},
   "source": [
    "#### add PCA features, gaze features, and partner distance to lever features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 for each PCA trace, calculate the length, curvature, and/or tortusity for comparison later\n",
    "# test hypothesis: 1. for testing if individual trial different was from gaze start time/stop time/gaze duration\n",
    "    \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "FRPCAfeatures_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','act_animal','succrate',\n",
    "                                                                    'bhv_name','bhv_id',\n",
    "                                                                    'PClength','PCcurv','PCtort','PCspeed','PCsmoothness',\n",
    "                                                                    'PCspeed_trace','PCcurv_trace',\n",
    "                                                                    ])\n",
    "FRPCAfeatures_gazeduration_corr_all_sessions_df = pd.DataFrame(columns=['condition','session','succrate',\n",
    "                                                                        'act_animal','bhv_name',])\n",
    "\n",
    "# add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "gaze_duration_type = 'before_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "# gaze_duration_type = 'around_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "\n",
    "#\n",
    "for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "    act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "    ind_animal = FRPCA_all_sessions_allevents_sum_df['act_animal']==act_animal_ana\n",
    "        \n",
    "    # get the dates\n",
    "    dates_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal]['session'])\n",
    "    ndates_toplot = np.shape(dates_toplot)[0]\n",
    "    \n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_sessions_allevents_sum_df['condition']==cond_ana\n",
    "\n",
    "        # get the dates\n",
    "        dates_ana = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal & ind_cond]['session'])\n",
    "        ndates_ana = np.shape(dates_ana)[0]\n",
    "    \n",
    "\n",
    "        for idate_ana in np.arange(0,ndates_ana,1):\n",
    "            date_ana = dates_ana[idate_ana]\n",
    "            ind_date = FRPCA_all_sessions_allevents_sum_df['session']==date_ana     \n",
    "            \n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_sessions_allevents_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                FRPCA_allevents_toana = FRPCA_all_sessions_allevents_sum_df[ind_ana]\n",
    "\n",
    "                bhv_ids = np.array(FRPCA_allevents_toana['bhv_id'])\n",
    "                nbhvevents = np.shape(bhv_ids)[0]\n",
    "\n",
    "                for ibhv_id in np.arange(0,nbhvevents,1):\n",
    "\n",
    "                    bhv_id = bhv_ids[ibhv_id]\n",
    "                    ind_bhvid = FRPCA_allevents_toana['bhv_id'] == bhv_id\n",
    "\n",
    "                    \n",
    "                    # \n",
    "                    # analyze some features based on 'otheranimals_otherlever_dist'\n",
    "                    pulltrig_conBhv_tgt = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "                    #\n",
    "                    \n",
    "                    if (pull_trig_events_tgtname == 'otherani_otherlever_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_lever_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'otherani_othertube_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_tube_dist') :\n",
    "                        partner_face_lever_speed = np.gradient(pulltrig_conBhv_tgt, 1/fps)\n",
    "                        \n",
    "                        x_full = np.arange(-4, 4, 1/fps)\n",
    "                        #\n",
    "                        if gaze_duration_type == 'before_pull':\n",
    "                            # Only use the pre-pull window (-4s to 0s)\n",
    "                            pre_mask = x_full <= 0\n",
    "                        elif gaze_duration_type == 'after_pull':\n",
    "                            # Only use the post-pull window (0s to 4s)\n",
    "                            pre_mask = x_full >= 0\n",
    "                        elif gaze_duration_type == 'around_pull':\n",
    "                            # the entire -4 to 4s\n",
    "                            pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "        \n",
    "                        partner_face_lever_meanspeed = np.nanmean(partner_face_lever_speed[pre_mask])\n",
    "        \n",
    "                        min_dist = np.arange(-4,4,1/fps)[np.argmin(pulltrig_conBhv_tgt)]\n",
    "                        max_dist = np.arange(-4,4,1/fps)[np.argmax(pulltrig_conBhv_tgt)]\n",
    "                        \n",
    "                        # find the partner lever approaching trend\n",
    "                        from scipy.stats import linregress\n",
    "                        # Define time range and extract window\n",
    "                        y_full = np.array(pulltrig_conBhv_tgt)                      \n",
    "                        #\n",
    "                        x_pre = x_full[pre_mask]\n",
    "                        y_pre = y_full[pre_mask]\n",
    "                        # Linear regression\n",
    "                        slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                        #\n",
    "                        approaching_slope = slope\n",
    "\n",
    "                        \n",
    "                    else:\n",
    "                        partner_face_lever_speed = np.ones(np.shape(np.arange(-4,4,1/fps)))*np,nan\n",
    "                        \n",
    "                        partner_face_lever_meanspeed = np.nan\n",
    "        \n",
    "                        min_dist = np.nan\n",
    "                        max_dist = np.nan\n",
    "                        \n",
    "                        approaching_slope = np.nan\n",
    "                    \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the pull triggered behavioral events\n",
    "                    #\n",
    "                    # pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "                    pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_gazeprob_name])[0]\n",
    "                            \n",
    "                    # if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                    if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                        # calculate the gaze start and gaze stop time, and finally gaze duration\n",
    "                        try:\n",
    "                            \n",
    "                            # Find the point of first increasing and last decrease to estimate gaze start and end\n",
    "                            #\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            #\n",
    "                            # if gaze_duration_type == 'before_pull':\n",
    "                            #     pulltrig_conBhv[timewins>0] = 0\n",
    "                            # elif gaze_duration_type == 'after_pull':\n",
    "                            #     pulltrig_conBhv[timewins<0] = 0                           \n",
    "                            \n",
    "                            if 1:\n",
    "                                first_increase_idx = np.where(np.diff(pulltrig_conBhv) > 0)[0][0] + 1\n",
    "                                #\n",
    "                                last_decrease_idx = np.where(np.diff(pulltrig_conBhv) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_increase_idx].copy()\n",
    "                                gazestop_time = timewins[last_decrease_idx].copy()\n",
    "                            if 0:\n",
    "                                # Find peaks\n",
    "                                peaks, _ = scipy.signal.find_peaks(pulltrig_conBhv)\n",
    "                                #\n",
    "                                # Get first and last peak\n",
    "                                first_peak = peaks[0] \n",
    "                                last_peak = peaks[-1]\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_peak].copy()\n",
    "                                gazestop_time = timewins[last_peak].copy()\n",
    "                            #\n",
    "                            # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                gazestart_time = gazestart_time\n",
    "                                gazestop_time = gazestop_time\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                if (gazestart_time > 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestop_time > 0):\n",
    "                                    gazestop_time = 0\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                if (gazestop_time < 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestart_time < 0):\n",
    "                                    gazestart_time = 0                                                \n",
    "                            #\n",
    "                            # if (gazestart_time == timewins[0]) | (gazestart_time == timewins[-1]):\n",
    "                            #     gazestart_time = np.nan\n",
    "                            # if (gazestop_time == timewins[0]) | (gazestop_time == timewins[-1]):\n",
    "                            #     gazestop_time = np.nan\n",
    "                            if (gazestop_time < gazestart_time):\n",
    "                                gazestart_time = np.nan\n",
    "                                gazestop_time = np.nan                           \n",
    "                        except:\n",
    "                            gazestart_time = np.nan\n",
    "                            gazestop_time = np.nan\n",
    "                            \n",
    "                        # calculate the gaze accumulation (use auc to estimate)\n",
    "                        try:\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            dt = 1 / fps  # sampling interval in seconds\n",
    "                            #\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv, dx=dt)\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins<0], dx=dt)\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins>0], dx=dt)\n",
    "                            #\n",
    "                            gaze_accum = auc\n",
    "                        #\n",
    "                        except:\n",
    "                            gaze_accum = np.nan\n",
    "                                \n",
    "                            \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the PCs \n",
    "                    FRPCA_ievent_toana = np.array(FRPCA_allevents_toana[ind_bhvid]['PCs'])[0]\n",
    "\n",
    "                    # smooth the pc trajectory\n",
    "                    if 0:\n",
    "                        FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                                 arr=FRPCA_ievent_toana, sigma=6)\n",
    "\n",
    "                    # calculate the length, curvature and tortuosity\n",
    "                    PC_traj = FRPCA_ievent_toana.copy()  # Shape (240, 3)\n",
    "                    \n",
    "                    #\n",
    "                    if gaze_duration_type == 'before_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[0:int(ntimepoints/2),:]\n",
    "                    elif gaze_duration_type == 'after_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[int(ntimepoints/2):,:]\n",
    "\n",
    "                    # Compute differences between consecutive points\n",
    "                    diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                    # Compute segment lengths\n",
    "                    segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                    total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                    # Compute curvature\n",
    "                    # First derivatives\n",
    "                    dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                    dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                    dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                    dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                    # Second derivatives\n",
    "                    d2X_dt2 = np.gradient(dX_dt)\n",
    "                    d2Y_dt2 = np.gradient(dY_dt)\n",
    "                    d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                    d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                    # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                    cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                    curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                    # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                    euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                    tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "                    \n",
    "                    # Compute speed \n",
    "                    dt = 1.0 / fps  # Time between frames\n",
    "                    # Velocity: first derivative of position\n",
    "                    velocity = np.gradient(PC_traj, axis=0) / dt\n",
    "                    # Speed: magnitude of velocity\n",
    "                    speed = np.linalg.norm(velocity, axis=1)\n",
    "                    \n",
    "                    # Compute Smoothness - A simple way to compute trajectory smoothness is to look at the jerk \n",
    "                    #  the third derivative of position (how quickly acceleration changes), \n",
    "                    # which reflects sudden directional/velocity shifts\n",
    "                    # Acceleration: second derivative\n",
    "                    acceleration = np.gradient(velocity, axis=0) / dt\n",
    "                    # Jerk: third derivative\n",
    "                    jerk = np.gradient(acceleration, axis=0) / dt\n",
    "                    # Smoothness metric: integrated squared jerk over time\n",
    "                    squared_jerk = np.linalg.norm(jerk, axis=1) ** 2\n",
    "                    smoothness = np.sum(squared_jerk) * dt\n",
    "\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df.append({\n",
    "                                                                                'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                                'bhv_id':ibhv_id,\n",
    "                                                                                'PClength':total_length,\n",
    "                                                                                'PCcurv':np.nanmean(curvature),\n",
    "                                                                                'PCtort':tortuosity,\n",
    "                                                                                'PCspeed':np.nanmean(speed),\n",
    "                                                                                'PCsmoothness':smoothness,\n",
    "                                                                                'PCspeed_trace':speed,\n",
    "                                                                                'PCcurv_trace':curvature,\n",
    "                                                                                'gazestart_time':gazestart_time,\n",
    "                                                                                'gazestop_time':gazestop_time,\n",
    "                                                                                'gaze_accum':gaze_accum,\n",
    "                                                                                'neuronNumBeforePCA':np.array(FRPCA_allevents_toana[ind_bhvid]['neuronNumBeforePCA'])[0],\n",
    "                                                                                pull_trig_gazeprob_name: pulltrig_conBhv,\n",
    "                                                                                pull_trig_events_tgtname: pulltrig_conBhv_tgt,\n",
    "                                                                                pull_trig_events_tgtname+'_speed':partner_face_lever_speed,\n",
    "                                                                                pull_trig_events_tgtname+'_meanspeed':partner_face_lever_meanspeed,\n",
    "                                                                                pull_trig_events_tgtname+'_max':max_dist,\n",
    "                                                                                pull_trig_events_tgtname+'_min':min_dist,\n",
    "                                                                                pull_trig_events_tgtname+'_appoachslope':approaching_slope,\n",
    "                                                                                }, ignore_index=True)\n",
    "      \n",
    "        \n",
    "               \n",
    "                # after pool all the events related data together do some plotting and calculate the correlation             \n",
    "                ind_sess_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == date_ana\n",
    "                ind_ani_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['act_animal'] == act_animal_ana\n",
    "                ind_bhv_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['bhv_name'] == bhvname_ana\n",
    "                ind_cond_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['condition'] == cond_ana\n",
    "                \n",
    "                ind_toplot = ind_sess_toplot & ind_ani_toplot & ind_bhv_toplot & ind_cond_toplot\n",
    "                FRPCAfeatures_toplot = FRPCAfeatures_all_sessions_allevents_sum_df[ind_toplot]\n",
    "                \n",
    "                yyy_types = [pull_trig_events_tgtname+'_meanspeed',pull_trig_events_tgtname+'_max',\n",
    "                             pull_trig_events_tgtname+'_min',pull_trig_events_tgtname+'_appoachslope',\n",
    "                            'gaze_accum']\n",
    "                nytypes = np.shape(yyy_types)[0]\n",
    "                \n",
    "                # figures \n",
    "                fig1, axs1 = plt.subplots(nytypes+1,1)\n",
    "                fig1.set_figheight(8*nytypes)\n",
    "                fig1.set_figwidth(8*1)\n",
    "                \n",
    "                # xxx_type = 'gaze_duration'\n",
    "                # xxx_type = 'gaze_accumulation'\n",
    "                xxx_type = 'gazestart_time'\n",
    "                \n",
    "\n",
    "                for iytype in np.arange(0,nytypes,1):\n",
    "                                        \n",
    "                    if xxx_type == 'gaze_duration':\n",
    "                        xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                        FRPCAfeatures_toplot['gaze_duration'] = xxx\n",
    "                    elif xxx_type == 'gaze_accumulation':\n",
    "                        xxx = FRPCAfeatures_toplot['gaze_accum']\n",
    "                        FRPCAfeatures_toplot['gaze_accumulation'] = xxx\n",
    "                    else:\n",
    "                        xxx = FRPCAfeatures_toplot[xxx_type]\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "\n",
    "                    ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "                    xxx = xxx[~ind_nan]\n",
    "                    yyy = yyy[~ind_nan]\n",
    "\n",
    "                    # Compute correlation\n",
    "                    if len(xxx) > 1:\n",
    "                        r, p = st.pearsonr(xxx, yyy)\n",
    "                        # Fit regression line\n",
    "                        slope, intercept = np.polyfit(xxx, yyy, 1)\n",
    "                        x_vals = np.array([min(xxx), max(xxx)])\n",
    "                        y_vals = slope * x_vals + intercept\n",
    "                        axs1[iytype].plot(x_vals, y_vals, color='red', linestyle='--', label='Regression line')\n",
    "                    else:\n",
    "                        r, p = np.nan, np.nan\n",
    "\n",
    "                    # Scatter plot\n",
    "                    axs1[iytype].plot(xxx, yyy, 'o', label='gaze '+gaze_duration_type)\n",
    "\n",
    "                    # Title and labels\n",
    "                    axs1[iytype].set_title(bhvname_ana + ' of ' + act_animal_ana + ' in ' +\n",
    "                                 cond_ana + ' ' + date_ana + '\\n neuron #=' +\n",
    "                                 str(FRPCAfeatures_toplot['neuronNumBeforePCA'].iloc[0]), fontsize=12)\n",
    "                    axs1[iytype].set_ylabel(yyy_type, fontsize=12)\n",
    "\n",
    "                    # Add correlation text\n",
    "                    axs1[iytype].text(0.05, 0.9, f\"r = {r:.3f}\\np = {p:.3f}\", transform=axs1[iytype].transAxes, fontsize=12,\n",
    "                            verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "                    # Optional: show legend\n",
    "                    axs1[iytype].legend()\n",
    "\n",
    "                    #\n",
    "                    # plot the one without any gaze as a comparison\n",
    "                    xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "                    \n",
    "                    ind_nan = (np.isnan(xxx)) & (~np.isnan(yyy))\n",
    "                    yyy = yyy[ind_nan]\n",
    "                    xxx = np.zeros(np.shape(yyy))\n",
    "                    \n",
    "                    axs1[iytype].plot(xxx, yyy, 'ro',label='no gaze '+gaze_duration_type)\n",
    "                    \n",
    "                    axs1[iytype].set_xlabel(xxx_type+' '+gaze_duration_type+' (s)',fontsize=12)  # Set xlabel only for the last subplot in the stack\n",
    "                    axs1[iytype].legend(loc='lower right')\n",
    "            \n",
    "                    # \n",
    "                    FRPCAfeatures_gazeduration_corr_all_sessions_df = FRPCAfeatures_gazeduration_corr_all_sessions_df.append({\n",
    "                                                                                'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                                'corr_'+yyy_type+'_vs_'+xxx_type:r,\n",
    "                                                                                'pcorr_'+yyy_type+'_vs_'+xxx_type:p,\n",
    "                                                                                'gazeduration_definition':gaze_duration_type,\n",
    "                                                                               }, ignore_index=True)\n",
    "                                                               \n",
    "            \n",
    "                # add a mean trace of the partner's distance to lever as a comparison\n",
    "                if 1:\n",
    "                    \n",
    "                    xxx = np.arange(-4, 4, 1/fps)\n",
    "                    data = np.stack(np.array(FRPCAfeatures_toplot[pull_trig_events_tgtname]))\n",
    "                    yyy = np.nanmean(data, axis=0)\n",
    "                    sem = np.nanstd(data, axis=0) / np.sqrt(np.sum(~np.isnan(data), axis=0))  # SEM\n",
    "\n",
    "                    # Plot mean trace\n",
    "                    axs1[nytypes].plot(xxx, yyy, color='black', linewidth=2, label='Mean '+pull_trig_events_tgtname)\n",
    "\n",
    "                    # Shaded error area\n",
    "                    axs1[nytypes].fill_between(xxx, yyy - sem, yyy + sem, color='gray', alpha=0.3, label=' SEM')\n",
    "\n",
    "                    # Labels\n",
    "                    axs1[nytypes].set_xlabel('Time (s)')\n",
    "                    # axs1[nytypes].set_ylabel('Mean partner distance to lever')\n",
    "                    axs1[nytypes].set_ylabel('Mean '+pull_trig_events_tgtname)\n",
    "                    axs1[nytypes].legend()\n",
    "                \n",
    "                if 0:\n",
    "                    from matplotlib.collections import LineCollection\n",
    "                    import matplotlib.cm as cm\n",
    "                    import matplotlib.colors as colors\n",
    "\n",
    "                    # Time vector\n",
    "                    xxx = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "                    # Extract data matrix (trials x time)\n",
    "                    data = np.stack(FRPCAfeatures_toplot[pull_trig_events_tgtname].to_numpy())\n",
    "\n",
    "                    # Define the column name used for sorting colors\n",
    "                    sort_var_name = xxx_type\n",
    "                    sort_values = np.array(FRPCAfeatures_toplot[sort_var_name])\n",
    "\n",
    "                    # Normalize for color mapping\n",
    "                    norm = colors.Normalize(vmin=np.nanmin(sort_values), vmax=np.nanmax(sort_values))\n",
    "                    cmap = cm.get_cmap('Purples')  # dark to light purple\n",
    "                    colors_list = cmap(norm(sort_values))\n",
    "\n",
    "                    # Prepare line segments\n",
    "                    segments = [np.column_stack((xxx, data[i])) for i in range(data.shape[0])]\n",
    "                    lc = LineCollection(segments, colors=colors_list, linewidths=0.8, alpha=0.6)\n",
    "\n",
    "                    # Plot using LineCollection\n",
    "                    axs1[nytypes].add_collection(lc)\n",
    "                    axs1[nytypes].set_xlim(xxx.min(), xxx.max())\n",
    "                    axs1[nytypes].set_ylim(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "                    # Add colorbar\n",
    "                    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "                    sm.set_array([])\n",
    "                    cbar = plt.colorbar(sm, ax=axs1[nytypes])\n",
    "                    cbar.set_label(f'{sort_var_name} (s)')\n",
    "\n",
    "                    # Labels\n",
    "                    axs1[nytypes].set_xlabel('Time (s)')\n",
    "                    axs1[nytypes].set_ylabel('Mean ' + pull_trig_events_tgtname) \n",
    "                    \n",
    "                    \n",
    "                fig1.tight_layout()\n",
    "\n",
    "\n",
    "                savefig = 1\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Anipose3d_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    merge_campairs[0]+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+date_ana+\"/\"\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    fig1.savefig(figsavefolder+'bhvevents_aligned__continuousBhv_'+bhvname_ana+'_'+pull_trig_gazeprob_name+'_'+\n",
    "                                 pull_trig_events_tgtname+'_'+xxx_type+'_'+gaze_duration_type+savefile_sufix+'.pdf')\n",
    "               \n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d44001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0080f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284cecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3e719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aff128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1ace5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8771e574",
   "metadata": {},
   "source": [
    "### sanity check plot: separating the gaze accumulation intro quantiles, and plot the mean social gaze and partner_lever distance traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax2 = ax.twinx()  # Secondary y-axis\n",
    "    \n",
    "    # === Scatter plot of gaze_accum vs partner_face_lever_min ===\n",
    "    fig2, ax_scatter = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250428_MC'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    session_example = '20240808'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20240606'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20250423'\n",
    "\n",
    "    # Filter data\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    # Gaze durations\n",
    "    gaze_durations = np.array(FRPCAfeatures_example['gaze_accum'])\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "\n",
    "    time_trace = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "    colors = ['b', 'r', 'y']\n",
    "    labels = ['low', 'mid', 'high']\n",
    "    inds = [ind_low, ind_mid, ind_high]\n",
    "\n",
    "    for ii in range(3):\n",
    "        # Social gaze trace (primary y-axis)\n",
    "        trials_socialgaze = np.stack(FRPCA_example[inds[ii]]['socialgaze_prob'], axis=0)\n",
    "        mean_trace = np.nanmean(trials_socialgaze, axis=0)\n",
    "        sem_trace = np.nanstd(trials_socialgaze, axis=0) / np.sqrt(trials_socialgaze.shape[0])\n",
    "\n",
    "        ax.plot(time_trace, mean_trace, color=colors[ii], linewidth=1.5, alpha=0.5, label=f\"{labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "        ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, color=colors[ii], alpha=0.15)\n",
    "\n",
    "        # Lever distance trace (secondary y-axis)\n",
    "        trials_dist = np.stack(FRPCAfeatures_example[inds[ii]]['partner_face_lever_speed'], axis=0)\n",
    "        # trials_dist = np.stack(FRPCA_example[inds[ii]]['otherani_otherlever_dist'], axis=0)\n",
    "        mean_dist = np.nanmean(trials_dist, axis=0)\n",
    "        sem_dist = np.nanstd(trials_dist, axis=0) / np.sqrt(trials_dist.shape[0])\n",
    "\n",
    "        # Thicker, dashed line with markers to highlight\n",
    "        ax2.plot(time_trace, mean_dist, color=colors[ii], linestyle='-', linewidth=3, marker='o', markersize=4, label=f\"{labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "        ax2.fill_between(time_trace, mean_dist - sem_dist, mean_dist + sem_dist, color=colors[ii], alpha=0.25)\n",
    "\n",
    "    # Axis labels and title\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Social gaze probability', color='gray')\n",
    "    ax2.set_ylabel('Other animal lever distance', color='black')\n",
    "    ax.set_title(f\"{animal_exmaple} - {session_example}\")\n",
    "\n",
    "    # Handle legends (combined from both axes)\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    \n",
    "    ## scatter plot\n",
    "    x = FRPCAfeatures_example['gaze_accum']\n",
    "    # y = FRPCAfeatures_example['partner_face_lever_max']\n",
    "    # y = FRPCAfeatures_example['partner_face_lever_min']\n",
    "    y = FRPCAfeatures_example['partner_lever_appoachslope']\n",
    "\n",
    "    # Drop NaNs\n",
    "    valid = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x_valid = np.array(x)[valid]\n",
    "    y_valid = np.array(y)[valid]\n",
    "\n",
    "    # Compute correlation\n",
    "    from scipy.stats import pearsonr\n",
    "    r_val, p_val = pearsonr(x_valid, y_valid)\n",
    "\n",
    "    # Plot scatter\n",
    "    ax_scatter.scatter(x_valid, y_valid, color='darkgreen', alpha=0.6, edgecolor='k')\n",
    "\n",
    "    # Regression line\n",
    "    slope, intercept = np.polyfit(x_valid, y_valid, 1)\n",
    "    x_line = np.linspace(np.min(x_valid), np.max(x_valid), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "    ax_scatter.plot(x_line, y_line, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Labels and title\n",
    "    ax_scatter.set_xlabel('Gaze accumulation (s)')\n",
    "    # ax_scatter.set_ylabel('Min partner lever distance')\n",
    "    ax_scatter.set_ylabel('partner lever approaching slope')\n",
    "    ax_scatter.set_title(f\"Correlation: r = {r_val:.2f}, p = {p_val:.3f}\")\n",
    "\n",
    "    # Optional grid\n",
    "    ax_scatter.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3502f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322e4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73692e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the single trial trajectories of an example session - compared the gaze_pull separating three quantile based on the gaze duration\n",
    "if 0:\n",
    "    # Create a 3D figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250415'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    session_example = '20240808'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20240606'\n",
    "    # animal_exmaple = 'kanga'\n",
    "    # session_example = '20250415'\n",
    "\n",
    "\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "    #\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session']==session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "    \n",
    "    #\n",
    "    # gaze_durations = FRPCAfeatures_example['gazestop_time'] - FRPCAfeatures_example['gazestart_time']\n",
    "    gaze_durations = FRPCAfeatures_example['gaze_accum']\n",
    "    gaze_durations = np.array(gaze_durations)\n",
    "    #\n",
    "    # Compute tertile (33rd and 67th percentiles), ignoring NaNs\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    #\n",
    "    # Separate data into three groups\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "    \n",
    "    for ii in np.arange(0,3,1):\n",
    "        if ii == 0:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_low]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'b'\n",
    "            traj_lab = 'low gaze duration ' + str(np.sum(ind_low))+' trials'\n",
    "        elif ii == 1:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_mid]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'r'\n",
    "            traj_lab = 'mid gaze duration ' + str(np.sum(ind_mid))+' trials'\n",
    "        elif ii == 2:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_high]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'y'\n",
    "            traj_lab = 'high gaze duration ' + str(np.sum(ind_high))+' trials'\n",
    "           \n",
    "        ntimepoints = np.shape(meanPCA_traj)[0]\n",
    "        \n",
    "        xxx = gaussian_filter1d(meanPCA_traj[:,0],6)\n",
    "        yyy = gaussian_filter1d(meanPCA_traj[:,1],6)\n",
    "        zzz = gaussian_filter1d(meanPCA_traj[:,2],6)\n",
    "        if gaze_duration_type == 'before_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),2],3)   \n",
    "        elif gaze_duration_type == 'after_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,2],3)   \n",
    "            \n",
    "        ax.plot3D(xxx, yyy, zzz, color=traj_clr, linewidth=2, label=traj_lab)\n",
    "        ax.plot3D(xxx[0], yyy[0], zzz[0], color=traj_clr, marker ='o',markersize = 12)\n",
    "        ax.plot3D(xxx[-1], yyy[-1], zzz[-1], color=traj_clr, marker ='s',markersize = 12)\n",
    "\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    savefig = 0\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+session_example+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'forExample_PCAtrajectory_in_gaze_accum_quantiles.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da12391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11809591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c14a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032d957e",
   "metadata": {},
   "source": [
    "#### sanity check plot, indibidual neurons' pull aligned FR and gaze related measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one needs to be run after the previous code becuase it need the definition of the \n",
    "# gaze_duration or gaze_accumulation from the previous code\n",
    "\n",
    "if 0:\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond_FRPCA = FRPCAfeatures_all_sessions_allevents_sum_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal_FRPCA = FRPCAfeatures_all_sessions_allevents_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "                ind_date_FRPCA = FRPCAfeatures_all_sessions_allevents_sum_df['session']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # get the gaze related features\n",
    "                features_ana = FRPCAfeatures_all_sessions_allevents_sum_df[ind_animal_FRPCA&ind_cond_FRPCA&ind_date_FRPCA]\n",
    "                \n",
    "                # Determine subplot grid (5 columns, dynamic rows)\n",
    "                ncols = 5\n",
    "                nrows = int(np.ceil(nneurons / ncols))\n",
    "\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "                axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "                \n",
    "                # === New heatmap plot per date for neuron correlation over time ===\n",
    "                fig_corr, ax_corr = plt.subplots(figsize=(10, max(6, 0.3 * nneurons)))\n",
    "\n",
    "                # Store r_trace and p_trace for each neuron\n",
    "                r_traces_all_neurons = []\n",
    "                p_traces_all_neurons = []\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                            \n",
    "                        #\n",
    "                        # load and plot bhv event ('gazestart'/'gazestop') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "\n",
    "                        FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "\n",
    "                        # separating FR based on the gaze related features (three quantiles)\n",
    "                        # xxx_type = 'gaze_duration'\n",
    "                        xxx_type = 'gaze_accumulation'\n",
    "                        if xxx_type == 'gaze_duration':\n",
    "                            gaze_durs_ineurons = np.array(features_ana['gazestop_time']-\\\n",
    "                                                          features_ana['gazestart_time'])\n",
    "                        elif xxx_type == 'gaze_accumulation':\n",
    "                            gaze_durs_ineurons = np.array(features_ana['gaze_accum'])\n",
    "                        \n",
    "                        ngazes = np.shape(gaze_durs_ineurons)[0]\n",
    "                        \n",
    "                        print(date_ana+' pull aligned FR # = '+str(nevents)+'; pull aligned gaze # = '+str(ngazes))\n",
    "                        \n",
    "                        # Compute quantiles\n",
    "                        q1, q2 = np.nanpercentile(gaze_durs_ineurons, [33, 66])\n",
    "\n",
    "                        # Get indices for each quantile group\n",
    "                        short_idx = np.where(gaze_durs_ineurons <= q1)[0]\n",
    "                        mid_idx   = np.where((gaze_durs_ineurons > q1) & (gaze_durs_ineurons <= q2))[0]\n",
    "                        long_idx  = np.where(gaze_durs_ineurons > q2)[0]\n",
    "\n",
    "                        quantile_groups = {\n",
    "                            'Short': short_idx,\n",
    "                            'Medium': mid_idx,\n",
    "                            'Long': long_idx\n",
    "                        }\n",
    "\n",
    "                        quantile_colors = {\n",
    "                            'Short': '#1f77b4',   # blue\n",
    "                            'Medium': '#ff7f0e',  # orange\n",
    "                            'Long': '#2ca02c'     # green\n",
    "                        }\n",
    "\n",
    "                        time_trace = np.arange(-4,4,1/fps)\n",
    "                        \n",
    "                        # Plot FRs by quantile\n",
    "                        for label, idx in quantile_groups.items():\n",
    "                            if len(idx) == 0:\n",
    "                                continue\n",
    "\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron[:, idx], axis=1)\n",
    "                            sem_trace = np.nanstd(FRsmoothed_allevents_ineuron[:, idx], axis=1) / np.sqrt(len(idx))\n",
    "\n",
    "                            ax.plot(time_trace, mean_trace, label=f\"{bhvname_ana} - {label} (n={len(idx)})\",\n",
    "                                    color=quantile_colors[label])\n",
    "                            ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace,\n",
    "                                            color=quantile_colors[label], alpha=0.3)\n",
    "                        \n",
    "                        \n",
    "                        # plot the correlation coefficient\n",
    "                        # === Compute time-varying correlation between FR and gaze duration ===\n",
    "                        n_timepoints = FRsmoothed_allevents_ineuron.shape[0]\n",
    "                        r_trace = np.full(n_timepoints, np.nan)\n",
    "                        p_trace = np.full(n_timepoints, np.nan)\n",
    "\n",
    "                        if xxx_type == 'gaze_duration':\n",
    "                            gaze_durs = np.array(features_ana['gazestop_time']-\\\n",
    "                                                          features_ana['gazestart_time'])\n",
    "                        elif xxx_type == 'gaze_accumulation':\n",
    "                            gaze_durs = np.array(features_ana['gaze_accum'])\n",
    "                            \n",
    "                        valid_gaze_mask = ~np.isnan(gaze_durs)\n",
    "\n",
    "                        for t in range(n_timepoints):\n",
    "                            fr_t = FRsmoothed_allevents_ineuron[t, :]\n",
    "                            valid_fr_mask = ~np.isnan(fr_t)\n",
    "                            valid_mask = valid_fr_mask & valid_gaze_mask\n",
    "\n",
    "                            if np.sum(valid_mask) > 2:\n",
    "                                r, p = st.pearsonr(fr_t[valid_mask], gaze_durs[valid_mask])\n",
    "                                r_trace[t] = r\n",
    "                                p_trace[t] = p\n",
    "                                \n",
    "                        r_traces_all_neurons.append(r_trace)\n",
    "                        p_traces_all_neurons.append(p_trace)\n",
    "\n",
    "                        # === Plot on right Y-axis ===\n",
    "                        ax2 = ax.twinx()\n",
    "                        ax2.plot(time_trace, r_trace, color='black', linestyle='-', linewidth=2, label='Gaze-FR Corr')\n",
    "                        ax2.set_ylabel(\"Corr(GazeDur, FR)\", color='black')\n",
    "                        ax2.tick_params(axis='y', labelcolor='black')\n",
    "                        ax2.set_ylim([-1, 1])\n",
    "                        ax2.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "                        # Plot significant time points (p < 0.01) as red dots\n",
    "                        sig_mask = (p_trace < 0.01) & (~np.isnan(p_trace)) & (~np.isnan(r_trace))\n",
    "                        ax2.plot(time_trace[sig_mask], r_trace[sig_mask], 'ro', markersize=4, label='p < 0.01')\n",
    "                        \n",
    "                    ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "                    ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                    # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                    ax.legend()\n",
    "                    #\n",
    "                    ax2.set_ylabel(\"Corr(GazeDur, FR)\", color='black')\n",
    "                    ax2.tick_params(axis='y', labelcolor='black')\n",
    "                    ax2.set_ylim([-1, 1])\n",
    "\n",
    "                # Hide empty subplots if nneurons < total grid size\n",
    "                for i in range(nneurons, len(axes)):\n",
    "                    fig.delaxes(axes[i])\n",
    "\n",
    "                # Figure title\n",
    "                fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)                \n",
    "                \n",
    "                \n",
    "                # Convert to numpy array for heatmap\n",
    "                r_traces_all_neurons = np.array(r_traces_all_neurons)\n",
    "\n",
    "                # === Sort r_traces by the time of their first peak ===\n",
    "                peak_times = []\n",
    "                for trace in r_traces_all_neurons:\n",
    "                    if np.all(np.isnan(trace)):\n",
    "                        peak_times.append(np.inf)\n",
    "                    else:\n",
    "                        peak_idx = np.nanargmax(trace)\n",
    "                        peak_times.append(time_trace[peak_idx])\n",
    "\n",
    "                # Get sorting indices based on peak times\n",
    "                sorted_indices = np.argsort(peak_times)\n",
    "                r_traces_sorted = r_traces_all_neurons[sorted_indices, :]\n",
    "\n",
    "                # Plot heatmap of r values\n",
    "                im = ax_corr.imshow(r_traces_sorted, aspect='auto', cmap='gray_r', interpolation='none',\n",
    "                                    extent=[time_trace[0], time_trace[-1], 0, nneurons],\n",
    "                                    vmin=-0.7, vmax=0.7)\n",
    "\n",
    "                # Overlay significance as red dots\n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    sig_times = np.where(p_traces_all_neurons[idx] < 0.01)[0]\n",
    "                    for t in sig_times:\n",
    "                        ax_corr.plot(time_trace[t], i + 0.5, 'r.', markersize=3)  # i+0.5 to center in the row\n",
    "\n",
    "                # Add vertical dashed line at time = 0\n",
    "                ax_corr.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                # Add vertical dashed line at time zero\n",
    "                ax_corr.axvline(x=0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "                ax_corr.set_xlabel(\"Time (s)\")\n",
    "                ax_corr.set_ylabel(\"Neuron (sorted by peak time)\")\n",
    "                ax_corr.set_title(f\"Neuron-wise Corr(Gaze, FR) Heatmap (Sorted): {act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                cbar = fig_corr.colorbar(im, ax=ax_corr)\n",
    "                cbar.set_label('Pearson r')\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                savefig = 1\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    fig.savefig(figsavefolder+'individualneurons_FR_in_quantiles_'+\n",
    "                                 pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+xxx_type+savefile_sufix+'.pdf')\n",
    "\n",
    "                    fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_' + \n",
    "                                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+xxx_type+savefile_sufix+'.pdf')\n",
    "\n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig)\n",
    "                plt.close(fig_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b973f7",
   "metadata": {},
   "source": [
    "## analysis with 'trial pooling' across sessions from the same condition\n",
    "### pool sessions for each task conditions together and then run PCA\n",
    "#### pool sessions based on quantiles of gaze-accumulation or gaze-length variables (e.g. 5 quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only analyze a subset of conditions\n",
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "# # the following analysis can only do one conditions \n",
    "# # multiple condition will be considered into one conditions for quantile and FR averaging analysis\n",
    "# conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['MC','SR',]\n",
    "###\n",
    "# For Kanga\n",
    "conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVermelho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "cond_toplot_type = 'allMC'\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "doOnlySigniNeurons = 0 # define the significant neurons using the previous code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "\n",
    "if 1:\n",
    "    # load and prepare the data\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.DataFrame(columns=['condition', 'session', 'act_animal',\n",
    "                                                                                 'bhv_name', 'bhv_id', 'FR_ievent',\n",
    "                                                                                 'clusterID', 'channelID',\n",
    "                                                                                 'gaze_accum_ievent', 'gaze_start_ievent',\n",
    "                                                                                 'gaze_stop_ievent',\n",
    "                                                                                 ])\n",
    "    #\n",
    "    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.DataFrame()\n",
    "\n",
    "    # step 1 for the bhvevents_aligned_FR_allevents_all_dates_df, get and gaze-related variables and calculate features\n",
    "\n",
    "    # it's better to match this variable with the previous one\n",
    "    # add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "    gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    xxx_type = 'gaze_accum'  # 'gaze_dur' or 'gaze_accum'\n",
    "\n",
    "    # special here, number of quantile to use for pooling across different days\n",
    "    num_quantiles = 5 # 10\n",
    "\n",
    "    #\n",
    "    for icond_ana in np.arange(0, nconds_to_ana, 1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition'] == cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0, nanimal_to_ana, 1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal'] == act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0, ndates_ana, 1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates'] == date_ana\n",
    "\n",
    "                # get the neurons\n",
    "                neurons_ana = np.unique(\n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0, nneurons, 1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID'] == clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0, nbhvnames_to_ana, 1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date\n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "                            ind_ana].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "                        if not bhvevents_aligned_FR_allevents_tgt.empty:\n",
    "                            channelID = bhvevents_aligned_FR_allevents_tgt['channelID'].iloc[0]\n",
    "\n",
    "                            #\n",
    "                            # load and plot bhv event ('pull') aligned FR\n",
    "                            FRs_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt['FR_allevents'].iloc[0])\n",
    "                            nevents_FR = np.shape(FRs_allevents_ineuron)[1]\n",
    "\n",
    "                            #\n",
    "                            # load and plot the pull aligned continuous bhv variables\n",
    "                            conBhv_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname].iloc[0])\n",
    "                            conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                            conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                            #\n",
    "                            nevents_bhv = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                            # if the pull aligned FR and bhv have different number\n",
    "                            if not nevents_FR == nevents_bhv:\n",
    "                                # print(date_ana+' mismatched number')\n",
    "                                if nevents_FR < nevents_bhv:\n",
    "                                    conBhv_allevents_ineuron = conBhv_allevents_ineuron[:, 0:nevents_FR]\n",
    "                                else:\n",
    "                                    FRs_allevents_ineuron = FRs_allevents_ineuron[:, 0:nevents_bhv]\n",
    "\n",
    "                            #\n",
    "                            nevents = np.min([nevents_FR, nevents_bhv])\n",
    "\n",
    "                            # get each bhv events\n",
    "                            for bhv_id in np.arange(0, nevents, 1):\n",
    "                                FRs_ievent_ineuron = FRs_allevents_ineuron[:, bhv_id]\n",
    "                                conBhv_ievent_ineuron = conBhv_allevents_ineuron[:, bhv_id]\n",
    "\n",
    "                                #\n",
    "                                # analyze the pull triggered behavioral events\n",
    "                                if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                                    # calculate the gaze start and gaze stop time, and finally gaze duration\n",
    "                                    try:\n",
    "                                        timewins = np.arange(-4, 4, 1 / fps)  # make sure it align with the setting in the previous section\n",
    "\n",
    "                                        if 1:\n",
    "                                            first_increase_idx = np.where(\n",
    "                                                np.diff(conBhv_ievent_ineuron) > 0)[0][0] + 1\n",
    "                                            #\n",
    "                                            last_decrease_idx = np.where(\n",
    "                                                np.diff(conBhv_ievent_ineuron) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                            #\n",
    "                                            gazestart_time = timewins[first_increase_idx].copy()\n",
    "                                            gazestop_time = timewins[last_decrease_idx].copy()\n",
    "                                        if 0:\n",
    "                                            # Find peaks\n",
    "                                            peaks, _ = scipy.signal.find_peaks(conBhv_ievent_ineuron)\n",
    "                                            #\n",
    "                                            # Get first and last peak\n",
    "                                            first_peak = peaks[0]\n",
    "                                            last_peak = peaks[-1]\n",
    "                                            #\n",
    "                                            gazestart_time = timewins[first_peak].copy()\n",
    "                                            gazestop_time = timewins[last_peak].copy()\n",
    "                                        #\n",
    "                                        # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                                        if gaze_duration_type == 'around_pull':\n",
    "                                            gazestart_time = gazestart_time\n",
    "                                            gazestop_time = gazestop_time\n",
    "                                        if gaze_duration_type == 'before_pull':\n",
    "                                            if (gazestart_time > 0):\n",
    "                                                gazestart_time = np.nan\n",
    "                                                gazestop_time = np.nan\n",
    "                                            elif (gazestop_time > 0):\n",
    "                                                gazestop_time = 0\n",
    "                                        if gaze_duration_type == 'after_pull':\n",
    "                                            if (gazestop_time < 0):\n",
    "                                                gazestart_time = np.nan\n",
    "                                                gazestop_time = np.nan\n",
    "                                            elif (gazestart_time < 0):\n",
    "                                                gazestart_time = 0\n",
    "\n",
    "                                        #\n",
    "                                        if (gazestop_time < gazestart_time):\n",
    "                                            gazestart_time = np.nan\n",
    "                                            gazestop_time = np.nan\n",
    "                                    except:\n",
    "                                        gazestart_time = np.nan\n",
    "                                        gazestop_time = np.nan\n",
    "\n",
    "                                    # calculate the gaze accumulation (use auc to estimate)\n",
    "                                    try:\n",
    "                                        timewins = np.arange(-4, 4, 1 / fps)  # make sure it align with the setting in the previous section\n",
    "                                        dt = 1 / fps  # sampling interval in seconds\n",
    "                                        #\n",
    "                                        if gaze_duration_type == 'around_pull':\n",
    "                                            auc = np.trapz(conBhv_ievent_ineuron, dx=dt)\n",
    "                                        if gaze_duration_type == 'before_pull':\n",
    "                                            auc = np.trapz(conBhv_ievent_ineuron[timewins < 0], dx=dt)\n",
    "                                        if gaze_duration_type == 'after_pull':\n",
    "                                            auc = np.trapz(conBhv_ievent_ineuron[timewins > 0], dx=dt)\n",
    "                                        #\n",
    "                                        gaze_accum = auc\n",
    "                                    #\n",
    "                                    except:\n",
    "                                        gaze_accum = np.nan\n",
    "\n",
    "                                bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(\n",
    "                                    [bhvevents_aligned_FR_and_eventFeatures_all_dates_df, pd.DataFrame({\n",
    "                                        'condition': [cond_ana],\n",
    "                                        'act_animal': [act_animal_ana],\n",
    "                                        'bhv_name': [bhvname_ana],\n",
    "                                        'session': [date_ana],\n",
    "                                        'bhv_id': [bhv_id],\n",
    "                                        'clusterID': [clusterID_ineuron],\n",
    "                                        'channelID': [channelID],\n",
    "                                        'gaze_accum_ievent': [gaze_accum],\n",
    "                                        'gaze_start_ievent': [gazestart_time],\n",
    "                                        'gaze_stop_ievent': [gazestop_time],\n",
    "                                        'FR_ievent': [FRs_ievent_ineuron],\n",
    "                                        pull_trig_events_tgtname:[conBhv_ievent_ineuron],\n",
    "                                    })], ignore_index=True)\n",
    "\n",
    "    # add the quantile information using the all sessions' data\n",
    "    # consider each date separately\n",
    "\n",
    "    # Create a list to store the DataFrames for each date\n",
    "    all_dates_dfs = []\n",
    "\n",
    "    # Get unique dates\n",
    "    unique_dates = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'].unique()\n",
    "\n",
    "    for date_ana in unique_dates:\n",
    "        date_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == date_ana].copy()\n",
    "\n",
    "        if xxx_type == 'gaze_accum':\n",
    "            quantile_tgt = date_df['gaze_accum_ievent'].dropna()  # Handle potential NaNs\n",
    "            n_unique = len(quantile_tgt.unique())\n",
    "            n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "            if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "                try:\n",
    "                    quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                    date_df['gaze_accum_quantile'] = pd.cut(\n",
    "                        date_df['gaze_accum_ievent'],\n",
    "                        bins=quantile_bins,\n",
    "                        labels=False,\n",
    "                        include_lowest=True,\n",
    "                        duplicates='drop'  # Drop duplicate bin edges\n",
    "                    )\n",
    "                    quantile_col = 'gaze_accum_quantile'\n",
    "                    title_prefix = 'Gaze Accumulation'\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                    date_df['gaze_accum_quantile'] = np.nan\n",
    "                    quantile_col = 'gaze_accum_quantile'\n",
    "                    title_prefix = 'Gaze Accumulation'\n",
    "            else:\n",
    "                date_df['gaze_accum_quantile'] = np.nan\n",
    "                quantile_col = 'gaze_accum_quantile'\n",
    "                title_prefix = 'Gaze Accumulation'\n",
    "                print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "\n",
    "        #\n",
    "        elif xxx_type == 'gaze_dur':\n",
    "            date_df['gaze_duration_ievent'] = \\\n",
    "                date_df['gaze_stop_ievent'] - \\\n",
    "                date_df['gaze_start_ievent']\n",
    "            quantile_tgt = date_df['gaze_duration_ievent'].dropna()  # Handle potential NaNs\n",
    "            n_unique = len(quantile_tgt.unique())\n",
    "            n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "            if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "                try:\n",
    "                    quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                    date_df['gaze_duration_quantile'] = pd.cut(\n",
    "                        date_df['gaze_duration_ievent'],\n",
    "                        bins=quantile_bins,\n",
    "                        labels=False,\n",
    "                        include_lowest=True,\n",
    "                        duplicates='drop'  # Drop duplicate bin edges\n",
    "                    )\n",
    "                    quantile_col = 'gaze_duration_quantile'\n",
    "                    title_prefix = 'Gaze Duration'\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                    date_df['gaze_duration_quantile'] = np.nan\n",
    "                    quantile_col = 'gaze_duration_quantile'\n",
    "                    title_prefix = 'Gaze Duration'\n",
    "            else:\n",
    "                date_df['gaze_duration_quantile'] = np.nan\n",
    "                quantile_col = 'gaze_duration_quantile'\n",
    "                title_prefix = 'Gaze Duration'\n",
    "                print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "\n",
    "        all_dates_dfs.append(date_df)\n",
    "\n",
    "    # Concatenate the DataFrames back together\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(all_dates_dfs, ignore_index=True)\n",
    "\n",
    "    # average for each neuron the firing rate of each quantile\n",
    "    if 'quantile_col' in locals():\n",
    "        # Iterate through unique (clusterID, session) pairs\n",
    "        for (cluster_id, session_id) in bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            ['clusterID', 'session']].drop_duplicates().itertuples(index=False):\n",
    "            neuron_session_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == session_id)\n",
    "            ].copy()\n",
    "\n",
    "            for q_val in neuron_session_df[quantile_col].dropna().unique():\n",
    "                quantile_df = neuron_session_df[neuron_session_df[quantile_col] == q_val]\n",
    "                if not quantile_df.empty:\n",
    "                    all_fr_traces = np.vstack(quantile_df['FR_ievent'].tolist())\n",
    "                    mean_fr_trace = np.nanmean(all_fr_traces, axis=0)\n",
    "\n",
    "                    # Get representative metadata\n",
    "                    condition = quantile_df['condition'].iloc[0]\n",
    "                    act_animal = quantile_df['act_animal'].iloc[0]\n",
    "                    bhv_name = quantile_df['bhv_name'].iloc[0]\n",
    "                    channelID = quantile_df['channelID'].iloc[0]\n",
    "\n",
    "                    new_row = {\n",
    "                        'condition': condition,\n",
    "                        'session': session_id,\n",
    "                        'act_animal': act_animal,\n",
    "                        'bhv_name': bhv_name,\n",
    "                        'clusterID': cluster_id,\n",
    "                        'channelID': channelID,\n",
    "                        quantile_col: int(q_val),\n",
    "                        'mean_FR_trace': mean_fr_trace\n",
    "                    }\n",
    "                    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.concat(\n",
    "                        [bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df,\n",
    "                         pd.DataFrame([new_row])],\n",
    "                        ignore_index=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01691733",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# do some big picture plot; here is to have a sense of the gaze-accumulation distribution of succ and failed pull\n",
    "# ...if the setting is set for that\n",
    "## \n",
    "if 1:\n",
    "    from scipy.stats import ks_2samp\n",
    "\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition_general\"] = \\\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    \n",
    "    # \n",
    "    plt.figure(figsize=(20,6))\n",
    "    data_toplot = bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "    # seaborn.kdeplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "    seaborn.kdeplot(data=data_toplot,x='gaze_accum_ievent',hue='condition_general',\n",
    "                   common_norm=False)  # don't normalize across groups\n",
    "    # seaborn.histplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "\n",
    "    # Compute group-wise quantiles (deciles from 10% to 90%)\n",
    "    # quantiles_df = data_toplot.groupby('bhv_name')['gaze_accum_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df = data_toplot.groupby('condition_general')['gaze_accum_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df.rename(columns={'gaze_accum_ievent': 'quantile_value'}, inplace=True)\n",
    "\n",
    "    # Draw vertical lines for each quantile\n",
    "    if 0:\n",
    "        # palette = dict(zip(data_toplot['bhv_name'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "        palette = dict(zip(data_toplot['condition_general'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "\n",
    "        for _, row in quantiles_df.iterrows():\n",
    "            plt.axvline(\n",
    "                row['quantile_value'],\n",
    "                # color=palette[row['bhv_name']],\n",
    "                color=palette[row['condition_general']],\n",
    "                linestyle='--',\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    plt.title('KDE Plot with Quantile Lines')\n",
    "    plt.show()\n",
    "    \n",
    "    # KS test: compare MC vs SR\n",
    "    if 0:\n",
    "        group_MC = data_toplot[data_toplot['condition_general'] == 'MC']['gaze_accum_ievent'].dropna()\n",
    "        group_SR = data_toplot[data_toplot['condition_general'] == 'SR']['gaze_accum_ievent'].dropna()\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(group_MC, group_SR)\n",
    "        print(f\"KS test: statistic={ks_stat:.4f}, p-value={ks_pval:.4g}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider the significant neurons based on the previous analysis\n",
    "if 1:\n",
    "    if doOnlySigniNeurons:\n",
    "        \n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = significant_bhv_df\n",
    "\n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = significant_bhv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98703bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "# bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4184afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - 2:\n",
    "# do a basic plot for sanity check - mean FR across all units in the pool condition for each quantile\n",
    "if 1:\n",
    "    from scipy.integrate import cumtrapz\n",
    "\n",
    "    doQuantMeanFRs = 1\n",
    "    # only do one example session, all neurons in that session, average\n",
    "    doExampleSession = 0\n",
    "    # only do one example neuron in one example cell \n",
    "    doExampleNeuron = 0\n",
    "    \n",
    "    timewins = np.arange(-4, 4, 1/30)\n",
    "    n_timepoints = len(timewins)\n",
    "        \n",
    "    #\n",
    "    if doExampleSession:\n",
    "        # examplesess = '20240606'\n",
    "        examplesess =  '20240808'\n",
    "        #\n",
    "        if doQuantMeanFRs:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col].unique())\n",
    "        else:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col].unique())      \n",
    "    # \n",
    "    elif doExampleNeuron:\n",
    "        examplesess =  '20240808'\n",
    "        exampleneuron = '129'\n",
    "        # \n",
    "        # has to do not QuantMeanFRs\n",
    "        ind_1 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "        ind_2 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID']==exampleneuron\n",
    "        quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col].unique()) \n",
    "    #\n",
    "    else:\n",
    "        if doQuantMeanFRs:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "        else:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col].unique())\n",
    "    \n",
    "    quantile_values = quantile_values[~np.isnan(quantile_values)]\n",
    "    \n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2 = ax.twinx()  # Secondary y-axis for AUC\n",
    "\n",
    "    for i_quantile, q_val in enumerate(quantile_values):\n",
    "    \n",
    "        #\n",
    "        if doExampleSession:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][ind_quantile]['FR_ievent'].tolist())\n",
    "        #\n",
    "        elif doExampleNeuron:\n",
    "            ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col] == q_val\n",
    "            fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile]['FR_ievent'].tolist())\n",
    "            gaze_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile][pull_trig_events_tgtname].tolist())\n",
    "     \n",
    "        #\n",
    "        else:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_quantile]['FR_ievent'].tolist())\n",
    "\n",
    "            \n",
    "        #     \n",
    "        mean_fr = np.nanmean(fr_traces_quantile, axis=0)\n",
    "        sem_fr = np.nanstd(fr_traces_quantile, axis=0) / np.sqrt(np.sum(~np.isnan(fr_traces_quantile[:, 0]))) # Standard Error of the Mean\n",
    "        \n",
    "        ax.plot(timewins, mean_fr, label=f'Quantile {int(q_val)}')\n",
    "        ax.fill_between(timewins, mean_fr - sem_fr, mean_fr + sem_fr, alpha=0.3)\n",
    "\n",
    "        \n",
    "        #\n",
    "        if doExampleNeuron:\n",
    "            # Accumulated AUC for each gaze trace, then average\n",
    "            auc_gaze_all = np.array([cumtrapz(trace, timewins, initial=0) for trace in gaze_traces_quantile])\n",
    "            mean_auc_gaze = np.nanmean(auc_gaze_all, axis=0)\n",
    "            sem_auc_gaze = np.nanstd(auc_gaze_all, axis=0) / np.sqrt(np.sum(~np.isnan(auc_gaze_all[:, 0])))\n",
    "            \n",
    "            # Plot AUC Gaze\n",
    "            ax2.plot(timewins, mean_auc_gaze, linestyle='--', color='lightblue', label=f'Gaze AUC {int(q_val)}')\n",
    "            ax2.fill_between(timewins, mean_auc_gaze - sem_auc_gaze, mean_auc_gaze + sem_auc_gaze, alpha=0.2, color='lightblue')\n",
    "\n",
    "        \n",
    "        \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'Mean Firing Rate by {title_prefix} Quantile '+gaze_duration_type)\n",
    "    ax.axvline(0, color='k', linestyle='--', linewidth=0.8, label='Pull Onset')\n",
    "    ax.legend()\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2.set_ylabel('Accumulated Gaze AUC')\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"\n",
    "    \n",
    "        if doExampleSession:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "        \n",
    "        if doExampleNeuron:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        if not doExampleNeuron:\n",
    "            fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_acrossNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        elif doExampleNeuron:\n",
    "            fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_exampleNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2889130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - calculate the PCA with the pooled data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import splprep, splev\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "if 1:\n",
    "    timewins = np.arange(-4, 4, 1 / 30)\n",
    "    n_timepoints = len(timewins)\n",
    "    quantile_col = 'gaze_accum_quantile'  # Or 'gaze_duration_quantile'\n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    title_prefix = 'Gaze Accumulation'  # Or 'Gaze Duration'\n",
    "    smooth_kernel_size = 6\n",
    "    n_bootstrap_iterations = 100\n",
    "    n_neurons_to_sample = 200\n",
    "    \n",
    "    # Get unique neurons\n",
    "    unique_neurons = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[['clusterID', 'session']].drop_duplicates()\n",
    "    n_neurons = len(unique_neurons)\n",
    "\n",
    "    # Get unique quantiles\n",
    "    unique_quantiles = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "    n_quantiles = len(unique_quantiles)\n",
    "\n",
    "    # Initialize the data matrix\n",
    "    data_matrix = np.empty((n_neurons, n_timepoints * n_quantiles))\n",
    "    neuron_index_lookup = {}\n",
    "\n",
    "    # Populate the data matrix\n",
    "    neuron_counter = 0\n",
    "    for neuron_row in unique_neurons.itertuples(index=False):\n",
    "        cluster_id = neuron_row.clusterID\n",
    "        session = neuron_row.session\n",
    "        neuron_index_lookup[(cluster_id, session)] = neuron_counter\n",
    "        neuron_counter += 1\n",
    "\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            # Get the mean FR trace for the current neuron and quantile\n",
    "            mean_fr_trace_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session'] == session) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val)\n",
    "            ]\n",
    "\n",
    "            if not mean_fr_trace_df.empty:\n",
    "                mean_fr_trace = mean_fr_trace_df['mean_FR_trace'].values[0]  # Take the first element\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = mean_fr_trace\n",
    "            else:\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = np.nan  # Handle missing data\n",
    "\n",
    "    # Prepare to store results\n",
    "    all_quantile_lengths = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_quantile_curvatures = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_boot_pca_data = np.zeros((n_bootstrap_iterations, 10, n_timepoints * n_quantiles))  # Store all PCA results\n",
    "\n",
    "    # Perform Bootstrapping\n",
    "    for boot_iter in range(n_bootstrap_iterations):\n",
    "        # 1. Randomly sample neurons\n",
    "        sampled_neuron_indices = np.random.choice(n_neurons, n_neurons_to_sample, replace=True)\n",
    "        sampled_data_matrix = data_matrix[sampled_neuron_indices, :].transpose()  # Neuron dimension becomes the columns\n",
    "\n",
    "        # 2. Run PCA\n",
    "        pca = PCA(n_components=10)  # Project to 10 PCs\n",
    "        pca.fit(np.nan_to_num(sampled_data_matrix))\n",
    "        pca_data = pca.transform(np.nan_to_num(sampled_data_matrix)).transpose()  # Project and transpose\n",
    "        all_boot_pca_data[boot_iter, :, :] = pca_data\n",
    "\n",
    "        # Calculate length and curvature for each quantile\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            start_col = i_quantile * n_timepoints\n",
    "            end_col = (i_quantile + 1) * n_timepoints\n",
    "            quantile_data = pca_data[:, start_col:end_col]  # (10, n_timepoints)\n",
    "\n",
    "            # \n",
    "            if gaze_duration_type == 'around_pull':\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, :], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, :], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'before_pull':\n",
    "                ind_tgt = timewins<0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'after_pull':\n",
    "                ind_tgt = timewins>0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "\n",
    "                    \n",
    "    # ---plotting---\n",
    "    # --- plotting number 1 ---\n",
    "    # Plot Length and Curvature (as before)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_lengths, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[0], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    #axes[0].boxplot(all_quantile_lengths)\n",
    "    # axes[0].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('PC Trajectory Length')\n",
    "    axes[0].set_title('Bootstrapped PC Trajectory Lengths')\n",
    "\n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_curvatures, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[1], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    # axes[1].boxplot(all_quantile_curvatures)\n",
    "    # axes[1].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('PC Trajectory Curvature')\n",
    "    axes[1].set_title('Bootstrapped PC Trajectory Curvatures')\n",
    "\n",
    "    # do the regression for each bootstrap iteration and then plot the average regression line\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Prepare x-axis (quantile indices)\n",
    "    quantiles = np.arange(0, n_quantiles).reshape(-1, 1)\n",
    "\n",
    "    # Store predicted regression lines from each bootstrap\n",
    "    predicted_lengths = []\n",
    "    predicted_curvatures = []\n",
    "    reg_coeff_lengths = []\n",
    "    reg_coeff_curvs = []\n",
    "\n",
    "    for i in range(n_bootstrap_iterations):\n",
    "        # Regression for length\n",
    "        y_len = all_quantile_lengths[i]\n",
    "        model_len = LinearRegression().fit(quantiles, y_len)\n",
    "        slope = model_len.coef_\n",
    "        reg_coeff_lengths.append(slope[0])\n",
    "        pred_len = model_len.predict(quantiles)\n",
    "        predicted_lengths.append(pred_len)\n",
    "\n",
    "        # Regression for curvature\n",
    "        y_curv = all_quantile_curvatures[i]\n",
    "        model_curv = LinearRegression().fit(quantiles, y_curv)\n",
    "        slope = model_curv.coef_\n",
    "        reg_coeff_curvs.append(slope[0])\n",
    "        pred_curv = model_curv.predict(quantiles)\n",
    "        predicted_curvatures.append(pred_curv)\n",
    "\n",
    "    # Convert to arrays\n",
    "    predicted_lengths = np.array(predicted_lengths)\n",
    "    predicted_curvatures = np.array(predicted_curvatures)\n",
    "\n",
    "    # Mean and 95% CI across bootstraps\n",
    "    mean_len = np.mean(predicted_lengths, axis=0)\n",
    "    ci_len_low = np.percentile(predicted_lengths, 2.5, axis=0)\n",
    "    ci_len_high = np.percentile(predicted_lengths, 97.5, axis=0)\n",
    "\n",
    "    mean_curv = np.mean(predicted_curvatures, axis=0)\n",
    "    ci_curv_low = np.percentile(predicted_curvatures, 2.5, axis=0)\n",
    "    ci_curv_high = np.percentile(predicted_curvatures, 97.5, axis=0)\n",
    "\n",
    "    # Length\n",
    "    axes[0].plot(quantiles, mean_len, color='red', label='Mean Regression')\n",
    "    axes[0].fill_between(quantiles.flatten(), ci_len_low, ci_len_high, color='red', alpha=0.3, label='95% CI')\n",
    "    axes[0].set_title('PC Trajectory Length')\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('Length')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Curvature\n",
    "    axes[1].plot(quantiles, mean_curv, color='blue', label='Mean Regression')\n",
    "    axes[1].fill_between(quantiles.flatten(), ci_curv_low, ci_curv_high, color='blue', alpha=0.3, label='95% CI')\n",
    "    axes[1].set_title('PC Trajectory Curvature')\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('Curvature')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_lengths, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the length regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "    #\n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_curvs, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the curvature regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "\n",
    "    # --- plotting number 1 end ---\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Average 3D Traces\n",
    "    fig2 = plt.figure(figsize=(10, 8))\n",
    "    ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "    mean_traces_3d = np.mean(all_boot_pca_data[:, :3, :], axis=0)  # Average across bootstrap iterations, use only first 3 PCs\n",
    "\n",
    "    for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "        start_col = i_quantile * n_timepoints\n",
    "        end_col = (i_quantile + 1) * n_timepoints\n",
    "        mean_quantile_data = mean_traces_3d[:, start_col:end_col]\n",
    "\n",
    "        # Smooth the trajectory\n",
    "        smooth_x = gaussian_filter1d(mean_quantile_data[0, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_y = gaussian_filter1d(mean_quantile_data[1, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_z = gaussian_filter1d(mean_quantile_data[2, ind_tgt], sigma=smooth_kernel_size)\n",
    "\n",
    "        # Plot the smoothed trajectory\n",
    "        ax.plot(smooth_x, smooth_y, smooth_z, label=f'Quantile {int(q_val)}')\n",
    "\n",
    "        # Mark start and end points\n",
    "        ax.scatter(smooth_x[0], smooth_y[0], smooth_z[0], marker='o', color='k')  # Start\n",
    "        ax.scatter(smooth_x[-1], smooth_y[-1], smooth_z[-1], marker='x', color='k')  # End\n",
    "\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_zlabel('PC 3')\n",
    "    ax.set_title(f'Average 3D Trajectories in Neuron-Reduced PC Space by {title_prefix} Quantile')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\n",
    "    print(\"\\n--- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\")\n",
    "\n",
    "    from scipy import stats\n",
    "    from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "    def perform_pairwise_paired_ttests(data, group_labels, alpha=0.01):\n",
    "        unique_groups = np.unique(group_labels)\n",
    "        n_groups = len(unique_groups)\n",
    "        p_values = []\n",
    "        comparisons = []\n",
    "\n",
    "        for i in range(n_groups):\n",
    "            for j in range(i + 1, n_groups):\n",
    "                group1_data = data[:, i]\n",
    "                group2_data = data[:, j]\n",
    "                t_stat, p_val = stats.ttest_rel(group1_data, group2_data)\n",
    "                p_values.append(p_val)\n",
    "                comparisons.append((unique_groups[i], unique_groups[j]))\n",
    "\n",
    "        reject, p_corrected, _, _ = multipletests(p_values, method='holm', alpha=alpha)\n",
    "\n",
    "        results_df = pd.DataFrame({'Comparison': comparisons,\n",
    "                                   'p_value': p_values,\n",
    "                                   'p_corrected': p_corrected,\n",
    "                                   'reject_null': reject})\n",
    "        return results_df\n",
    "\n",
    "    # Perform pairwise paired t-tests for Length\n",
    "    length_pairwise_results = perform_pairwise_paired_ttests(all_quantile_lengths, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Length:\")\n",
    "    print(length_pairwise_results)\n",
    "\n",
    "    # Perform pairwise paired t-tests for Curvature\n",
    "    curvature_pairwise_results = perform_pairwise_paired_ttests(all_quantile_curvatures, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Curvature:\")\n",
    "    print(curvature_pairwise_results)\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'bhvevents_aligned_PCfeatures_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_PCtrajectory_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sampled_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f380d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e511b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
