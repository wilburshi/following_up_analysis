{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### analyze the spike triggered pull and gaze ditribution\n",
    "#### the following detailed analysis focused on pull related behavioral events; with the specific focus on the partner Distance variables\n",
    "#### the pull action start events are defined based on the movement onset before each pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam import plot_continuous_bhv_var_singlecam\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b030e",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2ec5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "def keep_closest_cluster_single_trial(trace, time_trace):\n",
    "    \"\"\"\n",
    "    Keep only the contiguous region of non-zero gaze activity that's closest to time = 0.\n",
    "\n",
    "    Args:\n",
    "        trace (np.ndarray): 1D array of gaze values over time for one trial.\n",
    "        time_trace (np.ndarray): 1D array of time values corresponding to trace.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Same shape as trace, with only the closest cluster kept.\n",
    "    \"\"\"\n",
    "    # Step 1: Binary mask of non-zero gaze activity\n",
    "    binary = trace > 0\n",
    "\n",
    "    # Step 2: Label contiguous clusters of gaze\n",
    "    labeled, num_features = label(binary)\n",
    "\n",
    "    if num_features == 0:\n",
    "        return np.zeros_like(trace)\n",
    "\n",
    "    # Step 3: Identify cluster whose center is closest to time 0\n",
    "    closest_id = None\n",
    "    min_dist = np.inf\n",
    "\n",
    "    for region_id in range(1, num_features + 1):\n",
    "        inds = np.where(labeled == region_id)[0]\n",
    "        region_center_time = np.mean(time_trace[inds])\n",
    "        dist_to_zero = abs(region_center_time)\n",
    "        if dist_to_zero < min_dist:\n",
    "            min_dist = dist_to_zero\n",
    "            closest_id = region_id\n",
    "\n",
    "    # Step 4: Keep only the closest cluster\n",
    "    keep_mask = (labeled == closest_id)\n",
    "    return trace * keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04cf9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "def get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, time_point_juice1, time_point_juice2):\n",
    "    pull_infos = {}\n",
    "\n",
    "    def compute_infos(pull_times, juice_times, animal_name):\n",
    "        # Compute successful pulls: the last pull before each juice\n",
    "        successful_pulls = [pull_times[pull_times < juice].max() for juice in juice_times if (pull_times < juice).any()]\n",
    "        successful_pulls = pd.Series(successful_pulls).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "        failed_pulls = pull_times[~pull_times.isin(successful_pulls)]\n",
    "\n",
    "        # Initialize lists\n",
    "        num_preceding_failpull = []\n",
    "        time_from_last_reward = []\n",
    "        last_successful_pull_time = -np.inf\n",
    "\n",
    "        # Create lookup-friendly series\n",
    "        pull_times_sorted = pull_times.sort_values().reset_index(drop=True)\n",
    "\n",
    "        for curr_pull in pull_times_sorted:\n",
    "            # Update last successful pull if current one is successful\n",
    "            if curr_pull in successful_pulls.values:\n",
    "                last_successful_pull_time = curr_pull\n",
    "\n",
    "            # Count number of failed pulls between last successful pull and current pull\n",
    "            if np.isfinite(last_successful_pull_time):\n",
    "                failed_between = failed_pulls[(failed_pulls > last_successful_pull_time) & (failed_pulls < curr_pull)]\n",
    "            else:\n",
    "                failed_between = failed_pulls[failed_pulls < curr_pull]\n",
    "\n",
    "            num_preceding_failpull.append(len(failed_between))\n",
    "\n",
    "            # Time from most recent juice\n",
    "            past_juices = juice_times[juice_times < curr_pull]\n",
    "            if len(past_juices) == 0:\n",
    "                time_from_last_reward.append(np.nan)\n",
    "            else:\n",
    "                time_from_last_reward.append(curr_pull - past_juices.max())\n",
    "\n",
    "        # Save the data\n",
    "        pull_infos[(animal_name, 'num_preceding_failpull')] = pd.Series(num_preceding_failpull, index=pull_times_sorted.index)\n",
    "        pull_infos[(animal_name, 'time_from_last_reward')] = pd.Series(time_from_last_reward, index=pull_times_sorted.index)\n",
    "\n",
    "    # Process both animals\n",
    "    compute_infos(time_point_pull1, time_point_juice1, animal1)\n",
    "    compute_infos(time_point_pull2, time_point_juice2, animal2)\n",
    "\n",
    "    return pull_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a044a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "def find_sharp_increases_withinIPI(pull_data, speed_data, session_start_time, fps):\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.signal import argrelextrema\n",
    "\n",
    "    sharp_increase_frames = np.zeros_like(speed_data)\n",
    "    session_start_frame = int(np.round(session_start_time * fps))\n",
    "    pull_frames = np.where(pull_data)[0]\n",
    "    npulls = len(pull_frames)\n",
    "\n",
    "    for ipull in range(npulls):\n",
    "        pull_frame = pull_frames[ipull]\n",
    "        pullstart_frame = session_start_frame if ipull == 0 else pull_frames[ipull - 1]\n",
    "\n",
    "        try:\n",
    "            speed_IPI_tgt = speed_data[pullstart_frame:pull_frame-int(1*fps)] # allows a 1000ms error time window\n",
    "        except:\n",
    "            speed_IPI_tgt = speed_data[pullstart_frame:pull_frame-int(0.5*fps)] # allows a 500ms error time window\n",
    "\n",
    "        onset_global_idx = pullstart_frame  # default fallback\n",
    "\n",
    "        if len(speed_IPI_tgt) >= 3:\n",
    "            # Find local maxima\n",
    "            local_max_idx = argrelextrema(speed_IPI_tgt, np.greater_equal)[0]\n",
    "\n",
    "            if len(local_max_idx) > 0:\n",
    "                # Get the one closest to the current pull (end of interval)\n",
    "                peak_idx = local_max_idx[np.argmax(local_max_idx)]\n",
    "\n",
    "                # Go backward from the peak to find where it started rising\n",
    "                for j in range(peak_idx - 1, 0, -1):\n",
    "                    if speed_IPI_tgt[j] < speed_IPI_tgt[j - 1]:\n",
    "                        onset_local = j\n",
    "                        onset_global_idx = pullstart_frame + onset_local\n",
    "                        break\n",
    "\n",
    "        sharp_increase_frames[onset_global_idx] = 1\n",
    "\n",
    "    return sharp_increase_frames\n",
    "\n",
    "# take both speed and angle speed \n",
    "def find_sharp_increases_withinIPI_dual_speed(pull_data, speed_data, anglespeed_data, session_start_time, fps):\n",
    "    import numpy as np\n",
    "    from scipy.signal import argrelextrema\n",
    "\n",
    "    sharp_increase_frames = np.zeros_like(speed_data)\n",
    "    session_start_frame = int(np.round(session_start_time * fps))\n",
    "    pull_frames = np.where(pull_data)[0]\n",
    "    npulls = len(pull_frames)\n",
    "\n",
    "    for ipull in range(npulls):\n",
    "        pull_frame = pull_frames[ipull]\n",
    "        pullstart_frame = session_start_frame if ipull == 0 else pull_frames[ipull - 1]\n",
    "\n",
    "        try:\n",
    "            speed_segment = speed_data[pullstart_frame:pull_frame-int(1*fps)] # allows a 1000ms error time window\n",
    "            anglespeed_segment = anglespeed_data[pullstart_frame:pull_frame-int(1*fps)] # allows a 1000ms error time window\n",
    "\n",
    "        except:\n",
    "            speed_segment = speed_data[pullstart_frame:pull_frame-int(0.5*fps)] # allows a 500ms error time window\n",
    "            anglespeed_segment = anglespeed_data[pullstart_frame:pull_frame-int(0.5*fps)] # allows a 500ms error time window\n",
    "\n",
    "        def find_onset(segment):\n",
    "            onset = pullstart_frame  # default\n",
    "            if len(segment) >= 3:\n",
    "                local_max_idx = argrelextrema(segment, np.greater_equal)[0]\n",
    "                if len(local_max_idx) > 0:\n",
    "                    peak_idx = local_max_idx[np.argmax(local_max_idx)]\n",
    "                    for j in range(peak_idx - 1, 0, -1):\n",
    "                        if segment[j] < segment[j - 1]:\n",
    "                            return pullstart_frame + j\n",
    "            return onset\n",
    "\n",
    "        onset_speed = find_onset(speed_segment)\n",
    "        onset_anglespeed = find_onset(anglespeed_segment)\n",
    "\n",
    "        # Take the earlier one\n",
    "        final_onset = min(onset_speed, onset_anglespeed)\n",
    "        sharp_increase_frames[final_onset] = 1\n",
    "\n",
    "    return sharp_increase_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "pull1_intv_all_dates = np.zeros((ndates,1))\n",
    "pull2_intv_all_dates = np.zeros((ndates,1))\n",
    "pull1_minintv_all_dates = np.zeros((ndates,1))\n",
    "pull2_minintv_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_infos_all_dates = dict.fromkeys(dates_list, []) # keep some useful information about pulls - time from last reward, number of preceding failed pull etc\n",
    "\n",
    "pull_rts_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pullstart_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpullstart_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpullstart_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_pullstart_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvevents_pullstart_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all data\n",
      "analyze all dates\n",
      "load social gaze with camera-2 only of 20240508\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240508_Kanga_SR\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240509\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240509_Kanga_MC\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240513\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240513_Kanga_MC\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240514\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240514_Kanga_SR\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240523\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240523_Kanga_MC\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240524\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240524_Kanga_SR\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240606\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240606_Kanga_MC\n",
      "plot event aligned firing rate; pull start focus\n",
      "load social gaze with camera-2 only of 20240613\n",
      "use the gaze vector speed and face mass speed to define the start of the pull decision\n",
      "plot self pull start triggered bhv variables\n",
      "load spike data for 20240613_Kanga_MC_DannonAuto\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1836812/2414546528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_saved_subfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/pull_rts_all_dates_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0manimal1_fixedorders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0manimal2_fixedorders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mpull_rts_all_dates\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/data_saved_singlecam_wholebody_DLPFCs/camera-2/dannonkanga//pull_rts_all_dates_dannonkanga.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1836812/2414546528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mtotalsess_time_forFR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_look_ornot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'look_at_lever_or_not_merge'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dodson'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# to match the total time of the video recording\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n\u001b[0m\u001b[1;32m    699\u001b[0m                                                                                           spike_clusters_data, spike_time_data)\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/marmoset_tracking_DLCv2/following_up_analysis/3d_recontruction_analysis_self_and_coop_task_neural_analysis/ana_functions/spike_analysis_FR_calculation.py\u001b[0m in \u001b[0;36mspike_analysis_FR_calculation\u001b[0;34m(fs_spikes, FR_kernel, totalsess_time, spike_clusters_data, spike_time_data)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# xxx_plot = np.linspace(0, totalsess_time*fs_spikes, int(totalsess_time*fs_spikes/200))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gaussian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFR_kernel\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfs_spikes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 100ms bandwith gaussian kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlog_dens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxxx_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mFR_icluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfs_spikes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/sklearn/neighbors/_kde.py\u001b[0m in \u001b[0;36mscore_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0matol_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         log_density = self.tree_.kernel_density(\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "\n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_infos_all_dates  = pickle.load(f)  \n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_rts_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_rts_all_dates  = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pullstart_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/succpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpullstart_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/failpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpullstart_trig_events_all_dates = pickle.load(f)    \n",
    "    \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstart_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstart_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "        \n",
    "    print('all data from all dates are loaded; pull start focus')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        #\n",
    "        pull_rts_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        \n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        #\n",
    "        pulltime1 = np.array(bhv_data[(bhv_data['behavior_events']==1)]['time_points'])\n",
    "        pulltime2 = np.array(bhv_data[(bhv_data['behavior_events']==2)]['time_points'])\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        \n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        #\n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        #\n",
    "        ind_good_pull1_succ = time_point_pull1_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_succ = time_point_pull1_succ[ind_good_pull1_succ]\n",
    "        ind_good_pull2_succ = time_point_pull2_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_succ = time_point_pull2_succ[ind_good_pull2_succ]\n",
    "        #\n",
    "        ind_good_pull1_fail = time_point_pull1_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_fail = time_point_pull1_fail[ind_good_pull1_fail]\n",
    "        ind_good_pull2_fail = time_point_pull2_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_fail = time_point_pull2_fail[ind_good_pull2_fail]\n",
    "        \n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        pull_infos_all_dates[date_tgt] = pull_infos\n",
    "        \n",
    "            \n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "        # define variables and use them to find the 'onset' of pull decision\n",
    "        print('use the gaze vector speed and face mass speed to define the start of the pull decision')\n",
    "        #\n",
    "        gausKernelsize = 16\n",
    "        #\n",
    "        # clean the data\n",
    "        time_point_pull1_temp = np.array(time_point_pull1)+session_start_time\n",
    "        time_point_pull1_temp = time_point_pull1_temp[time_point_pull1_temp<totalsess_time]\n",
    "        time_point_pull2_temp = np.array(time_point_pull2)+session_start_time\n",
    "        time_point_pull2_temp = time_point_pull2_temp[time_point_pull2_temp<totalsess_time]\n",
    "        #\n",
    "        # organize the data into a time series\n",
    "        pull1_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull1_data[np.round(time_point_pull1_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        pull2_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull2_data[np.round(time_point_pull2_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        facemass1 = output_key_locations['facemass_loc_all_merge']['dodson'].transpose()\n",
    "        facemass1 = np.hstack((facemass1,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass1[:,1:]-facemass1[:,:-1])\n",
    "        speed1_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed1_data = scipy.ndimage.gaussian_filter1d(speed1_data,gausKernelsize)\n",
    "        #\n",
    "        facemass2 = output_key_locations['facemass_loc_all_merge']['scorch'].transpose()\n",
    "        facemass2 = np.hstack((facemass2,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass2[:,1:]-facemass2[:,:-1])\n",
    "        speed2_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed2_data = scipy.ndimage.gaussian_filter1d(speed2_data,gausKernelsize)\n",
    "        #\n",
    "        gazevect1 = np.array(output_allvectors['head_vect_all_merge']['dodson']).transpose()\n",
    "        gazevect1 = np.hstack((gazevect1, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect1[:, 1:]\n",
    "        at0 = gazevect1[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed1_data = np.full(nframes, np.nan)\n",
    "        eps = 1e-10\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed1_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed1_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed1_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed1_data[~nans])\n",
    "        anglespeed1_data = scipy.ndimage.gaussian_filter1d(anglespeed1_data, gausKernelsize)\n",
    "        #\n",
    "        gazevect2 = np.array(output_allvectors['head_vect_all_merge']['scorch']).transpose()\n",
    "        gazevect2 = np.hstack((gazevect2, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect2[:, 1:]\n",
    "        at0 = gazevect2[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed2_data = np.full(nframes, np.nan)\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed2_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed2_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed2_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed2_data[~nans])\n",
    "        anglespeed2_data = scipy.ndimage.gaussian_filter1d(anglespeed2_data, gausKernelsize)\n",
    "        \n",
    "        # find the transitional time point of angle speed and speed in IPI\n",
    "        speed1_increase = find_sharp_increases_withinIPI(pull1_data,speed1_data,session_start_time,fps)\n",
    "        anglespeed1_increase = find_sharp_increases_withinIPI(pull1_data,anglespeed1_data,session_start_time,fps)\n",
    "        # find the transitional time point using both angle speed and mass speed in IPI\n",
    "        pull1_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull1_data, speed1_data, anglespeed1_data, \n",
    "                                                                              session_start_time, fps)\n",
    "        #\n",
    "        speed2_increase = find_sharp_increases_withinIPI(pull2_data,speed2_data,session_start_time,fps)\n",
    "        anglespeed2_increase = find_sharp_increases_withinIPI(pull2_data,anglespeed2_data,session_start_time,fps)\n",
    "        # find the transitional time point using both angle speed and mass speed in IPI\n",
    "        pull2_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull2_data, speed2_data, anglespeed2_data, \n",
    "                                                                              session_start_time, fps)\n",
    "         \n",
    "        #\n",
    "        # store the pull reaction time information\n",
    "        pull_data_points = np.where(pull1_data)[0]\n",
    "        pullonset_data_points = np.where(pull1_action_onset_frames)[0]\n",
    "        pull1_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "        pull_rts_all_dates[date_tgt][animal1] = pull1_rt\n",
    "        #\n",
    "        pull_data_points = np.where(pull2_data)[0]\n",
    "        pullonset_data_points = np.where(pull2_action_onset_frames)[0]\n",
    "        pull2_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "        pull_rts_all_dates[date_tgt][animal2] = pull2_rt\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # replace time_point_pull_xxx to the pull onset\n",
    "        time_point_pull1 = np.array(np.round(time_point_pull1,1))\n",
    "        time_point_pull2 = np.array(np.round(time_point_pull2,1))\n",
    "        time_point_pull1_succ = np.array(time_point_pull1_succ)\n",
    "        time_point_pull2_succ = np.array(time_point_pull2_succ)\n",
    "        time_point_pull1_fail = np.array(time_point_pull1_fail)\n",
    "        time_point_pull2_fail = np.array(time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1_succ_idx = np.isin(time_point_pull1,time_point_pull1_succ)\n",
    "        time_point_pull2_succ_idx = np.isin(time_point_pull2,time_point_pull2_succ)\n",
    "        time_point_pull1_fail_idx = np.isin(time_point_pull1,time_point_pull1_fail)\n",
    "        time_point_pull2_fail_idx = np.isin(time_point_pull2,time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1 = np.where(pull1_action_onset_frames)[0]/fps - session_start_time\n",
    "        time_point_pull2 = np.where(pull2_action_onset_frames)[0]/fps - session_start_time\n",
    "        #\n",
    "        time_point_pull1_succ = time_point_pull1[time_point_pull1_succ_idx]\n",
    "        time_point_pull2_succ = time_point_pull2[time_point_pull2_succ_idx]\n",
    "        time_point_pull1_fail = time_point_pull1[time_point_pull1_fail_idx]\n",
    "        time_point_pull2_fail = time_point_pull2[time_point_pull2_fail_idx]\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            print('plot self pull start triggered bhv variables')\n",
    "            \n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+cameraID+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "\n",
    "            savefig = 0\n",
    "            \n",
    "            aligntwins = 4 # 5 second\n",
    "            \n",
    "            min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "\n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                    aligntwins, savefig, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pullstart_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                        aligntwins, savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpullstart_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpullstart_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                        aligntwins, savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpullstart_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpullstart_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 1:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate; pull start focus')\n",
    "                #\n",
    "                savefig = 0\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+\\\n",
    "                            animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_average_all,bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_pullstart_aligned_FR_all_dates[date_tgt] = bhvevents_aligned_FR_average_all\n",
    "                bhvevents_pullstart_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_rts_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_rts_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstart_trig_events_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstart_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstart_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_rts_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_rts_all_dates, f)  \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstart_trig_events_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstart_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstart_aligned_FR_allevents_all_dates, f) \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81289c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_pull2_succ_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3b16a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "# \n",
    "# mostly just for the sessions in which MC and SR are in the same session \n",
    "firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "#    \n",
    "# newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "\n",
    "# new total session time (instead of 600s) - total time of the video recording\n",
    "totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "\n",
    "# make sure all task code registered event, aka pulls, are within the video recording\n",
    "ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "\n",
    "time_point_pull1 = np.array(np.round(time_point_pull1,1))\n",
    "time_point_pull2 = np.array(np.round(time_point_pull2,1))\n",
    "time_point_pull1_succ = np.array(time_point_pull1_succ)\n",
    "time_point_pull2_succ = np.array(time_point_pull2_succ)\n",
    "time_point_pull1_fail = np.array(time_point_pull1_fail)\n",
    "time_point_pull2_fail = np.array(time_point_pull2_fail)\n",
    "#\n",
    "time_point_pull1_succ_idx = np.isin(time_point_pull1,time_point_pull1_succ)\n",
    "time_point_pull2_succ_idx = np.isin(time_point_pull2,time_point_pull2_succ)\n",
    "time_point_pull1_fail_idx = np.isin(time_point_pull1,time_point_pull1_fail)\n",
    "time_point_pull2_fail_idx = np.isin(time_point_pull2,time_point_pull2_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6026b719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(time_point_pull2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c3c17173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(time_point_pull2_succ_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "42859823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.5,  17.9,  24.5,  31. ,  37.4,  43.6,  50.1,  59.4,  65.4,\n",
       "        71.5,  78.9,  86. ,  92.2,  98.6, 105.1, 111.8, 117.9, 124.7,\n",
       "       131. , 138.1, 144.9, 151.9, 158.3, 165.4, 172.4, 185.2, 192.5,\n",
       "       199.6, 206.1, 213.1, 221.6, 229.9, 236.1, 244.1, 250.4, 256.9,\n",
       "       266.9, 274.3, 281.3, 289. , 305. , 311.5, 320.4, 326.8, 334.8,\n",
       "       341.4, 348.3, 354.8])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_point_pull2_succ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e1ed24d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48,)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(time_point_pull2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4cb3c2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_point_pull2_succ_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2218de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffa6d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(failpull_trig_events_all_dates['20240606'][('dannon', 'gaze_other_angle')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b996611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull1_intv_all_dates[np.isin(task_conditions,['MC','MC_withGinger','MC_withDodson','MC_withKoala','MC_withVermelho'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25fce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20240508', '20240509', '20240513', '20240514', '20240523', '20240524', '20240606', '20240613', '20240614', '20240617', '20240618', '20240619', '20240620', '20240621_1', '20240624', '20240626', '20240808', '20240809', '20240812', '20240813', '20240814', '20240815', '20240819', '20240821', '20240822', '20250415', '20250416', '20250417', '20250418', '20250421', '20250422', '20250422_SR', '20250423', '20250423_SR', '20250424', '20250424_MC', '20250424_SR', '20250425', '20250425_SR', '20250428_NV', '20250428_MC', '20250428_SR', '20250429_NV', '20250429_MC', '20250429_SR', '20250430_NV', '20250430_MC', '20250430_SR'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull_infos_all_dates['20240613'].keys()\n",
    "pull_infos_all_dates.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d9612c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_trig_events_all_dates['20240508'].keys()\n",
    "# pull_trig_events_all_dates['20240531'].keys()\n",
    "# pull_trig_events_all_dates['20240613'][('kanga', 'otherpull_prob')]\n",
    "# np.shape(pull_trig_events_all_dates['20240613'][('kanga', 'otherpull_prob')])\n",
    "# np.shape(pull_infos_all_dates['20240613'][('kanga', 'num_preceding_failpull')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "#### re-organized the data\n",
    "#### for the activity aligned at the different single behavioral events, mostly focus on self pull\n",
    "#### the ultamate goal is to analyze the difference in single trial and if gaze related variables related to any of them\n",
    "#### make some prilimary plot for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist','otherani_othertube_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "\n",
    "# use this variable as the main target to estimate the partner's intention \n",
    "#\n",
    "# pull_trig_events_tgtname = 'otherpull_prob' \n",
    "# pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "# pull_trig_events_tgtname = 'animal_lever_dist' \n",
    "# pull_trig_events_tgtname = 'otherani_othertube_dist' \n",
    "# pull_trig_events_tgtname = 'animal_tube_dist' \n",
    "# pull_trig_events_tgtname = 'animal_animal_dist' \n",
    "# pull_trig_events_tgtname = 'other_mass_move_speed'\n",
    "\n",
    "# change it to a list to work with \n",
    "#\n",
    "# List of pull_trig_events types to load\n",
    "pull_trig_events_tgtnames = [\n",
    "    'other_mass_move_speed', \n",
    "    'otherani_otherlever_dist',\n",
    "    'animal_animal_dist', \n",
    "    # 'mass_move_speed',\n",
    "]\n",
    "# only keep one for the further analysis\n",
    "pull_trig_events_tgtname = 'other_mass_move_speed'\n",
    "\n",
    "# Keep these as additional controls\n",
    "pull_trig_gazeprob_name = 'socialgaze_prob'\n",
    "pull_trig_otherpull_name = 'otherpull_prob'\n",
    "pull_trig_selfpull_name = 'selfpull_prob'\n",
    "pull_num_pre_failpull_name = 'num_preceding_failpull'\n",
    "pull_time_pre_reward_name = 'time_from_last_reward'\n",
    "\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=[\n",
    "                                                    'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                                    'succrate', 'clusterID', 'channelID', 'FR_allevents'\n",
    "                                                ])\n",
    "bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                            'succrate', 'clusterID', 'channelID', 'FR_average'\n",
    "                                        ])\n",
    "\n",
    "for idate in np.arange(0, ndates, 1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "    succrate = succ_rate_all_dates[idate]\n",
    "    bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "        clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if len(ibhv_type_split) == 3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1] + '_' + ibhv_type_split[2]\n",
    "\n",
    "        event_dict = {}\n",
    "        def load_event(source_dict, actor, name):\n",
    "            try:\n",
    "                return source_dict[date_tgt][(actor, name)]\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "        # determine which dataset to use\n",
    "        if ibhv_type_split[1] == 'pull':\n",
    "            source = pull_trig_events_all_dates\n",
    "        elif ibhv_type_split[1] == 'succpull':\n",
    "            source = succpull_trig_events_all_dates\n",
    "        elif ibhv_type_split[1] == 'failpull':\n",
    "            source = failpull_trig_events_all_dates\n",
    "        else:\n",
    "            source = None\n",
    "\n",
    "        if source is not None:\n",
    "            actor = ibhv_type_split[0]\n",
    "            for name in pull_trig_events_tgtnames:\n",
    "                event_dict[name] = load_event(source, actor, name)\n",
    "            \n",
    "            event_dict[pull_trig_gazeprob_name] = load_event(source, actor, pull_trig_gazeprob_name)\n",
    "            event_dict[pull_trig_otherpull_name] = load_event(source, actor, pull_trig_otherpull_name)\n",
    "            event_dict[pull_trig_selfpull_name] = load_event(source, actor, pull_trig_selfpull_name)\n",
    "        else:\n",
    "            for name in pull_trig_events_tgtnames + [\n",
    "                pull_trig_gazeprob_name, pull_trig_otherpull_name, pull_trig_selfpull_name\n",
    "            ]:\n",
    "                event_dict[name] = np.nan\n",
    "                \n",
    "        # for the pull information variables\n",
    "        #\n",
    "        source2 = pull_infos_all_dates\n",
    "        event_dict[pull_num_pre_failpull_name] = load_event(source2, actor, pull_num_pre_failpull_name)\n",
    "        event_dict[pull_time_pre_reward_name] = load_event(source2, actor, pull_time_pre_reward_name)\n",
    "        \n",
    "        #\n",
    "        for iclusterID in clusterIDs:\n",
    "            ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            row_data = {\n",
    "                'dates': date_tgt,\n",
    "                'condition': task_condition,\n",
    "                'act_animal': ibhv_type_split[0],\n",
    "                'bhv_name': ibhv_type_split[1],\n",
    "                'succrate': succrate,\n",
    "                'clusterID': iclusterID,\n",
    "                'channelID': ichannelID,\n",
    "                'FR_allevents': iFR_average\n",
    "            }\n",
    "            # Add all events into the row\n",
    "            row_data.update(event_dict)\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append(\n",
    "                row_data, ignore_index=True\n",
    "            )\n",
    "\n",
    "            # Also add to FR_average table\n",
    "            ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({\n",
    "                'dates': date_tgt,\n",
    "                'condition': task_condition,\n",
    "                'act_animal': ibhv_type_split[0],\n",
    "                'bhv_name': ibhv_type_split[1],\n",
    "                'succrate': succrate,\n",
    "                'clusterID': iclusterID,\n",
    "                'channelID': ichannelID,\n",
    "                'FR_average': iFR_average\n",
    "            }, ignore_index=True)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3829a5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'clusterID',\n",
       "       'channelID', 'FR_allevents', 'animal_animal_dist',\n",
       "       'num_preceding_failpull', 'other_mass_move_speed',\n",
       "       'otherani_otherlever_dist', 'otherpull_prob', 'selfpull_prob',\n",
       "       'socialgaze_prob', 'time_from_last_reward'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42bb0bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson',\n",
       "       'MC_withGinger', 'MC_withKoala', 'MC_withVermelho', 'NV',\n",
       "       'NV_withDodson', 'SR', 'SR_withDodson'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf229da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "# bhv_names_to_ana = ['failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "bhv_names_to_ana = ['pull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# \n",
    "# for dodson only\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "# 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33946e",
   "metadata": {},
   "source": [
    "### sanity check plot; focus on bhv only\n",
    "#### look at the relationship among pull_trig_events_tgtname (the variable that aim to explore other's intention), otherpull_prob (aligned at self pull), selfpull_prob, and socialgaze accumulation related quantification to see which variables correlated with self gaze accumulation the most and identify the best variables for estimating intention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33c20a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/tmp/ipykernel_369431/3490758758.py:207: RuntimeWarning: Mean of empty slice\n",
      "  tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:221: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:223: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/tmp/ipykernel_369431/3490758758.py:207: RuntimeWarning: Mean of empty slice\n",
      "  tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:221: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:223: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/tmp/ipykernel_369431/3490758758.py:207: RuntimeWarning: Mean of empty slice\n",
      "  tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:221: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:223: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/tmp/ipykernel_369431/3490758758.py:207: RuntimeWarning: Mean of empty slice\n",
      "  tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:221: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:223: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/tmp/ipykernel_369431/3490758758.py:207: RuntimeWarning: Mean of empty slice\n",
      "  tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:221: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:223: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n",
      "Not enough data for multivariable regression on 20250430_NV, NV_withDodson, kanga, pull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/tmp/ipykernel_369431/3490758758.py:207: RuntimeWarning: Mean of empty slice\n",
      "  tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:221: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
      "/tmp/ipykernel_369431/3490758758.py:223: RuntimeWarning: Mean of empty slice\n",
      "  tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_369431/3490758758.py:132: RuntimeWarning: All-NaN axis encountered\n",
      "  reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no regression line\n",
      "no regression line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/scipy/stats/_stats_mstats_common.py:194: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean**2)\n",
      "/home/ws523/.conda/envs/DLC/lib/python3.8/site-packages/seaborn/categorical.py:166: FutureWarning: Setting a gradient palette using color= is deprecated and will be removed in version 0.13. Set `palette='dark:k'` for same effect.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAGoCAYAAABMlYN4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADT2UlEQVR4nOydeXhTZfbHv+/NvnXfd7o3bQnQUgFBWRVEHbB22MTRccVxVECHccafC4MILgwiOuAuiICyyY6glL1IgRa6pC2F7vuWJk2bNsn9/ZEWC3QnadLyfp4nz9Pmvve857733txzz3vecwjLsqBQKBQKhUKhUO40GGsrQKFQKBQKhUKhWANqCFMoFAqFQqFQ7kioIUyhUCgUCoVCuSOhhjCFQqFQKBQK5Y6EGsIUCoVCoVAolDsSaghTKBQKhUKhUO5IqCFsoxBCWEJIcOvfIkLIHkKIihDyk7V1o9g+hJBxhJAsa+thCQgh3xJClt3G/hpCSKA5daJ0DiEkkRDydOvf8wghv3TRdtBetxQKxTahhrAFIYSMJYScbjVgawghpwghI/sg6lEA7gCcWZZNMLOaNkOrgdPcaqjUEEIOE0LC+yjrbULI9+bWsZs+8wghk3vY9rpxYKa+r784AQDLsidYlg0zl/yb+uK3jm8OIaSh9bi/JoQEWKK/26GjcWZZVsqy7FVr6WQLEELsCCGrCSEFrffbldb/XSzZL8uym1iWva+dHv123VIoFEpHUEPYQhBC7ADsBfAJACcA3gDeAaDrgzh/ANksy+rNp6HN8j7LslIAPgAqAHzbWwGEEK65lbpJPiGE3Mn3zjYADwOYC8AegALAeQCTeivo5nN1p4+tpa/d1j74AH4FEAlgKgA7AGMAVAOIs3T/FAqFYlOwLEs/FvgAiAVQ102bvwLIBFAL4BAA/3bbWADBMBnPzQBaAGgAPNWBHGcAewDUAzgHYBmAk+22fwygsHX7eQDj2m2ra5WrAdDQ2m9A67YHAaS0tjkNYGgnx7EOwIc3ffczgEWtfy8BUAxADSALwKRO5HwLYFm7/6cD0PTgGN6GyTj7vnX7izeNWWpru0QA/wFwqlWXXwC4tJMzqvU46wCkAhjfblsigHdb920EENyB/nkAJrf+/QSAkwA+bD2/1wBMa932LgADgKZW/da2fh8O4DCAmtZx+vNNY/MpgH2tup8FENS67XjreWtolTcLwHgARe32j2g9hjoA6QAe7onsDo5xcuvx+3ZxXXsB2N16HFcAPNPFuXq6o7HtwVgsa/3bEaYXzkoAtdNN4+DTzTizbecPJkN+Q+v++QDeAMB0dw47Oe5/AshtHcMMADNv2v4MTPd72/YR7a6bJQAuwfSizIXpRSO99XwlAohoJ6fD+wkmIza5dVzLAazqRM+nW7dLuziW69eL3DRePb5eAEwBoASgArAWwDEAT7cfU0tftwAIgP/C9DKtah3bqNv9Xacf+qGfwfexugKD9QOTl6UawHcApgFwvGn7DJiMhIjWB98bAE63297+Yf02gO+76GtL60cMQA6TwdjeEH4MJmOZC2AxgDIAwg7kLG99OPEAjGh9iNwFgAPgLzA9sAUd7HdPa5+k9X9HmAwaLwBhrdu8WrcFoHMj61v8YeBIAfwA4ER3x9A6Pi2tY8oAEHU0Zq0P1FwAoa1tEgGsaN3m3Xq+HmiVMaX1f9d2+xbA5EXjAuB1oH8ebjSEW2AyfjgAFgAoaTdGiWg1Dlr/l7SO05Ot8kcAqAIQ2W5samAydrgANgHY0tH10vr/eLQaFK3n8wqAfwHgA5gIk+EQ1hPZNx3jCgDHurn2jwH4DIAQwDCYjMw2Y62jc3Xz2Nr3YCyWsSyL5wD/BNO9IwYge9Q0DrtYlgUL3MsFTrQf5w7urQ0wvbTJYLo2s9H6stndOezguBNguuYZmIy6BgCe7bYVAxgJk5EWjNYXX5iumxQAvq3jEdq675TWc/eP1vPHRxf3E4AzAOa3u39GdfF78V1n5+8XwEkEFLVdL7+axst0vQD3Ck3XR4fXCwAXmAzxR1t1XwhAjw4MYUtetwDuh+mF2aF1vCPazgX90A/90E/7zx07BWlpWJatBzAWph/6LwBUEkJ2E0LcW5s8B+A9lmUzWVPIw3IAwwgh/r3phxDCARAP4C2WZbUsy2bAZHy31+V7lmWrWZbVsyz7EQABTA/U9nJmwTTVHc+ybNvDfz3LsmdZljWwLPsdTN6qUR2ocaL1OMe1/v8ogDMsy5bA5JETAJATQngsy+axLJvbxSG9Sgipg+kBKIXpwdmTYzjDsuwulmWNLMs2diH/G5Zls1vb/AiToQaYDO39LMvub5VxGCbv2gPt9v2WZdn0Vh1auuijjXyWZb9gWdYA0znxhCnWuyMeBJDHsuw3rfIvANgO01i2sYNl2d9br5dN7XTvjlEwjeUKlmWbWZb9DSYv6pw+yHYGUNpZR4QQX5iu+yUsyzaxLJsC4EsA89s16+hcXR9bmKbruxsLAMA6wOFHYC4LfMUCdv8GQIDxIGQzgE/FJmO8M105MBmsr7Msq2ZZNg/ARzfp2uNzyLLsTyzLlrQe11YAOfgj1OBpmMJ+zrEmrrAsm99u9zUsyxa2jscsAPtYlj3cep19CJOBPAZd308tAIIJIS4sy2pYlk3q5NC7PIffAw9LAI8WIJoFXCcCkAG/Pm3y5H/qYHpp6+x6eQBABsuy21p1Xw3TS2tfuJ3rtsWkNsJhenHJZFm202OmUCh3LtQQtiCtP75PsCzrAyAKJm/R6tbN/gA+JoTUtRp+NTB5Lry7kkkI+Vfr4hYNIWQdAFeYvCGF7ZoV3rTPYkJIZuuivTqYPG4u7bYPh2kKcybLspXt9Fvcpl/rfr6tx3DzcbIweZnaHlBzYXoogWXZKwBegckTWEEI2UIIuUVGOz5kWdaBZVkPlmUfbnvId3cMNx9zF7R/KGthetC2HW/CTcc7FibDp7d93NIXy7La1j+lnbT1B3DXTf3PA+DRA927wwtAIcuyxnbf5ePGa62nsqtx45h01FcNy7LqLvrqaBzbf9eTsQAAECCHAU7KgClSoOAeACxg32QKNRpab/KsdoYLTJ7G9gZpp+PS3TkkhDxOCElpp3MU/rhGfWGajeiM9sfv1V6n1vNWCMC7m/vpKZi8yUpCyDlCyIOd9NXlOdwANFYBF7mm0KFDAPA4cPd5035Dy0xhCp1dL17tj6X1t6G3900bfb5uW43mtTCFTpQTQj5vXbdBoVAoN0AN4X6CZVklTFN5Ua1fFQJ4rtXoa/uIWJY93Y2c5axp1buUZdnnYZp21sO0uKwN37Y/CCHjYIop/DNM4RkOMMXMkdbtrgB2AniRZdmL7WQUAnj3Jv3ELMtu7kS1zQAebfVo3wWTB69N5x9Ylh0Lk4HDAljZ1THeTHfH0NbNTbvd/H93FALYeNPxSliWXXEbMrviZlmFMIUctO9fyrLsAjP0VQLA96ZFaH4wTdX3liMA4gghPp1sLwHgRAiRddFXR+PY/rvejMViFgj9BXhHA2Qeb/3S0HVfbVTB5DlsPwvTp3Fpve6/gCk+3bn1Gk3DH9doIYCgLkS017OkvU6EEALTPV0MdH4/sSybw7LsHABurd9tI4RIOujrCID7O9nW1r9vM3DdAC0EeM6mOOnuKMWNvz+k/f+95LauW5Zl17AsGwNTyE0ogNf6qAeFQhnEUEPYQhBCwlu9mD6t//vC5DFtm65cB+B1Qkhk63Z7QkivU6O1TtnuAPA2IUTcmm7s8XZNZDAZypUAuISQN2GKX25bob4dwKbWqdz2fAHgeULIXa0r+SWEkOk3GTjt9bjY2seXAA6xLFvX2kcYIWQiIUQA06KlRtxgp/SITo+hC8oBBPQiA8H3AB4ihNxPCOEQQoSEkPFdGHy3SzmA9rls9wIIJYTMJ4TwWj8jCSERfZTXnrMweUb/0Sp3PICHYPLi9wqWZY/AtIhtJyEkhhDCJYTICCHPE0L+yrJsIUwLDt9rHcOhMHkqN/Wimx6PxRBgyL1ATDQw7lNg1jut33NNcfmprqZrrsNxab13fgTwbusx+ANYBNO10FskMBmllQBACHkSf7z0Aqb74tXWMSOEkOAuwqB+BDCdEDKJEMKDKSZeB+B0V/cTIeQxQohrqwe1rlVWR/faRpgM8+2tv1MMIcS5dbbpgTxA4ws4LgYWZQLTEwHsAbhLTd7zVA9T3G1n7AMQSQh5pPX35SV04Mlvh0Wu29br5a7W8WuAaax6+7tDoVDuAKghbDnUMHlGzxJCGmAygNNgeqiBZdmdMHltthBC6lu3TetjXy/CFCpQBtNDbjP+SNN2CMABmBYB5cP0QGibqvSBKa73FfJHuIWGEOLHsmwyTHHCa2HyBF1Ba7xuF2yGKavAD+2+E8C0wKqqVT83mBa/9IaujqEz2gqPVBNCLnTXQasB96dW3Spb5b8Gy90jH8PkQa8lhKxpDSW4D8BsmDxhZTBdH4IeynsbwHet0/J/br+BZdlmmLIQTIPpPHwG4PHWWYq+8CiA/QC2wuSZT4MpS8qR1u1zYFrEVQLTbMNbrCnmukf0ZizigJVpwBUZ8MCLwM6prd8LTaEUL7qZYn6vj3MH3f0dJkPpKkwZIn4A8HVPdW2ncwZMfZ2BybiLhikLRtv2n2DKYvEDTL8Nu2BKq9iRrCyYYtY/gel8PQTgodbz2NX9NBVAOiFEA9P1NZtl2aYO5Otguk+VML3U1AP4HaYwjrP+wNWXgGfWmjJGXHoBAAvMH82y0wG8WNdFuAnLslUwLQxcAVMoRUj7ceiAt2GZ69YOppf5Wph+M6phirWmUCiUG2hbwU4ZRBBCVgLwYFn2L9bWhULpVwhhwbKk+4aUHkPHlEKhDGKoR3gQ0Dq9ObR1yjUOpqnondbWi0Lpd6jBZn7omFIolEGMxasYUfoFGUxhCV4w5f79CKbcqBQKhUKhUCiUTqChERQKhUKhUCiUOxIaGkGhUCgUCoVCuSMZlKERLi4ubEBAgLXVoFAo/UBeXh7EUmcQ5o9Q1madFkI+B87OzlbUbOBSUFAAZ4E9uMwfvhJtSxP0XBaubm5W1GxwcP78+SqWZV2trQeFQhmkhnBAQACSk5OtrQaFQrEwzc3N+OtTT2H0fc/CVLvBRHmREk7iRrz80t+tqN3A5Ym//AWvxT4GAZd//TtldR7SUIx/vtHb7IeUmyGE5HffikKh9Ac0NIJCsQKvLnoFj89/DOfPn7e2KgMatVoNoVB8gxEMADyeEPX16k72onSFTqeD0ciCz+Hd8L2YJ4RarbGSVhQKhWIZqCFMofQzRqMR5RWVuMtDioqKCmurM6BRq9UQCES3fM/lC6HRUKOtL2g0GkiEolteLiQ8ER1TCoUy6KCGMIXSz2i1WvC5XMg4gLq+3trqDGg0Gg24fOEt3/OoIdxn1Go1xLxbx1TEFUCjpWNKoVAGF4MyRphCsWXUajWkAh6kPAY1dbXWVmdAo9FowOXdWoWayxOiUdtpJWBKFzQ0NEDUgSEs5PKha26G0WgEw1AfykDg/Pnzblwu90sAUaCOL8qdiRFAml6vfzomJqbDKVhqCFMo/YxKpYJMwIWMz+BabY211RnQNDQ0gOF0ZAjzodM1UaOtD2g0Goi4t44pQxgI+HxoNBrY2dlZQTNKb+FyuV96eHhEuLq61jIMQ4sGUO44jEYjqayslJeVlX0J4OGO2tAnBIXSz6hUKki5DGQ8Durq6qytzoDGZAjzb/meEAY8Hh9ardYKWg1sGhoaIGRuHVMAEPGEdEwHFlGurq711Aim3KkwDMO6urqqYJoV6bhNP+pDoVAA1NTUwJ7Lwl7AQa1KZW11BjQaTQOYm7IbtMHjC9DY2NjPGg18tFotBKTjyUIhT0AN4YEFQ41gyp1O6z3Qqb1LDWEKpZ+pqa4yGcJ8DlTqBhiNRmurNGDRNDSAw+3Ye8njC9HQQOOEe4u2QQsB0/HLhZBLvewUCmVwQQ1hCqWfqSwrhaOACy5DIBHwUFtLF8z1FW2DFlxex4Ywl8tHU1NTP2s08NE2NNxQSKM9Ag6fetkpg5q4uLiw7du33xAEv3TpUrfHHnvMryf7v/LKK167du2SddfH8ePHxTd/v2bNGufHH3+8R/1QzAc1hCmUfqayogLOQg4AwFksQGVlpZU1GrhoGxs7DY1guDxqCPeBRm0jBJ2MKZ/h0jGlDGoSEhKqN2/e7NT+u+3btzs99thj3a5s1uv1WL16dcmMGTNoNZ8BBDWEKZR+prK6Bs5CUwyms4BBeXm5lTUauDQ1NYHL7dho43CoIdwXmhobb6kq1wY1hCmDnfnz59f++uuv9o2NjQQAsrKy+BUVFbyNGzc6RUVFRQQHB0cuXLjQq629t7d39KuvvuoZExMT9vXXXzvGx8cHfPPNN44A8Oqrr3pGRUVFhISERM6ZM8e/fRjct99+6zx8+PDwkJCQyKNHj97iHS4pKeHef//9QVFRURFRUVERv/zyi6QfDv+OhBrCFEo/olarYTAYIOWZbj0XnhFlpSVW1mrgotPpOvcIM1zodLp+1mjgo9PpwO8kRphH6JhSBjceHh4GhULRsH37dnsA+O6775wefvjh2lWrVhWnpaVlKpXK9FOnTsnOnj17vaSlUCg0nj9/PuvZZ5+9Ic7ttddeq0hLS8vMyclJb2xsZLZs2WLftk2r1TIXL15UrlmzJv/ZZ58dcrMezz33nO+iRYvK09LSMnfu3Jn7/PPPB1jwsO9oaB5hCqUfKS0thatUeL18rZuQi8yCfCtrNXBpaW4Gw+n4Z4wwHDQ3N/ezRgOfZl0zeOKOx5RH6JhSBj9//vOfa7Zu3er42GOP1e3YscPpyy+/zPvuu++cvv32Wxe9Xk8qKyt5qampwrvuuqsRAB5//PEOF3ocOHBAtmrVKo+mpiamrq6OK5fLGwGoAGDu3Lk1ADBt2jSNRqNhqqqqOO33PXXqlF1OTs51Y1uj0XBqa2sZR0dHurrazFjVECaETAXwMQAOgC9Zll3RQZvxAFYD4AGoYln23n5UkUIxK0VFRfAQ/vF75yHh4XBBsRU1Gtg0tzSDw3DRrGvCtStpkEjt4OMfatpIONR72Qd0zc3gSjnQtejwe84lSIViDA+MBABwCQe6JjqmlMHNvHnz6t544w3fkydPipuamhgXFxf92rVr3c+fP5/p6upqiI+PD2hqaro+oy6TyW4xTrVaLVm8eLH/2bNnM4KDg1sWLVrk1X6fNmdIZ/+zLIvk5ORMqVRK099ZGKuFRhBCOAA+BTANgBzAHEKI/KY2DgA+A/Awy7KRABL6W08KxZwU5F2Dp8D0u+b2wgdwE3FRVVtHvWx9pKVFD8LhIEd5EQXXlMi8/DtUtVUATEU1WlparKzhwKOlpQU8hovfcy7h4tU0nMj4HdfKCwEAXIZLr1XKoMfe3t44atQo9dNPPx3wyCOP1NTW1nJEIpHRycnJUFhYyE1MTLTvToZWq2UAwMPDQ69SqZg9e/Y4tt++efNmRwA4dOiQVCaTGZydnQ3tt48dO7Z+5cqVbm3/nz59WgSKRbCmRzgOwBWWZa8CACFkC4A/Acho12YugB0syxYAAMuyHdaJplAGBPX1YBOPwiv8jwXJXIbAVSZC1fbt8Jo+HaCla3uFQa8Hw3AgEJieEQzDXE+nZgqNoIZwb9HrW8AhHEhax5QQApFACADgMgwa6ZhS7gBmz55d85e//CVo8+bNV4cPH94UFRWlDQkJifTz89PFxMRoutvfxcXFMG/evEq5XB7p4+PTrFAobkhq7ujoaBg+fHi4RqPhfP7559du3v/zzz8vfPrpp/1CQ0PlBoOB3HXXXeoxY8YUmPMYKSasaQh7Ayhs938RgLtuahMKgEcISQQgA/Axy7IbOhJGCHkWwLMA4OdH0/BRbA9jbi7u370XdukeaHp0AgCAqVPjmaNHYXfgMBAVBURHW1nLgYXeYDKEA0OHws7eCSKJDBKp6WWCoTHCfUKv14PLMBgeGAlHqT1EAiE8HFwBAFyGgxY6ppQ7gMcff7zu8ccfP9/2//bt2/M6aldcXHy5/f/t261Zs6ZkzZo1t6yG/v3337M6kvXSSy9VA6gGAE9PT/2+ffuu9kl5Sq+wZtYI0sF3N8fCcAHEAJgO4H4A/0cICe1IGMuyn7MsG8uybKyrq6t5NaVQzECpmxtWJjwCY+QQOHyyDQDg8Mk21If4YtPiRdQI7gNGgx6EYcAwDNw8/SCz+2P2kTAc6PV6K2o3MNEbDOAwHBBCMMTd97oRDAAcwoGBjimFQhlEWNMjXATAt93/PgBufnMqgmmBXAOABkLIcQAKANn9oyKFYj6ys7PhbycAqtq9AxLARcxDTnaHDgJKNxgMRhDS8fs8IQz0ekOH2yidYzAYwHQypgxhoDfQMaVQKIMHa3qEzwEIIYQMIYTwAcwGsPumNj8DGEcI4RJCxDCFTmT2s54UilmoOPwLntq4Ffz0a6j7+6MAgLoXH4XHlXy8snETGpKSrKzhwIJlWbBsF4Yww0BvoN7L3mIwGsDpZEw5hKEeYQqFMqiwmkeYZVk9IeRFAIdgSp/2Ncuy6YSQ51u3r2NZNpMQchDAJQBGmFKspVlLZwrldjhfUY6YRyfBLvqP3OlGBxnUTz2MX3/NQnRjI0ZYUb+BhsFgAMMwt6QdaoMQBgYDXdjVW4xGY+ceYYaBwUDTmFIolMGDVfMIsyy7H8D+m75bd9P/HwD4oD/1olDMTUVFBepZFrKogA6380K9cOlaLkZMmNC/ig1gjMbOvcGAKduBgU7j9xqD0Qimk5cLBgRGIx1TCoUyeKAllimUfuDy5csIcRDe4L2s+Oy163+HOQhwOSXVGqoNWEyGcMcGG9DmEabey95iCjfpxBCmLxcUCmWQQUssUyj9QMq53xFh1/l7p7eUh4YGFcrLy+Hu7t6Pmg1c2kIjOoMQ6r3sC4YuQiMIYWA00peLgcpn337lX91QLzSXPGeJXdMLTzxFa8RTBjTUEKZQLExLSwsylErMHOHSaRuGEEQ4CnHhwgVMmzatH7UbuBiNRpBuDWFqtPUG0wJEFkyH2S1N1ykd04FLdUO9sC7QWWo2gVerzSaqjffff99VLBYbX3zxxdsW7u3tHZ2cnJzp6elplhWe1tJt0aJFXlKp1LB06dLyV155xWv8+PHqGTNmqDtqu3HjRge5XN4UExPTdLs63ilQQ5hCsTBpaWnwshNByuN02S7SnoszZ05RQ7iHdBcjDMLAQNOn9QqWZUEI6SI0goGRhptQLMg//vGPSmvr0Bm2oNvq1atvKdDRnl27djno9XoVNYR7Do0RplAsTNKpkxhq13ksaxvhjkLkFRSivr6+H7Qa+HQfGsHAaLy5Rg+lKwytxTQ6g3qEKX1h8uTJQZGRkRHBwcGRH374oQsAiMXi4X//+9+9w8LC5AqFIrywsJALmLyfb775pjsAxMXFhT311FO+sbGxYYGBgZHHjh0T33fffUH+/v5RL730kldX8vuql63otmTJEo+AgICoMWPGhObk5Ajavo+Pjw/45ptvHAHghRde8A4KCooMDQ2VP/vssz6HDx+WHDlyxOGNN97wCQ8Pl6enpws6kt1T3T/77DOn6OjoiPDwcPncuXP92woUzZs3zy8qKioiODg4cuHChdfbe3t7Ry9cuNBLLpdHhIaGyi9evGi2MBxLQg1hCsWCNDc348KFCxjqIuq2LZ9DEOEsxtmzNJ9wT+hJjDCtLNc7uhtTDi2oQekDmzZtyktPT89MSUnJWL9+vXtZWRmnsbGRGT16tCYrKytj9OjRmk8++aTDkrB8Pt+YnJyc9eSTT1YmJCQEf/HFFwVKpTJ969atLmVlZZzO5PdVLwCwtm4nTpwQ79y50+ny5csZe/fuvZKamiq5uU15eTln//79jjk5OenZ2dkZy5cvL50yZUrD5MmT65YtW1akVCozIiMjdZ310Z3uFy5cEG7bts0pOTlZqVQqMxiGYdetW+cMAKtWrSpOS0vLVCqV6adOnZKdPXv2+gPOxcVFn5GRkfnXv/61csWKFQNiwQs1hCkUC5KSkgIvmQCOgp5FIcU4cnHy6FELazU4MBqNQBdZI0w5b6nR1htMVeW6GFNCx5TSe1auXOkeFhYmj4mJiSgrK+Olp6cLeTweO3v2bBUAxMTENOTn5/M72nfmzJl1AKBQKBqDg4Mb/f39W0QiEevr66u7evUqvzP5fdULAKyt29GjR6UPPPBAnUwmMzo5ORnvu+++upvbODk5GQQCgXH27Nn+3333nYNUKu3VVE13uh88eFCWlpYmVigUEeHh4fKTJ0/aXb16VQAA3333nZNcLo+Qy+XynJwcYWpq6vVjmjt3bi0AxMXFaQsLCzv0SNsaNEaYQrEgJ47+ihiHHjknAJjCI7ZcLaPZI3pAS0sLOEznP2GE4VCPcC/R6/XgdhEawSEMDLRaH6UX7N27V3bs2DFZcnKyUiaTGePi4sIaGxsZLpfLts0+cLlc6PX6Dt/AhEIhC5hebAUCwfVYJ4ZhoNfrSWfy+6pXqz5W1Q1Al6khAYDH4yElJSVz9+7ddlu2bHH83//+55aUlJTdE9k90Z1lWZKQkFD96aefFrffT6lU8teuXet+/vz5TFdXV0N8fHxAU1MTc7NcLpfLdjZutgb1CFMoFqK2thaZyiwM60FYRBschmCEqxCJv/1qQc0GB3q9Hgyni3hWagj3Gr1eDy6n85cLDsOBni5AHLA4S+yaHK5Wa8z1cZbYdbsgq66ujmNvb2+QyWTGixcvCjua5r8d+irf0nrdTh8TJ07U7Nu3z0Gj0ZDa2lrm8OHDDje3UalUTE1NDWfWrFmqdevWFWZmZooBQCqVGurr62/btps6dWr93r17HYuLi7mAKRQjOzubX1tbyxGJREYnJydDYWEhNzEx0f52+7I21CNMoViIY4mJULiIIOT27jdptKsQ/zv6G+IT/gwul96indHS0tJ1+jSGA30LLbHcG5qbm7tcLMdlOGjR0zEdqFgj5298fLzq888/dw0NDZUHBQU1KRSKBluQb2m9bqePsWPHamfOnFkTFRUV6e3trYuLi9Pc3Kauro7z4IMPBut0OgIAy5YtKwSAefPm1SxYsCBg3bp17tu2bcvtKk64K2JiYpreeOON4kmTJoUajUbweDx2zZo1BZMmTWqIiorShoSERPr5+eliYmJu0W2gQVh28K2qjo2NZZOTk62tBuUOxmAw4JW//w1/CRDCT9ZheFmXfJKhwvR5T2LUqFEW0G5wkJ6ejs+/2oiwEQ92uL1RU4dr6b9g7Sdr+lmzgUt+fj7WrFyFBdGPdLhd29KENSk/4suvv+pnzQYXhJDzLMvGWrqf1NTUPIVCUWXpfigUWyc1NdVFoVAEdLSNhkZQKBbgwoULsGPYPhnBADDOlYcDe342s1aDC51OB6aLGGGGw0VLc3M/ajTw0el04HF4nW7nMVw0Uy87hUIZRNB5VwrFAuzfvQvjXDs3KLoj2kWEny+W4+rVqwgMDDSjZoOHpqYmMF3Fs3J50FFDuFfodDrwuxhTLsOB0Wg0xRLTsB3KAKCsrIwzfvz4sJu/T0xMzPLw8LBqwLuldZs/f77fuXPnbqgkuGDBgvKXX37Z/CUBBzD0l4xCMTM5OTmoKi+DYphTn2VwCMG97kLs3rEdr7z6mhm1GzxotVownM497hwuDy3NOhiNxi5z41L+QKvVQtDFmBJCIOQL0NjYCJlM1o+aUSh9w8PDw6BUKjOsrUdHWFq3jRs3FlhK9mCCPh0oFDOze8c2jHcXgNNN+pvuGO0hRkZGBkpLS82k2eBCq9WCdDGNTwgDHo+PxsbGftRqYKPVaiFkug7nEfIF0Gq1/aQRhUKhWBZqCFMoZiQ/Px852dm4y11827IEHAZjPUTYtX2bGTQbfNTX14PThfcSAPgCETSaAb+oud9Qq9UQMl2H9Ih5QjqmFApl0EANYQrFjGzbuhkTPUXgc8xza93jKcbFCxdQVlZmFnmDidpaFXiCrl84BEIx6uvr+0mjgU99nQoSbteFryQ8ER1TCoUyaKAxwhSKmbh27RquZGdh1jAXs8kUc01e4e1bt+BvL79iNrmDgTqVCnyRX5dtuHxqtPWGutpauPO6LgAj4QqhUqn6SSOKOdn85f/8W+qqe1R+uCfwHJyb5jy9oN9zE1Mo5oQawhSKmdi88TtM8RSDzzFvVcnxnmIsT0lFfn4+/P39zSp7IFNbWwsPx/Au23B5ItTU1PSTRgOfmupaBAuDumwjYYR0TAcoLXXVwqmSemn3LXvGwbq+7VdVVcX58ssvnf75z39WAqZyxx999JH70aNHr5hLN0r/smbNGufk5GTJhg0bBtwCPRoaQaGYgcuXL6O8uBCjPbqPDS5TNWDnxSs4nVvSI9lCLoPJXiJs3vjd7ao5qKitrYFA1HXmAi5fjMpKWk+gp1RXV8NB0PWYOgikqCyv6CeNKIOR6upqzldffeVmLnktNLc15TaghjCFcpsYDAZs/OZrPOgjAofp3ht8Lq8cZfUNSC2qRJWmZxkNxnhIUFqYj9TU1NtVd1Cg1Wph0OvB5XU9yysQyVBGjbYeYTQaUVtfB3tB1w5De4EUleWV/aQVZTDw9ttvu4eEhESGhIRELl261G3x4sU+hYWFgvDwcPlzzz3nAwANDQ2cqVOnBg4ZMiTy4YcfHmI0GgEAJ06cEI8cOTIsMjIyYuzYsSH5+fk8AIiLiwt78cUXvUeOHBm2bNky9476jY+PD5g3b57fXXfdFerj4xO9b98+aUJCQkBgYGBkfHx8QFu7efPm+UVFRUUEBwdHLly40Kvt+xdeeME7KCgoMjQ0VP7ss8/6AMDXX3/tGBISEhkWFiaPjY29JQdwG2vWrHGePHly0MSJE4O9vb2jly9f7vr222+7R0REyBUKRXh5eTkHAE6fPi1SKBThoaGh8ilTpgRVVlZyLly4IIyOjo5ok5WVlcUPDQ2VdzUeHbFs2TK3Nv0ffPDBQABYtGiR14wZM4aMGjUq1N/fP+qjjz66Hsv3f//3f+5RUVERoaGh8vbj8NlnnzlFR0dHhIeHy+fOneuv1+sBAB9//LFzQEBA1MiRI8NOnz7d5Q9HT8/Fjh077IYNGxYul8sjpk2bFqhSqRgAePXVVz2joqIiQkJCIufMmePfdn3ExcWFLViwwDs6OjoiICAg6uDBg72e8aCGMIVymxz97TcIW7QY6tx1bGUb3o6m+9ROKICdsGeV57gMwZ98xdj4zVcwGKyaA94mqKiogNTOAaSbFHVCsR1daNhDampqIBYIweuioAYAOAntUFFJXy4oPePEiRPiH374wfn8+fOZycnJmRs2bHB9/fXXS319fXVKpTJj/fr1RQCQmZkp+vTTTwuvXLmSXlBQIDh8+LBUp9ORl156ye/nn3/OTU9Pz/zLX/5S9eqrr3q3ya6rq+OcO3cu65133invrH+VSsU9c+ZM9ooVKwpnzZoV8tprr5Xn5OSkK5VK0enTp0UAsGrVquK0tLRMpVKZfurUKdnZs2dF5eXlnP379zvm5OSkZ2dnZyxfvrwUAFasWOH5yy+/ZGdlZWUcPHiwy1CO7Oxs0fbt26+eO3cu87333vMWi8XGzMzMjNjY2Ib169c7A8ATTzwxZPny5UXZ2dkZkZGRjUuWLPEaMWJEU0tLC8nIyOADwIYNG5xmzJhR29143MyaNWs80tLSMrKzszO+/fbb67HcmZmZoiNHjuQkJSUpP/jgA6+8vDzejh077K5cuSK8dOlSZmZmZkZKSor4wIED0gsXLgi3bdvmlJycrFQqlRkMw7Dr1q1zzs/P561YscLr9OnTyhMnTmRnZ2d3+wDs7lyUlpZyly9f7nn8+PHsjIyMzBEjRmj/85//uAPAa6+9VpGWlpaZk5OT3tjYyGzZssW+Ta5eryeXL1/OXLlyZeHSpUu9OtegY2iMMIVyG2g0Gmz7cSueDZV2a5S1McLPDcGu9hDxueBxOD3uK9JJiBMV9fjl0CFMe+CBvqo8KCgpKYFI4thtO5HEERUV5WBZtsfn506lpKQErpLui8A4CmWoq1ehubkZfH7fSohT7hwSExOlDzzwQJ2dnZ0RAKZPn1579OjRW+JvoqOjG4KCgloAIDIyUpubm8t3cnLS5+TkiCZOnBgKmGYtXF1dr8dBzJkzp9tg9enTp9cxDIMRI0ZonZ2dW+Li4hoBIDQ0tDE3N1cwZsyYxu+++87p22+/ddHr9aSyspKXmpoqHDFiRKNAIDDOnj3bf/r06apZs2apACA2NlYzb968gPj4+Np58+bVdtX3mDFj1I6OjkZHR0ejVCo1JCQk1LUeq/bSpUvi6upqjlqt5kyfPl0DAM8880x1QkJCIADMmDGj5vvvv3davnx52c6dOx23bt169dKlS4KuxuNmwsLCGmfOnDnk4Ycfrps3b15d2/fTpk2rk0qlrFQq1Y8ePbr+xIkTkhMnTkiPHz9uJ5fL5QCg1WoZpVIpvHjxIklLSxMrFIoIAGhqamLc3Nz0x48fl4waNUrt5eWlB4BHHnmkJjs7u8spuu7ORX5+Pj83N1cYFxcXDgAtLS0kJiZGAwAHDhyQrVq1yqOpqYmpq6vjyuXyRgAqAEhISKhtHe+G1157rdc/StQQplBug60//IChjjz4SHt379mJBL3uixCCmf5irN2+DaNGj4ajY/eG4GClqLgYPIFdt+14fCEYhoO6uro7erx6QnFxMZx7MKYchgMnqT3Kysrg59d11g4KhWXZHrUTCATXG3I4HOj1esKyLAkODm5MSUlRdrSPTCYzdidXKBSybTL5fP71PhiGgV6vJ0qlkr927Vr38+fPZ7q6uhri4+MDmpqaGB6Ph5SUlMzdu3fbbdmyxfF///ufW1JSUvYPP/xQ8Ntvv0l2795tP2zYsMiUlJT0zsoh39xfmy5tfXel9/z582sTEhICZ8+eXUsIQXR0tO73338XdTUeN3P06NGcAwcOyHbt2uXw/vvve+Xk5KQBuMUpQAgBy7J45ZVXSl977bUbFlW8++67bgkJCdWffvppcfvvN27c6NBb50J354LD4bBjx46t37Nnz7X2+2m1WrJ48WL/s2fPZgQHB7csWrTIq6mpiblZLpfLhcFg6LXHg4ZGUCh95OrVqzh39gwe8JH0W58eYh7uchXg++++7bc+bZGrV69BJO2ZYSuzd0ZBwYBbyNzvFFzLg5vAoUdt3cROdEwHIDwH56aDDXYac314Ds5N3fU5ceJEzf79+x3UajVTX1/P7N+/3/Hee+/VNDQ0dGt/DB06tKmmpoZ75MgRCQDodDqSnJxstvRvAFBbW8sRiURGJycnQ2FhITcxMdEeAFQqFVNTU8OZNWuWat26dYWZmZliAEhPTxdMnDixYfXq1SWOjo76q1ev9nlaxNnZ2WBnZ2doi2v96quvnEePHq0BgMjISB3DMHjzzTe9Zs6cWQP0bjwMBgNyc3P5Dz30kPqzzz4rUqvVHJVKxQGAAwcOOGi1WlJWVsZJSkqSjR07tmHatGn1GzdudGmLyb127RqvuLiYO3Xq1Pq9e/c6FhcXcwGgvLyck52dzb/nnnsakpKSZGVlZRydTkd27tx5256G8ePHNyQnJ0vT0tIEAKBWq5lLly4JtFotAwAeHh56lUrF7Nmzx6xeDeoRplD6gF6vx+effYqHfMUQ8/r3ffI+Hwnev5SGS5cuYejQof3at62Qn5eP4GGRPWorlDjj6tVrUCgUFtZqYHPt6jXc7xrbo7YeAkdcy72KsWPHWlgrijmxRs7fsWPHaufOnVs9YsSICACYP39+5bhx47QxMTGakJCQyIkTJ6oeeuihDhNTC4VCdsuWLbkvvfSSn1qt5hgMBrJgwYLy2NjYbg3wnjJ69OjGqKgobUhISKSfn5+ubSq+rq6O8+CDDwbrdDoCAMuWLSsEgIULF/rk5eUJWJYlY8eOrR81atRt1XD/5ptvri1YsMD/pZdeYvz8/HSbN2/Oa9v2yCOP1PznP//xWblyZTHQu/HQ6/Vk7ty5Q9RqNYdlWfLcc8+Vu7i4GABg+PDhDZMmTQopKSnhv/rqq6UBAQEtAQEBLenp6cKRI0eGA4BYLDZu2rTpWkxMTNMbb7xRPGnSpFCj0Qgej8euWbOmYNKkSQ1LliwpGTVqVISrq2vL0KFDtX3xxrbHy8tLv379+rzZs2cHNjc3EwB46623iocOHaqaN29epVwuj/Tx8WlWKBQNt9PPzZCeTlsMJGJjY9nk5GRrq0EZxOzZ/TMuHtmP58LtrBJ7mlHTiJ0lBry/6r8QCHofZjGQqa+vx0svvYyRk/7ao7GvLMmBENVY8o9X+0G7gUlLSwuefuppLImbDz6n6xLLAHClthBntTl4a9k7/aDd4IMQcp5l2Z69ddwGqampeQqFguYPpFxn0aJFXlKp1LB06dJOFxgORlJTU10UCkVAR9toaASF0ktKS0uxe9cuPBogsdoCLLmTCH5CI37cstkq/VuTnJwcODh79njsZQ5uyM290uNYxTuRa9euwU3m1CMjGAC8pW7IK8xHWxolCoVCGajQ0AgKpRcYjUas/2wt7vMWwUVk3dtnpr8E7x8/hrtGj0FoaKhVdelPMjMzIZL1PBe/QGQHo5FFRUUF3N07TDd6x6PMVMJP0vMxFfEEcJLYIy8vD8HBwRbUjELpniVLlnj8/PPPN6Q8+dOf/lSzcuVKi+dO3L59u92///1vn/bf+fr66g4fPpxr6b7bmD9/vt+5c+duyJ+7YMGC8pdffrn65rarVq3qWSWnPmLNc9FXaGgEhdILDuzfj1P7d+FFuT0YG0jHlVKpxcFqghUffHTHpLL65+v/hp3HUDg4d5o+8xZyL/+Kh6bdi/Hjx1tOsQHMymXvIdzogUjXwB7vs+/aKQTdMxQPPvigBTUbnNDQCAqlf6GhERSKGSgpKcGObT9hTqDEJoxgABjmKoYXpwVbfvje2qr0CxqNBqWlJbBz6J1nV+LgiYsptCpfR7S0tCDrSjaGOPQuD/0QmRdSz6dYRikKhULpJ6ghTKH0AIPBgM/WfIypPmK4inoWR9lfxAdIcObkSaSnp1tbFYuTlpYGJxdvMN1UP7sZRxdfpF2+jLaynJQ/yM7OhpvMCeJuylXfTKCDF65cvQKdTmchzSgUCsXyUEOYQukBO7dvB19bh7s9xNZW5RYkPA5mDZFg3dpP0NBg1qwyNsf58xcg6aXnEgAEIhn4QgmuXr1qAa0GNhfPX0CgtPdjKuQK4GXvhoyMDAtoRaFQKP0DXSxHoXRDTk4Ojhw6gMXRTjYTEnEzcicRMlV6fP3F5/j7KwutrY5FMBgMuHDxIiLjZvZpf3tnP5w9+ztd3NUOlmVx7vdziPe/p0/7h8p88HvSWQwfPtzMmlEswRdfbfBX1WvNVpDC3k7c9MxTj/d7bmIKxZxQQ5hC6YLGxkZ8+vFqxAdIYS/gWFudLnnIT4pVaek4efIExo4dZ211zI5SqYRQJINQ3H0Z4I5wch+CM0nHMHfuHKulvbM1CgoKYGhugafEpU/7y12G4Ivzu2EwGMDh2Pb9QQFU9Vqh2DVK2n3LHsqrTDOXqF6xd+9emUAgME6ZMqUBAN5//31XsVhsfPHFF2/JktAXTp8+LSosLOTPmjVLBQCbNm2yT09PFy1fvtxmMx9Q+g41hCmULvj6i88RJDJC4dJ/ZZT7Cp9D8FiQFOu++QYhIaGDLlXY6dNnYO/i3+f9JXauaG5uQX5+PgICAsyn2AAm6UwSIhwD+vxi4Ci0g71AiszMTERFRZlZO8pgw2g0gmXZ235p+u2332RSqdTQZgj/4x//qDSLgq0kJyeLk5OTJW2G8Lx581QAOqx+Rxn40BhhCqUTTp48gSsZlzHD3/aN4DZ8pHxM9hJi7epVg6rYgV6vR9LZs3DxDOmzDEIInNyDcPzECTNqNnBhWRanTpxEtEvQbcmJchyCE8eOm0krymAjKyuLHxgYGPnYY4/5RUZGyv/xj394RkVFRYSGhsoXLlx4PTh98uTJQZGRkRHBwcGRH3744fUpim3bttnJ5fKIsLAw+ejRo0OzsrL4GzZscF23bp17eHi4/ODBg9JFixZ5vfnmm+6AyZurUCjCQ0ND5VOmTAmqrKzkAEBcXFzYggULvKOjoyMCAgKiDh482KFnvKmpibz33ntee/bscQwPD5d/8cUXjmvWrHF+/PHH/QAgPj4+YN68eX533XVXqI+PT/S+ffukCQkJAYGBgZHx8fEBbXJ27NhhN2zYsHC5XB4xbdq0QJVKRe0tG4WeGAqlA8rKyrDhm28wP0gKAWdg3Sb3ekog0Nbhp61brK2K2UhJSYFE6tjnsIg2XLxCcPLkKZo9AkBWVhY4RtLnsIg2hroGIzn5PM0eQemUvLw84ZNPPln97rvvFpWUlPAvXbqUmZmZmZGSkiI+cOCAFAA2bdqUl56enpmSkpKxfv1697KyMk5JSQn3xRdfDNixY0duVlZWxq5du3LDwsKaH3/88crnn3++XKlUZkydOlXTvq8nnnhiyPLly4uys7MzIiMjG5csWXLd2Nbr9eTy5cuZK1euLFy6dGmHK0SFQiH7+uuvlzz00EO1SqUy45lnnqm9uY1KpeKeOXMme8WKFYWzZs0Kee2118pzcnLSlUql6PTp06LS0lLu8uXLPY8fP56dkZGROWLECO1//vOfwTVFN4gYWE94CqUf0Ov1+OS/q3Cftwg+0oFXpIIQgjmBUpw4+hsuXbpkbXXMwq+/JcLB7fYXuYmlTuAJxLh8+bIZtBrYHDuaiGjHwNuOl5bxJfC1d8O5c+fMpBllsOHp6dk8adKkhoMHD9odP37cTi6XyyMjI+W5ublCpVIpBICVK1e6h4WFyWNiYiLKysp46enpwsTERElcXJw6PDy8GQDc3d0NXfVTXV3NUavVnOnTp2sA4JlnnqlOSkq67vlNSEioBYAxY8Y0FBUV9fnHffr06XUMw2DEiBFaZ2fnlri4uEYOh4PQ0NDG3NxcQWJioiQ3N1cYFxcXHh4eLt+yZYtzQUHBwHuY3CHQGGEK5Sa2bPoeEl09xvnLrK1Kn5HxOZgbJMX/1n6CFR98CHt7e2ur1Gdqa2uhzMzA8HsfM4s8J48wHDx0GAqFwizyBiJarRa///47XhyWYBZ5w51CceTQYYwdO9Ys8iiDC7FYbARM4TivvPJK6WuvvXZDtbu9e/fKjh07JktOTlbKZDJjXFxcWGNjI8OyrFkXtgqFQhYAuFwuDAZDnwW3yeFwOODz+dfL8zIMA71eTzgcDjt27Nj6PXv2XLt9rSmWhnqEKZR2XLhwAWdOHsecQIlZf4A1umakFFaiWtOIZr0BaSVVKK+3bM7fUAch4py5+HTN6gEdCvDbb0fh4hkMLtc8DhVXrxAoMzNQW3vLjOcdw6lTpxDo6AMZ3zx5scOd/VFaUori4mKzyKNYBns7cZO2Mk1jro+9nbipN/1PmzatfuPGjS5t8bLXrl3jFRcXc+vq6jj29vYGmUxmvHjxojA1NVUCABMmTGg4e/asTKlU8gGgvLycAwAymcygVqtvWXHn7OxssLOzM7TF/3711VfOo0eP1tzcrjvs7OwMGo2mz/bR+PHjG5KTk6VpaWkCAFCr1cylS5cEfZVHsSxW9QgTQqYC+BgAB8CXLMuu6KTdSABJAGaxLLutH1Wk3EHU1tbi8/99hr8EySDhmTcV1MG0PFRqGnGRy4WPoxRXKuvAIQRz4sIhE1puxmyqrxSfZhRjz88/408z+5Z/15oYDAYc+fVXDImcbDaZXC4fLp7B+PXX3/Doo/FmkztQYFkWvxw4hEku5sv9y2E4GO4WisOHfsETf33SbHIp5sXaOX8feeSR+vT0dOHIkSPDAZOneNOmTdfi4+NVn3/+uWtoaKg8KCioSaFQNACAl5eXfs2aNXkzZ84MNhqNcHZ2bjl9+nROfHx83aOPPhp04MABh9WrVxe07+Obb765tmDBAv+XXnqJ8fPz023evDmvt3pOmzZN/eGHH3qGh4fLFy9eXNrb/b28vPTr16/Pmz17dmBzczMBgLfeeqt46NChNJDeBiEsy3bfyhIdE8IBkA1gCoAiAOcAzGFZNqODdocBNAH4uieGcGxsLJucnGx+pSmDFqPRiHffeQv+LdWY6mu2NJvX+el8Nqo0jRBwOfC0lyKvWgWGEMwZGQY7kWUdBbVNeqxKq8XiJa8jNDTUon2Zm7Nnz+L7zdsREfuwWeU2qKuRc3E/Pv10LXg82yqZbWnS09PxxSfr8Leh8Wad9VDpNPjfpR1Ys/YTiMW2V4HRliCEnGdZNtbS/aSmpuYpFIqq7ltSKIOb1NRUF4VCEdDRNmuGRsQBuMKy7FWWZZsBbAHwpw7a/R3AdgAV/akc5c5i147t0NeU4z4fy6RKuz8yAHcN8cRDQwMxPswHIwM8MC1qiMWNYABwFHKRECDF2tX/hUbT61lCq7J79164ekeaXa5E5gyhxBFnzpwxu2xbZ//ufbjLTW72oiL2AimCHHyQmJhoVrkUCoViSaxpCHsDKGz3f1Hrd9chhHgDmAlgXXfCCCHPEkKSCSHJlZVmza1NGeQolUr8cmA/HguSWayEsp2QjxF+bnCViSHicRHr7w4/p/5bjDfURYQIKfDFus9grVmg3nLlyhVUVFbB2X2IReS7+URh9569A2Y8zEFpaSmyc7KhcOt7PuauuMtdjgN798Ng6HJxP4ViE2zfvt0uPDxc3v4zZcqU20usTRlwWNMQ7sjiuPmJtBrAEpZlu/1VZVn2c5ZlY1mWjXV1dTWHfpQ7AI1Gg08/Xo1ZQ2y/hPLt8rC/FCW52fjtt1+trUqP2LFzF9z9okEYy/xMObr5Q9PQdEelUtvz826MdI8An2OZcBA/Ow/IOCIkJSVZRD6FYk7i4+PrlUplRvvP4cOHc62tF6V/saYhXATAt93/PgBKbmoTC2ALISQPwKMAPiOEzOgX7SiDHpZl8cW6zxApI4h0FvVr3wYrZHHgMQTzg2XYsmkTioqK+r3/3lBSUgKlMgvuPhEW64MQAne/odixY5fF+rAlamtrcfbsWdzlYf5Qk/bc7R6N3Tt23VGedgqFMnCxpiF8DkAIIWQIIYQPYDaA3e0bsCw7hGXZAJZlAwBsA/ACy7K7+l1TyqAkMTERxbnZeMjf/IvjuiKztBpfnkzD9gs5aDH0r0HsIeZhuq8Ya1f/Fy0tLf3ad2/YtetnuPvIweFadiGbq1cIiktKceXKFYv2Ywvs37sPCtcQSPiWfekLcfJDi1aHlJQUi/ZDoVAo5sBqhjDLsnoALwI4BCATwI8sy6YTQp4nhDxvLb0odwalpaXYvHED5gfJwGMsExfcGTkVdTCyLCrUWqga+z+bzmh3MewNDfhxy+Z+77snVFZW4lxyMjz8oy3eF8Nw4OGvwI8/De6sjPX19Ug8moi7vYZavC+GEIzzVGDHj9uoV5hCodg8Vs0jzLLsfgD7b/quw4VxLMs+0R86UQY/BoMBn33yMaZ4i+Ap6f/UWdHeLqjT6uBuJ4aTRNjv/RNCMCtAgg+OHcWI2JGIiLBc+EFf2LFzF9x95ODx+2ds3H3CkXJyM65du4YhQyyzMM/a7N+3HxHOQ2Av6J/Zj0iXQCReuoi0tDRER1v+hYbSMzZ9ucFfV9dgthtL4CBpmve0dXMTUyi3Cy2xTLnj2LP7Z3A0tRgXbmeV/oe42GOIi3VLHkv5HPw5QIp1az/Big8/gkjUvzHSnVFVVYWzSUlQ3D273/pkOFx4+A/F1h9/wj+X/KPf+u0v1Go1fj1yBM9GdZSd0jIwhME9ngps2/oToqKizJ6qjdI3dHUNwnskcrO9DR2vy+hye1VVFefLL790+uc//1mZl5fHe/75530PHjx41Vz9d8fx48fFX3/9tfO3335b2H3rvrNmzRrn5ORkyYYNGwq6b/0HWVlZ/KNHj0qff/75Gkvp1hsWLVrkJZVKDUuXLi23ti79CS2xTLmjKCwsxP49ezBniMRiqdIGCpHOIgSJWHy/4Vtrq3Kdbdt3wM1HDp6gfw1zd99IXLmSi6tX++0Z3W/s3bMXEU5D4Cjs3xe/aNdg1FXVIC0trV/7pdgO1dXVnK+++soNAAICAlr60wgGgHvuuUdraSP4dsjJyRFs3brVqaNtll7DYTQaaZrDVqghTLljMBgMWLf2E0z3EcNRSCdDAOBP/hKknDuH9PR0a6uCiooKnD17Fp4Bij7t39SohV7ft4cHh8OFZ8AwbNnyY5/2t1Xq6+vx65FfcY9X38ZU06SFrqVvcewMYTDeczi2btpMY4XvUBYvXuxTWFgoCA8Pl0+bNi0wJCQkEjB5UCdPnhw0ceLEYG9v7+jly5e7vv322+4RERFyhUIRXl5ezgGA9PR0wbhx40IiIyMjYmJiwi5evNhpWMfXX3/tGBISEhkWFiaPjY0NA4C9e/fKJkyYEAyYvJ0JCQkBcXFxYT4+PtHLli1za9t37dq1zqGhofKwsDD5jBkzhgBASUkJ9/777w+KioqKiIqKivjll196VG3phx9+sB86dGh4RESEfMyYMaGFhYVcANi3b5+0LVdxRESEvLa2lvn3v//tnZycLA0PD5e/8847bmvWrHGeNm1a4MSJE4PHjRsXWl9fzyQkJARERUVFREREyL///nsHALj33nuDz549KwKAiIgI+auvvuoJAC+//LLXqlWrXFQqFTN69OhQuVweERoaen2/rKwsfmBgYORjjz3mFxkZKc/NzeUvWbLEIyAgIGrMmDGhOTk5lq/wZINQQ5hyx3Bg/35wG1UY5UHLv7Yh4jKID5Dgi/99Bp2u/xfutefHH7fBwzeqT7HBRQU5OPHrDpxO3ANdU2Of+nf3keNaXh5ycnL6tL8t8vPOnxHtEggHYe+Lt+SU5uGbX3/Et0e3o66hvk/9R7oGokGloRkk7lA++uijIl9fX51SqcxYvXr1DTkbs7OzRdu3b7967ty5zPfee89bLBYbMzMzM2JjYxvWr1/vDABPP/20/2effVaQnp6e+cEHHxQtWLDAr7O+VqxY4fnLL79kZ2VlZRw8eLDDNDBXrlwRHjt2LPvcuXOZH374oZdOpyPJycnCDz/80PPYsWPZWVlZGevXry8AgOeee8530aJF5WlpaZk7d+7Mff755wN6csxTpkzRpKSkKDMzMzMeffTRmqVLl3q0joXHmjVr8pVKZUZSUpJSKpUa33333eLY2FiNUqnMeOuttyoA4MKFC9LNmzdfS0pKyv7Xv/7lOWHChPq0tLTMEydOZL3xxhs+9fX1zN1336357bffpDU1NQyHw2GTkpKkAJCUlCSdNGmSWiwWG/ft23clIyMj89ixY9n/+te/fIytKTvz8vKETz75ZHVmZmZGeXk5d+fOnU6XL1/O2Lt375XU1FTLlFa1cahbjHJHUFVVhd07d+DlKEcar3gTUc4inK+px64d2zFrzlyr6FBcXIwLFy9g2Ng5fdq/pqoMAKBr0qJBo4JA2PvQCobDgeeQEdi0aTPefvvNPulhS9TU1ODYsUS8MDS+T/sXVZWCZVnomnWoVFXDQdL70AqGMJjgNQJbNm2GQqEAY6HiKJSBx5gxY9SOjo5GR0dHo1QqNSQkJNQBQHR0tPbSpUtilUrFXLx4UZqQkHC90ltzc3OnP96xsbGaefPmBcTHx9fOmzevtqM29913X51IJGJFIpHeycmppaioiHvo0CG7hx56qNbT01MPAO7u7gYAOHXqlF1OTs71HxKNRsOpra1lHB0du8x5ee3aNf6MGTN8Kisrec3NzYyvr68OAEaNGqV59dVXff/85z/XzJkzpzYoKKhDOePGjatv0yExMdHu0KFDDmvWrPEAAJ1OR65cucIfP368+uOPP3YPDAxsvu+++1SJiYl2arWaKSoqEigUCp1OpyOvvPKKT1JSkpRhGFRUVPCLioq4AODp6dk8adKkBgA4evSo9IEHHqiTyWTGtvHp6tgGK9QQptwRbPjmK4z1EMFVRC/5jviTnwQfHD6McfeOh5eXV7/3v3nzVnj6K8Dl9W1mbkhQJHSNWoildnBwcut+h05w8w7D5dOXkJaWhqioqD7LsQV2bNuOEW5hsBP0zckzbIgc1Zo6SAQiBLj7dr9DJ0Q4B+Bk2SWcPXsWo0eP7rMcyuCCz+dfj5dhGAZCoZBt+1uv1xODwQCZTKZXKpVdr8hr5Ycffij47bffJLt377YfNmxYZEpKyi3xXgKB4HqfHA4Her2esCwLQsgtsTssyyI5OTlTKpX2Kq7nxRdf9Hv55ZfL5s2bp9q7d69s6dKlXgCwfPnyshkzZqh+/vln+zFjxkQcPHgwu6P9xWLxdQOZZVls27btikKhuGG6rqmpiTz11FPi48eP6+6///76qqoq7urVq12io6MbAGD9+vVO1dXV3MuXL2cKBALW29s7urGxkblZPgDqGAINjaDcAVy+fBn5V7IxyfuOnPXpEQ4CDiZ7ibDh6y/7ve+rV69CqVTCw6/vhqfM3gkj774fkYrRt+V1ZBgOvAJj8P2mHwZ0XGt5eTnOJp3F2D7GBgOAo9Qej46ehmkjxoPH6fsLJCEEE71j8OPmrXRxjpUROEiajjdkaMz1EThImrrqz97e3tDQ0NCnG9LJycno4+PT/PXXXzsCpsVdZ86c6XSqJz09XTBx4sSG1atXlzg6OuqvXr3K70k/U6dOrd+9e7dTWVkZBwDa4pPHjh1bv3Llyutv1adPn+7RNJNareb4+fm1AMC3337r3F6/uLi4xnfffbcsOjq6IS0tTWhvb2/QaDSczmRNmDCh/qOPPnJvC2s4deqUCACEQiHr6enZsnv3bscJEyY0jBs3Tv3pp5963H333RoAUKlUHBcXlxaBQMDu2bNHVlJS0uFYTJw4UbNv3z4HjUZDamtrmcOHDzv05BgHG9Q9RhnUGI1GfP/t13jQR9zvhTM6o06rw/n8crjZiRHt7WJtda4zzlOC05fycenSJQwdavnCC218v+kHeAaOsHgVuZ7i4hmM9IJUnDt3DnFxcdZWp0/8uHkrRnlEQszr/zzVHRHk4A1p2SUcO3YMEydOtLY6dyz9nfPXw8PDEBMTowkJCYkMDg7udfD+5s2brz7zzDP+K1eu9NTr9WTmzJk1o0eP7lDOwoULffLy8gQsy5KxY8fWjxo1qnH//v3dBsfHxsY2LV68uHTcuHHhDMOwUVFR2u3bt+d9/vnnhU8//bRfaGio3GAwkLvuuks9ZsyYbtOj/fvf/y6ZM2dOkLu7e3NsbGxDQUGBAADef/99t9OnT9sxDMOGhoY2PvrooyqGYcDlctmwsDD53LlzqxwdHW94U1yxYkXJs88+6xceHi5nWZb4+Pjojh49egUARo8erT5+/LidTCYzTpkyRfPss8/yJkyYoAGAp59+umbatGnBUVFREZGRkdohQ4Z0+MIyduxY7cyZM2uioqIivb29dXFxcZrujm8wQgay16MzYmNj2eTkZGurMWjYs3cvVCoVHn7oIdjZWSf3bl9JTEzEkW0/4O9ye5uZAtp3+RoKakyLj2bFhlmlqEZnpFY14kgdF+998FG/xHNmZGRg9Zq1GDpmFhimU8dIv1NTkY/KgmSs+uhDcDi2o1dPyM/Px3tL38Xfh/8ZAo5tvFwAQGF9OX66ehSrP/kYfH6PnHWDFkLIeZZlYy3dT2pqap5CoaiydD8Uiq2TmprqolAoAjraRkMjKN3y449b8euJY8jPH1gFhPR6PXb89COm+4hsxggGAAeRKQ5WyOVCxLOtSZmhzkIwTQ34/fffLd4Xy7LYsPF7eA2JtSkjGAAcXf1gBA/Hjx+3tiq9ZvPGTRjrpbApIxgAfO3c4SV2waGDB62tCoVCoVyHGsKULjEYDDAajBA62aOpqctwMJvj1KmTcOIYEGRvW6kRxwR54qGhgXg0JgQivm0ZwoQQ3OclwM6ftqItLs1SnDt3DvVqLVy9QizaT18ghMA7KA4//vgTmpubra1Oj8nMzERRQRFiPWyrbHYbk3xisGf3HjQ0NFhbFcoAZcmSJR5t+XjbPkuWLPHoj74//vhj55v7nj9/fqcp3SgDA9t6ClNsDp1OBy6fB3AZq+eZ7Q0sy2LPzh2Y4WlbRjBgMrJ8HHuf17W/kDsKcaBYhdTUVAwfPtwifRgMBmz6YTO8g+Juy1tfmJeNkqJc+PqHwss3qPsdeoGdoweEEmccOvQLHnroQbPKtgQsy+KHjZswwXsEuLfhYc8qvoqL19IR7h2EYUPkZtQQcBU7IszRH7t/3o05c/uWKo9yZ7Ny5cqylStXllmj75dffrn65ZdfrrZG3xTLQT3ClC5pamoCh8cDOJwB5RFOT08HdI0IsRFvcIvBgN+Uhfg1swA6vW2vnCeEYJwrD4f27bFYH8ePH4cRfDi49D0tF8uyyM5IRn1dNbIzzptRuz/wChqJn3fvhlartYh8c5KcnAxtrRrRrsG3Jed4xu+oqKvCiYzfLTIrMN57OH49cgS1tR2meqVQKJR+hRrClC5pbGwEh88Dy2XQ2Ni3il3W4NC+vbjblWczscHKslpkldcgu6IW6SXVqGloQrMNG8TDXcW4evUqysvLzS67ubkZW3/8Cd5BI2/r/BBC4OjsDgAQS+xwPukIlJfNa7xJZE5wcPbFzz/vNptMS2A0GrFl02ZM8okFc5vXvK+LJwBT+rTd547gcMoJ6A16c6gJALAXSDHcLQzbf9pmNpkUCoXSV6ghTOmSxsZGMDwujFwGjX0sXdvfaDQapGdmYoSr7ZRSdpYIwRACAoKSOg22Jmdha3K2zXqH+RyC4S4inD51yuyyDx48BJHUGXaOtx/WN2zkBNw94U8QiiSoqSpDYX426moqzKDlH3gFxeLwYdv2YB4/fhxCAwchjn33sLdx/7B78PiEeHg6uKGgshiZRVdwrbzQDFr+wTgvBc4mnUVpaalZ5VIoFEpvoYYwpUsaGxtBeFwwPC7UmoGxwCU5ORmhTmKIuLZzeXs5SDF7ZBhmjwxDW8ZCja4ZmibbXYg13ImP0yeOmVWmVqvF7t274R040izyGIaBWCK77hnmC4SQSM2b4k8oksHVOxTbtu0wq1xz0dzcjG1bf8Ikn1izzIAQQuAgsYOPiycIIeBz+XCxczKDpn8g5gkxyiMKP27eala5FAqF0ltsx1Kg2CRarRYMnwvC50LTMDBybScnncZQe9tKxwUA9iIBHMQCxA3xgJeDFMN93eAs7VGxIqsQYMeHur7erOERu3fvgYOrP8Qy8xpWvgGhuHvCnzBm/MMQCM0/E+A1ZDjOnDljkVCR2+XI4SNwFzjAz868C+fDvAPxlwnx+MvEeDhK7c0qGwBGe0UhIz0DeXl5ZpdN6RubN292Gjt2bHR4eHjM2LFjozdv3mzeG7UHvP/++65r16517r5l93h7e0eXlpaaLSmAtXRbtGiR15tvvukOAK+88orXrl27Ol1tvXHjRofz58/bTnL6AQDNGkHpEq1WC5bLAcPjQVNn+x5ho9EIZXYOZij6/fe7xzhLhQh3d4KL1LZ/qxhCEOIoREZGBtzd3W9bnkqlwi+HDyNqVLwZtLsVscRymTh4fBHc/aKwectWvPLySxbrp7c0Njbi51278HjYVIvItxNbbkz5HB7u8VZg8/c/4PU3/mWxfig9Y/PmzU7vvfeev06nYwCgsrKS/9577/kDwJw5c2r6S49//OMflf3VV2+xBd1Wr15d0tX2Xbt2Oej1elVMTMzAWd1uZahHmNIlDQ0NMPI4A8YjXFBQADsBF3Z82/MIt3Eipxi/ZRVgx8UraNC1WFudLgmWMEhPTTGLrO3bd8LFMwRCke2mjusKz4ChuHzpMgoKuq2y2m/s37cPgXbecJeYxUnV78S4R6C4oBCZmZnWVuWO59NPP/VuM4Lb0Ol0zKeffup9u7InT54cFBkZGREcHBz54YcfugCAWCwe/ve//907LCxMrlAowgsLC7nAjd7PuLi4sKeeeso3NjY2LDAwMPLYsWPi++67L8jf3z/qpZde8upKfl/1shXdlixZ4hEQEBA1ZsyY0JycnOvpj+Lj4wO++eYbRwB44YUXvIOCgiJDQ0Plzz77rM/hw4clR44ccXjjjTd8wsPD5enp6R2mTeqp7p999plTdHR0RHh4uHzu3Ln+er1p0ey8efP8oqKiIoKDgyMXLlx4vb23t3f0woULveRyeURoaKj84sWLtu3taYUawpQuUavVYHkcMHwetA22n0IqPz8f3hLbqqh1M7oW0wI5g5GF3sJFK24XHykPBfl5ty2nsrISJ0+egNcQy+Ql7g+4XD48AhTYvHmLtVUBYLo3D+4/iAneA3hMGQ7u9RqOLZs2g20LnqdYhaqqqg7rXnf2fW/YtGlTXnp6emZKSkrG+vXr3cvKyjiNjY3M6NGjNVlZWRmjR4/WfPLJJ64d7cvn843JyclZTz75ZGVCQkLwF198UaBUKtO3bt3qUlZWxulMfl/1AgBr63bixAnxzp07nS5fvpyxd+/eK6mpqZKb25SXl3P279/vmJOTk56dnZ2xfPny0ilTpjRMnjy5btmyZUVKpTIjMjKy0+T/3el+4cIF4bZt25ySk5OVSqUyg2EYdt26dc4AsGrVquK0tLRMpVKZfurUKdnZs2evx/i5uLjoMzIyMv/6179Wrlix4vanEvsBaghTuqSuvh6MgAdGwIN2AFSDKirIhwfPto3LcSHeUPi4YorcD/Yi28hz3BnuIi7Kq2tgMNxedoufftoGNx85+ALbyeTRFzz8opBz5SquXLlibVWwe9fPiHAOgJPI/PG7/YnCLQSqqlqkpqZaW5U7GhcXlw5X7nb2fW9YuXKle1hYmDwmJiairKyMl56eLuTxeOzs2bNVABATE9OQn5/focE9c+bMOgBQKBSNwcHBjf7+/i0ikYj19fXVXb16ld+Z/L7qBQDW1u3o0aPSBx54oE4mkxmdnJyM9913X93NbZycnAwCgcA4e/Zs/++++85BKpX26sHXne4HDx6UpaWliRUKRUR4eLj85MmTdlevXhUAwHfffeckl8sj5HK5PCcnR5iamnr9mObOnVsLAHFxcdrCwkLbfsC1Qg1hSpfUq+vB8HlgBHw0aW0/fVplWSmchbYd+i4R8DAmyAtBrg7WVqVb+BwGYj4PdXV1fZZRWlqK5OTz8BqiMJ9iVoLD4cIzYBh+sLJXuK6uDkd/O4p7B7A3uA2GMBjvNRxbf9hCvcJW5G9/+1uxQCC4wZgSCATGv/3tb8W3I3fv3r2yY8eOyZKTk5VZWVkZERERjY2NjQyXy2UZxmSCcLlc6PX6DlOeCIVCFjBliBEIBNcvEIZhoNfrSWfy+6pXqz5W1Q1AtxlgeDweUlJSMuPj4+t27drlMH78+F7Vqu9Od5ZlSUJCQrVSqcxQKpUZeXl5aatWrSpRKpX8tWvXuh87diw7Ozs7Y+LEiaqmpibmZrlcLpftbNxsDWoIU7qkXq0GI+ADHNOlYuvV5dT1akh49LI2JxI+F2q1us/7//jjT3D3iwKXNyDCxbrF3TcCRUUlUCqVVtNh146dULiFwF4gtZoO5kTuEohmTROSk5Otrcody5w5c2pef/31fFdX12ZCCFxdXZtff/31/NtdKFdXV8ext7c3yGQy48WLF4UdTfNbQ76l9bqdPiZOnKjZt2+fg0ajIbW1tczhw4cdbm6jUqmYmpoazqxZs1Tr1q0rzMzMFAOAVCo11NfX3/ZDcOrUqfV79+51LC4u5gKmUIzs7Gx+bW0tRyQSGZ2cnAyFhYXcxMTEgT0lBZo1gtINarUajNDdlE9ULIRarYZQaLsGTUOjFiIZNYTNiZjH6XOJ4eLiYqReuoxhY+eYWSvrwTAceAYMxw+bt2DpO2/3e/81NTU4eeIk/jbs0X7v21IwhGCC13D8tOVHxMTEoM0bR+lf5syZU2PuDBHx8fGqzz//3DU0NFQeFBTUpFAozBpj11f5ltbrdvoYO3asdubMmTVRUVGR3t7euri4uFtWqtfV1XEefPDBYJ1ORwBg2bJlhQAwb968mgULFgSsW7fOfdu2bbldxQl3RUxMTNMbb7xRPGnSpFCj0Qgej8euWbOmYNKkSQ1RUVHakJCQSD8/P11MTIztr6LvBjIYp6JiY2NZ6lkwD089+wxkU+PAEQmgOXQO//j7KwgKCrK2Wp3yr9cW4xFXA/xkt72+g9LKJ0o15j7/EiIiInq9739Xf4zKegKfwBEW0Mx6sEYjLp3eildefrFP43I7fPPV19BmV+L+gFH92q+lYVkWn6f/jIS/zEFcXJy11bEohJDzLMvGWrqf1NTUPIVCUWXpfigUWyc1NdVFoVAEdLSNvnZTOsVoNKJJ2whGYMrCwAj5qK+vt7JWXcPhcGAYQC93LMuiTquDwYazRxiNLDic3qejKy4uxuXLafDwi7KAVtaFMAw8/Idh69af+rXf2tpanDp5Cnd7De3XfvsDQgju9RyG7Vt/gtGG7wcKhTK4oKERlE5Rq9XgCfkgbdOUQh5UKpV1leoGiUQCrd72FvVVqrVIzi+Hp70Uw3z/yMRzNKsIWeU1cLcTY+awYLOUyDU3DS0GSCS9D5/btn0HPPyiwOUOTu+8q3coLp1OgVKpRHh4eL/0uXvXzxjmFgopf2Bn3+iMMCd/HCtNwfnz5zFypHnKcFPubMrKyjjjx48Pu/n7xMTELA8Pj9tLh3ObWFq3+fPn+507d+6GhQQLFiwof/nll6tvV/ZgghrClE6pq6sDX/zHA9fI56K2ttaKGnWPk4sr6kptrwzu6aulKKnTIK+6HhyG4FJRFdztxChRmcKryuu10BtZ8Di2ZQibPNZNcHLqXaW+8vJypKamYtjYuRbSzPowDAcefgr8tG07/u+Nf1u8P5VKhRPHT+BvwyxTmc8WIIRgnMdQ7PxpO2JjY23yxZAysPDw8DAolcoMa+vREZbWbePGjbZT/ceGoaERlE6pq6sDR/SHN48I+aiqse0XSVcPT1TrbG9a1UViyjcu4fOQU16L+iYdcipqEenpDHc7McYEeYHHsb3bUdNiBI/HhUgk6r5xO3bu3AV3n0hweYPTG9yGm3cY8vMLcfXqVYv3tX/vPkS5BkHGN/vidpsi3HkIGtVaXL582dqqUCiUOwDbe/JSbIba2lpA+Ec+bEYsRFVNv5Wc7xMBAQEo1tmeF2lMkCceGR6MhNhQhHo4goDARSrCUB9XPDI8BAqfDgsXWZ1CTTP8fXpXYbWmpgZnf/8dHv7RFtLqDwwGAzIvn0XaxVNobu7/1H4MhwMP/2hs277Dov00NDTgt19/w92elo8NNhqNOJHxOw5cSIS6sf+L6DCEYKxHNHZus+yYUigUCkANYUoXVNfUwCD4Y5EURyRAtY17hIcMGYJClRZGG1swRwiBu50EIh4XUV4ueGpsJB4dEWKTXuD2FGpaMCQ4tFf77Nu/H65eoeDxLZ9mr6wkD0X5OSgtvoaCa9bJ6+vuI4dSqURpaanF+vj1yBEEO/rCUSizWB9t5FUW4eLVdOSUXEPylUsW768jolyCUFFaZhMV/CgUyuDGtp/CFKtSWVUJRnyjR7i+1rYXyzk4OEAmlaKkocXaqnQJj8MZEPGPV7RARGTPsz5otVocPZoID//+yWogldqDYRgQQsDhcNGs63+vMIfLg5tPJHbv2WsR+S0tLTiw/wDu9rC8hx0AHCR24HJMy0cEPD40TX3LIX07cBgORnlEYc+un/u9bwqFcmdBDWFKp1RUVYEj/sOrxxELoG3QwGCw6kLbblEMHw5lbZ9yiFuc8voGXKmoszmPdUfoDEYU1Gkhl8t7vM+vv/0GBxdfCEWW91wCgL2jC0aPfwg+/iG4okzB6WN70NTY/4abh18kks6csUh6wVOnTsFN6AgPqbPZZXeEk9QB88fPRGzwUCRfuYSNiTtQra7rl77bM8I9HBkZmSgvt73Fr4OVDRs2OA0bNiza29s7ZtiwYdEbNmzo3SrZHlJVVcVZsWLF9XiwvXv3yiZMmBBsib4o/cOaNWucH3/8cT9r69EXqCFM6ZTq6mpwJH8skiIMA75IhLq6Ousp1QOGx4xEutr2FsxVaxqxKyUXhzPzcT7f9h/umTVNCA4c0uNKggaDAQf2H4S7b//mDRaLZWhpaQYAtDTroNX2vRx0X+ELxHD2CMSRI7+aVS7Lsjiwdz/ucuv5y4g5kImkaNabZlVa9C2o0dT1a/8AIODwMNw1FIcOHOz3vu9ENmzY4PTWW2/5V1RU8FmWRUVFBf+tt97yt4QxXF1dzfnqq6/czCWvpcW2ZwAptg01hCmdoqqtBUdyoxHEk4pRVWXbhYoiIyNR2diC6ia9tVW5AZ3ecN0T3NRi2151ALhQZ8CYe8b3uP358+fBcIWQObhbTqlOCAyJhqu7N/wD5XB0MtvztVe4+0bh0KFfoNeb77pTKpVoUmsR7OhrNpk9JSYoCoEefojyD0Oge//3DwBxHnKcOH4CjY22lxt8sLFq1SpvnU53g02g0+mYVatW9W61bAe8/fbb7iEhIZEhISGRS5cudVu8eLFPYWGhIDw8XP7cc8/5AEBDQwNn6tSpgUOGDIl8+OGHh7QVVTlx4oR45MiRYZGRkRFjx44Nyc/P5wFAXFxc2Isvvug9cuTIsGXLlnX4oxMfHx8wb948v7vuuivUx8cnet++fdKEhISAwMDAyPj4+IC2dvPmzfOLioqKCA4Ojly4cKFX2/cvvPCCd1BQUGRoaKj82Wef9QGAr7/+2jEkJCQyLCxMHhsbe0sO4DbWrFnjPHny5KCJEycGe3t7Ry9fvtz17bffdo+IiJArFIrw8vJyDgCcPn1apFAowkNDQ+VTpkwJqqys5Fy4cEEYHR19vWRlVlYWPzQ0VN7VeHTEsmXL3Nr0f/DBBwMBYNGiRV4zZswYMmrUqFB/f/+ojz76yKWt/f/93/+5R0VFRYSGhsrbj8Nnn33mFB0dHREeHi6fO3euf9tv3Mcff+wcEBAQNXLkyLDTp09Lb1GgD+dix44ddsOGDQuXy+UR06ZNC1SpVAwAvPrqq55RUVERISEhkXPmzPFvuz7i4uLCFixY4B0dHR0REBAQdfDgwS716AhqCFM6RKvVwmgwgvBvvMcYidDmDWEul4u74kbhfKVtPTy9HKS4N9QHAc72KFdrkXTVcourbhdtixHZNdpelbrdt+8AXH0iLahV50ik9hg2cgJC5SOsFnstsXOBQGQHc5Z3P7jvAOLcIsBY4ZhkIikejJ2EidFjwGF6X1nQHDgIZQiw98TJkyet0v+dRGVlZYe5Djv7vqecOHFC/MMPPzifP38+Mzk5OXPDhg2ur7/+eqmvr69OqVRmrF+/vggAMjMzRZ9++mnhlStX0gsKCgSHDx+W6nQ68tJLL/n9/PPPuenp6Zl/+ctfql599dXrhnldXR3n3LlzWe+8806nU2wqlYp75syZ7BUrVhTOmjUr5LXXXivPyclJVyqVotOnT4sAYNWqVcVpaWmZSqUy/dSpU7KzZ8+KysvLOfv373fMyclJz87Ozli+fHkpAKxYscLzl19+yc7Kyso4ePBgl6s5s7OzRdu3b7967ty5zPfee89bLBYbMzMzM2JjYxvWr1/vDABPPPHEkOXLlxdlZ2dnREZGNi5ZssRrxIgRTS0tLSQjI4MPmLz1M2bMqO1uPG5mzZo1HmlpaRnZ2dkZ3377bX7b95mZmaIjR47kJCUlKT/44AOvvLw83o4dO+yuXLkivHTpUmZmZmZGSkqK+MCBA9ILFy4It23b5pScnKxUKpUZDMOw69atc87Pz+etWLHC6/Tp08oTJ05kZ2dnd5tjs7tzUVpayl2+fLnn8ePHszMyMjJHjBih/c9//uMOAK+99lpFWlpaZk5OTnpjYyOzZcsW+za5er2eXL58OXPlypWFS5cu9epcg46hhjClQyorKyGwk95iVBhFPFRUVlpJq54zYfJkJFU221wsrtzTGXqDEZVqLS4WVqBaY1vGehu/V2gxXKHocUW50tJSFBYVwdk90MKadY5GrbJKCrX2OHuF48DBX8wiS6VSIS09DQq3ELPI6wu1GhUarLBYrj0xruE4cvAXsDZ2Lw82XF1dm3vzfU9JTEyUPvDAA3V2dnZGe3t74/Tp02uPHj16yyKC6OjohqCgoBYOh4PIyEhtbm4u/9KlS4KcnBzRxIkTQ8PDw+UffPCBZ0lJyXXvzJw5c7rN5zl9+vQ6hmEwYsQIrbOzc0tcXFwjh8NBaGhoY25urgAAvvvuOye5XB4hl8vlOTk5wtTUVKGTk5NBIBAYZ8+e7f/dd985SKVSIwDExsZq5s2bF/DRRx+5dDf7M2bMGLWjo6PRy8tLL5VKDQkJCXWtx6rNy8sTVFdXc9RqNWf69OkaAHjmmWeqk5KSpAAwY8aMmu+//94JAHbu3Ok4f/78mu7G42bCwsIaZ86cOeSzzz5z4vF412+gadOm1UmlUtbT01M/evTo+hMnTkgOHjxod/z4cTu5XC6PjIyU5+bmCpVKpfDgwYOytLQ0sUKhiAgPD5efPHnS7urVq4Ljx49LRo0apfby8tILhUL2kUceue1zkZiYKMnNzRXGxcWFh4eHy7ds2eJcUFDAB4ADBw7Ihg4dGh4aGio/ffq0LC0t7brhnZCQUNs63g1FRUW9fnGjhjClQyorK8GV3fqCRyRClJaVWUGj3hEYGAh7Jydk1ljXMOoID3tTtT6pgA+p0PYKTrAsizNVzZgy7YEe73P48BG4eoWC4VjHc1hwTYkzx/bgTKJ1Fsu14eweiKLCQrOkUks8ehRylyEQcgXdN7YAyqJcbEzcgQ2JO6wSI9xGoIM3tOoG5ObmWk2HO4FFixYVCwSCGxZXCAQC46JFi4pvR25PX2AEAsH1hhwOB3q9nrAsS4KDgxuVSmWGUqnMyM7Ozjh16lROWzuZTNbtYhChUMi2yeTz+df7YBgGer2eKJVK/tq1a92PHTuWnZ2dnTFx4kRVU1MTw+PxkJKSkhkfH1+3a9cuh/Hjx4cAwA8//FCwbNmyksLCQv6wYcMiy8rKOv3Ru7m/Nl3a+u5K7/nz59fu2rXL8dKlSwJCCKKjo3XdjcfNHD16NOdvf/tb5fnz5yUKhULeFkt9s4OLEAKWZfHKK6+UtskuKChIW7hwYRXLsiQhIaG67fu8vLy0VatWlXQkpzu6Oxcsy2Ls2LH1bX3l5uam//jjj/larZYsXrzYf8eOHbnZ2dkZjz32WFVTUxNzs1wulwuDwdDr6TNqCFM6pLKyEqzo1gcwVypCaYXtG8IAcP/0h3C88racGRZhZIAHZo8Mw6zYUAi41jEcu0JZqwNfIkNYWKfhbzeg1+tx4sQJuPlEdN/YQqhqTeE6zc06aBvMn7mhpzAcDly8QvHbb0dvSw7Lsjh65DeMcOnZObAEZXWmmZ8WfYtVska0wRCC4S6h+PXwEavpcCfw+OOP17zzzjv5bm5uzYQQuLm5Nb/zzjv5jz/++G1VUZo4caJm//79Dmq1mqmvr2f279/veO+992oaGhq6tT+GDh3aVFNTwz1y5IgEAHQ6HUlOTjZrgvLa2lqOSCQyOjk5GQoLC7mJiYn2AKBSqZiamhrOrFmzVOvWrSvMzMwUA0B6erpg4sSJDatXry5xdHTUX716tc/eDGdnZ4OdnZ2hLa71q6++ch49erQGACIjI3UMw+DNN9/0mjlzZg3Qu/EwGAzIzc3lP/TQQ+rPPvusSK1Wc1QqFQcADhw44KDVaklZWRknKSlJNnbs2IZp06bVb9y40aUtJvfatWu84uJi7tSpU+v37t3rWFxczAWA8vJyTnZ2Nv+ee+5pSEpKkpWVlXF0Oh3ZuXOnY1/HoY3x48c3JCcnS9PS0gQAoFarmUuXLgm0Wi0DAB4eHnqVSsXs2bPntvtqD9ecwiiDh+LSErDiWw1hjlSMqgrLl5M1B2PGjMHWTd+jUNMMX6lteV4dxZYvNtFXjpbr8NCf5/f4bf/SpUsQiu0hkjhYVrEuCAyNhr6lGWKpHRyd+3+xXntcvEJx4sQhzJkzGwzTN19DTk4OWL0RPjLrLPwDgBGBkahv1EAiEFttsVwbCrcQ/O/cDjz51F/B59vWvTyYePzxx2tu1/C9mbFjx2rnzp1bPWLEiAgAmD9/fuW4ceO0MTExmpCQkMiJEyeqHnrooQ4T1AuFQnbLli25L730kp9areYYDAayYMGC8tjYWLNN9Y0ePboxKipKGxISEunn56eLiYnRAKb44wcffDBYpzOVKl22bFkhACxcuNAnLy9PwLIsGTt2bP2oUaNuK77tm2++ubZgwQL/l156ifHz89Nt3rw5r23bI488UvOf//zHZ+XKlcVA78ZDr9eTuXPnDlGr1RyWZclzzz1X7uLiYgCA4cOHN0yaNCmkpKSE/+qrr5YGBAS0BAQEtKSnpwtHjhwZDgBisdi4adOmazExMU1vvPFG8aRJk0KNRiN4PB67Zs2agkmTJjUsWbKkZNSoURGurq4tQ4cO1fbFG9seLy8v/fr16/Nmz54d2NzcTADgrbfeKh46dKhq3rx5lXK5PNLHx6dZoVCYteQlGYxxV7Gxsaw5F6zcibzz7jJUuAoh9L3xQcwajKjY+iu+/eYbcLm2/x61d88eZB7dh7+E2FlblRtQNzVDyOPaXGW5fHUzNlxrwn8/Wdvj8/vhR6ug0kng6WedhXK2SPrZ7Xjh+acRGdm3Mfnq8y+BPDXu9R1hZs0GLhuUB/DAnBkYNWqUtVW5bQgh51mWjbV0P6mpqXkKhcK2VzdT+pVFixZ5SaVSw9KlS20/h6cZSU1NdVEoFAEdbbPqU5gQMpUQkkUIuUII+WcH2+cRQi61fk4TQhTW0PNOpKKiApyOYoQ5DAQDIIVaG5OnTMGV+haUaW0nz+TFwgp8fzYTP53PRrPettKoHS5pwkMzZvbYCG5qakJa2mW4eARZWLOBhb1rEI4fP9GnfQ0GA86ePYuhrrS+QHuiHQNx8ljfxpRCoVA6w2ouPUIIB8CnAKYAKAJwjhCym2XZjHbNrgG4l2XZWkLINACfA7ir/7W9szAajaivrYWrVNzhdp6dBGVlZfDw8OhnzXqPUCjEtOkP4vDJXzA/uNPFtf1KUY0GAKBq1KG+qRku0m6zzvQLhepmFDca8drEiT3eJyUlBQ5OnuDxbTfUwxq4eATi/O+7YDAYwOnlAsLMzEw4CGVwFNrWLIa1CXPyx4ELW9DU1NTjIi+UO4MlS5Z4/PzzzzcU/vjTn/5Us3LlSosvaNm+fbvdv//9b5/23/n6+uoOHz7cb6s758+f73fu3Lkb8ucuWLCg/OWXX66+uW3bQjdLYc1z0VesObcdB+AKy7JXAYAQsgXAnwBcN4RZlj3drn0SgBsuNoplqKysBE8sAulsIZdEiLIBkDmijfvuvx8L9+1FWUMLPCTWN4Zj/N2gMxjgLhPDWWI7D/SDJU14eOYj4PF6PkanTp2BnXOA5ZQaoAjFdhCIpFAqlb0Ojzh7Jgnh9gOyUqlFEfOE8HXwQEpKyqAIj6CYj5UrV5ZZy9CKj4+vj4+Pz+i+peXYuHFjgTX7b481z0VfsWZohDeAwnb/F7V+1xlPATjQ2UZCyLOEkGRCSHLlAMhza8uUlZWBb3djcZaPHvjLH/9IhSgqvq2MOv2KWCzG9IcexsES28jZ6+UgxaMjQjAuxNtqxR9u5lq9DmU6YOKkST3eR6/XIy3tMpzcAyyn2ADGztmv18U1WJbFhfMXEEFfLjokTOaL5N/PWVsNCoUyiLCmIdyRBdDhyj1CyASYDOElnQljWfZzlmVjWZaNdXV1NZOKdyZlZWUQcgkCLys73B5WUoqKqwMrp+d999+PaxoDijS2l07NFjhQ3ISZCQm98gZfuXIFYqk9+IKOQ2judBxcfHHxYmqv9ikpKQFrMMJF5GAZpQY4wY4+SLucRotrUCgUs2FNQ7gIQPucPD4AboldIYQMBfAlgD+xLHtLvAvF/BQUFcK5uRHxa7/DYys/g12VKZuOXVUNHlv5GWZv/Rn6AZbcXigUYsYj8dhfbHsFNqxNdm0TVCwP9947vlf7paSmQubY1STOnY3U3hX19fWoru75z1ZqaiqCHXxsZqbA1nAS2YPPcFFQYDMzwRQKZYBjTUP4HIAQQsgQQggfwGwAu9s3IIT4AdgBYD7LstlW0PGOpKC4CGXhQfjws3eRNTwKz/7fhwCAZ//vQyhjhuLDz5Yhm8dHc/PA8q5OmjwZFc0EuSqdtVWxGViWxb7iJiTMmdvrRV2XL6fbjCFsNBpRXpKPepVZU6DeFoQwcHDxRmZmZo/3ybiUhiFSTwtq1XNYlsWV0nyU1lRYW5UbCLDzQkaGVUMyKRTKIMJqhjDLsnoALwI4BCATwI8sy6YTQp4nhDzf2uxNAM4APiOEpBBCaHLgfqCirBxcOwlAAOMNeW4JWAIQhoHIwc4sZWT7Ey6Xi/hZs7C/uIlOrbaSVtMEo1Da68VHer0eRYUFkDlYt3hFG7nZqbh04QTOnTqIBk2HufmtglDqCqUyq0dtWZbFldxc+NrZxpheuJqG/ed/w7Yz+1FaazvGsK/EFdmZPRtTSu/46quvnEJDQ6Pt7e1jQkNDo7/66iun7vfqf/bu3Ss7fPiwpO3/999/33Xt2rXO5pJ/+vRp0datW+3b/t+0aZP9v/71L9tPk0TpE1bNI8yy7H6WZUNZlg1iWfbd1u/WsSy7rvXvp1mWdWRZdljrx+IJyO90mpqaoNU0wKeyCov/9gYizl3C5/95FQDw+X8WI+LcJSz+2//Br0mLkhKLZmGxCOPG3QMtI4CyznpeYZZlUVKnQYPOurmNjSyLA8VNmDXvsV5XQMvPz4dE5gAuzzaqfDXrTCEvRqMR+hbbyRlt5+gBZVbPJrMqKytBWMBeIO2+cT/Q2Gy6R1iWha7FdmZ/fGXuyMnJsbYag46vvvrK6Z///Kd/WVkZn2VZlJWV8f/5z3/6m9MYNhqNMBhuP3f6b7/9Jjtx4sT1G+Uf//hH5Ysvvmi20Mnk5GTxvn37rhvC8+bNUy1fvnxAZUKg9BzbLw1G6VdKSkogcrRDnbsLdrzwOK5Gh1/fVu/ihO//+QICLytR1dAyoDJHtMEwDBLmzMWujV8j3EFglVjMM1dLkVpUCSGPi9kjwyDiWec2vFjZCJGDE4YPH97rffPz8yGWmc0Bc9uERAwHjyeARGoHe0cXa6tzHYmdC9LLS6HX67stUpKXlwdPmavNxAePDB4KQgCJQIwAN9vJXOksskeDVguNRgOp1DZeGgYDK1eu9NbpdDe8Eet0OmblypXeTz31VJ9jjrKysvjTpk0LGTNmjPr8+fPSBx54oPbQoUMOzc3NZPr06XX//e9/SwBg8uTJQaWlpXydTsc8//zz5a+++moVAGzbts3uzTff9DYYDMTJyUn/7bff5m3YsMGVYRj2xx9/dF69enXBL7/8YtdWLe306dOiBQsW+Dc2NjL+/v66H374Ic/V1dUQFxcXFhMTozl58qSdWq3mrFu3Lm/q1Kmam/Vtamoi7733nldTUxMTHh4uXbx4cWljYyOTnJws2bBhQ0F8fHyAUCg0XrlyRVhcXCxYv379tW+//dbl/PnzkuHDhzds3749DwB27Nhht3TpUq/m5mbi7++v27JlS569vb2xr+NIsRy2Vd+VYnVKSkrAsZOgSSK+wQhuz9XocDS7OSOvcGAuWImLiwMrlCK9xjoL52obTP02tejR2Ky3ig4GlsWhkibMmje/T4ZXfn4B+DaU2YDPFyJUPgLefrZVjY3D4UIslqG8vPtqpkWFRXDl204RDQGPj7vDYzFsiNzaqtwAIQRudk4oKiqytiqDivLy8g6ndzr7vjfk5eUJn3zyyep33323qKSkhH/p0qXMzMzMjJSUFPGBAwekALBp06a89PT0zJSUlIz169e7l5WVcUpKSrgvvvhiwI4dO3KzsrIydu3alRsWFtb8+OOPVz7//PPlSqUy42Zj9oknnhiyfPnyouzs7IzIyMjGJUuWeLVt0+v15PLly5krV64sXLp0qdfNegKAUChkX3/99ZKHHnqoVqlUZjzzzDO1N7dRqVTcM2fOZK9YsaJw1qxZIa+99lp5Tk5OulKpFJ0+fVpUWlrKXb58uefx48ezMzIyMkeMGKH9z3/+YxsxT5RboIYw5QYKi4pglAhu+X7x/u9u+J9rL0Fx8cB8EDEMg0dnz8GBEuvECo8O8kKQqwPGBHnByUoFNc5XaOHo5t7rYg9t5BcUQiy1yfBBm0Mi65nRVphfAFeRYz9oNPBxFTqgeADOSNky7u7uHca/dPZ9b/D09GyeNGlSw8GDB+2OHz9uJ5fL5ZGRkfLc3FyhUqkUAsDKlSvdw8LC5DExMRFlZWW89PR0YWJioiQuLk4dHh7e3KpLl3EV1dXVHLVazZk+fboGAJ555pnqpKSk69MGCQkJtQAwZsyYhqKioj4b+NOnT69jGAYjRozQOjs7t8TFxTVyOByEhoY25ubmChITEyW5ubnCuLi48PDwcPmWLVucCwoKbCOOjHILNDSCcgN5Bfng2Eu6bce1k6CqsgpGo7HX8aW2QGxsLLZv3Yz0miZEOfdviWMniRD3yf37tc/2GFkWR0p1eOaluX2ehq+qqkSA+zDzKnYbVJYXIe3iKYildoi5azK4vciHbGm4Ail6UuSnorwC0U6KftCoZxRXl2Fv8q+QCMV4ZNRUiAW2UQocABy5UlSU284CvsHAkiVLiv/5z3/6tw+PEAgExiVLltz2G4dYLDYCpnjzV155pfS1116rar997969smPHjsmSk5OVMpnMGBcXF9bY2MiwLGvWUCGhUMgCpoXTBoOhz4Lb5HA4HPD5/OveFIZhoNfrCYfDYceOHVu/Z8+ea7evNcXSDDwLhmJRiktKwLXvPu6O4XHBEwl79IC3RQgheOTPs/FLqe6OyyBxsbIRds6ukMv7NuXNsizqVXUQCG0nPrO0+Br0+hbU11VDparqfod+hMuXoLKye51q62phx+/+JbS/yCq5Cl1LM2rUdSiqtq0MMXYCCaoH6G+PrfLUU0/VrFixIt/Dw6OZEAIPD4/mFStW5N9OfPDNTJs2rX7jxo0uKpWKAYBr167xiouLuXV1dRx7e3uDTCYzXrx4UZiamioBgAkTJjScPXtWplQq+QBQXl7OAQCZTGZQq9W35Ht0dnY22NnZGQ4ePCgFgK+++sp59OjRt8QBd4ednZ1Bo9H02T4aP358Q3JysjQtLU0AAGq1mrl06dKtU60Um4AawpTr6PV6qGpqwbXrWaUwvr1sQE9PxsbGQs8TIduKGST6G5Zl8WuZDjMT/txnT0tDQwMYDhccru14Xb18gsDjC+Dg5AZ7B9tZLAcAAqEEVVVdL2g3Go2ob1BDyredKn0R3sEQC0RwsXOCr0uH4ZRWQ8aXoKbKdnJGDxaeeuqpmuzs7Msqlep8dnb2ZXMawQDwyCOP1CckJNSMHDkyPDQ0VD5z5syguro6Tnx8vEqv15PQ0FD5v/71Ly+FQtEAAF5eXvo1a9bkzZw5MzgsLEw+c+bMQACIj4+v27dvn0N4eLi8zeht45tvvrm2ZMkSn9DQUPmlS5dEK1as6HV6o2nTpqmzs7NF4eHh8i+++KLX8UpeXl769evX582ePTswNDRUHhMTE3758mXrxMFRuoUMRm9YbGwsm5xMUw73luLiYry1fBnsHxzdo/YN57MwXTEKDz30kIU1sxzHjh1D4o4fsCDcvvvGg4CMmkYcqOFixYer+mwIl5eX4//eegfDxs41s3aDk9rKAjSrruCdt9/stI1Wq8XfFvwN/x71RP8pNoApUVdif/nvWPHR+9ZWpU8QQs73RzrQ1NTUPIVCYVtTJBSKFUhNTXVRKBQBHW2jHmHKdYqLi8HrQVhEG0QmxrUBXur07rvvRoUOKNTYTp5US5JY3owHZzxyW3F3Op0OXCt6g1uadVDVVg2YkBYOlwedrutZh6amJgisGNesa2lGaU0FDMbbz/HaH/A5PDQ13zkzORQKxXJQQ5hynaKiIhilPQ9j4tpLUFhUaEGNLA+Xy8V9U6fiRPngf6iWNDSjQsdi9Oieefw7Q6fTgcOxjtGm17cg6cR+/H7qILLSO5710ahVyM64gJrq7lOW9QcMp3tDWKfTgWelMTUajfjx1F78dHoffkk50WGbeq0aJzLOIa/CNjLF8Dhc6KghTLlNtm/fbhceHi5v/5kyZUqQtfWi9C80awTlOtcKCsDY9XyxDtdeisqycph7ZW9/M3HSZCz8+WeofcWQ8W9ZfzFoOF6uw5T7p3Zb2MGWaWnWoamxAQBQX1eNelUNxGIZuDweGrUa8HgCpF08CXV9LYrys3HvfQngcKx/Tntyd1jrHmox6FHXUA8AqFTVoFJVDZlICiFfAHWjBjwOD4dTT6K4ugwpeRn468QESITWj2UmPRpVCqVz4uPj6+Pj4zOsrQfFugzcJyLF7BSXFIEb7dfj9oyQD8JhUFdXB0fHgZv/VCaTYeTIkUgqTsMUH5m11bEI2hYjUiu1eGLy5NuWxeFwwLLWKZAkEksRKo9BbXUZDAYDzp7YD7FEBh//UGRnnIdAIAJPYJrV4HJ5sIX3M9ZoBNONMc7hcKwWliDg8TExegyulOUDADaf2A2JUIy4EAUS05LA5/LgKHUAAPAYLrg28GJhZAdm2kYKhWJ70F8SCgDT9GhVeSW4dr1LiSVwsBvQmSPamHz/VCRVNsM4QOJOe8u5Ci0UiqGws7v9ymUcDgdGK8aS+gdGYNjICdC3mOK6tQ1qVFeYFobrdI0IDImGXDEasXffB4axvtHGssZuvdLWNIQBINIvFH+Km3I97rqhSYurZQVgWRa6lmZE+oZgsmIsEu6eDgHP+lmgDEajTRjkFApl4EM9whQAQHV1NbgCPhh+Ly8JmQglJSWIioqyjGL9RGBgIKR2Dsiu0yHccXBluWFZFknVLXhq7jSzyJNIJGixgfjMUHkMrl1Jg7OrF5xcPGBMT4ZEagdXd1+b8hbqW3SQSLoOOZJIJGi0gTEdEx4DAsDD0RVh3kE4evkMxAIRwrwDweXYzuOiUa+DWGz98AwKhTLwsZ2nBcWqlJSUgO/Q+7AAVipEwQAttdweQgjGT5mCc9Ut1lbF7BRqWqBneAgPDzeLPJlMhqYmrdWzNjg6u2PEXZPgHxgBmZ0jYkdPQUT0XTZlBANAS3Mj7Oy6vrcEAgEAFs0G615/bvbO+NNd9+Gu0OFwkNhh5qj7cf/we2zKCAYArb4JMtngDGMaTFRVVXFWrFjhCgB5eXm8qVOnBvZn/8ePHxc/8cQTvpbuZ82aNc6PP/54z+MKW8nKyuKvW7fOZmrVL1q0yOvNN990t7Ye/Y1tPTEoVqO0tBSQ9t4TyrWXoKBwYGeOaGPMmLuRUa1Fo9468a+W4lyVDvdMmGA2A1EgEICAwGBlo22g0NLcCEeHrvNUE0IgFUvR0NLUT1oNbBqaG2Fnf2fk/u5P1q1b5+Tl5RXNMEyMl5dX9O0aadXV1ZyvvvrKDQACAgJaDh48eNU8mvaMe+65R/vtt9/a7AMqJydHsHXr1g7HuKXFsr+vRqMRBsPASJdoaaghTAEA5BcV9s0QtpOgvKzMAhr1PzKZDPKIcFyqarS2KmbDYGSRUtWEsePuMatcRydn6LT1ZpU5WGnRaeDu3r2TxcXZGbVNdEx7Qq1ODTePO85xZVHWrVvntHDhQv/S0lI+y7IoLS3lL1y40P92jOHFixf7FBYWCsLDw+XTpk0LDAkJiQRMHtTJkycHTZw4Mdjb2zt6+fLlrm+//bZ7RESEXKFQhLeVUk5PTxeMGzcuJDIyMiImJibs4sWLnT6kvv76a8eQkJDIsLAweWxsbBgA7N27VzZhwoRgwOTtTEhICIiLiwvz8fGJXrZsmVvbvmvXrnUODQ2Vh4WFyWfMmDEEAEpKSrj3339/UFRUVERUVFTEL7/80qOUSj/88IP90KFDwyMiIuRjxowJLSws5ALAvn37pG0p2iIiIuS1tbXMv//9b+/k5GRpeHi4/J133nFbs2aN87Rp0wInTpwYPG7cuND6+nomISEhICoqKiIiIkL+/fffOwDAvffeG3z27FkRAERERMhfffVVTwB4+eWXvVatWuWiUqmY0aNHh8rl8ojQ0NDr+2VlZfEDAwMjH3vsMb/IyEh5bm4uf8mSJR4BAQFRY8aMCc3JybH+AgArQA1hCgCgqLgYnF6kTmuDIxGhQa1Bc/PgKEhx970TcFFlubfkFoMBjc16i8m/mew6HVzdXHtkiPUGdzc3NFJDuEe0NKl7NP4enp7UEO4hdfoGeHh6WFuNQcXSpUu9m5qabrAJmpqamKVLl3r3VeZHH31U5Ovrq1MqlRmrV6++IYYuOztbtH379qvnzp3LfO+997zFYrExMzMzIzY2tmH9+vXOAPD000/7f/bZZwXp6emZH3zwQdGCBQs6DT9YsWKF5y+//JKdlZWVcfDgwSsdtbly5Yrw2LFj2efOncv88MMPvXQ6HUlOThZ++OGHnseOHcvOysrKWL9+fQEAPPfcc76LFi0qT0tLy9y5c2fu888/H9CTY54yZYomJSVFmZmZmfHoo4/WLF261KN1LDzWrFmTr1QqM5KSkpRSqdT47rvvFsfGxmqUSmXGW2+9VQEAFy5ckG7evPlaUlJS9r/+9S/PCRMm1KelpWWeOHEi64033vCpr69n7r77bs1vv/0mrampYTgcDpuUlCQFgKSkJOmkSZPUYrHYuG/fvisZGRmZx44dy/7Xv/7lYzSaZjrz8vKETz75ZHVmZmZGeXk5d+fOnU6XL1/O2Lt375XU1NTeGwGDANsK/KJYjYrycggjev9gIQyByF6GsrIy+Pn1OkTK5hg+fDi+WL8O6maJ2XMKq5uasf1CDnR6AyaF+yHYzcGs8jviYm0Lxkycana53j5eyLpWZ3a5gxFtQx08PLq/tzx9vFBSmN0PGg18aprqzf5yd6dTVlbG7833t8uYMWPUjo6ORkdHR6NUKjUkJCTUAUB0dLT20qVLYpVKxVy8eFGakJBwvcBFc3NzpwkRY2NjNfPmzQuIj4+vnTdvXm1Hbe677746kUjEikQivZOTU0tRURH30KFDdg899FCtp6enHgDc3d0NAHDq1Cm7nJwcUdu+Go2GU1tbyzg6OnYZO3ft2jX+jBkzfCorK3nNzc2Mr6+vDgBGjRqlefXVV33//Oc/18yZM6c2KCioQznjxo2rb9MhMTHR7tChQw5r1qzxAACdTkeuXLnCHz9+vPrjjz92DwwMbL7vvvtUiYmJdmq1mikqKhIoFAqdTqcjr7zyik9SUpKUYRhUVFTwi4qKuADg6enZPGnSpAYAOHr0qPSBBx6ok8lkxrbx6erYBivUEKagubkZDWoNJBJR9407gGsvQWlp6aAwhAUCAYZGR+Fy9VWM8exdKrnuqFQ3orHF5A0uqlVb3BDWG1mkVzdi/l13mV32kIAAXEo/ana5fUVVW4WMS0mQSO0QNfxum0ibBpgyRuiatD0y2vz9/XEx8Ww/aNUzqtW1OHTxOCRCMaYOvxcCnkXsoV5jYI0or68aFL83toSHh0dzaWnpLSfZw8PDItN9fD7/+mpbhmEgFArZtr/1ej0xGAyQyWR6pVLZo4IXP/zwQ8Fvv/0m2b17t/2wYcMiU1JS0m9uIxAIrvfJ4XCg1+tJa0GoW1b+siyL5OTkTKlU2qtVwS+++KLfyy+/XDZv3jzV3r17ZUuXLvUCgOXLl5fNmDFD9fPPP9uPGTMm4uDBgx2+9YrF4usGMsuy2LZt2xWFQnFDSpmmpiby1FNPiY8fP667//7766uqqrirV692iY6ObgCA9evXO1VXV3MvX76cKRAIWG9v7+jGxkbmZvmA9Qr52BI0NIKCiooKiOxlIEzfbghWLEB5uW2UszUHo+4eh0v15l8w5+skQ6CLAxzFAhiMLPKrLTsNnqPSwcPdHc7OzmaXPWTIEDSoq8wut6/kX8uERl2H8tIC1NZUWFud62jqq+Dl5dOjhYoBAQEoqa+0ejaONlLzMlFVX4P8iiIUVNpOrvAqbR0cHRwhFA6uNIfW5s033ywWCoU3/PAJhULjm2++2eeTb29vb2hoaOiTneHk5GT08fFp/vrrrx0B0+KuM2fOdOqtSU9PF0ycOLFh9erVJY6OjvqrV6/26M1t6tSp9bt373YqKyvjAEBbfPLYsWPrV65ceT2O+PTp0z3yFKnVao6fn18LAHz77bfXf3zT09MFcXFxje+++25ZdHR0Q1pamtDe3t6g0Wg6fWufMGFC/UcffeTeFtZw6tQpEQAIhULW09OzZffu3Y4TJkxoGDdunPrTTz/1uPvuuzUAoFKpOC4uLi0CgYDds2ePrKSkpMOxmDhxombfvn0OGo2G1NbWMocPH3boyTEONqghTEFZWRm4stvIySkVorDEdh6Ut4tCoUBeXSO0Zs4eweMwuD/SH45iEbIranEgLQ/1jZbLHZtW14K77r7bIrK9vLzQ2KCGvsX6uW8BwNXNB4QQiMRSyOxsp8qhRlWB4KCeZYxydHQEl8tFnU5jYa16xhA3XzAMB2KBCB6OrtZW5zrF6goMCRhibTUGHc8//3zNf//733xPT89mQgg8PT2b//vf/+Y///zzNX2V6eHhYYiJidGEhIREvvLKKz693X/z5s1Xv/nmG5ewsDB5SEhI5Pbt2x06a7tw4UKf0NBQeUhISOSoUaPUo0aN6tGq59jY2KbFixeXjhs3LjwsLEz+wgsv+ALA559/XnjhwgVJaGioPCgoKHLt2rU9ugn+/e9/l8yZMycoJiYmzNnZ+fqCkPfff9+tbTGfSCQyPvroo6q4uLhGLpfLhoWFyd955x23m2WtWLGiRK/Xk/DwcHlISEjkG2+8cT1ee/To0WoXFxe9TCYzTpkyRVNeXs6bMGGCBgCefvrpmtTUVElUVFTE999/7zRkyJAO09GMHTtWO3PmzJqoqKjIBx98MCguLs42fnz6GWIr3gdzEhsbyyYnJ1tbjQHD3r17sffiGUhMC21vQFVYisqsa5C4OsFjaFiH0yi6kirYF9Th3XeW9oe6/cIHy5chqqUMMW7mT9r/m7IAWeW14DIM5sSFQSow/5SzkWXxzoVqvPmfd+Hp6Wl2+QDw1lvvgGcfBCe3AIvI7y0tLc3gcDg2ExYBADmphzArfjpGjRrVo/b//XAVfDUyDHMPtbBmPUPX0gwOw9hUHuFdV49Dcd8oTJkyxdqq9BlCyHmWZWMt3U9qamqeQqGwnakbCsVKpKamuigUioCOtlGPMAWl5WWApOOsKbX5JTC06FFfUgF9U8feP45MjOqqwfVbGzv6bqTVWyZ7xLgQb9wT4oOHFUEWMYIBUxENiURiMSMYAKKjo6CutZ3UeTwe36aMYJZlUVdV0qtCJvLoSOQ32E6YkYDHtykjGADy60sRERFhbTUoFMoggRrCFJSWl4Ej7djzae/jAUIIJK5O4Ao6NpY5EiE09Wro9f2XFszSDB8+HFk1WuiN5p8x4XE4iPRyhrud5UrEptXoEBNn/kVy7YmMlKNBVWrRPgYyDfVVkNnJ4ODg0ON95HI5rqlKbCZO2Naoa1JDZ2iBt3efM3pRBjhLlizxaMvH2/ZZsmRJv+TS+/jjj51v7nv+/Pl01eYAx7Ze9SlWobKyEpyAjqdiHQO84eDv1eXKUsIwEEhEqK6uHjQpjRwcHODh5oZclQ5hjgNvUU5GvQFPx460aB8hISFoUNeiWacFX2A5o36gUltZgBHDh/dqHx8fH7AMUN2ogovYwTKKDWByagsxdOhQutL9DmblypVlK1eutMpU1Msvv1z98ssvV1ujb4rloB7hOxyWZaH6//buOzzOLDsP/Hsqo5BzBkECYG6QBEAwNXPoJptN9szIlrSWPbbskb3W2tbKa1mW7XV65J2VvF5bliV7VmFGsixZliXPSBqPQit5OjM0A5gAEjlWROV89w8Ue9hsgigA9dVX4f09Dx6iqr6696AIEge37j3H7YWxYvUDsZn80DFXlsNZZNsjBoYP496yNm0uI/EEUhqt+rkjCSzHEujr69Nk/KdMJhN27d4Dj2NK03kKVcAzg8HBgXU9R0Swf/9+PPLwNX2RscAsBoYG9Q6jkKRSqRR/a6CSlv43sOrpdybCJS4QCECMBhjMm3tzQOxWuFzF9Yvy4NAQRrzxrL9NfWfWiV98dwT/9fojxDXo9X7PHcH+ffsyKtm1WcMHB+F3M2l7XiwaQsDnWtf+4KcGDg7ikW9ag6gKWywZx7h7Fv39/XqHUkjuOhyOaibDVKpSqZQ4HI5qAHdXu4ZbI0qcy+WCtXLzXRVTNjMcTkcWIsofXV1dSIkRi+EEWuzmrI077lwGALiDESyHY2h4yWr8RtzzK5x7Q9v9wU8NDQ3hq1/9GpKJOIym7L1G6+VyzmPk5ruwV1Rh/8FTMOkYCwC4Fp6gf98+mM3rj+OVV17Bz/h/BoFYCBUW/baczHuW8D9u/Ans1jJcHT6PMou+W4RG3dPo2daDiorsNropZolE4q8tLCz83MLCwl5w4YtKUwrA3UQi8ddWu4CJcIlzOp0w2jf/A07sViws5U8jg2wQERwYGMTI5M2sJsIHOpsQiiXQVGlHfXl2k4toMoXHniB+OEerZhUVFdi2rQcexyQaWntzMueLzE6NIRoNIxoNY9nrRH2DdtUyMuFzTeDKhS9s6LkWiwX7+/fhvmsCB1t3ZzmyzN2fGUMgHEQgHMS0cw7b2zKrh6yVe8sTOPLGSV1jKDSDg4NLAK7oHQdRPuNviCXO4/FAlW2+hJfRboOzyLZGAMDg8DDu+bO7NaKzrhLfc3AHzuzszPqhn4eeKHq6u2G3524l8fjxY/A6nuRsvhdpaeuG0WhCZVUtqqqz30lvPWKRIHxex6bewj9y/ChGvONZjGr9+lq3wmKyoLq8Cu11OTmUv6poMo4x1zSGhjQvvUtEJYaJcIlzud1IWTf/xoDRboPH48lCRPll9+7dmPOFEYhrU1M420Z8CQwePpLTOYeHh+F2TCMRf2HzopxoaunE6de/G4dPvAGzWZvazJlyzI9icGAQ1lXKDWZi//79WAy64Y34sxjZ+nQ2tOKvv/a/4Iunv4Bym75VQe47x7Fj+w5UVVXpGgcRFR8mwiVuyemAsWzjP7CfMpRZ4VtezkJE+cVisWD3rl2459YvyctUSincc0UwOJjbU/Xl5eXYu/cVOObGcjrv8/KlpJZ36TFOn97cW/gmkwmHDh3CLcdolqLamHx5TW97HuPEGW6LIKLsYyJc4tweDwzZSIStFkTDkaJqqvHU0OEjuOtbtfJK3pjwxVBdU42mps+0rNfc2TOn4VnUNxHOBwGfE8lENCudz06ePoVbrrGSb67hjfgx73diYGB9peiIiDLBRLjELS8vZyURFoPAUmaD36/fW7laGRgYwCN3CLFkfifDdzwxHMzxtoin+vv7EY8FEfQV3z7x9XDOPsDZM6ezUrqut7cXFrsNEyXeve/m0kMcPXYUFou+W16IqDgxES5xAb8fRtvqP2CUUogs+5HMYKXXXGbDchFuj6isrMTWrk489Eb1DmVVSinc8cZxUOO2yqsxGo04feoUHHMPdJk/HySTCTjmR3H69OmsjCciOHP+LG44H2VlvEKUUincdI7izLmzeodCREWKiXAJU0ohHAzB8JJEeOneGCbfvYmpd28itcaKqMFmgc/ny3aYeWH42Ku47c3fbR+zwThgMmPLli26xXD27Bk45h4hmdSmG1++c86PoaenBw0NDVkb8/jx43jknkQwHs7amIXkkXsKtXW1un5fE1FxWzURFpGtuQyEci8SicBgMECMxtWv8a5sdYgFw0jFX57giNWMQCCQ1RjzxcGDwxhxhZFI5ed+zdvuKIYPHdH1cFNjYyP6+vp0PzSnF9fcfVy6+HpWx6ysrMTgwCBuLj7M6riF4rrzIS5cyu5rSkT0rJetCP8GAIjI2zmKhXLM7/fDXPbyhg6Nu3pQXl+Dxp3bYLK9fC+xMhuLco8wANTV1aGttQWPvPlXPUIphVueBA4fPap3KLh08XW45u6X3AEvv3cJqUQU+/fvz/rYFy6+hmuOh0ip/N6jnm3u8DJm/Q4cOaLPvnciKg0vKyBrEJF/AmC7iPzw8w8qpf61dmFRLgQCAZhesi0CAOx11bAPZ9YYIGU2wlekiTAAHDl+Arfe/m3srtvcOEopLPlDqLJZUWbZfA3n+VACCTGgp6dn02NtVn9/PwQJ+L0LqKrVt7tbLi3NjOC1185n5ZDc83p6elBVU4VH7insrO/O+vj56sPF+zhx8gQPyRGRpl72v/b3AIhgJVmufMEHFbhgMAiD5eWtg8MeH2au3YFnYnbN8cRiKtqtEQBw6NBh3MnC9oj3xxfwmzfH8F+uPUQkvvl9xzedERw6cjQvar4aDAZcuvg6lmZG9A4lZ2LRENyL4zh7VpsDXSKC1y9fwoeO+5qMn4+iyTg+XnqE17K81YSI6HmrJsJKqYdKqf8bwPcrpf7Z8x85jJE0EgqFAPPLVySX7o8h6PBg6f5jJCIvr5pgsJjhK+JEuK6uDp3t7bjv2dz2CHdg5eBTOJ5AKLa5RFgphZvuGI6+enxT42TTqVOn4HVMIxou3u+FZy1O38OhQ4dQUVGh2RxHjhzBUtiDpaBbsznyya3FR9i1axcaGxv1DoWIilwm7+PdEJGfF5H/AQAisltE/mo2JheR10XkoYiMiciPvuBxEZGfSj9+W0RYUT2LVhLh1Q/KAYCtauWHu9lug8H88tVjg8WMQLB4t0YAwNGTp3DTs7nk9fC2Vmytr8bhra2oK3/5Hu21TAfiMFpt2Lo1f8622u12HHv1VSxO39U7FM2lkkkszdzD5ctvaDqP2WzGufPn8f5i8a+0p5TC+0v3cOlNbV9TIiIgs0T4qwB+D0Bb+vYjAD+02YlFxAjg3wO4CGA3gO8Vkd3PXXYRQF/64wcA/Oxm56XvCIVCUKaXfws07elD15H92HJ0AAbjy68VswmhcHGXeTp06BDuu0KIbqK5Rn1FGV7f240DXZvvAHfdFcXR4yfyYlvEsy6/cQlLMw+QTBR3KTXH/Ci2bOlCR0eH5nOdv3AeI84nCMaK+9/YI/ckyqsrsHPnTr1DIaISkEki3KCU+nUAKQBQSiUAJLMw9zCAMaXUE6VUDMCvAbj63DVXAfySWvE+gBoRKZ0TOBoLh8NIrZXciqCspgrGNbZQAICYjYiE86+qQjZVVVVhe18f7rj0T0ZSSuGmM4JjebQt4qnm5mbs2LkTizPaNdjwuh34+KM/xtT4A8TjMTx+dBsLcxOIhIO4/v4f4uOP/gSJNUr+bYZSCkvTd3D1ypuazfGs6upqDA8P48OFe5rN4fS58dsfvY0PR28hnkzgw9FbGJkeRTgWwdc/+H389w9+H8FISLP5AeD9pXt448rlvPvljoiKUyaJcFBE6gEoABCRwwCy0T6sHcD0M7dn0vet9xqk4/oBEbkmItccDkcWwit+wXAIkkGCmykxmxCNFHciDADHT5/BDY/+paxGvVHU1dWjra1t7Yt18NbVN+GYuQulUdmvh/euwbE4i4cj1zBy6308eXQbd258Gw9HrsPtXIBjcQYL8xOazA0AXucMbBYT+vszq6qSDW+8eRnXlu4jntKmucs7969hfHEK7z+8gT+89W28//AG3r71bfzxnfcw6ZjFlGMW92a0qxM953fAE/Pj8OHDms1BRPSsTBLhHwbwDQA9IvIOgF8C8LeyMPeLft1//jh+Jtes3KnUV5RSQ0qpIR6wyEwoHIaYXr5HeD0MJiOi0fxtQ5wtg4ODmFgOwx/b2BsjgWgM74zNYXTRs6k4rrvjOH76zKbG0NL27dtRV1sD1+K4JuNXVq3UsSuzV8BmXdlrLQYDahuaYTAYYDKZUVOr3f8FS9N3cPXqmzlduezo6MC2bdtwa3FUk/Ebq+sBAGXWMlTYyj+5v62uGSajCUajEe11zZrMDQDvLt7FxTcuwWTK3i/oREQvs+b/NkqpGyJyEsAOrCSmD5VS2Xi/cQZA5zO3OwDMbeAa2qBwJJLVRFhMRsRjxb0nFACsVisGBg7g+uJDnGpff6WA/zk6iwmXDwJBY6UdNfaXNyp5kVgyhbuuEP5yHjTRWI2I4OrVN/Gf/vNvoKEl+zWOd70yjPbOHtjLK2E0mVBZU4/y8krU1DWhpXULxGCA2axNDdqgz4lw0I1jx45pMv7LXH7rCr7y734WAy07YchyEn505yB6WragsqwcNrMVDVW1KLfZ0dXQhp3tPVBQKLNs7oDnarwRPx57ZvC3z/6IJuMTEb3ImivCImIG8DcB/DMA/xTAX0/ft1kfAegTka0iYsFK3eJvPHfNNwD8pXT1iMMAlpVS81mYmwBEo9GsJ8Ja7snMJ8dPncGNDVaPKEtvRzEaBOY19miv5o4rgt5tPaipqdnQ83NleHgYKhWFz539f7YiguraBpgtVhgMRrR39qCmbuUAosVq0ywJBoDFqTu4dPF1mNeopKKF3bt3w15VgUfuSU3Gb65pgN1aBoPBgF0dvehqWNl6Y7NYNUuCAeD9hbs4eeok7Ha7ZnMQET0vk5/CPwtgEMDPpD8GkYXqDelDd/8bVipS3Afw60qpERH5GyLyN9KXfRPAEwBjAP4/rCTklCWRaHZXhGEwQKVSSCazcZYyv+3Zswe+hMJCcP2J/6t97Ti/awu+MNCHcuvGEqlrngSOn9GmgUM2GQwGXHnzMhanb+sdStZEwwG4lyZw7tw5XeYXEbz51hW8V0Sl1MLxKD5eGsXFNy7pHQoRlZhMEuGDSqkvKqX+KP3xVwAczMbkSqlvKqW2K6V6lFI/nr7vPyil/kP6c6WU+sH0468opa5lY15aEYvFIBtckXwREYHBZEQsFsvamPnKYDDg6LHjuO5a/+FAk8GA3qaaDdcQ9seSmFiOYHBwcEPPz7VTp07B71lEKLC5PdEv4/d5cO2938e92x8gldL2IOPC1B2cOHFC0wYaazl06BB8yRCmfYuazeEN+vCb738L37r5p0gktTmc99RHi/exf/9+1NfXazoPEdHzMsmCkiLyyQY/EdmG7JRPI53F43GIMYsrwgCMJhPipbI94uRJXHfGkFKba7m8XtcdYQwOHIDNpt3b1NlktVpx/vw5LE7d0WyOibEReFxLmJ0ahde9pNk8iXgUS7MP8IbOK5dGoxGXLr+Bdxe1a1py88kIZpzzeDT7BOOL02s/YYMSqSQ+XBzBm29d0WwOIqLVZJII/z0AfywifyIifwrgjwD8XW3DolyIx+JZXREGAIPJVBIrwgCwZcsW2CsrMe7L7dd7w5PA8VP5Wy3iRV5//TU458cQi2pTg7a2fmVvsNVahvKKak3mAFbaKe/r35cXrX9PnzmNieU5uMLZqGb5WR31LRARWM2WT6pJaOH20ig6u7qwZcsWzeYgIlpNJlUj3haRPnynasQDpVTx18gqAYl4HKZsJ8JGY8msCAPAqydP4/q3v4We6vVXftiIxVAcy3GFPXv25GS+bKmursbhI0cwPnUXXX3DWR+/Y8t21De2wWy2wqTRAbZUKonF6bv40j/8B5qMv142mw1nz53Dezfv4vLW7Fev6GvbipbaJpiNJtgs2nx/p5TCe0sj+P6/+SVNxiciWksmVSN+EECZUuq2UuoWALuI8NBaEUgkEkCWE2ExGkoqET726qu45QwjkcrN9ojrzgiOHjsGgyG7f2+5cOXNy1iauY9kUpvvjzJ7hWZJMAA458fQ0dGO7u5uzeZYr9cvvo47jjEE49p0OqwsK9csCQaAMc80LOU27N27V7M5iIheJpOfpl9SSnmf3lBKeQDw1/cikEwkIFlOqAxGw0qCXSLq6+vR0d6O+x7tO+oppXDDFcOx4yc0n0sLra2t2LF9O5ZmHuodyroppbA0dQefe+v5LvD6qqmpwfDBYXy0cF/vUDbk3cW7ePOtK2ynTES6ySQLMsgz/0uJiBGAdgU6KWcSiWTW9wiX2oowABw9eQo3N1hTeD2mAnEYrTZs3bpV87m08tZbV7A0fUeztsta8TpnYLUYc9pOOVNvXLmMjxbvadZ2WStzAbZTJiL9ZZIF/R6AXxeRsyJyBsCvAviWtmFRLqSSSSDbb7EbSmtFGFgpZXXPFUI0qW1yd8MVxdHjJwp69Wz79u2ora2Ge3FC71DWZWkm9+2UM9XR0YGt27bh9tKY3qGsy3sLI7h46SLbKRORrjLJgv4+gLcB/K8AfjD9OXtgFoFkUpsV4VJLhKuqqtDXsw0jG6gpnKmUUrjliuLI0dy39M0mEcHVK29iaUa7sl/ZFvS7Efa7dGmnnKnLV9/E+0sjUDku5bdRy9EARj1TOH22sKqfEFHxWTMLUkql0k0uvksp9QWl1H9USrGOcIFLpVJQqRSQ7RUug5RcIgwAR46fxMde7f5ZTPhiKK+sREdHh2Zz5Mrw8DCSsRD8Xu2aQWTT4tQdXHjtgi7tlDO1Z88emO1WjHm0q/ebTR8u3MPx48dRXl6udyhEVOIK7+g5ZUUikYDBaMz+W71SeivCADA0NIRHHu22R9zyxHD42HFNxs41o9GIi5dex9JM/rcIjkfDcC08xoXz5/UO5aVEBJfefAMfOPL/0FwsGceNpYd4/dJFvUMhImIiXKqeJsJZZyzNFeGKigr0bduKe+7sb49QSuG2O4pDRXSo6Mzp0/AsTSIaCeodykstzNzDweFhVFVV6R3Kmo4ePYqFoAuOkHatrLPh1tIoduzYgebmZr1DISJ6eSIsIkYR+clcBUO5s5IIa/B7kJRe1Yinho8dx53l7G+PmA7EYbHZ0d7envWx9VJeXo5jx45hcTp/V4VTqSQc0yO4rHM75UxZLBacPX8OHyzm72uqlMIHS/dw6c039A6FiAjAGolwei/woOTjUWnalHg8rs2KcInuEQaAwcFBPHCHst5c4447iuHDh/OyYsFmXLz4Ohwz95FK5uf3i2vhCdrb29HV1aV3KBk7d/4c7jqeIJzIz+afj72zsJTbsGvXLr1DISICkNnWiJsAvi4if1FEPv/0Q+vASFtabY1QBinZFeGamhq0tbRidDm7Scjd5QQGD2a/LbHe2tra0N3dDcd8fpb9csyO4PLlwlgNfqq2thb79u3DjcX8bFryoeMeXn/jYtH9UkdEhSuTRLgOgAvAGQBvpj8uaxkUaS8ej2e9dBqwkgiX6oowAAweOox73uz9IuCKJBBMpNDb25u1MfPJ5cuX4Jy9p3cYnxFYdiAZD2NgYEDvUNbt9Tcu4trSfaTyrJSaO+zDtG8xr8vQEVHpWXOPMACnUuqvPPfx/TmKjzSSSCS0SYSldFeEAeDAwADueWNZq+c64opg3779MGS78Ume2LdvH5CK510ptaWZe7hw/jyMWmwf0lhvby/KqyvxOM9KqV1buo+TJ0/CarXqHQoR0Scy2SNceEsitKZYLAYxaVE1woBYLJb9cQtEZ2cnlMGExXB2VsUfBBUGinBbxFMGgwEXLpyHYyZ/VoUT8QhcC49x5sxpvUPZEBHBa5dex0fOB3qH8ol4KoGbS49w7kJ+l6EjotKTyTLTxyLyDe4RLi6JRAJiyH4iLEYDovHSTYRFBPv278cDz+bLqMWSCk/cIezduzcLkeWv06dPwbk4jnhMu85867E48xD9+/ahurpa71A27OjRo5heXoQ34tc7FADAPecTdHd3o6WlRe9QiIg+hXuES1QsFoMYs39gRYwGRKP5eWI9V/YPDuFBYPPjPPFF0dneWvTdt6qqqrB/334szep/wEspBdf8Q7z+2gW9Q9kUq9WKo8eO4caS/q8pAFx3PsKFi6/pHQYR0Wdk0mL5+f3B3CNcBGKxGKDB/kcxGREp8UR49+7dGPcEEd9kGbVHy3H0DwxlKar89tpr5+Gaf5i1vdUb5fPMw2o2YufOnbrGkQ3nLpzDTccjJJU23Q4ztRT0wBP148CBA7rGQUT0ImsmwiLSISK/JSJLIrIoIv9NRDpyERxpJxqNAhkclgu5vJj+4BZcj6cyGleMxpJfES4vL0dHawvGN1lG7VEgiVf6+7MUVX7buXMnLGYD/J4FXeNwzT3E+fPniqK8V2dnJxqbGjHqzuzfrlZuOB7g5OlTMJlMusZBRPQimWyN+EUA3wDQBqAdwG+n76MCtrIivPZfv+PBE4Tcy3A+mkAisnZixxXhFXv3H8Cob+PVMwLxJFzBGLZt25bFqPKXiODc2TNwzuv3Vn4iHoVzcRwnThzXLYZsO33+LG66RnWbP5FK4rZjDKcL9OAhERW/TBLhRqXULyqlEumPrwJo1Dgu0lgkEoHKIBEuq60CAFgr7DCYzWtev5II58ehJz3t2fsKxoIbf5t/zBtFX8+2klpFO3nyJFwLT5DQ6bClY24Ue/fsRVVVlS7za+HIkSOY8M7BHwvqMv8D1wQ6OjvR3Nysy/xERGvJJBF2isj3iYgx/fF9WDk8RwUsEokglcFhuabdveg+PoiuIwdgyCBxFpMR0QxWjovd9u3bMecLIZrc2P7Mx4Ek9uzfn92g8lx1dTV27twF58JjXeb3LI7i3LkzusytFZvNhuGDw7i1pM+q8C33GE4X2WtKRMUlk0T4+wH8eQALAOYBfFf6PipgoXA44zrC1opyGDK8VsxGRCJcEbZYLOhqb8Okb2Orm08CSezatTvLUeW/s2dPw7uY+6Qt5HcjHg2hvwj3ZJ88cwq33I9zfhDRFw1i2reI4eHirYNNRIUvk6oRU0qpK0qpRqVUk1LqLaXUZC6CI+0EwyGIOftvuxvMJsS4RxgAsOuVfoy9YJ/w2JIXv/jOCH73zjiSqc+uGIfiKbhCUXR3d+cgyvyyf/9+hINehIPLOZ3XMf8IJ04cL8oOfjt27EDKCMwGHDmd95ZjFAcPHmQnOSLKa5lUjfiaiNQ8c7tWRH5B06hIc4FgEAYNEmExmxDlijAAYMfOXZgIf/b+e/MuRBIJTLl98IQ++0vDuD+KbVu6Smp/8FMmkwlHjx6FYy53h+aUSsE1P4qTJ0/kbM5cEhGcPH0St5xjOZtTKYXb7sc4efpUzuYkItqITJY/+pVS3qc3lFIeACwIWeBCIW1WhMVkQiIWR+oFK52lpq+vD5PeIJLP1RPe2VIHk8GA1uoK1Ng/u1r2xJ/Ajj2v5CrMvHPy5Am4F3P3Vr7XOYva2hp0dBRvVchjr76Ku87HSKSSOZlvPuhEQlLYvn17TuYjItqoTDIhg4jUphNgiEhdhs+jPBYMBSGW7HcsE4PAaDYjHA4XfUe0tVRUVKChtgazwTi6Ki2f3L+9uRZ9TTWr1qqdDCl8VxE0dNiorVu3osxqgd+zgKq61oye419248HINZRXVGHn3uF1bXHwLI7hzKmTGw23IDQ3N6O1pRVjnmnsrO/O6DneoA9v334H5TY7zvUfg8mY+X/7t52P8WqRbjUhouKSyf9S/w+Ad0XkX4jIPwfwLoCf0DYs0looGILBsnY5tI0w26wIBvUp15Rv+nbsxMQLDsytlgQnUwrTyyH09vZqHVreEhGcPHkCrsXM38offzwCr3sJs1Nj8LqXMn5eMhmHc3EcR48e3UioBeX46RO443mS8fU3n4xg1rWAR7NPML44nfHzUiqFu67HePX4qxsJk4gopzI5LPdLAL4AYBGAA8DnlVK/rHVgpK1wKASDVZtE2Gi1MBFO275rNybDmb/FPxeKo762Bna7XcOo8t+xY0fhWngCleEWm7r6lTq1VmsZyiuqM57HszSJrVu3oqamZiNhFpRDhw5hzDWNaDKzRi8d9S0QEVjNFjRW12c8z8TyPGpqa9He3r7RUImIciaj97qUUvcA3NM4FsoRpRSi4TCqNFoRNljNCAQCmoxdaHp7e/Fbgcw7zE36YujtK72yac9rbm5GU2MjvK4Z1DZ2rXl9x5btaGhqh8lkgSmDxi9PeZae4OobpVHntqqqCtv7+vDANY59TWvv3e1r24qW2iZYTCZYzZlXfrjjfoJXi6g7HxEVN27gKkGRSAQQybiO8HqJ1Qy/36/J2IWmra0N/mgcgXhmh5SmwkDfzl0aR1UYTpx4FZ7FzJtr2MrK15UEJ+JReBzTJVXn9tjJ4xjxTmR8fWVZ+bqS4EQqifvOcRw5emQD0RER5R4T4RLk8/lgKSvTbHxlMTIRTjMYDNja1Ykpf2aNNaaDCfT09GgcVWE4fPgwXIsTSCW1qXTgXprAjp27SmobysDAACY8cwgntKn1/cQ7i7bWVtTXZ76VgohIT0yES5Df74epTLsi9ymzCcvLuW2IkM96tu/EdAbbIyKJFFzBSFGX8VqPuro6tHe0w+Nc+6CW1+3Ae3/627h17U+RzDBxXnaM48TxY5sNs6DY7Xbs3rUbD1wTa17r9LnxK3/2dfz3D34f0Xhmv8iNeMZxpMReUyIqbEyES9Dy8jIMNsvaF26QocwCl8ej2fiFZltvL2aia/9Tmw3G0dHaXJKNNFbz6rGjWHaMr3nd1MQDBPzLWFqYhtezdtWIRDwKj2sWAwMD2QizoBx59SjuLa/dHPT25AO4fG5MOWYx6ZhZ8/pEKomHrkkcOnQoG2ESEeUEE+EStLy8DGhUMQIAjDYrXB63ZuMXmq1bt2LGv3a3velADNt6+3IQUeE4dOgQnIvja26PaGrugBgMKLNXoLKqds1x3UsT2LljF8o03CKUrw4cOIDJDLZHbGvugsFgRLnNjtbapjXHXdkW0Ya6urpshUpEpDkuPZUgj9eLlFW7v3pDmRXeCadm4xeapqYmRBIp+GNJVFpWP6A4ExEM9LET17Nqa2vR1tYOr2sadU3dq17X0r4VDU0dMBiNGTVxWHZO4OyVC1mMtHDY7Xbs2rETD12T2N+8+vdbd1MHfuDC98JoMMBoWPtg7T3PBA6f5iE5IiosXBEuQQ6XE6Lp1ggrfB7uEX5KRNDd2YGZ4Mv3Cc8GE+ju7s5NUAXk2NEj8DrWfivfZDZnlAQnE3F4HDMluS3iqUPHjuCBb2rN6ywmc0ZJcFKl8NA9geFDpVOBg4iKAxPhEuRwOmEst2k2vrHMilAwkPGhpVKwpacXs4HVDxzFkgquYJgH5V5gePgg3EsTGTfXWIvHMYltPb0l3QJ8YGAAT1wziGXYXGMtk8vzaGxoRENDQ1bGIyLKFSbCJcjtdsNo1y4RFqMBZpuNlSOe0b11G+Ziq/9zWwjF0dxQz4NyL9DY2Ii6ujr4PPNZGW/ZOYmjR0r7QFdFRQW2bd2KMU/mrZNf5oFnEsMl/poSUWHSJREWkToR+QMRGU3/+ZnTLSLSKSJ/LCL3RWRERP6OHrEWo2WvFwYNE2EAsFTY4XbzwNxTW7ZswdxLtkbMBePY0r01hxEVlsOHhuFxrr09Yi2pVBKupUkMDg5mIarCdvDIITxYXnt7xFqUUnjgmcTQwYNZiIqIKLf0WhH+UQBvK6X6ALydvv28BIC/q5TaBeAwgB8UEfae3aRIJIJ4NKZp+TQAMJTb4HTywNxT7e3tcAUiiCXVCx+fC6fQtY2NNFZz8OAQvI5JKPXi1y9Ty645tDS3oLZ27coSxW5oaAij7mkk1ea2nMwHnbBYLWhvb89SZEREuaNXInwVwNfSn38NwFvPX6CUmldK3Uh/7gdwHwD/p90kp9MJW1UFRETTeVSZBUtLa9dzLRUmkwlNDXVYCr94VXghCnR1deU4qsLR1dUFowEIBTb3LsOycxKHDvNAFwDU19ejvr4eU8sLmxrngXsSQ8MHNf8/hYhIC3olws1KqXlgJeEF8NIilSLSDeAAgA9ecs0PiMg1EbnmcDiyGWtRcTgcMJXnoHaq3Yq5xc39gC02nZ2dWAi9OBGe90fQ2dmZ44gKh4hgcGAAnqWNb49QSsHrnMQQt0V8Ymh4CI+8m9seMeqbweDBoSxFRESUW5olwiLyhyJy9wUfV9c5TgWA/wbgh5RSvtWuU0p9RSk1pJQaamxs3Gz4RWtpaQmqXLv2yk8ZK+2YX2Ai/KyO7m2YD322koY/lkQSQE1NTc5jKiQHDw7Bv4nDXaGAGyaTkZU5njEwOIhHy2t3jVuNLxqEJ+xDXx8bwRBRYdLsiLpS6txqj4nIooi0KqXmRaQVwAvfQxcRM1aS4F9RSv2mRqGWlNn5uZwkwqZKOxy31m6NW0o6Ojrw8Nuf/d1zMZRAW3MT31pew65du+BfdiEeC8NsWf+7Gh7HJAYOHODr/Izu7m5EUzG4wsuoL6te9/NHPVN4Ze9eVjshooKl19aIbwD4YvrzLwL4+vMXyMpPq58HcF8p9a9zGFtRm5mbg6lS+/qpxvIy+JaXkUgkNJ+rULS3t2Mh+NlawguhONq5LWJNFosFO3bsgMexsVXhgGcWQ0PcFvEsg8GAffv2YdS9sdd0zD+LAW6LIKICplci/GUA50VkFMD59G2ISJuIfDN9zTEAfxHAGRH5OP1xSZ9wi8fiwgKMlfaMr1+eWcDY2+9h/taDdZ3YF6MB1nI7D8w9o7m5GZ5QBInUp19HR1Shvatbn6AKzNDgAAKe9b+Vn4hH4fMsYfduFp553sDQIB4H59b9vEQqiSfuWfT392sQFRFRbuiSCCulXEqps0qpvvSf7vT9c0qpS+nPv62UEqVUv1Jqf/rjmy8fmV4mkUhg2euFaR2JsGdiFslYHL65JSQi0XXNZ6muxPx8dpogFAOTyYT6mmo4I59eJXfEgdbWVp2iKiz79++He2lq3WXUvM4ZbOvphcWibdnAQvTKK69gwj2LeHJ9795M+xbR3NSM6ur1b6kgIsoX7CxXQhYXF2GrrIAYM/9rr25vhghQ3lgLk3V9e4tVhRVzc+tfaSpmLc3NcISfS4RDcSbCGWpsbERFZQWCvvXVqPZ7ZjE0eECjqAqb3W5HZ3snJn3rO9z6eHkG+wb2axMUEVGOMBEuIbOzszBXV6zrObVbO9D32nF0DL0CMazvkJFU2jE+tfluYMWkpb0DzmcS4WRKwROKoKnppRUE6Rn79u2D15n5nlalFJZdM3wL/yX6D+zD43VWj3gSmEf/Pr6mRFTYmAiXkKnpaaQq1t9aeaOn7E01FZia3ni5q2LU0tYO5zOlhN3RBGoqK3jqfh327+tHcDnzdxoiIR+AFDufvUT/vn6MBzLfxhSKR+AMeFg2jYgKHhPhEvJ4YhzGGu0rRjxlqqnA0sICUqnNtXAtJk1NTXA/kwi7Ikk0NjToF1AB2rVrF7yuBaQy3NO67JrBnt17WDbtJbZt2wZ3cBnBeDij68eX59DX08df4Iio4DERLiEzMzMw1VTmbD6DxQyTzQp2+vuOxsZGuJ7ZGuGKJNDU0qJjRIXHbrejta0dPu9iRtcHl+exfz/fwn8Zk8mEHX3bMe7NbKV93D+PVw7wNSWiwsdEuETEYjF4XW6Yqta3IpyIROEam0TIvbyhea21VZia2lwL12LS0NAATzDySdUDTzSFxhYelFuvff174XPPrnndSlvlWZZNy8Cefa9gIpDZgblJ/zxfUyIqCkyES8TU1BTK6qrXVTECAOZvP4RzdBKz1+4gGYuv/YTnpKrseDLODnNP2Ww2WC1mBOMr20W8KQMaGtgSfL12796NkG/tFeFw0AuLxQK2XV/brl27MBVY+zUNxMLwRYLo7u7WPigiIo0xES4Rk5OTMNasr2IEgO9UihBZ+VgnY20FHj0eW/fzilldTTU80SQAwBtTqK+v1zmiwtPX14dl9yJSyeRLr/O557Fr184cRVXYuru74Q35EIpHXnrdlG8efT29MBj444OICh//JysRj8bGoKozb6TxVGv/TjTt6kHncD+M5vUfjDHXVWFqkiXUnlVbW4vl2EoCtxyJo66uTueICo/dbkdjUzMCyy/vXBjyL2LPHr6Fnwmj0YierT2YWqOe8FRgCTv38jUlouLARLhEjD4ehbl+/R2gjBYzarvbYave2CE7Y0UZ4vE4PB7Php5fjOoaGuGNJlf2r4ajqK2t1TukgrRr5w74vC9P2vzeRezYsSNHERW+Hbt3Yjrw8l8uZkJLfE2JqGgwES4BsVgMjsUlmGtzVzHiKRGBrbEWT548yfnc+aqmvgH+eAqRpILBYIDNtv7azgTs3LkDkcDqFUli0RBikRDa2tpyGFVh275jO2ZCq7+m8VQCC8tObNu2LYdRERFph4lwCZicnERZTRXEZNRlflVTjkejo7rMnY9qamrgTxngiyVRXZG7us7Fpre3Fz7P6oe7At4lbOneyr2s69Db24u55SUk1Ytrf88HnGhpaoF1ne3WiYjyFX9ClIDR0VEY6qt0m9/UUI2RB/d1mz/fVFdXI5AAAvEUqipzv0pfLJqamqBSSUQjwRc+HvAtYccOdj5bD7vdjpqqajhDL97KNOd3oKevJ8dRERFph4lwCbh7/x6kXr+Ey9JQjenJSXaYS6uqqkIgkVpJhKv0+wWl0IkItmzpXvXAXDToQm8Pk7b12tbTg1n/i7dHzEVc6N3OXy6IqHgwES5ySimMjY3B0lCjWwwGqwXm8jI21kirrKxEIJZEMJ5EZfX6DzDSd/Rt70XQ53zhYz6vA1u3bs1xRIWvZ3svFsLuFz62EHLzNSWiosJEuMg5nU7EEwkYK9dfOi2bTA01ePjwoa4x5IuKigqEYnEEEylUVtfoHU5B29rdjVj4s2/jx6IhKJVijeYN2LJlCxYjn31N48kEXAEvOjo6dIiKiEgbTISL3P3792FrroNsoBlGNklDFW7dvaNrDPmivLwcoVgcoSRQUcmtEZuxZcsWBJY/+zZ+0OdER0eH7t/3hWjLli2YW3YglW4D/tRSyIPmhkaYTOuvJ05ElK+YCBe5O/dGoHTcH/yUpakWjx49gnruh2spMplMMBmNWI6vJMW0cc3NzYhGQkjEY5+6P+h3swXwBlVUVMBms8IXDXzq/qWQG51dXTpFRUSkDSbCRW7k3j1YmvXvXGasKEMKwPz8vN6h5IUyqxWeaBJlZWV6h1LQDAYDGptaEA5++q38eMSLLV2dOkVV+Npb27H0XOWIpbAHnd1MhImouDARLmJOpxOhYBCmmooNj+GdmsPoH7yDuZv3NrWaKyKwtNRhZGRkw2MUk8rKCkwvh1HJ8mmb1tnRgaD/04e7IkEv2tvbdYqo8HVu6cRS6NOvqSvm4/5gIio6TISL2L1792BrbdjUPknv1DxSiST8C04kItFNxSON1bh5+9amxigWP/aP/wn+5Ze/jD179ugdSsHr6upANLT8yW2lFPw+NzvKbUJ7ZwfcMf+n7nOEvHxNiajoMBEuYjdufQw0bO4wVk1XKwxGAyqa62Gyba6blLWlDg8ePGA9Yax0l+vo6GDXsyxobW1FPPqdpC0Ri0BEuNq+CS0tLXDFfJ/cTqSSWA750NTUpGNURETZx+O/RUophZG7I7CfPbCpcWq62lDTlZ1VIGN5GYxWCyYnJ1mLlLKmpaUFkZD3k9vhkBeNjU2sGLEJra2tcAW9n9z2RvyoqaphxQgiKjpcjipSMzMzSBkAk871g59nbK7F7du39Q6DikhTUxMCPu8ne9gjIR9ampt1jqqw1dbWIhSNIJ5MAADcER+aGrkaTETFh4lwkbp95w7MLfpXi3iesaUWH928oXcYVETsdjtMJhMSsQgAIBL2o7WVifBmGAwG1NXUwpvecuKN+NHcwteUiIoPE+Ei9eG1j2DMw0TY0lyHqYkJhMNhvUOhIlJXV49IeGVPazIW5F7WLGiob4Ankk6EYwE0tvA1JaLiw0S4CEUiEYw/GYelNf/ayxrMJtib63H37l29Q6EiUldfj2hkpQFEPBpka+UsaGhsgC8aBAD4k2E0NDToHBERUfYxES5CIyMjsDfWwmDOz4Mtqqka125wewRlT2NDA2KRlaQtFgmiri7/3g0pNPWNDfDFVl5TXzyI2tpanSMiIso+JsJF6KPr14HmGr3DWJWtvRE3P77JdsuUNY2N9YinVy9DIT+Ttiyoq6+DP7WyhcnHXy6IqEgxES4ySincuHkD1o5GvUNZlbGqHEkBJicn9Q6FikRNTQ2SiQhSyQSSiTgqKjbeTZFW1NTUIBhfOYDoDwdQU1Ojb0BERBpgIlxkJicnkTIITNX5mwiICMxt9bh+47reoVCRqK6uRiIeQSwWRnl5JWsIZ0F1dTUC8RCiyTgUAJvNpndIRERZx0S4yFy7dg2mPDwk9zxTWwPe+/BDvcOgIlFdXY14NIR4NMSOcllSXV2NQDSEYCyMqgr+ckFExYmJcJF578MPYGrP/0TY0lwLx9ISPB6P3qFQEaisrEQsGkY8FmEinCWVlZUIRcMIJSKoKM/fd5iIiDaDiXARcblccLlcsDTl/0EhMRhQ1t6E69e5PYI2r7KyEtFIGLFoEJVVTISzwWq1IqVScId9qKxkIkxExYmJcBG5fv06bO2NEENh/LUa2urwzgfv6x0GFQGr1YqqqipM3H8HnR0deodTFEQE7a3t+PrjP0N7J19TIipO+VloljbknQ/eg6Et/7dFPGVta8STD/8MoVAIdrtd73CogIkIfvqn/53eYRSd/+snvqx3CEREmiqMpUNaUyAQwMTjcVjbCqf7k8FiQllzPW7fvq13KERERFSCmAgXiY8//hhlrQ15201uVa21+Pb77+kdBREREZUgJsJF4p3334O0FV7nJ1tnM+7evoNEIqF3KERERFRimAgXgVgshnsj92DtaNI7lHUzlllhra3EyMiI3qEQERFRiWEiXATu3LkDW301jDaL3qFsiLTW4d33WT2CiIiIckuXRFhE6kTkD0RkNP3nqoVvRcQoIjdF5HdyGWMhee+DDwpyW8RT1s6VesKpVErvUIiIiKiE6LUi/KMA3lZK9QF4O317NX8HwP2cRFWAUqkUbt68AWtn4W2LeMpUVQ6xmjE2NqZ3KERERFRC9EqErwL4WvrzrwF460UXiUgHgDcA/Fxuwio8jx49gtFug6misOvwGtvr8cFHH+odBhEREZUQvRLhZqXUPACk/1xtOfPfAPgRAGu+Zy4iPyAi10TkmsPhyFqg+e79Dz8oqCYaq7F0NOL9Dz7QOwwiIiIqIZolwiLyhyJy9wUfVzN8/mUAS0qp65lcr5T6ilJqSCk11NjYuKnYC4VSCh9+9BEsHYX/9ZrqqhCORDA7O6t3KERERFQiNOu+oJQ6t9pjIrIoIq1KqXkRaQWw9ILLjgG4IiKXANgAVInIf1JKfZ9GIRec2dlZROIx1NRWaj6XUgoiotn4IgJLRyOuXb+O9vZ2zeYhIiIiekqvrRHfAPDF9OdfBPD15y9QSv0DpVSHUqobwPcA+CMmwZ927do1WNoaNE1QAcAzPoPR3/s2Zm+MQCml2Tym9nq89wHLqBEREVFu6JUIfxnAeREZBXA+fRsi0iYi39QppoLz3kcfwtSe/f3ByVgck+9cx9gfvouQy4vlmQUopRBYdCEZi2d9vqcszfWYn5uDz+fTbA4iIiKip3RJhJVSLqXUWaVUX/pPd/r+OaXUpRdc/ydKqcu5jzR/+f1+LMzNwdKc/frBYc8yIr4gkvEEfLOLKG+uh8FoRFVbE4wWc9bne0qMBpS1NuL27duazUFERET0FDvLFajbt2+jrLUBYjRmfeyy2mrYqipgtJgRj0ThfjwNs92Glle2a74NQ1pq8P61jzSdg4iIiAhgIlywPrx+DWiu0WRso8WMLccG0Hv2CFLxla0QUX8QCQ23RTxlbW/EyJ277DJHREREmmMiXIBSqRRG7o7A2tqg+VwN27fCVlOJ+t4umG1Wzecz2m0w2W0YHx/XfC4iIiIqbZqVTyPtTE1NAWYjTJXad5Mrb6xDeWP29yG/jLG5Fh/fuoWenp6czktERESlhSvCBej2nTswNdfqHYZmjC21uP7xTb3DICIioiLHRLgAfXznNgxNNXqHoRlLUy1mpqYQi8X0DoWIiIiKGBPhApNMJvFkdAyWpuJdETaYTSirq8bY2JjeoRAREVERYyJcYCYmJmCuKIOxTPuDa7pqqMbdkRG9oyAiIqIixkS4wDx69AjGhmq9w9CcqaEaIw/u6x0GERERFTEmwgXm3sMHkLpKvcPQnLmhGpPjE6wnTERERJphIlxgxsbGYGms0XSOWDCMqfduYubaHSRz0ETjRYxlVphsFszNzekyPxERERU/JsIFJBAIIBgIwlhVruk8yzPzCHv9CDo88M0twb/gQDwU0XTOFzHXV7OxBhEREWmGiXABmZqagr2+BiKi6Tz2+loYjAYYzSb455cwd/M+Jt+7iVQiqem8z0tV2vBkgokwERERaYOd5QrI5OQkUJ2DbnINtdh2+jBEBDPX7gAAUvEEUskkDCaj5vM/ZaqrwuPxJzmbj4iIiEoLE+EC8mRyAqjSPhEGAKN55VujZe92eCZnYa+vgclqycncT5lrKjD30cOczklERESlg4lwAZmdm4Opuy6nc1oq7Gje05fTOZ8y2G2IhiOIRCKw2Wy6xEBERETFi3uEC4hzyQFjZW5WhPOBiKCsphKLi4t6h0JERERFiIlwgYjFYggFgzDaS2tl1FhpZyJMREREmmAiXCA8Hg+s5XaIQduKEflG2czweDx6h0FERERFiIlwgfB6vTDpsBqcjMXhmZhFZNmf87kBIGU1w+126zI3ERERFTcelisQy8vLMJTltmoDAMzffoCgwwODyYhtpw59Uk0iV4w2KxxuV07nJCIiotLAFeECEQgEgBwnoQCgUir9iVr5yDGD1Qx/IJDzeYmIiKj4cUW4QESjUShj7n9vae3fgeWZBZTV1cBoMed8fjEZEYkGcz4vERERFT8mwgUiEokgZcp9ImyyWVHfuyXn8z4lJiMikYhu8xMREVHx4taIAhGNxYASqxgBAGI0IB6L6x0GERERFSEmwgXCYCjdvyqR0vsFgIiIiLRXutlVgTEaDLocVtObUqX9SwARERFphxlGgTAYDJDSy4MBpUquiQgRERHlBhPhAmGz2SCJlN5h5JyKJ1BWVqZ3GERERFSEmAgXiPLy8pJMhFOxOCrs5XqHQUREREWIiXCBsNvtQDyhdxg5p2IJVFRU6B0GERERFSEmwgWiuroaqXBU7zByLhWOor62Tu8wiIiIqAixoUaBaGhoQMyf2w5r3qk5OEcnUdFcj5a923M691OGSBxNjY26zE1ERETFjSvCBaK6uhqJWAwqkczZnJ7xWSRjcSxPLyCpV1OLcBT19fX6zE1ERERFjYlwgTAYDKiqrUUiENJ8LpVSiAVCqGxrgghQVlcNMRo1n/dFYr4gmpqadJmbiIiIihsT4QLS0d6OhDeg+TxzN0cw/j+vIerzo3ZrJ8LuZUx/eAsqlduqFal4ArFQGM3NzTmdl4iIiEoDE+EC0rN1K5Je7fcJh1zeT/4MOd0AgIjXj0SOt0ckvAE0NjezsxwRERFpghlGAdnStQUGv/ZbI5p296KsphJNu3tR37sF1go7arvbYbZZNZ/7WQmPH1u6unI6JxEREZUOVo0oIL29vQgveVCmFES0aztsLrNBjEYk4wlUd7SgorlBs7leRnn82H1sWJe5iYiIqPgxES4g9fX1sJjNSPpDMFVp123N8eAxIr4gQi4vDEYDPOOzsDfWonl3r2ZzvkjS6cP27fqUbSMiIqLix60RBaavrw8xh1fTOWw1VQAAS0UZvFNziIXC8E7OIR7JXUOPVCSGeDCMzs7OnM1JREREpUWXRFhE6kTkD0RkNP1n7SrX1YjIb4jIAxG5LyJHch1rvtn/Sj/U0rKmczTt7sWWYwPYcmQAlS0rzSzKaqtgslg0nfdZ0QUXerb38qAcERERaUavLONHAbytlOoD8Hb69ov8WwDfUkrtBLAPwP0cxZe3+vv7EZ13Qiml2RwiAltVBQwmI2q62tCwvRtNu3sgBu32JT8vtejBwQODOZuPiIiISo9eifBVAF9Lf/41AG89f4GIVAE4AeDnAUApFVNKeXMUX95qbm5GWVkZEh5/Tuabv/0QzkcTmPngNpLxRE7mVEohNudCf39/TuYjIiKi0qRXItyslJoHgPSfL2odtg2AA8AvishNEfk5EVn1hJiI/ICIXBORaw6HQ5uo88TggQOIzubma1TJlZbOSilAw1XoZyU8flgtFrS2tuZkPiIiIipNmiXCIvKHInL3BR9XMxzCBGAAwM8qpQ4ACGL1LRRQSn1FKTWklBpqbGzMwleQv44ePgI158rJXC39O1Df04X2wb0wWsw5mTM2vYQjhw5rWiKOiIiISLPyaUqpc6s9JiKLItKqlJoXkVYASy+4bAbAjFLqg/Tt38BLEuFSsmPHDiSDEST8IZgq7ZrOZS6zoWF7t6ZzPC8568Thz39fTuckIiKi0qPX1ohvAPhi+vMvAvj68xcopRYATIvIjvRdZwHcy014+c1gMGBo6CCiU4t6h5J1cW8Akkihtze3NYuJiIio9OiVCH8ZwHkRGQVwPn0bItImIt985rq/BeBXROQ2gP0A/mWuA81Xp06cQHJySdPqEXqIjc/j1WOvsmwaERERaU6XznJKKRdWVnifv38OwKVnbn8MYCh3kRWOHTt2wJhSSHj8MNdV6R1OViilEJtcxKnv+5LeoRAREVEJ4LJbgTIYDDh5/ARi4wt6h5I1sQU3aqqq0NXVpXcoREREVAKYCBew06dOIToxD5VM6R1KViTG53H+zGfeKCAiIiLSBBPhAtbS0oLOzk5EiuDQXCoSQ3jWgePHj+sdChEREZUIJsIF7tJrryOp4faIoNODqfc/hmtsUrM5ACD8ZA4DAwOoqKjQdB4iIiKip5gIF7ihoSEkl4OIewOajO98+ARhjw/O0UnEI1FN5lBKIf54DhcvvKbJ+EREREQvwkS4wJlMJpw/dw6x0RlNxi+rqwEAWCvtMGnUWS4650RtZRX6+vo0GZ+IiIjoRZgIF4EL588jMrGAVCye9bGbdvVg64khdB05ANGotm9idA5vXnqDLZWJiIgop5gIF4Ha2lr07+tHeGxWk/Et5XYYjEZNxk4sB5Dw+HDkyBFNxiciIiJaDRPhIvHWm1cQezQDlSqsUmrRh9N47fwFWCwWvUMhIiKiEsNEuEhs27YNbS0tiEwWToONZDiKyNQiXn+Nh+SIiIgo95gIF5HPXbmK+MMZKKX0DiUjkdFpHBo+hKqq4mgRTURERIWFiXAROXDgAMoMJsQW3HqHsqZUPIHo6CzeunJF71CIiIioRDERLiIGgwGfu/oW4g+m9A5lTeGxWezcuROtra16h0JEREQliolwkXn12DEofxhx17LeoaxKpVKIPZrGF976nN6hEBERUQljIlxkzGYzLl96A7EH03qHsqrwxALamlvQ29urdyhERERUwpgIF6Hz584htuBGwhfUO5TPUEoh/mAKf+7zX9A7FCIiIipxTISLUFlZGS5cOI9oHu4Vjs44UF1Wjv7+fr1DISIiohLHRLhIXXr9IiJTi0iGInqH8olnV4PZTpmIiIj0xkS4SFVVVeHE8eOI5NGqcGzRDUsSGB4e1jsUIiIiIibCxezqm1cQeTyHVDSmdygAgPiDaXz+rc/BYOC3HREREemPGUkRq6+vx9DBIYQfba6CRCqR3HS3urhrGcoXwonjxzc1DhEREVG2MBEucp+7chXRRzNIxRMber7r8RRG/+AdzHx4Gyq18WQ49mAKVy6/CZPJtOExiIiIiLKJiXCRa29vx86dOxF+PLuh5/vnHQCAkHsZiWh0Q2MkfEHEFj04d/bshp5PREREpAUmwiXgC299DrGH01Cp1LqfW9fTCXOZDTVdrTCX2TY0f/TBFC6cPw+bbWPPJyIiItIC36cuAb29vWhraYV7cgFlW9vW9dyq1iZUtTZteO5kOIrI5AIu/e8XNzwGERERkRa4IlwivnD1LcQfzmz60Nt6RR5O48iRI6iqqsrpvERERERrYSJcIvbt24cyoxmxBVfO5kzFE4g8nsWVy2/mbE4iIiKiTDERLhEGgwGfv3IV8YczOZsz8ngWO3fsQGtra87mJCIiIsoUE+EScvToUSQ9fsSXA5rPpZRCbHQWb715RfO5iIiIiDaCiXAJsVgsuHD+PGI5WBWOzjpQW1mFHTt2aD4XERER0UYwES4xr114DZHJec3bLidGZ3H18psQEU3nISIiItooJsIlprq6GvsPHNhwg41MxJcDSC4HceTIEc3mICIiItosJsIl6PLFS4iPzWlWSi02OouzZ86wnTIRERHlNSbCJainpwe1VdWIzjmzPnYqnkBkfA4Xzp3P+thERERE2cREuASJCN54/SKST+azPnZkfB47du5EfX191scmIiIiyiYmwiXqyJEjiC66kQxFsjpucnwBr5+/kNUxiYiIiLTARLhE2Ww2DA8fQuTxXNbGjLt9kFgC+/bty9qYRERERFphIlzCXjt/HvHxhawdmos9mceZU6dhMPDbioiIiPIfM5YStnXrVpTbbIg7vJseSyVTiEwu4NTJk5sPjIiIiCgHRKsSWnoSEQeASb3jKARlZWUt9urKNlOFfVOdL1KxBGK+QHjZ672XrdgobzQAyH6JEaLStUUp1ah3EERUpIkwEWWPiFxTSg3pHQcREVG2cWsEEREREZUkJsJEREREVJKYCBPRWr6idwBERERa4B5hIiIiIipJXBEmIiIiopLERJiIiIiIShITYSICAIiIEpFffua2SUQcIvI7z9x3UUSuich9EXkgIv9Kn2iJiIg2j4kwET0VBLBXRMrSt88DmH36oIjsBfDTAL5PKbULwF4AT3IeJRERUZYwESaiZ/0PAG+kP/9eAL/6zGM/AuDHlVIPAEAplVBK/UyO4yMiIsoaJsJE9KxfA/A9ImID0A/gg2ce2wvgui5RERERaYCJMBF9Qil1G0A3VlaDv6lvNERERNpiIkxEz/sGgH+FT2+LAIARAIO5D4eIiEgbTISJ6Hm/AOCfK6XuPHf/TwL4MRHZDgAiYhCRH855dERERFli0jsAIsovSqkZAP/2BfffFpEfAvCrImIHoAD8bo7DIyIiyhq2WCYiIiKiksStEURERERUkpgIExEREVFJYiJMRERERCWJiTARERERlSQmwkRERERUkpgIExEREVFJYiJMVARE5Ksi8l3pz39ORHanP/+x5657V4/4ckFE/qmI/B96x0FERIWDiTBRkVFK/TWl1L30zR977rGjOoSUNSJi1DsGIiIqHkyEiXQgIn9JRG6LyC0R+WUR2SIib6fve1tEutLXfVVEfkpE3hWRJ8+s+oqI/LSI3BOR3wXQ9MzYfyIiQyLyZQBlIvKxiPxK+rHAM8//SRG5KyJ3ROS70/efSj//N0TkgYj8iojIS76OS+nrvp2O83fS95eLyC+IyEciclNErqbv/8si8psi8i0RGRWRn3hmrAsi8p6I3BCR/yoiFen7J0Tk/xSRbwP4cyLypfS4t0Tkv6W73BEREa0bE2GiHBORPQD+IYAzSql9AP4OgJ8G8EtKqX4AvwLgp555SiuAVwFcBvDl9H2fA7ADwCsAvgTgMyu9SqkfBRBWSu1XSv2F5x7+PID9APYBOAfgJ0WkNf3YAQA/BGA3gG0Ajq3yddgA/EcAF5VSrwJofObhfwjgj5RSBwGcTo9fnn5sP4DvTsf+3SLSKSINAP4RgHNKqQEA1wD88DPjRZRSryqlfg3AbyqlDqZfu/sA/uqL4iMiIloLE2Gi3DsD4DeUUk4AUEq5ARwB8J/Tj/8yVhLfp/67UiqV3u7QnL7vBIBfVUollVJzAP5onTG8+szzFwH8KYCD6cc+VErNKKVSAD4G0L3KGDsBPFFKjadv/+ozj10A8KMi8jGAPwFgA9CVfuxtpdSyUioC4B6ALQAOYyXxfif9nC+m73/qvzzz+V4R+Z8icgfAXwCwZx1fNxER0SdMegdAVIIEgFrjmmcfjz733Bdds5EYVvPsfEms/v/Ey8YQAF9QSj381J0ih1YZXwD8gVLqe1cZL/jM518F8JZS6paI/GUAp14SBxER0aq4IkyUe28D+PMiUg8AIlIH4F0A35N+/C8A+PYaY/wZgO8REWN6S8PpVa6Li4h5led/d/r5jVhZYf5wnV/HAwDbRKQ7ffu7n3ns9wD8raf7i0XkwBpjvQ/gmIj0pq+3i8j2Va6tBDCf/rqe3/JBRESUMa4IE+WYUmpERH4cwJ+KSBLATQB/G8AviMjfA+AA8FfWGOa3sLLF4g6AR1jZ2vAiXwFwW0RuPLdP+Lewsh3jFlZWln9EKbUgIjvX8XWEReRvAviWiDjx6UT6XwD4N+m5BcAEVvY4rzaWI726+6siYk3f/Y/SX9vz/jGADwBMYuXrr8w0ZiIiomeJUpt5d5WISpmIVCilAulk998DGFVK/b96x0VERJQJbo0gos34Uvpw2wiAaqxUkSAiIioIXBEmojWJyG8B2Prc3X9fKfV7esRDRESUDUyEiYiIiKgkcWsEEREREZUkJsJEREREVJKYCBMRERFRSWIiTEREREQl6f8HtPVpzetnFHAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 1:\n",
    "    # only look at pull aligned events that has no preceding self pull \n",
    "    doSingleSelfPulls = 1\n",
    "    \n",
    "    # define which time window will be analyzed\n",
    "    gaze_duration_type = 'before_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "\n",
    "    # time window time points\n",
    "    x_full = np.arange(-4, 4, 1/fps)\n",
    "    #\n",
    "    if gaze_duration_type == 'before_pull':\n",
    "        # Only use the pre-pull window (-4s to 0s)\n",
    "        pre_mask = x_full <= -0.15\n",
    "    elif gaze_duration_type == 'after_pull':\n",
    "        # Only use the post-pull window (0s to 4s)\n",
    "        pre_mask = x_full >= 0.15\n",
    "    elif gaze_duration_type == 'around_pull':\n",
    "        # the entire -4 to 4s\n",
    "        pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "    \n",
    "    # sampling interval in seconds\n",
    "    dt = 1 / fps  # sampling interval in seconds\n",
    "    \n",
    "    # initialize some dataframe for saving data\n",
    "    bhvevents_aligned_bhvonly_eachevents_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name', 'bhv_id',\n",
    "                                        ])\n",
    "    selfgaze_partnerIntention_corr_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name', 'selfgazeVarName',\n",
    "                                        ])\n",
    "    selfgaze_partnerIntention_multivar_corr_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                        ])\n",
    "    \n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "\n",
    "                # only look at one neuron since it's behavioral related exploration\n",
    "                for ineuron in np.arange(0,1,1):\n",
    "                # for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                        \n",
    "                        # self gaze distribution\n",
    "                        pull_trig_gazeprob_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_trig_gazeprob_name])[0])\n",
    "                        # self pull distribution\n",
    "                        pull_trig_selfpull_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_trig_selfpull_name])[0])\n",
    "                        # other pull distribution\n",
    "                        pull_trig_otherpull_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_trig_otherpull_name])[0])\n",
    "                        # number of preceding failed pulls\n",
    "                        pull_num_pre_failpull_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_num_pre_failpull_name])[0])\n",
    "                        # time between the current pull and the previous juice\n",
    "                        pull_time_pre_reward_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_time_pre_reward_name])[0])\n",
    "                        \n",
    "                        #\n",
    "                        nevents = np.shape(pull_trig_gazeprob_tgt)[0]\n",
    "                        \n",
    "                        for ievent in np.arange(0,nevents,1):\n",
    "                            \n",
    "                            # number of preceding failed pulls\n",
    "                            pull_num_pre_failpull_ievent = pull_num_pre_failpull_tgt[ievent]\n",
    "                            \n",
    "                            # time between the current pull and the previous juice\n",
    "                            pull_time_pre_reward_ievent = pull_time_pre_reward_tgt[ievent]\n",
    "                            lastreward_time = -pull_time_pre_reward_ievent\n",
    "                            \n",
    "                            #\n",
    "                            # calculate the mean self pull number\n",
    "                            #\n",
    "                            from scipy.signal import find_peaks\n",
    "                            #\n",
    "                            pull_trig_selfpull_ievent = pull_trig_selfpull_tgt[ievent,:]\n",
    "                            try:\n",
    "                                # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                                data = pull_trig_selfpull_ievent[pre_mask]\n",
    "                                peaks, _ = find_peaks(data)\n",
    "                                selfpull_num = len(peaks)\n",
    "                            except:\n",
    "                                selfpull_num = np.nan\n",
    "                            #\n",
    "                            if selfpull_num > 0:\n",
    "                                lastselfpull_time = x_full[peaks[-1]]\n",
    "                            else:\n",
    "                                lastselfpull_time =  np.nan\n",
    "                            \n",
    "                            #\n",
    "                            # calculate the mean other pull number\n",
    "                            pull_trig_otherpull_ievent = pull_trig_otherpull_tgt[ievent,:]\n",
    "                            try:\n",
    "                                # otherpull_num = np.trapz(pull_trig_otherpull_ievent[pre_mask], dx=dt)\n",
    "                                data = pull_trig_otherpull_ievent[pre_mask]\n",
    "                                peaks, _ = find_peaks(data)\n",
    "                                otherpull_num = len(peaks)\n",
    "                            except:\n",
    "                                otherpull_num = np.nan\n",
    "                                                              \n",
    "                            #\n",
    "                            # reaction time: time since last reward or last pull\n",
    "                            reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #\n",
    "                            # calculate the gaze accumulation level\n",
    "                            pull_trig_gazeprob_ievent = pull_trig_gazeprob_tgt[ievent,:]\n",
    "                            \n",
    "                            # \n",
    "                            # only consider gaze that's within reaction time\n",
    "                            if 0:\n",
    "                                ind_beforeRT = pull_trig_gazeprob_ievent < -reaction_time\n",
    "                                pull_trig_gazeprob_ievent[ind_beforeRT]=0     \n",
    "                            #\n",
    "                            # for the socialgaze_prob, only use the meaningful ones\n",
    "                            if 0:\n",
    "                                trace = pull_trig_gazeprob_ievent\n",
    "                                time_trace = np.arange(-4, 4, 1/30)\n",
    "                                filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                pull_trig_gazeprob_ievent = filtered_trace\n",
    "                                \n",
    "                            \n",
    "                            #\n",
    "                            try:\n",
    "                                gaze_accum = np.trapz(pull_trig_gazeprob_ievent[pre_mask], dx=dt)\n",
    "                            except:\n",
    "                                gaze_accum = np.nan\n",
    "                            # calculate the gaze start stop and duration\n",
    "                            try:\n",
    "                                first_increase_idx = np.where(np.diff(pull_trig_gazeprob_ievent) > 0)[0][0] + 1\n",
    "                                #\n",
    "                                last_decrease_idx = np.where(np.diff(pull_trig_gazeprob_ievent) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                #\n",
    "                                gazestart_time = x_full[first_increase_idx].copy()\n",
    "                                gazestop_time = x_full[last_decrease_idx].copy()\n",
    "                                #    \n",
    "                                # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                                if gaze_duration_type == 'around_pull':\n",
    "                                    gazestart_time = gazestart_time\n",
    "                                    gazestop_time = gazestop_time\n",
    "                                if gaze_duration_type == 'before_pull':\n",
    "                                    if (gazestart_time > 0):\n",
    "                                        gazestart_time = np.nan\n",
    "                                        gazestop_time = np.nan\n",
    "                                    elif (gazestop_time > 0):\n",
    "                                        gazestop_time = 0\n",
    "                                if gaze_duration_type == 'after_pull':\n",
    "                                    if (gazestop_time < 0):\n",
    "                                        gazestart_time = np.nan\n",
    "                                        gazestop_time = np.nan\n",
    "                                    elif (gazestart_time < 0):\n",
    "                                        gazestart_time = 0                                                \n",
    "                                #\n",
    "                                # if (gazestart_time == timewins[0]) | (gazestart_time == timewins[-1]):\n",
    "                                #     gazestart_time = np.nan\n",
    "                                # if (gazestop_time == timewins[0]) | (gazestop_time == timewins[-1]):\n",
    "                                #     gazestop_time = np.nan\n",
    "                                if (gazestop_time < gazestart_time):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan                           \n",
    "                            except:\n",
    "                                gazestart_time = np.nan\n",
    "                                gazestop_time = np.nan\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                                \n",
    "                            #\n",
    "                            # for variables in the pull_trig_events_tgtnames do some more detailed analysis to define metrics\n",
    "                            event_dict = {}\n",
    "                            for name in pull_trig_events_tgtnames:\n",
    "                                pull_trig_tgt_ievent = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [name])[0])[ievent]\n",
    "                                \n",
    "                                # calculate the mean value\n",
    "                                tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
    "                                event_dict[name+'_mean'] = tgt_mean\n",
    "                                \n",
    "                                # more detailed metric\n",
    "                                if (name == 'otherani_otherlever_dist') | \\\n",
    "                                   (name == 'animal_lever_dist') | \\\n",
    "                                   (name == 'otherani_othertube_dist') | \\\n",
    "                                   (name == 'animal_tube_dist') | \\\n",
    "                                   (name == 'animal_animal_dist') :\n",
    "\n",
    "                                    tgt_velocity = np.gradient(pull_trig_tgt_ievent, 1/fps)\n",
    "                                    #\n",
    "                                    tgt_speed = np.abs(tgt_velocity)\n",
    "                                    #\n",
    "                                    tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
    "                                    #\n",
    "                                    tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
    "\n",
    "                                    min_dist = x_full[np.argmin(pull_trig_tgt_ievent)]\n",
    "                                    max_dist = x_full[np.argmax(pull_trig_tgt_ievent)]\n",
    "                                    #\n",
    "                                    pre_min_mask = x_full <= min_dist\n",
    "\n",
    "                                    # find the time that the dramatic change starts, if could not find it, use the -4s\n",
    "                                    percentile = 95\n",
    "                                    dt = 1 / fps\n",
    "                                    abs_derivative = np.abs(np.gradient(pull_trig_tgt_ievent[x_full <= 0], dt))\n",
    "                                    threshold = np.percentile(abs_derivative, percentile)\n",
    "                                    # Start from the first time point (index 0)\n",
    "                                    idx_change = np.where(abs_derivative > threshold)[0]\n",
    "                                    if len(idx_change) > 0:\n",
    "                                        onset_idx = idx_change[0]\n",
    "                                        change_time = x_full[x_full <= 0][onset_idx]\n",
    "                                    else:\n",
    "                                        change_time = x_full[0]\n",
    "                                    #\n",
    "                                    tgt_changetime = change_time\n",
    "                                    #\n",
    "                                    post_changetime_mask = x_full >= change_time\n",
    "                                    #\n",
    "                                    # min and max after the change_time\n",
    "                                    min_dist_post_changetime = x_full[x_full>=change_time][np.argmin(pull_trig_tgt_ievent[x_full>=change_time])]\n",
    "                                    max_dist_post_changetime = x_full[x_full>=change_time][np.argmax(pull_trig_tgt_ievent[x_full>=change_time])]\n",
    "\n",
    "                                    #\n",
    "                                    # find the partner lever approaching trend\n",
    "                                    from scipy.stats import linregress\n",
    "                                    # Define time range and extract window\n",
    "                                    y_full = np.array(pull_trig_tgt_ievent)                      \n",
    "                                    #\n",
    "                                    # x_pre = x_full[pre_mask]\n",
    "                                    # y_pre = y_full[pre_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask & post_changetime_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask & post_changetime_mask]    \n",
    "                                    x_pre = x_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]\n",
    "                                    y_pre = y_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]                        \n",
    "                                    # Linear regression\n",
    "                                    slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                                    # slope = (y_pre[-1] - y_pre[0])/(x_pre[-1] - x_pre[0])\n",
    "                                    #\n",
    "                                    approaching_slope = slope\n",
    "                                \n",
    "                                    #\n",
    "                                    event_dict[name+'_speed'] = tgt_speed \n",
    "                                    event_dict[name+'_meanspeed'] = tgt_meanspeed \n",
    "                                    event_dict[name+'_velocity'] = tgt_velocity \n",
    "                                    event_dict[name+'_meanvelocity'] = tgt_meanvelocity \n",
    "                                    event_dict[name+'_max'] = max_dist \n",
    "                                    event_dict[name+'_min'] = min_dist \n",
    "                                    event_dict[name+'_changetime'] = tgt_changetime \n",
    "                                    event_dict[name+'_appoachslope'] = approaching_slope \n",
    "                                    \n",
    "                            \n",
    "                            # put the data together\n",
    "                            row_data = {'dates': date_ana, \n",
    "                                        'condition':cond_ana,\n",
    "                                        'act_animal':act_animal_ana,\n",
    "                                        'bhv_name': bhvname_ana,\n",
    "                                        'bhv_id':ievent,\n",
    "                                        'gaze_accum':gaze_accum,\n",
    "                                        'gazestart_time':gazestart_time,\n",
    "                                        'gazestop_time':gazestop_time,\n",
    "                                        'gaze_duration':gazestop_time-gazestart_time,\n",
    "                                        'selfpull_num':selfpull_num,\n",
    "                                        'otherpull_num':otherpull_num, \n",
    "                                        'prefailpull_num':pull_num_pre_failpull_ievent,\n",
    "                                        'time_since_lastreward':pull_time_pre_reward_ievent,\n",
    "                                        'reaction_time':reaction_time,\n",
    "                                       }\n",
    "                            # Add all events into the row\n",
    "                            row_data.update(event_dict)\n",
    "                            #\n",
    "                            bhvevents_aligned_bhvonly_eachevents_df = bhvevents_aligned_bhvonly_eachevents_df.append(\n",
    "                                                                       row_data, ignore_index=True)\n",
    "                            \n",
    "                        # remove events that has multiple self pulls before the aligned self pulls    \n",
    "                        if doSingleSelfPulls: \n",
    "                            ind_singlepull = bhvevents_aligned_bhvonly_eachevents_df['selfpull_num']==0\n",
    "                            bhvevents_aligned_bhvonly_eachevents_df = bhvevents_aligned_bhvonly_eachevents_df[ind_singlepull]\n",
    "    \n",
    "                        \n",
    "        \n",
    "                        #\n",
    "                        # do some regression analysis and do some plotting for each date\n",
    "                        # do some single regression\n",
    "                        import seaborn as sns\n",
    "                        from sklearn.linear_model import LinearRegression\n",
    "                        import math\n",
    "\n",
    "                        ind_cond_plot = bhvevents_aligned_bhvonly_eachevents_df['condition']==cond_ana\n",
    "                        ind_date_plot = bhvevents_aligned_bhvonly_eachevents_df['dates']==date_ana\n",
    "                        ind_ani_plot  = bhvevents_aligned_bhvonly_eachevents_df['act_animal']==act_animal_ana\n",
    "                        ind_bhv_plot  = bhvevents_aligned_bhvonly_eachevents_df['bhv_name']==bhvname_ana\n",
    "                        #\n",
    "                        ind_plot = ind_cond_plot & ind_date_plot & ind_ani_plot & ind_bhv_plot\n",
    "                        #\n",
    "                        bhvevents_aligned_bhvonly_df_toplot = bhvevents_aligned_bhvonly_eachevents_df[ind_plot]\n",
    "                        \n",
    "                        #\n",
    "                        xxx_plot_name = 'gaze_accum'\n",
    "                        # Automatically select possible Y variables\n",
    "                        excluded_keys = {'dates', 'condition', 'act_animal', 'bhv_name', 'bhv_id', \n",
    "                                        'gaze_accum', 'gaze_duration', 'gazestart_time', 'gazestop_time',}\n",
    "                        yyy_plot_names = [col for col in bhvevents_aligned_bhvonly_df_toplot.columns\n",
    "                                if col not in excluded_keys and pd.api.types.is_numeric_dtype(bhvevents_aligned_bhvonly_df_toplot[col])]\n",
    "\n",
    "                        # Prepare subplots\n",
    "                        ncols = 4\n",
    "                        nplots = len(yyy_plot_names)\n",
    "                        nrows = math.ceil(nplots / ncols)\n",
    "\n",
    "                        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5*ncols, 4*nrows))\n",
    "                        axes = axes.flatten()\n",
    "\n",
    "                        # Loop through Y variables\n",
    "                        yyy_corrs_dict = {}\n",
    "                        for i, yyy_plot_name in enumerate(yyy_plot_names):\n",
    "                            ax = axes[i]\n",
    "\n",
    "                            df_plot = bhvevents_aligned_bhvonly_df_toplot[[xxx_plot_name, yyy_plot_name]].dropna()\n",
    "                            if df_plot.empty or len(df_plot) < 2:\n",
    "                                ax.set_title(f'{yyy_plot_name} (No data)')\n",
    "                                ax.axis('off')\n",
    "                                continue\n",
    "\n",
    "                            # Linear regression using scipy\n",
    "                            x = df_plot[yyy_plot_name].values  # now independent variable\n",
    "                            y = df_plot[xxx_plot_name].values  # now dependent variable\n",
    "                            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "                            y_pred = slope * x + intercept\n",
    "\n",
    "                            # Plot\n",
    "                            sns.scatterplot(x=x, y=y, alpha=0.6, ax=ax)\n",
    "                            try:\n",
    "                                sns.lineplot(x=x, y=y_pred, color='red', ax=ax)\n",
    "                            except:\n",
    "                                print('no regression line')\n",
    "\n",
    "                            # Set title with R and p-value\n",
    "                            ax.set_title(f'{yyy_plot_name}  {xxx_plot_name}\\nR={r_value**2:.2f}, p={p_value:.3g}')\n",
    "                            ax.set_xlabel(yyy_plot_name)\n",
    "                            ax.set_ylabel(xxx_plot_name)\n",
    "\n",
    "                            # Store correlation\n",
    "                            yyy_corrs_dict[yyy_plot_name] = r_value\n",
    "                            \n",
    "                        # Turn off unused subplots\n",
    "                        for j in range(i+1, len(axes)):\n",
    "                            axes[j].axis('off')\n",
    "\n",
    "                        plt.suptitle(f'Regression: {xxx_plot_name} vs other features\\n{cond_ana}, {date_ana}, {act_animal_ana}, {bhvname_ana}', fontsize=16)\n",
    "                        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                        \n",
    "                        savefig = 1\n",
    "                        if savefig:\n",
    "                            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                            cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                            if not os.path.exists(figsavefolder):\n",
    "                                os.makedirs(figsavefolder)\n",
    "\n",
    "                            if not doSingleSelfPulls:\n",
    "                                fig.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                            '_and_partnerIntentionVariables_correlations.pdf')\n",
    "                            elif doSingleSelfPulls:\n",
    "                                fig.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                            '_and_partnerIntentionVariables_correlations_singleselfpulls.pdf')\n",
    "                        \n",
    "                        plt.close(fig)\n",
    "                        \n",
    "                        # organize the summarizing correlation result\n",
    "                        corr_data = {'dates': date_ana, \n",
    "                                    'condition':cond_ana,\n",
    "                                    'act_animal':act_animal_ana,\n",
    "                                    'bhv_name': bhvname_ana,\n",
    "                                    'selfgazeVarName':xxx_plot_name,    \n",
    "                                   }\n",
    "                        # Add all events into the row\n",
    "                        corr_data.update(yyy_corrs_dict)\n",
    "                        #\n",
    "                        selfgaze_partnerIntention_corr_df = selfgaze_partnerIntention_corr_df.append(\n",
    "                                                                   corr_data, ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #\n",
    "                        # Do some multiple regression\n",
    "                        # Setup variable names\n",
    "                        y_var = 'gaze_accum'\n",
    "                        excluded_keys = {'dates', 'condition', 'act_animal', 'bhv_name', 'bhv_id',\n",
    "                                         'gaze_accum', 'gaze_duration', 'gazestart_time', 'gazestop_time'}\n",
    "                        # x_vars = [col for col in bhvevents_aligned_bhvonly_df_toplot.columns\n",
    "                        #           if col not in excluded_keys and pd.api.types.is_numeric_dtype(bhvevents_aligned_bhvonly_df_toplot[col])]\n",
    "                        x_vars = [   # 'animal_animal_dist_mean',\n",
    "                                     'other_mass_move_speed_mean',\n",
    "                                     # 'otherani_otherlever_dist_mean',\n",
    "                                     # 'otherpull_num',\n",
    "                                     # 'prefailpull_num',\n",
    "                                     # 'selfpull_num',\n",
    "                                     'time_since_lastreward',\n",
    "                                     'reaction_time',\n",
    "                                    ]\n",
    "                            \n",
    "                        # Drop rows with missing values\n",
    "                        df_multivar = bhvevents_aligned_bhvonly_df_toplot[[y_var] + x_vars].dropna()\n",
    "                        if len(df_multivar) < 2:\n",
    "                            print(f'Not enough data for multivariable regression on {date_ana}, {cond_ana}, {act_animal_ana}, {bhvname_ana}')\n",
    "                        else:\n",
    "                            # Fit linear regression model\n",
    "                            X = df_multivar[x_vars].values\n",
    "                            y = df_multivar[y_var].values\n",
    "                            model = LinearRegression().fit(X, y)\n",
    "\n",
    "                            # Predictions and metrics\n",
    "                            y_pred = model.predict(X)\n",
    "                            r_value = np.corrcoef(y, y_pred)[0, 1]\n",
    "                            r_squared = model.score(X, y)\n",
    "\n",
    "                            # Compute standardized beta coefficients\n",
    "                            x_std = df_multivar[x_vars].std().values\n",
    "                            y_std = df_multivar[y_var].std()\n",
    "                            std_betas = model.coef_ * (x_std / y_std)\n",
    "\n",
    "                            # Plot predicted vs actual\n",
    "                            fig3, ax = plt.subplots(figsize=(6, 6))\n",
    "                            sns.scatterplot(x=y, y=y_pred, alpha=0.7, ax=ax)\n",
    "                            ax.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')\n",
    "                            ax.set_xlabel('Actual gaze_accum')\n",
    "                            ax.set_ylabel('Predicted gaze_accum')\n",
    "                            ax.set_title(f'Multivariable regression\\nR={r_value:.2f}, R={r_squared:.2f}')\n",
    "                            plt.tight_layout()\n",
    "\n",
    "                            # Save figure\n",
    "                            savefig = 1\n",
    "                            if savefig:\n",
    "                                figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\" + \\\n",
    "                                                cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + '/' + date_ana + '/'\n",
    "                                if not os.path.exists(figsavefolder):\n",
    "                                    os.makedirs(figsavefolder)\n",
    "                                if not doSingleSelfPulls:\n",
    "                                    fig3.savefig(figsavefolder + bhvname_ana + '_aligned_' + y_var + '_multiRegress_predictors.pdf')\n",
    "                                elif doSingleSelfPulls:\n",
    "                                    fig3.savefig(figsavefolder + bhvname_ana + '_aligned_' + y_var + '_multiRegress_predictors_singleselfpulls.pdf')\n",
    "                            plt.close(fig3)\n",
    "\n",
    "                            # Save results\n",
    "                            multi_corr_data = {\n",
    "                                'dates': date_ana,\n",
    "                                'condition': cond_ana,\n",
    "                                'act_animal': act_animal_ana,\n",
    "                                'bhv_name': bhvname_ana,\n",
    "                                'selfgazeVarName': y_var,\n",
    "                                'R': r_value,\n",
    "                                'R_squared': r_squared\n",
    "                            }\n",
    "                            for xi, coef, std_beta in zip(x_vars, model.coef_, std_betas):\n",
    "                                multi_corr_data[f'coef_{xi}'] = coef\n",
    "                                multi_corr_data[f'std_beta_{xi}'] = std_beta\n",
    "\n",
    "                            # Append to global results dataframe\n",
    "                            selfgaze_partnerIntention_multivar_corr_df = selfgaze_partnerIntention_multivar_corr_df.append(\n",
    "                                multi_corr_data, ignore_index=True\n",
    "                            )\n",
    "    \n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    # do the correlation coeffient summary plot \n",
    "    #####\n",
    "    from scipy.stats import ttest_1samp\n",
    "\n",
    "    # examine to multivariate regression coeffient\n",
    "    domultiVar = 0\n",
    "    \n",
    "    # Generalize conditions\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond\n",
    "\n",
    "    # Apply to dataframe\n",
    "    selfgaze_partnerIntention_corr_df[\"condition_general\"] = \\\n",
    "        selfgaze_partnerIntention_corr_df[\"condition\"].apply(generalize_condition)\n",
    "    #\n",
    "    selfgaze_partnerIntention_multivar_corr_df[\"condition_general\"] = \\\n",
    "        selfgaze_partnerIntention_multivar_corr_df[\"condition\"].apply(generalize_condition)\n",
    "\n",
    "    # Melt dataframe for seaborn\n",
    "    if not domultiVar:\n",
    "        plot_df = selfgaze_partnerIntention_corr_df[['condition_general'] + yyy_plot_names].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars='condition_general',\n",
    "                                      value_vars=yyy_plot_names,\n",
    "                                      var_name='Variable',\n",
    "                                      value_name='corr coef')\n",
    "    if domultiVar:\n",
    "        std_beta_names = ['std_beta_' + name for name in x_vars]\n",
    "        plot_df = selfgaze_partnerIntention_multivar_corr_df[['condition_general'] + std_beta_names].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars='condition_general',\n",
    "                                      value_vars=std_beta_names,\n",
    "                                      var_name='Variable',\n",
    "                                      value_name='corr coef') \n",
    "    \n",
    "    \n",
    "    # if only plot certain condition\n",
    "    onlyplotMC = 1\n",
    "    if onlyplotMC:\n",
    "        plot_df_melted = plot_df_melted[plot_df_melted['condition_general']=='MC']\n",
    "    \n",
    "    # if only plot a selection of the variables\n",
    "    onlyvarisubset = 1\n",
    "    if onlyvarisubset:\n",
    "        variable_subset = ['other_mass_move_speed_mean','animal_animal_dist_mean','time_since_lastreward','reaction_time']\n",
    "        #\n",
    "        if domultiVar:\n",
    "            variable_subset = ['other_mass_move_speed_mean','time_since_lastreward']\n",
    "            variable_subset = ['std_beta_' + name for name in variable_subset]\n",
    "        #\n",
    "        ind_subset = np.isin(plot_df_melted['Variable'],variable_subset)\n",
    "        plot_df_melted = plot_df_melted[ind_subset]\n",
    "        # Set up figure\n",
    "        fig2, ax = plt.subplots(figsize=(10, 6))\n",
    "    else:\n",
    "        # Set up figure\n",
    "        fig2, ax = plt.subplots(figsize=(20, 6))\n",
    "        \n",
    "       \n",
    "\n",
    "    # Violin plot\n",
    "    sns.violinplot(data=plot_df_melted,\n",
    "                   x='condition_general',\n",
    "                   y='corr coef',\n",
    "                   hue='Variable',\n",
    "                   inner=None,\n",
    "                   palette='Set2',\n",
    "                   linewidth=1,\n",
    "                   ax=ax)\n",
    "\n",
    "    # Swarm plot\n",
    "    sns.swarmplot(data=plot_df_melted,\n",
    "                  x='condition_general',\n",
    "                  y='corr coef',\n",
    "                  hue='Variable',\n",
    "                  dodge=True,\n",
    "                  color='k',\n",
    "                  alpha=0.5,\n",
    "                  size=3,\n",
    "                  ax=ax)\n",
    "\n",
    "    # Remove duplicate legend entries\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    n_vars = len(yyy_plot_names)\n",
    "    ax.legend(handles[:n_vars],\n",
    "              labels[:n_vars],\n",
    "              title=\"Variable\",\n",
    "              bbox_to_anchor=(1.05, 1),\n",
    "              loc='upper left')\n",
    "\n",
    "    # ---- Add significance stars ---- #\n",
    "\n",
    "    # Get all condition levels and hue levels (in the same order seaborn uses them)\n",
    "    conditions = plot_df_melted[\"condition_general\"].unique()\n",
    "    variables = plot_df_melted[\"Variable\"].unique()\n",
    "    condition_order = list(plot_df_melted[\"condition_general\"].unique())\n",
    "    variable_order = list(plot_df_melted[\"Variable\"].unique())\n",
    "\n",
    "    # Build mapping for positions\n",
    "    group_spacing = 1.0\n",
    "    dodge_width = 0.8 / len(variable_order)  # spacing per hue within group\n",
    "\n",
    "    for i, cond in enumerate(condition_order):\n",
    "        for j, var in enumerate(variable_order):\n",
    "            # Filter group\n",
    "            subset = plot_df_melted[\n",
    "                (plot_df_melted[\"condition_general\"] == cond) &\n",
    "                (plot_df_melted[\"Variable\"] == var)\n",
    "            ]\n",
    "            y_vals = subset[\"corr coef\"].dropna()\n",
    "            if len(y_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            # T-test against 0\n",
    "            t_stat, p_val = ttest_1samp(y_vals, 0)\n",
    "\n",
    "            if p_val < 0.05:\n",
    "                # Compute x-position using dodge\n",
    "                x_pos = i - 0.4 + (j + 0.5) * dodge_width\n",
    "                y_max = y_vals.max()\n",
    "                ax.text(x_pos, y_max + 0.05, '*',\n",
    "                        ha='center', va='bottom', fontsize=18, color='red')\n",
    "\n",
    "    ax.set_title('Self-gaze vs Partner Intention Correlation across Conditions')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/SelfGaze_PartnerIntention_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        \n",
    "        if not domultiVar:\n",
    "            if not doSingleSelfPulls: \n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly.pdf')\n",
    "\n",
    "            elif doSingleSelfPulls:\n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_singleselfpulls.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly_singleselfpulls.pdf')\n",
    "        \n",
    "        elif domultiVar:\n",
    "            if not doSingleSelfPulls: \n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_multivars.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly_multivars.pdf')\n",
    "\n",
    "            elif doSingleSelfPulls:\n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_singleselfpulls_multivars.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly_singleselfpulls_multivars.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8a97c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pull_trig_gazeprob_ievent[pull_trig_gazeprob_ievent <1]=0\n",
    "pull_trig_gazeprob_ievent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5db83ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>condition</th>\n",
       "      <th>act_animal</th>\n",
       "      <th>bhv_name</th>\n",
       "      <th>bhv_id</th>\n",
       "      <th>animal_animal_dist_appoachslope</th>\n",
       "      <th>animal_animal_dist_changetime</th>\n",
       "      <th>animal_animal_dist_max</th>\n",
       "      <th>animal_animal_dist_mean</th>\n",
       "      <th>animal_animal_dist_meanspeed</th>\n",
       "      <th>...</th>\n",
       "      <th>otherani_otherlever_dist_meanspeed</th>\n",
       "      <th>otherani_otherlever_dist_meanvelocity</th>\n",
       "      <th>otherani_otherlever_dist_min</th>\n",
       "      <th>otherani_otherlever_dist_speed</th>\n",
       "      <th>otherani_otherlever_dist_velocity</th>\n",
       "      <th>otherpull_num</th>\n",
       "      <th>prefailpull_num</th>\n",
       "      <th>reaction_time</th>\n",
       "      <th>selfpull_num</th>\n",
       "      <th>time_since_lastreward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20240509</td>\n",
       "      <td>MC</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>0</td>\n",
       "      <td>33.216606</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.633333</td>\n",
       "      <td>1134.273537</td>\n",
       "      <td>139.732760</td>\n",
       "      <td>...</td>\n",
       "      <td>25.561634</td>\n",
       "      <td>-13.404889</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>[81.59463994849375, 81.45969120607901, 79.9143...</td>\n",
       "      <td>[-81.59463994849375, -81.45969120607901, -79.9...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20240509</td>\n",
       "      <td>MC</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>1</td>\n",
       "      <td>-491.189920</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1066.863197</td>\n",
       "      <td>173.635954</td>\n",
       "      <td>...</td>\n",
       "      <td>53.507961</td>\n",
       "      <td>-3.443239</td>\n",
       "      <td>-1.433333</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.01891507555001226, 0.0724109...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.01891507555001226, 0.0724109...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240509</td>\n",
       "      <td>MC</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>2</td>\n",
       "      <td>-570.364059</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-1.433333</td>\n",
       "      <td>1159.697277</td>\n",
       "      <td>129.007600</td>\n",
       "      <td>...</td>\n",
       "      <td>14.245021</td>\n",
       "      <td>-9.252242</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>[3.1770918368849266, 3.411990841382533, 3.9146...</td>\n",
       "      <td>[-3.1770918368849266, -3.411990841382533, -3.9...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.606320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.606320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20240509</td>\n",
       "      <td>MC</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>3</td>\n",
       "      <td>-262.714369</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1154.696534</td>\n",
       "      <td>117.578697</td>\n",
       "      <td>...</td>\n",
       "      <td>5.606696</td>\n",
       "      <td>-0.076055</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>[3.055039173617047, 2.738858991800015, 2.00220...</td>\n",
       "      <td>[-3.055039173617047, -2.738858991800015, -2.00...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.867670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.867670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20240509</td>\n",
       "      <td>MC</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>4</td>\n",
       "      <td>-414.036270</td>\n",
       "      <td>-0.766667</td>\n",
       "      <td>-1.466667</td>\n",
       "      <td>1143.231585</td>\n",
       "      <td>169.435678</td>\n",
       "      <td>...</td>\n",
       "      <td>43.401704</td>\n",
       "      <td>-37.613137</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>[9.54298716877787, 8.720974736136924, 6.937020...</td>\n",
       "      <td>[9.54298716877787, 8.720974736136924, 6.937020...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.944809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.944809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>20250430_SR</td>\n",
       "      <td>SR_withDodson</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>12</td>\n",
       "      <td>-149.894090</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>899.226877</td>\n",
       "      <td>151.852472</td>\n",
       "      <td>...</td>\n",
       "      <td>39.560777</td>\n",
       "      <td>-23.646170</td>\n",
       "      <td>-1.400000</td>\n",
       "      <td>[3.78222478618369, 4.457849395930253, 5.637946...</td>\n",
       "      <td>[3.78222478618369, 4.457849395930253, 5.637946...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.804287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.804287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>20250430_SR</td>\n",
       "      <td>SR_withDodson</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>13</td>\n",
       "      <td>-153.915942</td>\n",
       "      <td>-3.100000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>891.720702</td>\n",
       "      <td>137.120405</td>\n",
       "      <td>...</td>\n",
       "      <td>45.189779</td>\n",
       "      <td>-30.273937</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>[6.49377958895343, 6.747877347360571, 7.660562...</td>\n",
       "      <td>[-6.49377958895343, -6.747877347360571, -7.660...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.867724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.867724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>20250430_SR</td>\n",
       "      <td>SR_withDodson</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>14</td>\n",
       "      <td>-452.265386</td>\n",
       "      <td>-1.933333</td>\n",
       "      <td>3.966667</td>\n",
       "      <td>972.883645</td>\n",
       "      <td>182.652753</td>\n",
       "      <td>...</td>\n",
       "      <td>50.950662</td>\n",
       "      <td>-31.620437</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>[2.4736419080130645, 1.3355800755429925, 0.983...</td>\n",
       "      <td>[2.4736419080130645, 1.3355800755429925, -0.98...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.556073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.556073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>20250430_SR</td>\n",
       "      <td>SR_withDodson</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>15</td>\n",
       "      <td>-443.776462</td>\n",
       "      <td>-1.533333</td>\n",
       "      <td>-3.300000</td>\n",
       "      <td>973.387548</td>\n",
       "      <td>174.339319</td>\n",
       "      <td>...</td>\n",
       "      <td>54.106701</td>\n",
       "      <td>-30.932537</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>[1.3148943060130591, 0.5405719353285576, 0.473...</td>\n",
       "      <td>[1.3148943060130591, 0.5405719353285576, -0.47...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.389881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.389881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154</th>\n",
       "      <td>20250430_SR</td>\n",
       "      <td>SR_withDodson</td>\n",
       "      <td>kanga</td>\n",
       "      <td>pull</td>\n",
       "      <td>16</td>\n",
       "      <td>-523.825511</td>\n",
       "      <td>-1.866667</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>962.853799</td>\n",
       "      <td>166.771250</td>\n",
       "      <td>...</td>\n",
       "      <td>56.599904</td>\n",
       "      <td>-11.640772</td>\n",
       "      <td>-2.133333</td>\n",
       "      <td>[3.716279099702433, 3.09440758736244, 1.871419...</td>\n",
       "      <td>[-3.716279099702433, -3.09440758736244, -1.871...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.405673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.405673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3155 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dates      condition act_animal bhv_name bhv_id  \\\n",
       "0        20240509             MC      kanga     pull      0   \n",
       "1        20240509             MC      kanga     pull      1   \n",
       "2        20240509             MC      kanga     pull      2   \n",
       "3        20240509             MC      kanga     pull      3   \n",
       "4        20240509             MC      kanga     pull      4   \n",
       "...           ...            ...        ...      ...    ...   \n",
       "3150  20250430_SR  SR_withDodson      kanga     pull     12   \n",
       "3151  20250430_SR  SR_withDodson      kanga     pull     13   \n",
       "3152  20250430_SR  SR_withDodson      kanga     pull     14   \n",
       "3153  20250430_SR  SR_withDodson      kanga     pull     15   \n",
       "3154  20250430_SR  SR_withDodson      kanga     pull     16   \n",
       "\n",
       "      animal_animal_dist_appoachslope  animal_animal_dist_changetime  \\\n",
       "0                           33.216606                      -1.000000   \n",
       "1                         -491.189920                      -1.100000   \n",
       "2                         -570.364059                      -0.800000   \n",
       "3                         -262.714369                      -0.866667   \n",
       "4                         -414.036270                      -0.766667   \n",
       "...                               ...                            ...   \n",
       "3150                      -149.894090                      -3.333333   \n",
       "3151                      -153.915942                      -3.100000   \n",
       "3152                      -452.265386                      -1.933333   \n",
       "3153                      -443.776462                      -1.533333   \n",
       "3154                      -523.825511                      -1.866667   \n",
       "\n",
       "      animal_animal_dist_max  animal_animal_dist_mean  \\\n",
       "0                  -1.633333              1134.273537   \n",
       "1                   0.933333              1066.863197   \n",
       "2                  -1.433333              1159.697277   \n",
       "3                   2.600000              1154.696534   \n",
       "4                  -1.466667              1143.231585   \n",
       "...                      ...                      ...   \n",
       "3150                3.733333               899.226877   \n",
       "3151               -4.000000               891.720702   \n",
       "3152                3.966667               972.883645   \n",
       "3153               -3.300000               973.387548   \n",
       "3154               -3.333333               962.853799   \n",
       "\n",
       "      animal_animal_dist_meanspeed  ...  otherani_otherlever_dist_meanspeed  \\\n",
       "0                       139.732760  ...                           25.561634   \n",
       "1                       173.635954  ...                           53.507961   \n",
       "2                       129.007600  ...                           14.245021   \n",
       "3                       117.578697  ...                            5.606696   \n",
       "4                       169.435678  ...                           43.401704   \n",
       "...                            ...  ...                                 ...   \n",
       "3150                    151.852472  ...                           39.560777   \n",
       "3151                    137.120405  ...                           45.189779   \n",
       "3152                    182.652753  ...                           50.950662   \n",
       "3153                    174.339319  ...                           54.106701   \n",
       "3154                    166.771250  ...                           56.599904   \n",
       "\n",
       "      otherani_otherlever_dist_meanvelocity otherani_otherlever_dist_min  \\\n",
       "0                                -13.404889                     3.200000   \n",
       "1                                 -3.443239                    -1.433333   \n",
       "2                                 -9.252242                     0.966667   \n",
       "3                                 -0.076055                     0.566667   \n",
       "4                                -37.613137                    -0.133333   \n",
       "...                                     ...                          ...   \n",
       "3150                             -23.646170                    -1.400000   \n",
       "3151                             -30.273937                    -0.200000   \n",
       "3152                             -31.620437                    -0.200000   \n",
       "3153                             -30.932537                    -0.266667   \n",
       "3154                             -11.640772                    -2.133333   \n",
       "\n",
       "                         otherani_otherlever_dist_speed  \\\n",
       "0     [81.59463994849375, 81.45969120607901, 79.9143...   \n",
       "1     [0.0, 0.0, 0.0, 0.01891507555001226, 0.0724109...   \n",
       "2     [3.1770918368849266, 3.411990841382533, 3.9146...   \n",
       "3     [3.055039173617047, 2.738858991800015, 2.00220...   \n",
       "4     [9.54298716877787, 8.720974736136924, 6.937020...   \n",
       "...                                                 ...   \n",
       "3150  [3.78222478618369, 4.457849395930253, 5.637946...   \n",
       "3151  [6.49377958895343, 6.747877347360571, 7.660562...   \n",
       "3152  [2.4736419080130645, 1.3355800755429925, 0.983...   \n",
       "3153  [1.3148943060130591, 0.5405719353285576, 0.473...   \n",
       "3154  [3.716279099702433, 3.09440758736244, 1.871419...   \n",
       "\n",
       "                      otherani_otherlever_dist_velocity  otherpull_num  \\\n",
       "0     [-81.59463994849375, -81.45969120607901, -79.9...            0.0   \n",
       "1     [0.0, 0.0, 0.0, 0.01891507555001226, 0.0724109...            1.0   \n",
       "2     [-3.1770918368849266, -3.411990841382533, -3.9...            0.0   \n",
       "3     [-3.055039173617047, -2.738858991800015, -2.00...            0.0   \n",
       "4     [9.54298716877787, 8.720974736136924, 6.937020...            0.0   \n",
       "...                                                 ...            ...   \n",
       "3150  [3.78222478618369, 4.457849395930253, 5.637946...            1.0   \n",
       "3151  [-6.49377958895343, -6.747877347360571, -7.660...            1.0   \n",
       "3152  [2.4736419080130645, 1.3355800755429925, -0.98...            1.0   \n",
       "3153  [1.3148943060130591, 0.5405719353285576, -0.47...            1.0   \n",
       "3154  [-3.716279099702433, -3.09440758736244, -1.871...            1.0   \n",
       "\n",
       "      prefailpull_num  reaction_time  selfpull_num  time_since_lastreward  \n",
       "0                 0.0            NaN           0.0                    NaN  \n",
       "1                 0.0            NaN           0.0                    NaN  \n",
       "2                 0.0      -4.606320           0.0               4.606320  \n",
       "3                 0.0      -4.867670           0.0               4.867670  \n",
       "4                 0.0      -4.944809           0.0               4.944809  \n",
       "...               ...            ...           ...                    ...  \n",
       "3150              0.0      -8.804287           0.0               8.804287  \n",
       "3151              0.0      -7.867724           0.0               7.867724  \n",
       "3152              0.0      -7.556073           0.0               7.556073  \n",
       "3153              0.0      -7.389881           0.0               7.389881  \n",
       "3154              0.0      -7.405673           0.0               7.405673  \n",
       "\n",
       "[3155 rows x 33 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhvevents_aligned_bhvonly_eachevents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c48138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative sanity check plot, plot the self pull aligned traces\n",
    "\n",
    "if 0: \n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "\n",
    "                # only look at one neuron since it's behavioral related exploration\n",
    "                for ineuron in np.arange(0,1,1):\n",
    "                # for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "    \n",
    "                        #\n",
    "                        contbhv_var = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname])[0]\n",
    "                        contbhv_var = np.array(contbhv_var)\n",
    "                        # \n",
    "                        otherpull_var = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_otherpull_name])[0]\n",
    "                        otherpull_var = np.array(otherpull_var)\n",
    "                        \n",
    "                        \n",
    "                        # plot\n",
    "                        time_full = np.arange(-4, 4, 1/fps)  # 240 time points\n",
    "                        speed = np.abs(np.diff(contbhv_var, axis=1))  # shape: (17, 239)\n",
    "                        time_diff = time_full[:-1]\n",
    "\n",
    "                        # Average speed between -4s and 0s\n",
    "                        start_idx = 0\n",
    "                        end_idx = np.searchsorted(time_diff, 0)\n",
    "                        mean_speed = np.nanmean(speed[:, start_idx:end_idx], axis=1)\n",
    "\n",
    "                        # Quantile thresholds\n",
    "                        q1, q2 = np.quantile(mean_speed, [1/3, 2/3])\n",
    "\n",
    "                        # Trial indices per quantile\n",
    "                        idx1 = np.where(mean_speed <= q1)[0]\n",
    "                        idx2 = np.where((mean_speed > q1) & (mean_speed <= q2))[0]\n",
    "                        idx3 = np.where(mean_speed > q2)[0]\n",
    "\n",
    "                        # Mean and SEM function\n",
    "                        def mean_sem(data):\n",
    "                            mean = np.nanmean(data, axis=0)\n",
    "                            sem = np.nanstd(data, axis=0) / np.sqrt(data.shape[0])\n",
    "                            return mean, sem\n",
    "\n",
    "                        # Get averages\n",
    "                        cont1_mean, cont1_sem = mean_sem(contbhv_var[idx1])\n",
    "                        cont2_mean, cont2_sem = mean_sem(contbhv_var[idx2])\n",
    "                        cont3_mean, cont3_sem = mean_sem(contbhv_var[idx3])\n",
    "\n",
    "                        other1_mean, other1_sem = mean_sem(otherpull_var[idx1])\n",
    "                        other2_mean, other2_sem = mean_sem(otherpull_var[idx2])\n",
    "                        other3_mean, other3_sem = mean_sem(otherpull_var[idx3])\n",
    "\n",
    "                        # Plotting\n",
    "                        fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "                        # Left y-axis: contbhv_var\n",
    "                        ax1.plot(time_full, cont1_mean, label=pull_trig_events_tgtname+' speed Low', color='blue')\n",
    "                        ax1.fill_between(time_full, cont1_mean - cont1_sem, cont1_mean + cont1_sem, color='blue', alpha=0.2)\n",
    "\n",
    "                        ax1.plot(time_full, cont2_mean, label=pull_trig_events_tgtname+' speed Medium', color='green')\n",
    "                        ax1.fill_between(time_full, cont2_mean - cont2_sem, cont2_mean + cont2_sem, color='green', alpha=0.2)\n",
    "\n",
    "                        ax1.plot(time_full, cont3_mean, label=pull_trig_events_tgtname+' speed High', color='red')\n",
    "                        ax1.fill_between(time_full, cont3_mean - cont3_sem, cont3_mean + cont3_sem, color='red', alpha=0.2)\n",
    "\n",
    "                        ax1.set_xlabel('Time (s)')\n",
    "                        ax1.set_ylabel(pull_trig_events_tgtname)\n",
    "                        ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                        # Right y-axis: otherpull_var\n",
    "                        ax2 = ax1.twinx()\n",
    "                        ax2.plot(time_full, other1_mean, '--', \n",
    "                                 label=pull_trig_otherpull_name+' in '+pull_trig_events_tgtname+' speed Low', color='blue')\n",
    "                        ax2.fill_between(time_full, other1_mean - other1_sem, other1_mean + other1_sem, color='blue', alpha=0.1)\n",
    "\n",
    "                        ax2.plot(time_full, other2_mean, '--', \n",
    "                                 label=pull_trig_otherpull_name+' in '+pull_trig_events_tgtname+' speed Medium', color='green')\n",
    "                        ax2.fill_between(time_full, other2_mean - other2_sem, other2_mean + other2_sem, color='green', alpha=0.1)\n",
    "\n",
    "                        ax2.plot(time_full, other3_mean, '--', \n",
    "                                 label=pull_trig_otherpull_name+' in '+pull_trig_events_tgtname+' speed High', color='red')\n",
    "                        ax2.fill_between(time_full, other3_mean - other3_sem, other3_mean + other3_sem, color='red', alpha=0.1)\n",
    "\n",
    "                        ax2.set_ylabel(pull_trig_otherpull_name)\n",
    "                        ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                        # Combine legends from both axes\n",
    "                        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "                        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "                        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center left', bbox_to_anchor=(1.12, 0.5), borderaxespad=0)\n",
    "\n",
    "                        # Final touches\n",
    "                        plt.title(f\"{act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                        ax1.grid(False)\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        savefig = 0\n",
    "                        if savefig:\n",
    "                            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                            cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                            if not os.path.exists(figsavefolder):\n",
    "                                os.makedirs(figsavefolder)\n",
    "\n",
    "                            fig.savefig(figsavefolder+'OtherIntention_and_otherPull_comparison_'+bhvname_ana+'_'+\n",
    "                                        pull_trig_events_tgtname+'_'+pull_trig_otherpull_name+'.pdf')\n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc22bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfad2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f94652a",
   "metadata": {},
   "source": [
    "### sanity check plot; mean pull aligned firing rate and pull aligned gaze events (3 gaussian kernel smoothed)\n",
    "#### add the option to look at gaze accumulation over time\n",
    "#### also use this code to defined significant neurons - label neurons that significantly encode gaze accumulation before pull, this is for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:   \n",
    "    # make sure that the significance is defined based on 'pull_trig_gazeprob_name', not 'pull_trig_events_tgtname'\n",
    "    \n",
    "    # gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    gaze_duration_type = 'around_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    \n",
    "    # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "    doGazeAccum = 0\n",
    "        \n",
    "    significant_neurons_data_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                        'clusterID','significance_or_not',\n",
    "                                                        'gaze_duration_type','gaze_variable_name'])\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # Determine subplot grid (5 columns, dynamic rows)\n",
    "                ncols = 5\n",
    "                nrows = int(np.ceil(nneurons / ncols))\n",
    "\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "                axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "                \n",
    "                # === New heatmap plot per date for neuron correlation over time ===\n",
    "                fig_corr, ax_corr = plt.subplots(figsize=(10, max(6, 0.3 * nneurons)))\n",
    "\n",
    "                # Store r_trace and p_trace for each neuron\n",
    "                r_traces_all_neurons = []\n",
    "                p_traces_all_neurons = []\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                        #\n",
    "                        # load and plot bhv event ('pull') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot the results\n",
    "                            time_trace = np.arange(-4,4,1/fps)  # Assuming time is just indices\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            ax.plot(time_trace, mean_trace, label=bhvname_ana+'(n='+str(nevents)+')', \n",
    "                                    color=bhvname_clrs[ibhvname_ana])\n",
    "                            ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                            color=bhvname_clrs[ibhvname_ana], alpha=0.3)\n",
    "                        #\n",
    "                        else:\n",
    "                            FRsmoothed_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # load and plot the pull aligned continuous bhv variables\n",
    "                        # conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_gazeprob_name])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                        \n",
    "                        #\n",
    "                        # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "                        if doGazeAccum:\n",
    "                            from sklearn.metrics import auc\n",
    "                            # if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                            if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                                num_time_points = conBhv_allevents_ineuron.shape[0]\n",
    "                                num_conbhv = conBhv_allevents_ineuron.shape[1]\n",
    "                                accumulated_auc = np.zeros((num_time_points, num_conbhv))\n",
    "                                # Create a time axis (assuming equal spacing)\n",
    "                                time_ind = np.arange(num_time_points)\n",
    "                                #\n",
    "                                for i in range(num_conbhv):\n",
    "                                    for t in range(1, num_time_points):\n",
    "                                        # Calculate AUC up to the current time point for the i-th conbhv\n",
    "                                        y = conBhv_allevents_ineuron[:t+1, i]\n",
    "                                        accumulated_auc[t, i] = auc(time_ind[:t+1], y)\n",
    "                            #\n",
    "                            conBhv_allevents_ineuron = accumulated_auc\n",
    "                            \n",
    "                        # zscored the behavioral events\n",
    "                        # Flatten the data\n",
    "                        flattened = conBhv_allevents_ineuron.flatten()\n",
    "                        # Z-score the entire dataset as a single distribution\n",
    "                        flattened_z = np.full_like(flattened, np.nan)\n",
    "                        valid_mask = ~np.isnan(flattened)\n",
    "                        flattened_z[valid_mask] = st.zscore(flattened[valid_mask])\n",
    "                        # Reshape back to original shape\n",
    "                        conBhv_allevents_ineuron_z = flattened_z.reshape(conBhv_allevents_ineuron.shape)\n",
    "                        # \n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron_z\n",
    "    \n",
    "                        try:\n",
    "                            nevents = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents = 0\n",
    "                        \n",
    "                        try:\n",
    "                            FRconBhv_allevents_ineuron = gaussian_filter1d(conBhv_allevents_ineuron, sigma=6, axis=0)\n",
    "                        except:\n",
    "                            FRconBhv_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        # if the pull aligned FR and bhv have different number\n",
    "                        try:\n",
    "                            nevents_fr = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents_fr = 0\n",
    "                            \n",
    "                        if not  nevents_fr == nevents: \n",
    "                            print(date_ana+' mismatched number')\n",
    "                            if nevents_fr < nevents:\n",
    "                                FRconBhv_allevents_ineuron = FRconBhv_allevents_ineuron[:,0:nevents_fr]\n",
    "                            else:\n",
    "                                FRs_allevents_ineuron = FRs_allevents_ineuron[:,0:nevents]\n",
    "                            \n",
    "                        \n",
    "                        # Compute correlation coefficient between FR and behavior at each time point\n",
    "                        try:\n",
    "                            corrs = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "                            pvals = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "\n",
    "                            for t in range(FRsmoothed_allevents_ineuron.shape[0]):\n",
    "                                fr_t = FRsmoothed_allevents_ineuron[t, :]\n",
    "                                bhv_t = FRconBhv_allevents_ineuron[t, :]\n",
    "\n",
    "                                valid_mask = ~np.isnan(fr_t) & ~np.isnan(bhv_t)\n",
    "                                if np.sum(valid_mask) > 5:  # Only compute if enough data points\n",
    "                                    r, p = st.pearsonr(fr_t[valid_mask], bhv_t[valid_mask])\n",
    "                                    corrs[t] = r\n",
    "                                    pvals[t] = p    \n",
    "                        except:\n",
    "                            time_trace = np.arange(-4,4,1/fps)\n",
    "                            corrs = np.full(time_trace.shape[0], np.nan)\n",
    "                            pvals = np.full(time_trace.shape[0], np.nan)\n",
    "\n",
    "\n",
    "                        r_traces_all_neurons.append(corrs)\n",
    "                        p_traces_all_neurons.append(pvals)\n",
    "\n",
    "                        # decide if this neuron is significant or not\n",
    "                        if gaze_duration_type == 'around_pull':\n",
    "                            significant_neuron = np.sum(pvals<0.01)>0\n",
    "                        elif gaze_duration_type == 'before_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace<0]<0.01)>0\n",
    "                        elif gaze_duration_type == 'after_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace>0]<0.01)>0\n",
    "                                                \n",
    "                        #\n",
    "                        # put information about the significance \n",
    "                        if doGazeAccum:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':'gaze_accum',     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        else:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':pull_trig_gazeprob_name,     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            if doGazeAccum:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_gazeprob_name+'_AUC(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "                            else:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_gazeprob_name+'(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "\n",
    "\n",
    "                            # Create a twin axis for the correlation plot\n",
    "                            ax2 = ax.twinx()                   \n",
    "\n",
    "                            # Plot correlation coefficient trace on the right y-axis\n",
    "                            ax2.plot(time_trace, corrs, color='black', linestyle='--', label='FRBhv r')\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "\n",
    "                            # Highlight significant timepoints (p < 0.01) with red dots on the right y-axis\n",
    "                            significant_mask = (pvals < 0.01) & ~np.isnan(pvals)\n",
    "                            ax2.plot(time_trace[significant_mask], corrs[significant_mask], 'ro', label='p < 0.01')\n",
    "\n",
    "                            # Set the label for the right axis\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "                            ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                            # Optionally adjust limits or formatting if necessary\n",
    "                            ax2.set_ylim(-1, 1)  # Adjust this as necessary for your data range\n",
    "\n",
    "\n",
    "                    ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "                    ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                    # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                    ax.legend()\n",
    "\n",
    "                # Hide empty subplots if nneurons < total grid size\n",
    "                for i in range(nneurons, len(axes)):\n",
    "                    fig.delaxes(axes[i])\n",
    "\n",
    "                # Figure title\n",
    "                fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)\n",
    "\n",
    "                \n",
    "                # Convert to numpy array for heatmap\n",
    "                r_traces_all_neurons = np.array(r_traces_all_neurons)\n",
    "\n",
    "                # === Sort r_traces by the time of their first peak ===\n",
    "                peak_times = []\n",
    "                for trace in r_traces_all_neurons:\n",
    "                    if np.all(np.isnan(trace)):\n",
    "                        peak_times.append(np.inf)\n",
    "                    else:\n",
    "                        peak_idx = np.nanargmax(trace)\n",
    "                        peak_times.append(time_trace[peak_idx])\n",
    "\n",
    "                # Get sorting indices based on peak times\n",
    "                sorted_indices = np.argsort(peak_times)\n",
    "                r_traces_sorted = r_traces_all_neurons[sorted_indices, :]\n",
    "\n",
    "                # Plot heatmap of r values\n",
    "                im = ax_corr.imshow(r_traces_sorted, aspect='auto', cmap='gray_r', interpolation='none',\n",
    "                                    extent=[time_trace[0], time_trace[-1], 0, nneurons],\n",
    "                                    vmin=-0.7, vmax=0.7)\n",
    "\n",
    "                # Overlay significance as red dots\n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    sig_times = np.where(p_traces_all_neurons[idx] < 0.01)[0]\n",
    "                    for t in sig_times:\n",
    "                        ax_corr.plot(time_trace[t], i + 0.5, 'r.', markersize=3)  # i+0.5 to center in the row\n",
    "\n",
    "                # Add vertical dashed line at time = 0\n",
    "                ax_corr.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                # Add vertical dashed line at time zero\n",
    "                ax_corr.axvline(x=0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "                ax_corr.set_xlabel(\"Time (s)\")\n",
    "                ax_corr.set_ylabel(\"Neuron (sorted by peak time)\")\n",
    "                ax_corr.set_title(f\"Neuron-wise Corr(Gaze, FR) Heatmap (Sorted): {act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                cbar = fig_corr.colorbar(im, ax=ax_corr)\n",
    "                cbar.set_label('Pearson r')\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                savefig = 0\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    if doGazeAccum:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_gazeprob_name+'_auc'+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_gazeprob_name+'_auc'+savefile_sufix+'.pdf')\n",
    "                    else:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_gazeprob_name+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_gazeprob_name+savefile_sufix+'.pdf')\n",
    "\n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig)\n",
    "                plt.close(fig_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_neurons_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from scipy.integrate import cumtrapz  # Add this import\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    ind1 = bhvevents_aligned_FR_allevents_all_dates_df['condition']=='MC_withGinger'\n",
    "    ind2 = bhvevents_aligned_FR_allevents_all_dates_df['dates']=='20240808'\n",
    "    ind3 = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == 'pull'\n",
    "    ind4 = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']=='kanga'\n",
    "    conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_events_tgtname])[0]\n",
    "    conBhv_gazeprob_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_gazeprob_name])[0]\n",
    "\n",
    "    np.shape(conBhv_gazeprob_ineuron)\n",
    "    \n",
    "    # Setup\n",
    "    plotID = 3\n",
    "    trace = conBhv_gazeprob_ineuron[plotID]\n",
    "    trace2 = conBhv_allevents_ineuron[plotID]\n",
    "    time_trace = np.arange(-4, 4, 1/30)\n",
    "\n",
    "    # Compute accumulated AUC using trapezoidal integration\n",
    "    accum_auc = cumtrapz(trace, time_trace, initial=0)\n",
    "\n",
    "    # Create plot with dual y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Plot the gaze distribution\n",
    "    color1 = 'tab:blue'\n",
    "    line1, = ax1.plot(time_trace, trace, color=color1, label='Gaze Distribution')\n",
    "    fill = ax1.fill_between(time_trace, trace, alpha=0.3, color=color1)\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Gaze Distribution', color=color1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.axvline(0, color='k', linestyle='--', linewidth=1)\n",
    "    ax1.set_title('Self pull aligned social gaze')\n",
    "\n",
    "    # Create a second y-axis for accumulated AUC\n",
    "    if 1:\n",
    "        ax2 = ax1.twinx()\n",
    "        color2 = 'tab:red'\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Patner distance to lever')\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Self distance to lever')\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Partner distance to tube')\n",
    "        line2, = ax2.plot(time_trace, trace2, color=color2, label=pull_trig_events_tgtname)\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Self distance to tube')\n",
    "        ax2.set_ylabel('distance (a.u.)', color=color2)\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Legend: lines and manual patch for shaded area\n",
    "    legend_elements = [\n",
    "        line1,\n",
    "        Patch(facecolor=color1, alpha=0.3, label='AUC Area'),\n",
    "        line2\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_PartnerLeverDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_SelfLeverDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_PartnerTubeDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_SelfTubeDist.pdf')\n",
    "        fig.savefig(figsavefolder+'example_event_SelfGazeAccum_'+pull_trig_events_tgtname+'.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b24c5b",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately for each condition\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### run PCA for all bhvevent together combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "        \n",
    "    # Step 1 - run PCA separately\n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','succrate','act_animal',\n",
    "                                                                'bhv_name','bhv_id','PCs',])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana         \n",
    "\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    succrate = np.array(bhvevents_aligned_FR_allevents_tgt['succrate'])[0][0]\n",
    "                    \n",
    "                    # to better combine different bhv events, choose the same amount\n",
    "                    nbhv_topick = 50\n",
    "\n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    valid_bhvs = ~np.any(np.isnan(data_array), axis=(0, 1))  # Shape (144,)\n",
    "                    data_array = data_array[:, :, valid_bhvs]\n",
    "                    # in case no bhv events were there\n",
    "                    if np.shape(valid_bhvs)[0]==0:\n",
    "                        a, b, _ = data_array.shape\n",
    "                        data_array = np.full((a, b, 1), np.nan)\n",
    "\n",
    "                    nneurons = np.shape(data_array)[0]\n",
    "                    timepointnums = np.shape(data_array)[1]\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "                        \n",
    "                    # Randomly select bhv events with replacement, once for all neurons\n",
    "                    selected_bhvs = np.random.choice(mbhv_total, nbhv_topick, replace=True)\n",
    "                    sampled_data = data_array[:, :, selected_bhvs]\n",
    "\n",
    "                    # Reshape by flattening the last two dimensions\n",
    "                    final_array = sampled_data.reshape(nneurons, -1)\n",
    "\n",
    "                    PCA_dataset_ibv = final_array\n",
    "\n",
    "                    # combine all bhv for running PCA in the same neural space\n",
    "                    if ibhvname_ana == 0:\n",
    "                        PCA_dataset = PCA_dataset_ibv\n",
    "                    else:\n",
    "                        PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # Run PCA on this concatenated data \n",
    "                pca = PCA(n_components=3)\n",
    "                try:\n",
    "                    pca.fit(PCA_dataset)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "                totalneuronNum = np.shape(PCA_dataset)[1]\n",
    "\n",
    "                # project on the individual events\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    # get the pull triggered continuous variable of target\n",
    "                    data_array_conBhv = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname]))\n",
    "                    data_array_conBhv = np.nanmean(data_array_conBhv,axis=0)\n",
    "                    data_array_conBhv = data_array_conBhv.transpose()\n",
    "\n",
    "                    # get the pull triggered gaze distribution \n",
    "                    data_array_gazeprob = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_gazeprob_name]))\n",
    "                    data_array_gazeprob = np.nanmean(data_array_gazeprob,axis=0)\n",
    "                    data_array_gazeprob = data_array_gazeprob.transpose()\n",
    "                    \n",
    "                    # get the self and other pull variable\n",
    "                    data_array_otherpull = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_otherpull_name]))\n",
    "                    data_array_otherpull = np.nanmean(data_array_otherpull,axis=0)\n",
    "                    data_array_otherpull = data_array_otherpull.transpose()\n",
    "                    #\n",
    "                    data_array_selfpull = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_selfpull_name]))\n",
    "                    data_array_selfpull = np.nanmean(data_array_selfpull,axis=0)\n",
    "                    data_array_selfpull = data_array_selfpull.transpose()\n",
    "                    \n",
    "                    # get the prefailpull number variable\n",
    "                    data_prefailpull_num = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_num_pre_failpull_name]))\n",
    "                    data_prefailpull_num = np.nanmean(data_prefailpull_num,axis=0)\n",
    "                    \n",
    "                    # get the time_since_lastreward variable\n",
    "                    data_lastreward_time = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_time_pre_reward_name]))\n",
    "                    data_lastreward_time = np.nanmean(data_lastreward_time,axis=0)\n",
    "                    \n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                    for ibhv in np.arange(0,mbhv_total,1):\n",
    "\n",
    "                        data_ibhv = data_array[:,:,ibhv]\n",
    "\n",
    "                        #\n",
    "                        # get the pull triggered continous variables of target for individual events\n",
    "                        try:\n",
    "                            data_array_conBhv_ibhv = data_array_conBhv[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_conBhv_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        \n",
    "                        #\n",
    "                        # get the pull triggered gaze distribution\n",
    "                        try:\n",
    "                            data_array_gazeprob_ibhv = data_array_gazeprob[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_gazeprob_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                            \n",
    "                        #\n",
    "                        # for the socialgaze_prob, only use the meaningful ones\n",
    "                        if 0:\n",
    "                            if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                                trace = data_array_gazeprob_ibhv\n",
    "                                time_trace = np.arange(-4, 4, 1/30)\n",
    "                                filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                data_array_gazeprob_ibhv = filtered_trace\n",
    "                               \n",
    "                        #    \n",
    "                        # get the pull triggered continous variables of target for individual events\n",
    "                        try:\n",
    "                            data_array_conBhv_ibhv = data_array_conBhv[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_conBhv_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        \n",
    "                        # get the pull triggered self and pther pull for individual events\n",
    "                        try:\n",
    "                            data_array_otherpull_ibhv = data_array_otherpull[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_otherpull_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        #\n",
    "                        try:\n",
    "                            data_array_selfpull_ibhv = data_array_selfpull[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_selfpull_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                            \n",
    "                        # get the prefailpull number variable\n",
    "                        try:\n",
    "                            data_prefailpull_num_ibhv = data_prefailpull_num[ibhv]\n",
    "                        except:\n",
    "                            data_prefailpull_num_ibhv = np.nan\n",
    "                            \n",
    "                        # get the time_since_lastreward variable\n",
    "                        try:\n",
    "                            data_lastreward_time_ibhv = data_lastreward_time[ibhv]\n",
    "                        except:\n",
    "                            data_lastreward_time_ibhv = np.nan\n",
    "                        \n",
    "                            \n",
    "                        #\n",
    "                        # for firing rate, project on the PC space    \n",
    "                        try:\n",
    "                            PCA_proj_ibhv = pca.transform(np.transpose(data_ibhv))\n",
    "                        except:\n",
    "                            PCA_proj_ibhv = np.full((timepointnums, 3), np.nan)\n",
    "\n",
    "                        FRPCA_all_sessions_allevents_sum_df = FRPCA_all_sessions_allevents_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':succrate,\n",
    "                                                                                'bhv_id':ibhv,\n",
    "                                                                                'PCs':PCA_proj_ibhv,\n",
    "                                                                                'neuronNumBeforePCA':totalneuronNum,\n",
    "                                                                                pull_trig_events_tgtname: data_array_conBhv_ibhv,\n",
    "                                                                                pull_trig_gazeprob_name: data_array_gazeprob_ibhv,\n",
    "                                                                                pull_trig_otherpull_name:data_array_otherpull_ibhv,\n",
    "                                                                                pull_trig_selfpull_name:data_array_selfpull_ibhv,\n",
    "                                                                                pull_num_pre_failpull_name:data_prefailpull_num_ibhv,\n",
    "                                                                                pull_time_pre_reward_name:data_lastreward_time_ibhv,\n",
    "                                                                                                         \n",
    "                                                                                 }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480a20f",
   "metadata": {},
   "source": [
    "#### add PCA features, gaze features, and partner distance to lever features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 for each PCA trace, calculate the length, curvature, and/or tortusity for comparison later\n",
    "# test hypothesis: 1. for testing if individual trial different was from gaze start time/stop time/gaze duration\n",
    "    \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "FRPCAfeatures_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','act_animal','succrate',\n",
    "                                                                    'bhv_name','bhv_id',\n",
    "                                                                    'PClength','PCcurv','PCtort','PCspeed','PCsmoothness',\n",
    "                                                                    'PCspeed_trace','PCcurv_trace',\n",
    "                                                                    ])\n",
    "FRPCAfeatures_gazeduration_corr_all_sessions_df = pd.DataFrame(columns=['condition','session','succrate',\n",
    "                                                                        'act_animal','bhv_name',])\n",
    "\n",
    "# newly added control!!\n",
    "# only look at pull aligned events that has no preceding self pull \n",
    "doSingleSelfPulls = 1\n",
    "\n",
    "# add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "gaze_duration_type = 'before_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "# gaze_duration_type = 'around_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "\n",
    "#\n",
    "for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "    act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "    ind_animal = FRPCA_all_sessions_allevents_sum_df['act_animal']==act_animal_ana\n",
    "        \n",
    "    # get the dates\n",
    "    dates_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal]['session'])\n",
    "    ndates_toplot = np.shape(dates_toplot)[0]\n",
    "    \n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_sessions_allevents_sum_df['condition']==cond_ana\n",
    "\n",
    "        # get the dates\n",
    "        dates_ana = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal & ind_cond]['session'])\n",
    "        ndates_ana = np.shape(dates_ana)[0]\n",
    "    \n",
    "\n",
    "        for idate_ana in np.arange(0,ndates_ana,1):\n",
    "            date_ana = dates_ana[idate_ana]\n",
    "            ind_date = FRPCA_all_sessions_allevents_sum_df['session']==date_ana     \n",
    "            \n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_sessions_allevents_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                FRPCA_allevents_toana = FRPCA_all_sessions_allevents_sum_df[ind_ana]\n",
    "\n",
    "                bhv_ids = np.array(FRPCA_allevents_toana['bhv_id'])\n",
    "                nbhvevents = np.shape(bhv_ids)[0]\n",
    "\n",
    "                for ibhv_id in np.arange(0,nbhvevents,1):\n",
    "\n",
    "                    bhv_id = bhv_ids[ibhv_id]\n",
    "                    ind_bhvid = FRPCA_allevents_toana['bhv_id'] == bhv_id\n",
    "\n",
    "                    # \n",
    "                    # count self pull before self pull, the goal is to make sure the effect we will find later is not because of the preceding pulls\n",
    "                    from scipy.signal import find_peaks\n",
    "                    #\n",
    "                    pulltrig_selfpull = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_selfpull_name])[0]\n",
    "                    #\n",
    "                    x_full = np.arange(-4,4,1/fps)\n",
    "                    pre_mask = x_full<-0.15\n",
    "                    try:\n",
    "                        # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                        data = pulltrig_selfpull[pre_mask]\n",
    "                        peaks, _ = find_peaks(data)\n",
    "                        selfpull_num = len(peaks)\n",
    "                    except:\n",
    "                        selfpull_num = np.nan\n",
    "                    \n",
    "                    #\n",
    "                    x_full = np.arange(-4, 4, 1/fps)\n",
    "                    #\n",
    "                    if gaze_duration_type == 'before_pull':\n",
    "                        # Only use the pre-pull window (-4s to 0s)\n",
    "                        pre_mask = x_full <= 0\n",
    "                    elif gaze_duration_type == 'after_pull':\n",
    "                        # Only use the post-pull window (0s to 4s)\n",
    "                        pre_mask = x_full >= 0\n",
    "                    elif gaze_duration_type == 'around_pull':\n",
    "                        # the entire -4 to 4s\n",
    "                        pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "                \n",
    "                    # \n",
    "                    # analyze some features based on 'pull_trig_events_tgtname'\n",
    "                    pulltrig_conBhv_tgt = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "                    \n",
    "                    partner_face_lever_mean = np.nanmean(pulltrig_conBhv_tgt[pre_mask])\n",
    "                    \n",
    "                    \n",
    "                    if (pull_trig_events_tgtname == 'otherani_otherlever_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_lever_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'otherani_othertube_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_tube_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_animal_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'otherpull_prob') :\n",
    "                        \n",
    "                        partner_face_lever_velocity = np.gradient(pulltrig_conBhv_tgt, 1/fps)\n",
    "                        #\n",
    "                        partner_face_lever_speed = np.abs(partner_face_lever_velocity)\n",
    "                        #\n",
    "                        partner_face_lever_meanvelocity = np.nanmean(partner_face_lever_velocity[pre_mask])\n",
    "                        #\n",
    "                        partner_face_lever_meanspeed = np.nanmean(partner_face_lever_speed[pre_mask])\n",
    "        \n",
    "                        min_dist = np.arange(-4,4,1/fps)[np.argmin(pulltrig_conBhv_tgt)]\n",
    "                        max_dist = np.arange(-4,4,1/fps)[np.argmax(pulltrig_conBhv_tgt)]\n",
    "                        #\n",
    "                        pre_min_mask = x_full <= min_dist\n",
    "                        \n",
    "                        # find the time that the dramatic change starts, if could not find it, use the -4s\n",
    "                        percentile = 95\n",
    "                        dt = 1 / fps\n",
    "                        abs_derivative = np.abs(np.gradient(pulltrig_conBhv_tgt[x_full <= 0], dt))\n",
    "                        threshold = np.percentile(abs_derivative, percentile)\n",
    "                        # Start from the first time point (index 0)\n",
    "                        idx_change = np.where(abs_derivative > threshold)[0]\n",
    "                        if len(idx_change) > 0:\n",
    "                            onset_idx = idx_change[0]\n",
    "                            change_time = x_full[x_full <= 0][onset_idx]\n",
    "                        else:\n",
    "                            change_time = x_full[0]\n",
    "                        #\n",
    "                        partner_face_lever_changetime = change_time\n",
    "                        #\n",
    "                        post_changetime_mask = x_full >= change_time\n",
    "                        #\n",
    "                        # min and max after the change_time\n",
    "                        min_dist_post_changetime = x_full[x_full>=change_time][np.argmin(pulltrig_conBhv_tgt[x_full>=change_time])]\n",
    "                        max_dist_post_changetime = x_full[x_full>=change_time][np.argmax(pulltrig_conBhv_tgt[x_full>=change_time])]\n",
    "                        \n",
    "                        #\n",
    "                        # find the partner lever approaching trend\n",
    "                        from scipy.stats import linregress\n",
    "                        # Define time range and extract window\n",
    "                        y_full = np.array(pulltrig_conBhv_tgt)                      \n",
    "                        #\n",
    "                        # x_pre = x_full[pre_mask]\n",
    "                        # y_pre = y_full[pre_mask]\n",
    "                        # x_pre = x_full[pre_min_mask]\n",
    "                        # y_pre = y_full[pre_min_mask]\n",
    "                        # x_pre = x_full[pre_min_mask & post_changetime_mask]\n",
    "                        # y_pre = y_full[pre_min_mask & post_changetime_mask]    \n",
    "                        x_pre = x_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]\n",
    "                        y_pre = y_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]                        \n",
    "                        # Linear regression\n",
    "                        slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                        # slope = (y_pre[-1] - y_pre[0])/(x_pre[-1] - x_pre[0])\n",
    "                        #\n",
    "                        approaching_slope = slope\n",
    "                      \n",
    "                    else:\n",
    "                        partner_face_lever_velocity = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "                        #\n",
    "                        partner_face_lever_speed = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "                        \n",
    "                        partner_face_lever_meanspeed = np.nan\n",
    "                        partner_face_lever_meanvelocity = np.nan\n",
    "        \n",
    "                        min_dist = np.nan\n",
    "                        max_dist = np.nan\n",
    "                        \n",
    "                        approaching_slope = np.nan\n",
    "                        \n",
    "                        partner_face_lever_changetime = np.nan                  \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the pull triggered behavioral events\n",
    "                    #\n",
    "                    # pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "                    pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_gazeprob_name])[0]\n",
    "                            \n",
    "                    # if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                    if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                        # calculate the gaze start and gaze stop time, and finally gaze duration\n",
    "                        try:\n",
    "                            \n",
    "                            # Find the point of first increasing and last decrease to estimate gaze start and end\n",
    "                            #\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            #\n",
    "                            # if gaze_duration_type == 'before_pull':\n",
    "                            #     pulltrig_conBhv[timewins>0] = 0\n",
    "                            # elif gaze_duration_type == 'after_pull':\n",
    "                            #     pulltrig_conBhv[timewins<0] = 0                           \n",
    "                            \n",
    "                            if 1:\n",
    "                                first_increase_idx = np.where(np.diff(pulltrig_conBhv) > 0)[0][0] + 1\n",
    "                                #\n",
    "                                last_decrease_idx = np.where(np.diff(pulltrig_conBhv) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_increase_idx].copy()\n",
    "                                gazestop_time = timewins[last_decrease_idx].copy()\n",
    "                            if 0:\n",
    "                                # Find peaks\n",
    "                                peaks, _ = scipy.signal.find_peaks(pulltrig_conBhv)\n",
    "                                #\n",
    "                                # Get first and last peak\n",
    "                                first_peak = peaks[0] \n",
    "                                last_peak = peaks[-1]\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_peak].copy()\n",
    "                                gazestop_time = timewins[last_peak].copy()\n",
    "                            #\n",
    "                            # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                gazestart_time = gazestart_time\n",
    "                                gazestop_time = gazestop_time\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                if (gazestart_time > 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestop_time > 0):\n",
    "                                    gazestop_time = 0\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                if (gazestop_time < 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestart_time < 0):\n",
    "                                    gazestart_time = 0                                                \n",
    "                            #\n",
    "                            # if (gazestart_time == timewins[0]) | (gazestart_time == timewins[-1]):\n",
    "                            #     gazestart_time = np.nan\n",
    "                            # if (gazestop_time == timewins[0]) | (gazestop_time == timewins[-1]):\n",
    "                            #     gazestop_time = np.nan\n",
    "                            if (gazestop_time < gazestart_time):\n",
    "                                gazestart_time = np.nan\n",
    "                                gazestop_time = np.nan                           \n",
    "                        except:\n",
    "                            gazestart_time = np.nan\n",
    "                            gazestop_time = np.nan\n",
    "                            \n",
    "                        # calculate the gaze accumulation (use auc to estimate)\n",
    "                        try:\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            dt = 1 / fps  # sampling interval in seconds\n",
    "                            #\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv, dx=dt)\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins<0], dx=dt)\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins>0], dx=dt)\n",
    "                            #\n",
    "                            gaze_accum = auc\n",
    "                        #\n",
    "                        except:\n",
    "                            gaze_accum = np.nan\n",
    "                                \n",
    "                         \n",
    "                    # get the information about prefailpull_num\n",
    "                    prefailpull_num_ibhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_num_pre_failpull_name])[0]\n",
    "                    \n",
    "                    # get the information about last reward time\n",
    "                    lastreward_time_ibhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_time_pre_reward_name])[0]\n",
    "                    \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the PCs \n",
    "                    FRPCA_ievent_toana = np.array(FRPCA_allevents_toana[ind_bhvid]['PCs'])[0]\n",
    "\n",
    "                    # smooth the pc trajectory\n",
    "                    if 0:\n",
    "                        FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                                 arr=FRPCA_ievent_toana, sigma=6)\n",
    "\n",
    "                    # calculate the length, curvature and tortuosity\n",
    "                    PC_traj = FRPCA_ievent_toana.copy()  # Shape (240, 3)\n",
    "                    \n",
    "                    #\n",
    "                    if gaze_duration_type == 'before_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[0:int(ntimepoints/2),:]\n",
    "                    elif gaze_duration_type == 'after_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[int(ntimepoints/2):,:]\n",
    "\n",
    "                    # Compute differences between consecutive points\n",
    "                    diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                    # Compute segment lengths\n",
    "                    segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                    total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                    # Compute curvature\n",
    "                    # First derivatives\n",
    "                    dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                    dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                    dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                    dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                    # Second derivatives\n",
    "                    d2X_dt2 = np.gradient(dX_dt)\n",
    "                    d2Y_dt2 = np.gradient(dY_dt)\n",
    "                    d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                    d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                    # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                    cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                    curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                    # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                    euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                    tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "                    \n",
    "                    # Compute speed \n",
    "                    dt = 1.0 / fps  # Time between frames\n",
    "                    # Velocity: first derivative of position\n",
    "                    velocity = np.gradient(PC_traj, axis=0) / dt\n",
    "                    # Speed: magnitude of velocity\n",
    "                    speed = np.linalg.norm(velocity, axis=1)\n",
    "                    \n",
    "                    # Compute Smoothness - A simple way to compute trajectory smoothness is to look at the jerk \n",
    "                    #  the third derivative of position (how quickly acceleration changes), \n",
    "                    # which reflects sudden directional/velocity shifts\n",
    "                    # Acceleration: second derivative\n",
    "                    acceleration = np.gradient(velocity, axis=0) / dt\n",
    "                    # Jerk: third derivative\n",
    "                    jerk = np.gradient(acceleration, axis=0) / dt\n",
    "                    # Smoothness metric: integrated squared jerk over time\n",
    "                    squared_jerk = np.linalg.norm(jerk, axis=1) ** 2\n",
    "                    smoothness = np.sum(squared_jerk) * dt\n",
    "\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df.append({\n",
    "                                                                'condition':cond_ana,\n",
    "                                                                'act_animal':act_animal_ana,\n",
    "                                                                'bhv_name': bhvname_ana,\n",
    "                                                                'session':date_ana,\n",
    "                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                'bhv_id':ibhv_id,\n",
    "                                                                'PClength':total_length,\n",
    "                                                                'PCcurv':np.nanmean(curvature),\n",
    "                                                                'PCtort':tortuosity,\n",
    "                                                                'PCspeed':np.nanmean(speed),\n",
    "                                                                'PCsmoothness':smoothness,\n",
    "                                                                'PCspeed_trace':speed,\n",
    "                                                                'PCcurv_trace':curvature,\n",
    "                                                                'gazestart_time':gazestart_time,\n",
    "                                                                'gazestop_time':gazestop_time,\n",
    "                                                                'gaze_accum':gaze_accum,\n",
    "                                                                'neuronNumBeforePCA':np.array(FRPCA_allevents_toana[ind_bhvid]['neuronNumBeforePCA'])[0],\n",
    "                                                                'selfpull_num':selfpull_num,\n",
    "                                                                pull_trig_gazeprob_name: pulltrig_conBhv,\n",
    "                                                                pull_trig_events_tgtname: pulltrig_conBhv_tgt,\n",
    "                                                                pull_trig_events_tgtname+'_mean':partner_face_lever_mean,\n",
    "                                                                pull_trig_events_tgtname+'_speed':partner_face_lever_speed,\n",
    "                                                                pull_trig_events_tgtname+'_meanspeed':partner_face_lever_meanspeed,\n",
    "                                                                pull_trig_events_tgtname+'_velocity':partner_face_lever_velocity,\n",
    "                                                                pull_trig_events_tgtname+'_meanvelocity':partner_face_lever_meanvelocity,\n",
    "                                                                pull_trig_events_tgtname+'_max':max_dist,\n",
    "                                                                pull_trig_events_tgtname+'_min':min_dist,\n",
    "                                                                pull_trig_events_tgtname+'_changetime':partner_face_lever_changetime,\n",
    "                                                                pull_trig_events_tgtname+'_appoachslope':approaching_slope,\n",
    "                                                                pull_num_pre_failpull_name:prefailpull_num_ibhv,\n",
    "                                                                pull_time_pre_reward_name:lastreward_time_ibhv,\n",
    "                    \n",
    "                                                                }, ignore_index=True)\n",
    "      \n",
    "        \n",
    "                # \n",
    "                # remove events that has multiple self pulls before the aligned self pulls \n",
    "                if doSingleSelfPulls:\n",
    "                    ind_good = FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num']==0\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df[ind_good]\n",
    "                \n",
    "               \n",
    "                # after pool all the events related data together do some plotting and calculate the correlation             \n",
    "                ind_sess_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == date_ana\n",
    "                ind_ani_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['act_animal'] == act_animal_ana\n",
    "                ind_bhv_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['bhv_name'] == bhvname_ana\n",
    "                ind_cond_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['condition'] == cond_ana\n",
    "                \n",
    "                ind_toplot = ind_sess_toplot & ind_ani_toplot & ind_bhv_toplot & ind_cond_toplot\n",
    "                FRPCAfeatures_toplot = FRPCAfeatures_all_sessions_allevents_sum_df[ind_toplot]\n",
    "                \n",
    "                #\n",
    "                doNeuroPCA_plot = 1\n",
    "                #\n",
    "                if not doNeuroPCA_plot:\n",
    "                    # xxx_type = 'gaze_duration'\n",
    "                    xxx_type = 'gaze_accumulation'\n",
    "                    # xxx_type = 'gazestart_time'\n",
    "\n",
    "                    yyy_types = [pull_trig_events_tgtname+'_meanspeed',pull_trig_events_tgtname+'_max',\n",
    "                                 pull_trig_events_tgtname+'_min',pull_trig_events_tgtname+'_mean',\n",
    "                                 pull_trig_events_tgtname+'_changetime',pull_trig_events_tgtname+'_appoachslope',\n",
    "                                'gazestart_time',pull_num_pre_failpull_name,pull_time_pre_reward_name]\n",
    "                    #\n",
    "                    # yyy_types = [pull_trig_events_tgtname+'_mean',\n",
    "                    #             'gazestart_time']\n",
    "                #\n",
    "                elif doNeuroPCA_plot:\n",
    "                    # xxx_type = pull_trig_events_tgtname+'_appoachslope'\n",
    "                    # xxx_type = pull_trig_events_tgtname+'_meanspeed'\n",
    "                    # xxx_type = pull_trig_events_tgtname+'_mean'\n",
    "                    # xxx_type = pull_num_pre_failpull_name\n",
    "                    xxx_type = pull_time_pre_reward_name\n",
    "                    \n",
    "                    yyy_types = ['PCcurv','PClength','PCsmoothness','PCspeed']\n",
    "            \n",
    "                nytypes = np.shape(yyy_types)[0]\n",
    "                \n",
    "                # figures \n",
    "                fig1, axs1 = plt.subplots(nytypes+1,1)\n",
    "                fig1.set_figheight(8*nytypes)\n",
    "                fig1.set_figwidth(8*1)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                for iytype in np.arange(0,nytypes,1):\n",
    "                                        \n",
    "                    if xxx_type == 'gaze_duration':\n",
    "                        xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                        FRPCAfeatures_toplot['gaze_duration'] = xxx\n",
    "                    elif xxx_type == 'gaze_accumulation':\n",
    "                        xxx = FRPCAfeatures_toplot['gaze_accum']\n",
    "                        FRPCAfeatures_toplot['gaze_accumulation'] = xxx\n",
    "                    else:\n",
    "                        xxx = FRPCAfeatures_toplot[xxx_type]\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "\n",
    "                    ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "                    xxx = xxx[~ind_nan]\n",
    "                    yyy = yyy[~ind_nan]\n",
    "\n",
    "                    # Compute correlation\n",
    "                    try:\n",
    "                        r, p = st.pearsonr(xxx, yyy)\n",
    "                        # Fit regression line\n",
    "                        slope, intercept = np.polyfit(xxx, yyy, 1)\n",
    "                        x_vals = np.array([min(xxx), max(xxx)])\n",
    "                        y_vals = slope * x_vals + intercept\n",
    "                        axs1[iytype].plot(x_vals, y_vals, color='red', linestyle='--', label='Regression line')\n",
    "                    except:\n",
    "                        r, p = np.nan, np.nan\n",
    "\n",
    "                    # Scatter plot\n",
    "                    axs1[iytype].plot(xxx, yyy, 'o', label='gaze '+gaze_duration_type)\n",
    "\n",
    "                    # Title and labels\n",
    "                    axs1[iytype].set_title(bhvname_ana + ' of ' + act_animal_ana + ' in ' +\n",
    "                                 cond_ana + ' ' + date_ana + '\\n neuron #=' +\n",
    "                                 str(FRPCAfeatures_toplot['neuronNumBeforePCA'].iloc[0]), fontsize=12)\n",
    "                    axs1[iytype].set_ylabel(yyy_type, fontsize=12)\n",
    "\n",
    "                    # Add correlation text\n",
    "                    axs1[iytype].text(0.05, 0.9, f\"r = {r:.3f}\\np = {p:.3f}\", transform=axs1[iytype].transAxes, fontsize=12,\n",
    "                            verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "                    # Optional: show legend\n",
    "                    axs1[iytype].legend()\n",
    "\n",
    "                    #\n",
    "                    # plot the one without any gaze as a comparison\n",
    "                    xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "                    \n",
    "                    ind_nan = (np.isnan(xxx)) & (~np.isnan(yyy))\n",
    "                    yyy = yyy[ind_nan]\n",
    "                    xxx = np.zeros(np.shape(yyy))\n",
    "                    \n",
    "                    axs1[iytype].plot(xxx, yyy, 'ro',label='no gaze '+gaze_duration_type)\n",
    "                    \n",
    "                    axs1[iytype].set_xlabel(xxx_type+' '+gaze_duration_type+' (s)',fontsize=12)  # Set xlabel only for the last subplot in the stack\n",
    "                    axs1[iytype].legend(loc='lower right')\n",
    "            \n",
    "                    # \n",
    "                    FRPCAfeatures_gazeduration_corr_all_sessions_df = FRPCAfeatures_gazeduration_corr_all_sessions_df.append({\n",
    "                                                                                'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                                'corr_'+yyy_type+'_vs_'+xxx_type:r,\n",
    "                                                                                'pcorr_'+yyy_type+'_vs_'+xxx_type:p,\n",
    "                                                                                'gazeduration_definition':gaze_duration_type,\n",
    "                                                                               }, ignore_index=True)\n",
    "                                                               \n",
    "            \n",
    "                # add a mean trace of the partner's distance to lever as a comparison\n",
    "                # clean the dataframe if needed\n",
    "                FRPCAfeatures_toplot = FRPCAfeatures_toplot[\n",
    "                        FRPCAfeatures_toplot[pull_trig_events_tgtname].apply(\n",
    "                            lambda x: not all(np.isnan(i) for i in x if isinstance(i, float) or isinstance(i, np.floating))\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                if 1:\n",
    "                    \n",
    "                    xxx = np.arange(-4, 4, 1/fps)\n",
    "                    data = np.stack(np.array(FRPCAfeatures_toplot[pull_trig_events_tgtname]))\n",
    "                    yyy = np.nanmean(data, axis=0)\n",
    "                    sem = np.nanstd(data, axis=0) / np.sqrt(np.sum(~np.isnan(data), axis=0))  # SEM\n",
    "\n",
    "                    # Plot mean trace\n",
    "                    axs1[nytypes].plot(xxx, yyy, color='black', linewidth=2, label='Mean '+pull_trig_events_tgtname)\n",
    "\n",
    "                    # Shaded error area\n",
    "                    axs1[nytypes].fill_between(xxx, yyy - sem, yyy + sem, color='gray', alpha=0.3, label=' SEM')\n",
    "\n",
    "                    # Labels\n",
    "                    axs1[nytypes].set_xlabel('Time (s)')\n",
    "                    # axs1[nytypes].set_ylabel('Mean partner distance to lever')\n",
    "                    axs1[nytypes].set_ylabel('Mean '+pull_trig_events_tgtname)\n",
    "                    axs1[nytypes].legend()\n",
    "                \n",
    "                if 0:\n",
    "                    from matplotlib.collections import LineCollection\n",
    "                    import matplotlib.cm as cm\n",
    "                    import matplotlib.colors as colors\n",
    "\n",
    "                    # Time vector\n",
    "                    xxx = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "                    # Extract data matrix (trials x time)\n",
    "                    data = np.stack(FRPCAfeatures_toplot[pull_trig_events_tgtname].to_numpy())\n",
    "\n",
    "                    # Define the column name used for sorting colors\n",
    "                    sort_var_name = xxx_type\n",
    "                    sort_values = np.array(FRPCAfeatures_toplot[sort_var_name])\n",
    "\n",
    "                    # Normalize for color mapping\n",
    "                    norm = colors.Normalize(vmin=np.nanmin(sort_values), vmax=np.nanmax(sort_values))\n",
    "                    cmap = cm.get_cmap('Purples')  # dark to light purple\n",
    "                    colors_list = cmap(norm(sort_values))\n",
    "\n",
    "                    # Prepare line segments\n",
    "                    segments = [np.column_stack((xxx, data[i])) for i in range(data.shape[0])]\n",
    "                    lc = LineCollection(segments, colors=colors_list, linewidths=0.8, alpha=0.6)\n",
    "\n",
    "                    # Plot using LineCollection\n",
    "                    axs1[nytypes].add_collection(lc)\n",
    "                    axs1[nytypes].set_xlim(xxx.min(), xxx.max())\n",
    "                    axs1[nytypes].set_ylim(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "                    # Add colorbar\n",
    "                    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "                    sm.set_array([])\n",
    "                    cbar = plt.colorbar(sm, ax=axs1[nytypes])\n",
    "                    cbar.set_label(f'{sort_var_name} (s)')\n",
    "\n",
    "                    # Labels\n",
    "                    axs1[nytypes].set_xlabel('Time (s)')\n",
    "                    axs1[nytypes].set_ylabel('Mean ' + pull_trig_events_tgtname) \n",
    "                    \n",
    "                    \n",
    "                fig1.tight_layout()\n",
    "\n",
    "\n",
    "                savefig = 1\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+date_ana+\"/\"\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    if not doNeuroPCA_plot:\n",
    "                        fig1.savefig(figsavefolder+'bhvevents_aligned__continuousBhv_'+bhvname_ana+'_'+pull_trig_gazeprob_name+'_'+\n",
    "                                 pull_trig_events_tgtname+'_'+xxx_type+'_'+gaze_duration_type+savefile_sufix+'.pdf')\n",
    "                    elif doNeuroPCA_plot:\n",
    "                        fig1.savefig(figsavefolder+'bhvevents_aligned_PCspace_trajectory_features_and_continuousBhv_'+bhvname_ana+'_'+pull_trig_gazeprob_name+'_'+\n",
    "                                 xxx_type+'_'+gaze_duration_type+savefile_sufix+'.pdf')\n",
    "                     \n",
    "            \n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPCAfeatures_gazeduration_corr_all_sessions_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the session-by-session correlation between PC trajectory features and gaze duration \n",
    "if 1:    \n",
    "    import itertools\n",
    "    \n",
    "    #####\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    FRPCAfeatures_gazeduration_corr_all_sessions_df[\"condition_general\"] = \\\n",
    "        FRPCAfeatures_gazeduration_corr_all_sessions_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    if not doNeuroPCA_plot:\n",
    "        # corr_type = 'corr_otherani_otherlever_dist_meanspeed_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_animal_animal_dist_meanspeed_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_animal_animal_dist_mean_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_animal_animal_dist_min_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_other_mass_move_speed_mean_vs_gaze_accumulation'\n",
    "        corr_type = 'corr_other_mass_move_speed_mean_vs_gaze_accumulation'\n",
    "    elif doNeuroPCA_plot:\n",
    "        # corr_type = 'corr_PClength_vs_animal_animal_dist_appoachslope'\n",
    "        # corr_type = 'corr_PClength_vs_otherani_otherlever_dist_appoachslope'\n",
    "        # corr_type = 'corr_PClength_vs_otherani_otherlever_dist_meanspeed'\n",
    "        # corr_type = 'corr_PClength_vs_animal_lever_dist_meanspeed'\n",
    "        # corr_type = 'corr_PClength_vs_other_mass_move_speed_mean'\n",
    "        # corr_type = 'corr_PClength_vs_num_preceding_failpull'\n",
    "        corr_type = 'corr_PClength_vs_time_from_last_reward'    \n",
    "        \n",
    "    fig2, axs2 = plt.subplots(1, 1)\n",
    "    fig2.set_figheight(5)\n",
    "    fig2.set_figwidth(12)\n",
    "\n",
    "    # Define a consistent color\n",
    "    box_color = 'skyblue'\n",
    "\n",
    "    # Plot boxplot and swarmplot\n",
    "    seaborn.violinplot(ax=axs2, data=FRPCAfeatures_gazeduration_corr_all_sessions_df,\n",
    "                x='condition_general', y=corr_type,\n",
    "                color=box_color)\n",
    "\n",
    "    seaborn.swarmplot(ax=axs2, data=FRPCAfeatures_gazeduration_corr_all_sessions_df,\n",
    "                   x='condition_general', y=corr_type,\n",
    "                   color='b', size=6)\n",
    "\n",
    "    # Rotate x-axis ticks\n",
    "    axs2.set_xticklabels(axs2.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Significance from 0 (Wilcoxon signed-rank)\n",
    "    conditions = FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'].unique()\n",
    "    y_max = FRPCAfeatures_gazeduration_corr_all_sessions_df[corr_type].max()\n",
    "    y_min = FRPCAfeatures_gazeduration_corr_all_sessions_df[corr_type].min()\n",
    "\n",
    "    y_offset = (y_max - y_min) * 0.15  # vertical spacing for significance bars\n",
    "\n",
    "    for i, cond in enumerate(conditions):\n",
    "        data = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond][corr_type].dropna()\n",
    "        if len(data) > 0 and np.any(data != 0):  # Wilcoxon requires non-zero variation\n",
    "            try:\n",
    "                stat, p = st.wilcoxon(data)\n",
    "                if p < 0.01:\n",
    "                    axs2.text(i, data.max() + y_offset, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "            except ValueError:\n",
    "                pass  # skip if data not suitable for Wilcoxon\n",
    "\n",
    "    # Pairwise comparisons (MannWhitney U)\n",
    "    pairs = list(itertools.combinations(range(len(conditions)), 2))\n",
    "    for j, (i1, i2) in enumerate(pairs):\n",
    "        cond1 = conditions[i1]\n",
    "        cond2 = conditions[i2]\n",
    "        data1 = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond1][corr_type].dropna()\n",
    "        data2 = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond2][corr_type].dropna()\n",
    "\n",
    "        if len(data1) > 0 and len(data2) > 0:\n",
    "            stat, p = st.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "            if p < 0.05:\n",
    "                y = max(data1.max(), data2.max()) + y_offset * (j + 2)\n",
    "                axs2.plot([i1, i2], [y, y], lw=1.5, c='black')\n",
    "                axs2.text((i1 + i2) / 2, y + y_offset * 0.2, '*', ha='center', va='bottom', fontsize=16)\n",
    "     \n",
    "    # fig2.tight_layout()\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/SelfGaze_PartnerIntention_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "         \n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_continuousBhv_corrSummary_'+bhvname_ana+'_'+\n",
    "                     pull_trig_gazeprob_name+'_'+pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+\n",
    "                     corr_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "\n",
    "    # to see if there's correlation between correlation coefficient of PC features vs gaze duration, and succ rate \n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    # Drop rows where either column is NaN\n",
    "    subset_df = FRPCAfeatures_gazeduration_corr_all_sessions_df[['condition_general', 'succrate', corr_type]].dropna()\n",
    "\n",
    "    # Group by condition and calculate Pearson correlation and p-value\n",
    "    def compute_corr_pval(group):\n",
    "        if len(group) < 2:\n",
    "            return pd.Series({'r': None, 'p': None})  # Not enough data to correlate\n",
    "        r, p = pearsonr(group['succrate'], group[corr_type])\n",
    "        return pd.Series({'r': r, 'p': p})\n",
    "\n",
    "    grouped_corrs = subset_df.groupby('condition_general').apply(compute_corr_pval)\n",
    "\n",
    "    # Display the result\n",
    "    print(f'Correlation between {corr_type} and succrate:')\n",
    "    print(grouped_corrs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvname_ana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761373cb",
   "metadata": {},
   "source": [
    "### sanity check plot: separating the gaze accumulation intro quantiles, and plot the mean social gaze and partner_lever distance traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675edaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax2 = ax.twinx()  # Secondary y-axis\n",
    "    \n",
    "    # === Scatter plot of gaze_accum vs partner_face_lever_min ===\n",
    "    fig2, ax_scatter = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # session_example = '20250428_MC'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    # session_example = '20240523'\n",
    "    session_example = '20240606'\n",
    "    # session_example = '20240808'\n",
    "    # session_example = '20240809'\n",
    "    # session_example = '20240812'\n",
    "    # session_example = '20250415'\n",
    "    # session_example = '20250428_MC'\n",
    "\n",
    "    \n",
    "    # Filter data\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    if doSingleSelfPulls:\n",
    "        shared_cols = ['condition', 'session', 'act_animal', 'bhv_name', 'bhv_id']\n",
    "        FRPCA_example_subset = FRPCA_example.merge(\n",
    "                                FRPCAfeatures_example[shared_cols].drop_duplicates(),\n",
    "                                on=shared_cols,\n",
    "                                how='inner'\n",
    "                            )\n",
    "        FRPCA_example = FRPCA_example_subset\n",
    "        \n",
    "    # Gaze durations\n",
    "    gaze_durations = np.array(FRPCAfeatures_example['gaze_accum'])\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "\n",
    "    time_trace = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "    colors = ['b', 'r', 'y']\n",
    "    labels = ['low', 'mid', 'high']\n",
    "    inds = [ind_low, ind_mid, ind_high]\n",
    "\n",
    "    for ii in range(3):\n",
    "        # Social gaze trace (primary y-axis)\n",
    "        trials_socialgaze = np.stack(FRPCA_example[inds[ii]][pull_trig_gazeprob_name], axis=0)\n",
    "        mean_trace = np.nanmean(trials_socialgaze, axis=0)\n",
    "        sem_trace = np.nanstd(trials_socialgaze, axis=0) / np.sqrt(trials_socialgaze.shape[0])\n",
    "\n",
    "        ax.plot(time_trace, mean_trace, color=colors[ii], linewidth=1.5, alpha=0.5, label=pull_trig_gazeprob_name+f\" when {labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "        ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, color=colors[ii], alpha=0.15)\n",
    "\n",
    "        # Lever distance trace (secondary y-axis)\n",
    "        # trials_dist = np.stack(FRPCAfeatures_example[inds[ii]][pull_trig_events_tgtname+'_speed'], axis=0)\n",
    "        trials_dist = np.stack(FRPCA_example[inds[ii]][pull_trig_events_tgtname], axis=0)\n",
    "        mean_dist = np.nanmean(trials_dist, axis=0)\n",
    "        sem_dist = np.nanstd(trials_dist, axis=0) / np.sqrt(trials_dist.shape[0])\n",
    "\n",
    "        # Thicker, dashed line with markers to highlight\n",
    "        if 0:\n",
    "            #\n",
    "            ax2.plot(time_trace, mean_dist, color=colors[ii], linestyle='-', linewidth=3, marker='o', markersize=4, \n",
    "                     label=pull_trig_events_tgtname+f\" when {labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "            #\n",
    "            # ax2.plot(time_trace, mean_dist, color=colors[ii], linestyle='-', linewidth=3, marker='o', markersize=4, \n",
    "            #         label=pull_trig_events_tgtname+'_speed'+f\" when {labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "            #\n",
    "            ax2.fill_between(time_trace, mean_dist - sem_dist, mean_dist + sem_dist, color=colors[ii], alpha=0.25)\n",
    "\n",
    "    # Axis labels and title\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Social gaze probability', color='gray')\n",
    "    if 0:\n",
    "        ax2.set_ylabel(pull_trig_events_tgtname+'_speed', color='black')\n",
    "        # ax2.set_ylabel(pull_trig_events_tgtname, color='black')\n",
    "    ax.set_title(f\"{animal_exmaple} - {session_example}\")\n",
    "\n",
    "    # Handle legends (combined from both axes)\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left',bbox_to_anchor=(1.15, 1))\n",
    "    \n",
    "    \n",
    "    ## scatter plot\n",
    "    if not doNeuroPCA_plot:\n",
    "        xxx_plotname = 'gaze_accum'\n",
    "        x = FRPCAfeatures_example[xxx_plotname]\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_appoachslope'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_meanspeed'\n",
    "        yyy_plotname = pull_trig_events_tgtname+'_mean'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_max'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_min'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_changetime'\n",
    "        y = FRPCAfeatures_example[yyy_plotname]\n",
    "    #\n",
    "    elif doNeuroPCA_plot:\n",
    "        # xxx_plotname = pull_trig_events_tgtname+'_appoachslope'\n",
    "        # xxx_plotname = pull_trig_events_tgtname+'_meanspeed'\n",
    "        xxx_plotname = pull_trig_events_tgtname+'_mean'\n",
    "        x = FRPCAfeatures_example[xxx_plotname]\n",
    "        yyy_plotname = 'PClength'\n",
    "        y = FRPCAfeatures_example[yyy_plotname]\n",
    "\n",
    "    # Drop NaNs\n",
    "    valid = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x_valid = np.array(x)[valid]\n",
    "    y_valid = np.array(y)[valid]\n",
    "\n",
    "    # Compute correlation\n",
    "    from scipy.stats import pearsonr\n",
    "    r_val, p_val = pearsonr(x_valid, y_valid)\n",
    "\n",
    "    # Plot scatter\n",
    "    ax_scatter.scatter(x_valid, y_valid, color='darkgreen', alpha=0.6, edgecolor='k')\n",
    "\n",
    "    # Regression line\n",
    "    slope, intercept = np.polyfit(x_valid, y_valid, 1)\n",
    "    x_line = np.linspace(np.min(x_valid), np.max(x_valid), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "    ax_scatter.plot(x_line, y_line, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Labels and title\n",
    "    if not doNeuroPCA_plot:\n",
    "        ax_scatter.set_xlabel('Gaze accumulation (s)')\n",
    "        ax_scatter.set_ylabel(yyy_plotname)\n",
    "    elif doNeuroPCA_plot:\n",
    "        ax_scatter.set_xlabel(xxx_plotname)\n",
    "        ax_scatter.set_ylabel(yyy_plotname)\n",
    "    ax_scatter.set_title(f\"Correlation: r = {r_val:.2f}, p = {p_val:.3f}\")\n",
    "\n",
    "    # Optional grid\n",
    "    ax_scatter.grid(True)\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+session_example+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'forExample_trace_between_'+pull_trig_gazeprob_name+'_and_'+pull_trig_events_tgtname+'.pdf')\n",
    "        #\n",
    "        fig2.savefig(figsavefolder+'forExample_scatter_between_'+xxx_plotname+'_and_'+yyy_plotname+'.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_socialgaze = np.stack(FRPCA_example[pull_trig_gazeprob_name], axis=0)\n",
    "mean_trace = np.nanmean(trials_socialgaze, axis=0)\n",
    "sem_trace = np.nanstd(trials_socialgaze, axis=0) / np.sqrt(trials_socialgaze.shape[0])\n",
    "plt.plot(time_trace, mean_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73692e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the single trial trajectories of an example session - compared the gaze_pull separating three quantile based on the gaze duration\n",
    "if 0:\n",
    "    # Create a 3D figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # session_example = '20250428_MC'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    # session_example = '20240523'\n",
    "    session_example = '20240606'\n",
    "    # session_example = '20240808'\n",
    "    # session_example = '20240809'\n",
    "    # session_example = '20240812'\n",
    "    # session_example = '20250415'\n",
    "    # session_example = '20250428_MC'\n",
    "\n",
    "\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "    #\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session']==session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "    \n",
    "    if doSingleSelfPulls:\n",
    "        shared_cols = ['condition', 'session', 'act_animal', 'bhv_name', 'bhv_id']\n",
    "        FRPCA_example_subset = FRPCA_example.merge(\n",
    "                                FRPCAfeatures_example[shared_cols].drop_duplicates(),\n",
    "                                on=shared_cols,\n",
    "                                how='inner'\n",
    "                            )\n",
    "        FRPCA_example = FRPCA_example_subset\n",
    "        \n",
    "    #\n",
    "    # xxx_type = 'gaze_accum'\n",
    "    # xxx_type = 'animal_animal_dist_appoachslope'\n",
    "    # xxx_type = 'otherani_otherlever_dist_appoachslope'\n",
    "    # xxx_type = 'otherani_otherlever_dist_meanspeed'\n",
    "    xxx_type = 'other_mass_move_speed_mean'\n",
    "\n",
    "    #\n",
    "    # gaze_durations = FRPCAfeatures_example['gazestop_time'] - FRPCAfeatures_example['gazestart_time']\n",
    "    gaze_durations = FRPCAfeatures_example[xxx_type]\n",
    "    gaze_durations = np.array(gaze_durations)\n",
    "    #\n",
    "    # Compute tertile (33rd and 67th percentiles), ignoring NaNs\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    #\n",
    "    # Separate data into three groups\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "    \n",
    "    for ii in np.arange(0,3,1):\n",
    "        if ii == 0:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_low]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'b'\n",
    "            traj_lab = 'low '+xxx_type+' ' + str(np.sum(ind_low))+' trials'\n",
    "        elif ii == 1:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_mid]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'r'\n",
    "            traj_lab = 'mid '+xxx_type+' ' + str(np.sum(ind_mid))+' trials'\n",
    "        elif ii == 2:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_high]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'y'\n",
    "            traj_lab = 'high '+xxx_type+' ' + str(np.sum(ind_high))+' trials'\n",
    "           \n",
    "        ntimepoints = np.shape(meanPCA_traj)[0]\n",
    "        \n",
    "        xxx = gaussian_filter1d(meanPCA_traj[:,0],6)\n",
    "        yyy = gaussian_filter1d(meanPCA_traj[:,1],6)\n",
    "        zzz = gaussian_filter1d(meanPCA_traj[:,2],6)\n",
    "        if gaze_duration_type == 'before_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),2],3)   \n",
    "        elif gaze_duration_type == 'after_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,2],3)   \n",
    "            \n",
    "        ax.plot3D(xxx, yyy, zzz, color=traj_clr, linewidth=2, label=traj_lab)\n",
    "        ax.plot3D(xxx[0], yyy[0], zzz[0], color=traj_clr, marker ='o',markersize = 12)\n",
    "        ax.plot3D(xxx[-1], yyy[-1], zzz[-1], color=traj_clr, marker ='s',markersize = 12)\n",
    "\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+session_example+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'forExample_PCAtrajectory_in_'+xxx_type+'_quantiles.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57b52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a7ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b973f7",
   "metadata": {},
   "source": [
    "## analysis with 'trial pooling' across sessions from the same condition\n",
    "### pool sessions for each task conditions together and then run PCA\n",
    "#### pool sessions based on quantiles of gaze-accumulation or gaze-length variables or partner-intention related variables  (e.g. 5 quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only analyze a subset of conditions\n",
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['pull']\n",
    "# bhv_names_to_ana = ['failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "# # the following analysis can only do one conditions \n",
    "# # multiple condition will be considered into one conditions for quantile and FR averaging analysis\n",
    "# conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['MC','SR',]\n",
    "###\n",
    "# For Kanga\n",
    "conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVermelho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "cond_toplot_type = 'allMC'\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "doOnlySigniNeurons = 0 # define the significant neurons using the previous code\n",
    "\n",
    "# newly added control!!\n",
    "# only look at pull aligned events that has no preceding self pull \n",
    "doSingleSelfPulls = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a92ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "\n",
    "if 1:\n",
    "    # load and prepare the data\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.DataFrame(columns=['condition', 'session', 'act_animal',\n",
    "                                                                                 'bhv_name', 'bhv_id', 'FR_ievent',\n",
    "                                                                                 'clusterID', 'channelID',\n",
    "                                                                                pull_trig_events_tgtname+'_mean_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_meanspeed_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_meanvelocity_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_max_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_min_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_changetime_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_appoachslope_ievent',\n",
    "                                                                                 ])\n",
    "    #\n",
    "    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.DataFrame()\n",
    "\n",
    "    # step 1 for the bhvevents_aligned_FR_allevents_all_dates_df, get and gaze-related variables and calculate features\n",
    "\n",
    "    # it's better to match this variable with the previous one\n",
    "    # add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "    gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    # xxx_type = pull_trig_events_tgtname+'_appoachslope'\n",
    "    # xxx_type = pull_trig_events_tgtname+'_meanspeed'\n",
    "    xxx_type = pull_trig_events_tgtname+'_mean'\n",
    "    # xxx_type = pull_num_pre_failpull_name\n",
    "    # xxx_type = pull_time_pre_reward_name\n",
    "    \n",
    "    # special here, number of quantile to use for pooling across different days\n",
    "    num_quantiles = 3 # 10\n",
    "\n",
    "    #\n",
    "    for icond_ana in np.arange(0, nconds_to_ana, 1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition'] == cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0, nanimal_to_ana, 1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal'] == act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0, ndates_ana, 1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates'] == date_ana\n",
    "\n",
    "                # get the neurons\n",
    "                neurons_ana = np.unique(\n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0, nneurons, 1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID'] == clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0, nbhvnames_to_ana, 1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date\n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "                            ind_ana].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "                        if not bhvevents_aligned_FR_allevents_tgt.empty:\n",
    "                            channelID = bhvevents_aligned_FR_allevents_tgt['channelID'].iloc[0]\n",
    "\n",
    "                            #\n",
    "                            # load and plot bhv event ('pull') aligned FR\n",
    "                            FRs_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt['FR_allevents'].iloc[0])\n",
    "                            nevents_FR = np.shape(FRs_allevents_ineuron)[1]\n",
    "\n",
    "                            #\n",
    "                            # load and plot the pull aligned continuous bhv variables\n",
    "                            conBhv_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname].iloc[0])\n",
    "                            conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                            conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                            #\n",
    "                            nevents_bhv = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                            \n",
    "                            #\n",
    "                            # load the self pull variables\n",
    "                            selfpull_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_selfpull_name].iloc[0])\n",
    "                            selfpull_allevents_ineuron = np.array(selfpull_allevents_ineuron)\n",
    "                            selfpull_allevents_ineuron = selfpull_allevents_ineuron.transpose()\n",
    "                            \n",
    "                            # load the prefailpull number and time since the last reward\n",
    "                            prefailpullnum_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_num_pre_failpull_name].iloc[0])\n",
    "                            prefailpullnum_allevents_ineuron = np.array(prefailpullnum_allevents_ineuron)\n",
    "                            #\n",
    "                            lastrewardtime_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_time_pre_reward_name].iloc[0])\n",
    "                            lastrewardtime_allevents_ineuron = np.array(lastrewardtime_allevents_ineuron)\n",
    "                            \n",
    "                            # if the pull aligned FR and bhv have different number\n",
    "                            if not nevents_FR == nevents_bhv:\n",
    "                                # print(date_ana+' mismatched number')\n",
    "                                if nevents_FR < nevents_bhv:\n",
    "                                    conBhv_allevents_ineuron = conBhv_allevents_ineuron[:, 0:nevents_FR]\n",
    "                                else:\n",
    "                                    FRs_allevents_ineuron = FRs_allevents_ineuron[:, 0:nevents_bhv]\n",
    "\n",
    "                            #\n",
    "                            nevents = np.min([nevents_FR, nevents_bhv])\n",
    "\n",
    "                            # get each bhv events\n",
    "                            for bhv_id in np.arange(0, nevents, 1):\n",
    "                                FRs_ievent_ineuron = FRs_allevents_ineuron[:, bhv_id]\n",
    "                                conBhv_ievent_ineuron = conBhv_allevents_ineuron[:, bhv_id]\n",
    "                                selfpull_ievent_ineuron = selfpull_allevents_ineuron[:, bhv_id]\n",
    "\n",
    "                                # load the prefailpull number and time since the last reward\n",
    "                                prefailpullnum_ievent_ineuron = prefailpullnum_allevents_ineuron[bhv_id]\n",
    "                                lastrewardtime_ievent_ineuron = lastrewardtime_allevents_ineuron[bhv_id]\n",
    "                                \n",
    "                                #\n",
    "                                # count self pull before self pull, \n",
    "                                # the goal is to make sure the effect we will find later is not because of the preceding pulls\n",
    "                                from scipy.signal import find_peaks\n",
    "                                #\n",
    "                                x_full = np.arange(-4,4,1/fps)\n",
    "                                pre_mask = x_full<-0.15\n",
    "                                try:\n",
    "                                    # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                                    data = selfpull_ievent_ineuron[pre_mask]\n",
    "                                    peaks, _ = find_peaks(data)\n",
    "                                    selfpull_num = len(peaks)\n",
    "                                except:\n",
    "                                    selfpull_num = np.nan\n",
    "                                    \n",
    "                                    \n",
    "                                #\n",
    "                                # analyze the pull triggered behavioral events\n",
    "                                #\n",
    "                                x_full = np.arange(-4, 4, 1/fps)\n",
    "                                #\n",
    "                                if gaze_duration_type == 'before_pull':\n",
    "                                    # Only use the pre-pull window (-4s to 0s)\n",
    "                                    pre_mask = x_full <= 0\n",
    "                                    # pre_mask = (x_full <= 0) & (x_full >= -2) # manipulate the size of the time window\n",
    "                                elif gaze_duration_type == 'after_pull':\n",
    "                                    # Only use the post-pull window (0s to 4s)\n",
    "                                    pre_mask = x_full >= 0\n",
    "                                elif gaze_duration_type == 'around_pull':\n",
    "                                    # the entire -4 to 4s\n",
    "                                    pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "\n",
    "                                #\n",
    "                                partner_face_lever_mean = np.nanmean(conBhv_ievent_ineuron[pre_mask])\n",
    "                                \n",
    "                                if (pull_trig_events_tgtname == 'otherani_otherlever_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'animal_lever_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'otherani_othertube_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'animal_tube_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'animal_animal_dist'):\n",
    "\n",
    "                                    partner_face_lever_velocity = np.gradient(conBhv_ievent_ineuron, 1/fps)\n",
    "                                    #\n",
    "                                    partner_face_lever_speed = np.abs(partner_face_lever_velocity)\n",
    "                                    #\n",
    "                                    partner_face_lever_meanvelocity = np.nanmean(partner_face_lever_velocity[pre_mask])\n",
    "                                    #\n",
    "                                    partner_face_lever_meanspeed = np.nanmean(partner_face_lever_speed[pre_mask])\n",
    "\n",
    "                                    min_dist = np.arange(-4,4,1/fps)[np.argmin(conBhv_ievent_ineuron)]\n",
    "                                    max_dist = np.arange(-4,4,1/fps)[np.argmax(conBhv_ievent_ineuron)]\n",
    "                                    #\n",
    "                                    pre_min_mask = x_full <= min_dist\n",
    "\n",
    "                                    # find the time that the dramatic change starts, if could not find it, use the -4s\n",
    "                                    percentile = 95\n",
    "                                    dt = 1 / fps\n",
    "                                    abs_derivative = np.abs(np.gradient(conBhv_ievent_ineuron[x_full <= 0], dt))\n",
    "                                    threshold = np.percentile(abs_derivative, percentile)\n",
    "                                    # Start from the first time point (index 0)\n",
    "                                    idx_change = np.where(abs_derivative > threshold)[0]\n",
    "                                    if len(idx_change) > 0:\n",
    "                                        onset_idx = idx_change[0]\n",
    "                                        change_time = x_full[x_full <= 0][onset_idx]\n",
    "                                    else:\n",
    "                                        change_time = x_full[0]\n",
    "                                    #\n",
    "                                    partner_face_lever_changetime = change_time\n",
    "                                    #\n",
    "                                    post_changetime_mask = x_full >= change_time\n",
    "                                    #\n",
    "                                    # min and max after the change_time\n",
    "                                    min_dist_post_changetime = x_full[x_full>=change_time][np.argmin(conBhv_ievent_ineuron[x_full>=change_time])]\n",
    "                                    max_dist_post_changetime = x_full[x_full>=change_time][np.argmax(conBhv_ievent_ineuron[x_full>=change_time])]\n",
    "\n",
    "                                    #\n",
    "                                    # find the partner lever approaching trend\n",
    "                                    from scipy.stats import linregress\n",
    "                                    # Define time range and extract window\n",
    "                                    y_full = np.array(conBhv_ievent_ineuron)                      \n",
    "                                    #\n",
    "                                    # x_pre = x_full[pre_mask]\n",
    "                                    # y_pre = y_full[pre_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask & post_changetime_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask & post_changetime_mask]    \n",
    "                                    x_pre = x_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]\n",
    "                                    y_pre = y_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]                        \n",
    "                                    # Linear regression\n",
    "                                    # slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                                    slope = (y_pre[-1] - y_pre[0])/(x_pre[-1] - x_pre[0])\n",
    "                                    #\n",
    "                                    approaching_slope = slope\n",
    "\n",
    "                                else:\n",
    "                                    partner_face_lever_velocity = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "                                    #\n",
    "                                    partner_face_lever_speed = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "\n",
    "                                    partner_face_lever_meanspeed = np.nan\n",
    "\n",
    "                                    min_dist = np.nan\n",
    "                                    max_dist = np.nan\n",
    "\n",
    "                                    approaching_slope = np.nan\n",
    "\n",
    "                                    partner_face_lever_changetime = np.nan\n",
    "                                \n",
    "                                #\n",
    "                                bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(\n",
    "                                    [bhvevents_aligned_FR_and_eventFeatures_all_dates_df, pd.DataFrame({\n",
    "                                        'condition': [cond_ana],\n",
    "                                        'act_animal': [act_animal_ana],\n",
    "                                        'bhv_name': [bhvname_ana],\n",
    "                                        'session': [date_ana],\n",
    "                                        'bhv_id': [bhv_id],\n",
    "                                        'clusterID': [clusterID_ineuron],\n",
    "                                        'channelID': [channelID],\n",
    "                                        'FR_ievent': [FRs_ievent_ineuron],\n",
    "                                        'selfpull_num_ievent': [selfpull_num],\n",
    "                                        pull_trig_events_tgtname:[conBhv_ievent_ineuron],\n",
    "                                        pull_trig_events_tgtname+'_mean_ievent':[partner_face_lever_mean],\n",
    "                                        pull_trig_events_tgtname+'_speed_ievent':[partner_face_lever_speed],\n",
    "                                        pull_trig_events_tgtname+'_meanspeed_ievent':[partner_face_lever_meanspeed],\n",
    "                                        pull_trig_events_tgtname+'_velocity_ievent':[partner_face_lever_velocity],\n",
    "                                        pull_trig_events_tgtname+'_meanvelocity_ievent':[partner_face_lever_meanvelocity],\n",
    "                                        pull_trig_events_tgtname+'_max_ievent':[max_dist],\n",
    "                                        pull_trig_events_tgtname+'_min_ievent':[min_dist],\n",
    "                                        pull_trig_events_tgtname+'_changetime_ievent':[partner_face_lever_changetime],\n",
    "                                        pull_trig_events_tgtname+'_appoachslope_ievent':[approaching_slope],\n",
    "                                        pull_num_pre_failpull_name+'_ievent':prefailpullnum_ievent_ineuron,\n",
    "                                        pull_time_pre_reward_name+'_ievent':lastrewardtime_ievent_ineuron,\n",
    "                                    })], ignore_index=True)\n",
    "\n",
    "                                \n",
    "    # only look at the pulls that has no pull preceeding\n",
    "    if doSingleSelfPulls:\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['selfpull_num_ievent']==0\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "                                \n",
    "                \n",
    "    # add the quantile information using the all sessions' data\n",
    "    # consider each date separately\n",
    "\n",
    "    # Create a list to store the DataFrames for each date\n",
    "    all_dates_dfs = []\n",
    "\n",
    "    # Get unique dates\n",
    "    unique_dates = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'].unique()\n",
    "\n",
    "    for date_ana in unique_dates:\n",
    "        date_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == date_ana].copy()\n",
    "\n",
    "        # separating quantiles\n",
    "        quantile_tgt = date_df[xxx_type+'_ievent'].dropna()  # Handle potential NaNs\n",
    "        n_unique = len(quantile_tgt.unique())\n",
    "        n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "        if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "            try:\n",
    "                quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                date_df[xxx_type+'_quantile'] = pd.cut(\n",
    "                    date_df[xxx_type+'_ievent'],\n",
    "                    bins=quantile_bins,\n",
    "                    labels=False,\n",
    "                    include_lowest=True,\n",
    "                    duplicates='drop'  # Drop duplicate bin edges\n",
    "                )\n",
    "                quantile_col = xxx_type+'_quantile'\n",
    "                title_prefix = xxx_type\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                date_df[xxx_type+'_quantile'] = np.nan\n",
    "                quantile_col = xxx_type+'_quantile'\n",
    "                title_prefix = xxx_type\n",
    "        else:\n",
    "            date_df['gaze_accum_quantile'] = np.nan\n",
    "            quantile_col = xxx_type+'_quantile'\n",
    "            title_prefix = xxx_type\n",
    "            print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "\n",
    "\n",
    "        all_dates_dfs.append(date_df)\n",
    "\n",
    "    # Concatenate the DataFrames back together\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(all_dates_dfs, ignore_index=True)\n",
    "\n",
    "    # average for each neuron the firing rate of each quantile\n",
    "    if 'quantile_col' in locals():\n",
    "        # Iterate through unique (clusterID, session) pairs\n",
    "        for (cluster_id, session_id) in bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            ['clusterID', 'session']].drop_duplicates().itertuples(index=False):\n",
    "            neuron_session_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == session_id)\n",
    "            ].copy()\n",
    "\n",
    "            for q_val in neuron_session_df[quantile_col].dropna().unique():\n",
    "                quantile_df = neuron_session_df[neuron_session_df[quantile_col] == q_val]\n",
    "                if not quantile_df.empty:\n",
    "                    all_fr_traces = np.vstack(quantile_df['FR_ievent'].tolist())\n",
    "                    mean_fr_trace = np.nanmean(all_fr_traces, axis=0)\n",
    "\n",
    "                    # Get representative metadata\n",
    "                    condition = quantile_df['condition'].iloc[0]\n",
    "                    act_animal = quantile_df['act_animal'].iloc[0]\n",
    "                    bhv_name = quantile_df['bhv_name'].iloc[0]\n",
    "                    channelID = quantile_df['channelID'].iloc[0]\n",
    "\n",
    "                    new_row = {\n",
    "                        'condition': condition,\n",
    "                        'session': session_id,\n",
    "                        'act_animal': act_animal,\n",
    "                        'bhv_name': bhv_name,\n",
    "                        'clusterID': cluster_id,\n",
    "                        'channelID': channelID,\n",
    "                        quantile_col: int(q_val),\n",
    "                        'mean_FR_trace': mean_fr_trace\n",
    "                    }\n",
    "                    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.concat(\n",
    "                        [bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df,\n",
    "                         pd.DataFrame([new_row])],\n",
    "                        ignore_index=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e562fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_and_eventFeatures_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01691733",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# do some big picture plot; here is to have a sense of the gaze-accumulation distribution of succ and failed pull\n",
    "# ...if the setting is set for that\n",
    "## \n",
    "if 0:\n",
    "    from scipy.stats import ks_2samp\n",
    "\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition_general\"] = \\\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    \n",
    "    # \n",
    "    plt.figure(figsize=(20,6))\n",
    "    data_toplot = bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "    # seaborn.kdeplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "    seaborn.kdeplot(data=data_toplot,x=xxx_type+'_ievent',hue='condition_general',\n",
    "                   common_norm=False)  # don't normalize across groups\n",
    "    # seaborn.histplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "\n",
    "    # Compute group-wise quantiles (deciles from 10% to 90%)\n",
    "    # quantiles_df = data_toplot.groupby('bhv_name')['gaze_accum_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df = data_toplot.groupby('condition_general')[xxx_type+'_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df.rename(columns={xxx_type+'_ievent': 'quantile_value'}, inplace=True)\n",
    "\n",
    "    # Draw vertical lines for each quantile\n",
    "    if 0:\n",
    "        # palette = dict(zip(data_toplot['bhv_name'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "        palette = dict(zip(data_toplot['condition_general'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "\n",
    "        for _, row in quantiles_df.iterrows():\n",
    "            plt.axvline(\n",
    "                row['quantile_value'],\n",
    "                # color=palette[row['bhv_name']],\n",
    "                color=palette[row['condition_general']],\n",
    "                linestyle='--',\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    plt.title('KDE Plot with Quantile Lines')\n",
    "    plt.show()\n",
    "    \n",
    "    # KS test: compare MC vs SR\n",
    "    if 0:\n",
    "        group_MC = data_toplot[data_toplot['condition_general'] == 'MC']['gaze_accum_ievent'].dropna()\n",
    "        group_SR = data_toplot[data_toplot['condition_general'] == 'SR']['gaze_accum_ievent'].dropna()\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(group_MC, group_SR)\n",
    "        print(f\"KS test: statistic={ks_stat:.4f}, p-value={ks_pval:.4g}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider the significant neurons based on the previous analysis\n",
    "if 1:\n",
    "    if doOnlySigniNeurons:\n",
    "        \n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = significant_bhv_df\n",
    "\n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = significant_bhv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98703bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "# bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4184afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - 2:\n",
    "# do a basic plot for sanity check - mean FR across all units in the pool condition for each quantile\n",
    "if 1:\n",
    "    from scipy.integrate import cumtrapz\n",
    "\n",
    "    doQuantMeanFRs = 1\n",
    "    # only do one example session, all neurons in that session, average\n",
    "    doExampleSession = 0\n",
    "    # only do one example neuron in one example cell \n",
    "    doExampleNeuron = 0\n",
    "    \n",
    "    timewins = np.arange(-4, 4, 1/30)\n",
    "    n_timepoints = len(timewins)\n",
    "        \n",
    "    #\n",
    "    if doExampleSession:\n",
    "        # examplesess = '20240606'\n",
    "        examplesess =  '20240808'\n",
    "        #\n",
    "        if doQuantMeanFRs:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col].unique())\n",
    "        else:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col].unique())      \n",
    "    # \n",
    "    elif doExampleNeuron:\n",
    "        examplesess =  '20240808'\n",
    "        exampleneuron = '129'\n",
    "        # \n",
    "        # has to do not QuantMeanFRs\n",
    "        ind_1 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "        ind_2 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID']==exampleneuron\n",
    "        quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col].unique()) \n",
    "    #\n",
    "    else:\n",
    "        if doQuantMeanFRs:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "        else:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col].unique())\n",
    "    \n",
    "    quantile_values = quantile_values[~np.isnan(quantile_values)]\n",
    "    \n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2 = ax.twinx()  # Secondary y-axis for AUC\n",
    "\n",
    "    for i_quantile, q_val in enumerate(quantile_values):\n",
    "    \n",
    "        #\n",
    "        if doExampleSession:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][ind_quantile]['FR_ievent'].tolist())\n",
    "        #\n",
    "        elif doExampleNeuron:\n",
    "            ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col] == q_val\n",
    "            fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile]['FR_ievent'].tolist())\n",
    "            gaze_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile][pull_trig_events_tgtname].tolist())\n",
    "     \n",
    "        #\n",
    "        else:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_quantile]['FR_ievent'].tolist())\n",
    "\n",
    "            \n",
    "        #     \n",
    "        mean_fr = np.nanmean(fr_traces_quantile, axis=0)\n",
    "        sem_fr = np.nanstd(fr_traces_quantile, axis=0) / np.sqrt(np.sum(~np.isnan(fr_traces_quantile[:, 0]))) # Standard Error of the Mean\n",
    "        \n",
    "        ax.plot(timewins, mean_fr, label=xxx_type+f' Quantile {int(q_val)}')\n",
    "        ax.fill_between(timewins, mean_fr - sem_fr, mean_fr + sem_fr, alpha=0.3)\n",
    "\n",
    "        \n",
    "        #\n",
    "        if doExampleNeuron:\n",
    "            # Accumulated AUC for each gaze trace, then average\n",
    "            auc_gaze_all = np.array([cumtrapz(trace, timewins, initial=0) for trace in gaze_traces_quantile])\n",
    "            mean_auc_gaze = np.nanmean(auc_gaze_all, axis=0)\n",
    "            sem_auc_gaze = np.nanstd(auc_gaze_all, axis=0) / np.sqrt(np.sum(~np.isnan(auc_gaze_all[:, 0])))\n",
    "            \n",
    "           #  # Plot AUC Gaze\n",
    "           #  ax2.plot(timewins, mean_auc_gaze, linestyle='--', color='lightblue', label=f'Gaze AUC {int(q_val)}')\n",
    "           #  ax2.fill_between(timewins, mean_auc_gaze - sem_auc_gaze, mean_auc_gaze + sem_auc_gaze, alpha=0.2, color='lightblue')\n",
    "\n",
    "        \n",
    "        \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'Mean Firing Rate by {title_prefix} Quantile '+gaze_duration_type)\n",
    "    ax.axvline(0, color='k', linestyle='--', linewidth=0.8, label='Pull Onset')\n",
    "    ax.legend()\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2.set_ylabel(xxx_type)\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_quantile_fig/\"\n",
    "    \n",
    "        if doExampleSession:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "        \n",
    "        if doExampleNeuron:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        if not doExampleNeuron:\n",
    "            fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_acrossNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        elif doExampleNeuron:\n",
    "            fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_exampleNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2889130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - calculate the PCA with the pooled data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import splprep, splev\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "if 1:\n",
    "    timewins = np.arange(-4, 4, 1 / 30)\n",
    "    n_timepoints = len(timewins)\n",
    "    quantile_col = xxx_type+'_quantile'  # Or 'gaze_duration_quantile'\n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    title_prefix = xxx_type  # Or 'Gaze Duration'\n",
    "    smooth_kernel_size = 6\n",
    "    n_bootstrap_iterations = 100\n",
    "    n_neurons_to_sample = 200\n",
    "    \n",
    "    # Get unique neurons\n",
    "    unique_neurons = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[['clusterID', 'session']].drop_duplicates()\n",
    "    n_neurons = len(unique_neurons)\n",
    "\n",
    "    # Get unique quantiles\n",
    "    unique_quantiles = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "    n_quantiles = len(unique_quantiles)\n",
    "\n",
    "    # Initialize the data matrix\n",
    "    data_matrix = np.empty((n_neurons, n_timepoints * n_quantiles))\n",
    "    neuron_index_lookup = {}\n",
    "\n",
    "    # Populate the data matrix\n",
    "    neuron_counter = 0\n",
    "    for neuron_row in unique_neurons.itertuples(index=False):\n",
    "        cluster_id = neuron_row.clusterID\n",
    "        session = neuron_row.session\n",
    "        neuron_index_lookup[(cluster_id, session)] = neuron_counter\n",
    "        neuron_counter += 1\n",
    "\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            # Get the mean FR trace for the current neuron and quantile\n",
    "            mean_fr_trace_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session'] == session) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val)\n",
    "            ]\n",
    "\n",
    "            if not mean_fr_trace_df.empty:\n",
    "                mean_fr_trace = mean_fr_trace_df['mean_FR_trace'].values[0]  # Take the first element\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = mean_fr_trace\n",
    "            else:\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = np.nan  # Handle missing data\n",
    "\n",
    "    # Prepare to store results\n",
    "    all_quantile_lengths = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_quantile_curvatures = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_boot_pca_data = np.zeros((n_bootstrap_iterations, 10, n_timepoints * n_quantiles))  # Store all PCA results\n",
    "\n",
    "    # Perform Bootstrapping\n",
    "    for boot_iter in range(n_bootstrap_iterations):\n",
    "        # 1. Randomly sample neurons\n",
    "        sampled_neuron_indices = np.random.choice(n_neurons, n_neurons_to_sample, replace=True)\n",
    "        sampled_data_matrix = data_matrix[sampled_neuron_indices, :].transpose()  # Neuron dimension becomes the columns\n",
    "\n",
    "        # 2. Run PCA\n",
    "        pca = PCA(n_components=10)  # Project to 10 PCs\n",
    "        pca.fit(np.nan_to_num(sampled_data_matrix))\n",
    "        pca_data = pca.transform(np.nan_to_num(sampled_data_matrix)).transpose()  # Project and transpose\n",
    "        all_boot_pca_data[boot_iter, :, :] = pca_data\n",
    "\n",
    "        # Calculate length and curvature for each quantile\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            start_col = i_quantile * n_timepoints\n",
    "            end_col = (i_quantile + 1) * n_timepoints\n",
    "            quantile_data = pca_data[:, start_col:end_col]  # (10, n_timepoints)\n",
    "\n",
    "            # \n",
    "            if gaze_duration_type == 'around_pull':\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, :], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, :], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'before_pull':\n",
    "                ind_tgt = timewins<0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'after_pull':\n",
    "                ind_tgt = timewins>0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "\n",
    "                    \n",
    "    # ---plotting---\n",
    "    # --- plotting number 1 ---\n",
    "    # Plot Length and Curvature (as before)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_lengths, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[0], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    #axes[0].boxplot(all_quantile_lengths)\n",
    "    # axes[0].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('PC Trajectory Length')\n",
    "    axes[0].set_title('Bootstrapped PC Trajectory Lengths')\n",
    "\n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_curvatures, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[1], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    # axes[1].boxplot(all_quantile_curvatures)\n",
    "    # axes[1].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('PC Trajectory Curvature')\n",
    "    axes[1].set_title('Bootstrapped PC Trajectory Curvatures')\n",
    "\n",
    "    # do the regression for each bootstrap iteration and then plot the average regression line\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Prepare x-axis (quantile indices)\n",
    "    quantiles = np.arange(0, n_quantiles).reshape(-1, 1)\n",
    "\n",
    "    # Store predicted regression lines from each bootstrap\n",
    "    predicted_lengths = []\n",
    "    predicted_curvatures = []\n",
    "    reg_coeff_lengths = []\n",
    "    reg_coeff_curvs = []\n",
    "\n",
    "    for i in range(n_bootstrap_iterations):\n",
    "        # Regression for length\n",
    "        y_len = all_quantile_lengths[i]\n",
    "        model_len = LinearRegression().fit(quantiles, y_len)\n",
    "        slope = model_len.coef_\n",
    "        reg_coeff_lengths.append(slope[0])\n",
    "        pred_len = model_len.predict(quantiles)\n",
    "        predicted_lengths.append(pred_len)\n",
    "\n",
    "        # Regression for curvature\n",
    "        y_curv = all_quantile_curvatures[i]\n",
    "        model_curv = LinearRegression().fit(quantiles, y_curv)\n",
    "        slope = model_curv.coef_\n",
    "        reg_coeff_curvs.append(slope[0])\n",
    "        pred_curv = model_curv.predict(quantiles)\n",
    "        predicted_curvatures.append(pred_curv)\n",
    "\n",
    "    # Convert to arrays\n",
    "    predicted_lengths = np.array(predicted_lengths)\n",
    "    predicted_curvatures = np.array(predicted_curvatures)\n",
    "\n",
    "    # Mean and 95% CI across bootstraps\n",
    "    mean_len = np.mean(predicted_lengths, axis=0)\n",
    "    ci_len_low = np.percentile(predicted_lengths, 2.5, axis=0)\n",
    "    ci_len_high = np.percentile(predicted_lengths, 97.5, axis=0)\n",
    "\n",
    "    mean_curv = np.mean(predicted_curvatures, axis=0)\n",
    "    ci_curv_low = np.percentile(predicted_curvatures, 2.5, axis=0)\n",
    "    ci_curv_high = np.percentile(predicted_curvatures, 97.5, axis=0)\n",
    "\n",
    "    # Length\n",
    "    axes[0].plot(quantiles, mean_len, color='red', label='Mean Regression')\n",
    "    axes[0].fill_between(quantiles.flatten(), ci_len_low, ci_len_high, color='red', alpha=0.3, label='95% CI')\n",
    "    axes[0].set_title('PC Trajectory Length')\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('Length')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Curvature\n",
    "    axes[1].plot(quantiles, mean_curv, color='blue', label='Mean Regression')\n",
    "    axes[1].fill_between(quantiles.flatten(), ci_curv_low, ci_curv_high, color='blue', alpha=0.3, label='95% CI')\n",
    "    axes[1].set_title('PC Trajectory Curvature')\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('Curvature')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_lengths, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the length regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "    #\n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_curvs, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the curvature regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "\n",
    "    # --- plotting number 1 end ---\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Average 3D Traces\n",
    "    fig2 = plt.figure(figsize=(10, 8))\n",
    "    ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "    mean_traces_3d = np.mean(all_boot_pca_data[:, :3, :], axis=0)  # Average across bootstrap iterations, use only first 3 PCs\n",
    "\n",
    "    for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "        start_col = i_quantile * n_timepoints\n",
    "        end_col = (i_quantile + 1) * n_timepoints\n",
    "        mean_quantile_data = mean_traces_3d[:, start_col:end_col]\n",
    "\n",
    "        # Smooth the trajectory\n",
    "        smooth_x = gaussian_filter1d(mean_quantile_data[0, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_y = gaussian_filter1d(mean_quantile_data[1, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_z = gaussian_filter1d(mean_quantile_data[2, ind_tgt], sigma=smooth_kernel_size)\n",
    "\n",
    "        # Plot the smoothed trajectory\n",
    "        ax.plot(smooth_x, smooth_y, smooth_z, label=f'Quantile {int(q_val)}')\n",
    "\n",
    "        # Mark start and end points\n",
    "        ax.scatter(smooth_x[0], smooth_y[0], smooth_z[0], marker='o', color='k')  # Start\n",
    "        ax.scatter(smooth_x[-1], smooth_y[-1], smooth_z[-1], marker='x', color='k')  # End\n",
    "\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_zlabel('PC 3')\n",
    "    ax.set_title(f'Average 3D Trajectories in Neuron-Reduced PC Space by {title_prefix} Quantile')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\n",
    "    print(\"\\n--- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\")\n",
    "\n",
    "    from scipy import stats\n",
    "    from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "    def perform_pairwise_paired_ttests(data, group_labels, alpha=0.01):\n",
    "        unique_groups = np.unique(group_labels)\n",
    "        n_groups = len(unique_groups)\n",
    "        p_values = []\n",
    "        comparisons = []\n",
    "\n",
    "        for i in range(n_groups):\n",
    "            for j in range(i + 1, n_groups):\n",
    "                group1_data = data[:, i]\n",
    "                group2_data = data[:, j]\n",
    "                t_stat, p_val = stats.ttest_rel(group1_data, group2_data)\n",
    "                p_values.append(p_val)\n",
    "                comparisons.append((unique_groups[i], unique_groups[j]))\n",
    "\n",
    "        reject, p_corrected, _, _ = multipletests(p_values, method='holm', alpha=alpha)\n",
    "\n",
    "        results_df = pd.DataFrame({'Comparison': comparisons,\n",
    "                                   'p_value': p_values,\n",
    "                                   'p_corrected': p_corrected,\n",
    "                                   'reject_null': reject})\n",
    "        return results_df\n",
    "\n",
    "    # Perform pairwise paired t-tests for Length\n",
    "    length_pairwise_results = perform_pairwise_paired_ttests(all_quantile_lengths, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Length:\")\n",
    "    print(length_pairwise_results)\n",
    "\n",
    "    # Perform pairwise paired t-tests for Curvature\n",
    "    curvature_pairwise_results = perform_pairwise_paired_ttests(all_quantile_curvatures, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Curvature:\")\n",
    "    print(curvature_pairwise_results)\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_Pullfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_quantile_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'bhvevents_aligned_PCfeatures_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_PCtrajectory_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d132a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sampled_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f380d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e511b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
