{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### basic neural analysis for LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import scipy.io\n",
    "\n",
    "from scipy.signal import welch\n",
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec3831",
   "metadata": {},
   "source": [
    "### define LFP analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csd(lfp_data, spacing=0.1, smooth_sigma=1):\n",
    "    \"\"\"\n",
    "    Compute CSD using the second spatial derivative of LFP.\n",
    "    \n",
    "    Parameters:\n",
    "    - lfp_data: 2D array (channels x time) of LFP signals.\n",
    "    - spacing: Distance (mm) between electrodes (default: 0.1 mm).\n",
    "    - smooth_sigma: Smoothing factor for Gaussian filtering.\n",
    "\n",
    "    Returns:\n",
    "    - csd: 2D array (CSD estimate, same shape as LFP).\n",
    "    \"\"\"\n",
    "    # Compute second spatial derivative (Finite Difference Method)\n",
    "    csd = -np.diff(lfp_data, n=2, axis=0) / spacing**2  \n",
    "\n",
    "    # Optional: Apply Gaussian smoothing along the spatial (channel) axis\n",
    "    csd_smoothed = gaussian_filter1d(csd, sigma=smooth_sigma, axis=0)\n",
    "\n",
    "    return csd_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29990f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csd_for_windows(lfp_data, window_size, spacing=0.1, smooth_sigma=1):\n",
    "    \"\"\"\n",
    "    Compute CSD for each 5-second window of LFP data.\n",
    "    \n",
    "    Parameters:\n",
    "    - lfp_data: 2D array (channels x time) of LFP signals.\n",
    "    - window_size: Number of samples per window (for 5 seconds, 5000 samples).\n",
    "    - spacing: Distance between electrodes in mm.\n",
    "    - smooth_sigma: Gaussian smoothing factor for CSD.\n",
    "\n",
    "    Returns:\n",
    "    - csd_windows: List of CSD arrays, one for each window.\n",
    "    \"\"\"\n",
    "    num_windows = lfp_data.shape[1] // window_size  # Number of windows in the data\n",
    "    csd_windows = []\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * window_size\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        # Extract LFP for the current window\n",
    "        lfp_window = lfp_data[:, start_idx:end_idx]\n",
    "        \n",
    "        # Compute CSD for this window\n",
    "        csd_window = compute_csd(lfp_window, spacing=spacing, smooth_sigma=smooth_sigma)\n",
    "        csd_windows.append(csd_window)\n",
    "    \n",
    "    return csd_windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### define the analysis dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define analysis dates and conditions\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "neural_record_conditions = [\n",
    "                            # '20240508_Kanga_SR',\n",
    "                            \n",
    "                            # '20240513_Kanga_chair',\n",
    "                        \n",
    "                            # '20240524_Kanga_SR',\n",
    "    \n",
    "                            # '20240528_Kanga_chaired',\n",
    "\n",
    "                            #'20240606_Kanga_MC',\n",
    "       \n",
    "                            # '20250204_Dodson_MC_withKoala', \n",
    "                            # '20250206_Dodson_MC_withKoala', \n",
    "                            # '20250206_Dodson_chaired', # good LFP\n",
    "                            # '20250207_Dodson_chaired', \n",
    "                            '20250210_Dodson_SR_withKoala', # good LFP\n",
    "                            # '20250210_Dodson_chaired', # good LFP\n",
    "                            # '20250211_Dodson_MC_withKoala', \n",
    "                            # '20250212_Dodson_SR_withKoala',\n",
    "                            # '20250213_Dodson_MC_withKoala',\n",
    "                            # '20250214_Dodson_MC_withKoala', # good LFP\n",
    "                            # '20250217_Dodson_SR_withKoala',\n",
    "                            # '20250217_Dodson_chaired',\n",
    "                            # '20250218_Dodson_MC_withKoala',\n",
    "                            # '20250218_Dodson_chaired',\n",
    "\n",
    "                           ]\n",
    "n_record_conditions = np.shape(neural_record_conditions)[0]\n",
    "\n",
    "# define the information for bhv analysis\n",
    "dates_list = [\n",
    "              # '20240508',\n",
    "            \n",
    "              # '20240513',\n",
    "    \n",
    "              # '20240524',\n",
    "    \n",
    "              # '20240528',\n",
    "\n",
    "              # '20240606',\n",
    "    \n",
    "              # '20250204',\n",
    "              # '20250206',\n",
    "              # '20250206',\n",
    "              # '20250207',\n",
    "              '20250210',\n",
    "              # '20250210',\n",
    "              # '20250211', \n",
    "              # '20250212',\n",
    "              # '20250213',\n",
    "              # '20250214',\n",
    "              # '20250217',\n",
    "              # '20250217',\n",
    "              # '20250218',\n",
    "              # '20250218',\n",
    "             ]\n",
    "session_start_times_camera = [ # need to update - 12042023\n",
    "                               0, \n",
    "                             ]\n",
    "animal1_fixedorder = ['dodson']\n",
    "animal2_fixedorder = ['koala']\n",
    "# animal1_fixedorder = ['dannon']\n",
    "# animal2_fixedorder = ['kanga']\n",
    "\n",
    "\n",
    "animal1_filename = \"Dodson\"\n",
    "animal2_filename = \"Koala\"\n",
    "# animal1_filename = \"Dannon\"\n",
    "# animal2_filename = \"Kanga\"\n",
    "\n",
    "\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "kilosortver = 4\n",
    "\n",
    "# the total time of the analyzed sessions \n",
    "total_session_time = 600 # in the unit of s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313a2fe",
   "metadata": {},
   "source": [
    "### get the basic neural data and behavioral data\n",
    "### LFP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot types\n",
    "plot_allpulljuice = 0\n",
    "plot_succ_fail_pull = 0\n",
    "\n",
    "\n",
    "chanMapFile = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "# chanMapFile = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_mirroredMap_2.mat'\n",
    "\n",
    "\n",
    "# Load .mat file\n",
    "chan_map = scipy.io.loadmat(chanMapFile)\n",
    "\n",
    "# load the neural activity for each condition\n",
    "\n",
    "for icondition in np.arange(0,n_record_conditions,1):\n",
    "    \n",
    "    neural_record_condition = neural_record_conditions[icondition]\n",
    "    date_tgt = dates_list[icondition]\n",
    "    \n",
    "    # not use it for now, pre-process in matlab code first\n",
    "    neural_record_filename = glob.glob(neural_data_folder+neural_record_condition+'/*.bin')[0]\n",
    "    \n",
    "    # # load filtered lfp\n",
    "    lfp_filt_filename = neural_data_folder+neural_record_condition+'/lfp_filt.txt'\n",
    "    lfp_filt_data = np.loadtxt(lfp_filt_filename, delimiter=',')\n",
    "    \n",
    "    lfp_filt_data[:,0:200*fs_lfp]=np.nan # arbitrually remove some big noise\n",
    "    # lfp_filt_data[:,500*fs_lfp:]=np.nan # arbitrually remove some big noise\n",
    "    \n",
    "    \n",
    "    #\n",
    "    ntimewins = np.shape(lfp_filt_data)[1]\n",
    "    timewins = (np.arange(0,ntimewins,1))/fs_lfp # in the unit of second\n",
    "    \n",
    "    \n",
    "    if 1:\n",
    "        # load the behavioral data\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)           \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1_frombhv = session_info['lever1_animal'][0].lower()\n",
    "        animal2_frombhv = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "        #\n",
    "        # fix misslabeled successful trial\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number'])-1,1):\n",
    "            #\n",
    "            # initialize  \n",
    "            ind = bhv_data[\"trial_number\"]==itrial+2\n",
    "            bhv_data_itrial = bhv_data[ind]\n",
    "            #\n",
    "            if itrial == 0:\n",
    "                ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                bhv_data_pre_itrial = bhv_data[ind]\n",
    "                #\n",
    "                bhv_data_fixed = bhv_data_pre_itrial\n",
    "            #\n",
    "            # use the bhv_data_fixed for the bhv_data_pre_itrial\n",
    "            #\n",
    "            ind_pre_itrial = bhv_data_fixed[\"trial_number\"]==itrial+1\n",
    "            bhv_data_pre_itrial = bhv_data_fixed[ind_pre_itrial]\n",
    "\n",
    "            # examine the itrial\n",
    "            # the misslabeled successful trial, but miss one pull, need to fix \n",
    "            if np.sum((bhv_data_itrial['behavior_events']==1)|(bhv_data_itrial['behavior_events']==2))==1:\n",
    "                # \n",
    "                # modify when the trial starts\n",
    "                bhv_data_pre_itrial['trial_number'][bhv_data_pre_itrial['behavior_events']==9] = itrial+2\n",
    "                bhv_data_pre_itrial['behavior_events'][bhv_data_pre_itrial['behavior_events']==9] = 0\n",
    "                #\n",
    "                # if animal 2 pull is missing\n",
    "                if np.sum((bhv_data_itrial['behavior_events']==1))==1:\n",
    "                    # modify the animal 2 pull\n",
    "                    bhv_data_itrial['behavior_events'][bhv_data_itrial['behavior_events']==0] = 2\n",
    "                    # modify the end of previous trial\n",
    "                    bhv_data_pre_itrial['behavior_events'].iloc[np.where(bhv_data_pre_itrial['behavior_events']==2)[0][-1]]=9\n",
    "\n",
    "                # if animal 1 pull is missing\n",
    "                elif np.sum((bhv_data_itrial['behavior_events']==2))==1:\n",
    "                    # modify the animal 1 pull\n",
    "                    bhv_data_itrial['behavior_events'][bhv_data_itrial['behavior_events']==0] = 1\n",
    "                    # modify the end of previous trial\n",
    "                    bhv_data_pre_itrial['behavior_events'].iloc[np.where(bhv_data_pre_itrial['behavior_events']==1)[0][-1]]=9\n",
    "                #\n",
    "                bhv_data_fixed[ind_pre_itrial] = bhv_data_pre_itrial\n",
    "                bhv_data_fixed = pd.concat([bhv_data_fixed,bhv_data_itrial])\n",
    "                #\n",
    "            else:\n",
    "\n",
    "                bhv_data_fixed = pd.concat([bhv_data_fixed,bhv_data_itrial])\n",
    "                bhv_data_old = bhv_data.copy()\n",
    "                \n",
    "        bhv_data = bhv_data_fixed\n",
    "\n",
    "        # analyze behavior results\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        #\n",
    "        juiceid = np.array(bhv_data[(bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4)][\"behavior_events\"])\n",
    "        juicetime = np.array(bhv_data[(bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4)][\"time_points\"])\n",
    "        #\n",
    "        # successful trial\n",
    "        trial_num_succ = np.array(trial_record_clean[trial_record_clean['rewarded']>0]['trial_number'])\n",
    "        bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trial_num_succ)]\n",
    "        #\n",
    "        pullid_succ = np.array(bhv_data_succ[(bhv_data_succ['behavior_events']==1)|(bhv_data_succ['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime_succ = np.array(bhv_data_succ[(bhv_data_succ['behavior_events']==1)|(bhv_data_succ['behavior_events']==2)][\"time_points\"])\n",
    "        #\n",
    "        juiceid_succ = np.array(bhv_data_succ[(bhv_data_succ['behavior_events']==3)|(bhv_data_succ['behavior_events']==4)][\"behavior_events\"])\n",
    "        juicetime_succ = np.array(bhv_data_succ[(bhv_data_succ['behavior_events']==3)|(bhv_data_succ['behavior_events']==4)][\"time_points\"])\n",
    "        # \n",
    "        #failed trial\n",
    "        trial_num_fail = np.array(trial_record_clean[trial_record_clean['rewarded']==0]['trial_number'])\n",
    "        bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trial_num_fail)]\n",
    "        #\n",
    "        pullid_fail = np.array(bhv_data_fail[(bhv_data_fail['behavior_events']==1)|(bhv_data_fail['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime_fail = np.array(bhv_data_fail[(bhv_data_fail['behavior_events']==1)|(bhv_data_fail['behavior_events']==2)][\"time_points\"])\n",
    "        #\n",
    "        juiceid_fail = np.array(bhv_data_fail[(bhv_data_fail['behavior_events']==3)|(bhv_data_fail['behavior_events']==4)][\"behavior_events\"])\n",
    "        juicetime_fail = np.array(bhv_data_fail[(bhv_data_fail['behavior_events']==3)|(bhv_data_fail['behavior_events']==4)][\"time_points\"])\n",
    "\n",
    "        # \n",
    "        if animal1_frombhv == animal1_fixedorder[0]:\n",
    "            # all pulls \n",
    "            npulls1 = sum(pullid==1)\n",
    "            pulltimes1 = pulltime[pullid==1]\n",
    "            npulls2 = sum(pullid==2)\n",
    "            pulltimes2 = pulltime[pullid==2]\n",
    "            # all juices\n",
    "            njuice1 = sum(juiceid==3)\n",
    "            juicetimes1 = juicetime[juiceid==3]\n",
    "            njuice2 = sum(juiceid==4)\n",
    "            juicetimes2 = juicetime[juiceid==4]\n",
    "            # all succ pulls \n",
    "            npulls1_succ = sum(pullid_succ==1)\n",
    "            pulltimes1_succ = pulltime_succ[pullid_succ==1]\n",
    "            npulls2_succ = sum(pullid_succ==2)\n",
    "            pulltimes2_succ = pulltime_succ[pullid_succ==2]\n",
    "            # all fail pulls \n",
    "            npulls1_fail = sum(pullid_fail==1)\n",
    "            pulltimes1_fail = pulltime_fail[pullid_fail==1]\n",
    "            npulls2_fail = sum(pullid_fail==2)\n",
    "            pulltimes2_fail = pulltime_fail[pullid_fail==2]\n",
    "            #\n",
    "        elif animal1_frombhv == animal2_fixedorder[0]:\n",
    "            # all pulls \n",
    "            npulls1 = sum(pullid==2)\n",
    "            pulltimes1 = pulltime[pullid==2]\n",
    "            npulls2 = sum(pullid==1)\n",
    "            pulltimes2 = pulltime[pullid==1]\n",
    "            # all juices\n",
    "            njuice1 = sum(juiceid==4)\n",
    "            juicetimes1 = juicetime[juiceid==4]\n",
    "            njuice2 = sum(juiceid==3)\n",
    "            juicetimes2 = juicetime[juiceid==3]\n",
    "            # all succ pulls \n",
    "            npulls1_succ = sum(pullid_succ==2)\n",
    "            pulltimes1_succ = pulltime_succ[pullid_succ==2]\n",
    "            npulls2_succ = sum(pullid_succ==1)\n",
    "            pulltimes2_succ = pulltime_succ[pullid_succ==1]\n",
    "            # all fail pulls \n",
    "            npulls1_fail = sum(pullid_fail==2)\n",
    "            pulltimes1_fail = pulltime_fail[pullid_fail==2]\n",
    "            npulls2_fail = sum(pullid_fail==1)\n",
    "            pulltimes2_fail = pulltime_fail[pullid_fail==1]\n",
    "\n",
    "\n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "\n",
    "\n",
    "    \n",
    "    else:\n",
    "        neural_start_time_session_start_offset = 0\n",
    "    \n",
    "    # align the LFP recording time stamps\n",
    "    LFP_timewins_aligned = timewins+neural_start_time_session_start_offset # in the unit of second\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "    # # load spike sorting results\n",
    "    print('load spike data for '+neural_record_condition)\n",
    "    if kilosortver == 2:\n",
    "        spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "        spike_time_data = np.load(spike_time_file)\n",
    "    elif kilosortver == 4:\n",
    "        spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "        # spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch_mirroredMap/spike_times.npy'\n",
    "        spike_time_data = np.load(spike_time_file)\n",
    "    # \n",
    "    # align the FR recording time stamps\n",
    "    spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "    # down-sample the spike recording resolution to the same as the lfp\n",
    "    spike_time_data = spike_time_data/fs_spikes*fs_lfp\n",
    "    spike_time_data = np.round(spike_time_data)/fs_lfp\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "        spike_clusters_data = np.load(spike_clusters_file)\n",
    "        spike_channels_data = np.copy(spike_clusters_data)\n",
    "    elif kilosortver == 4:\n",
    "        spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "        # spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch_mirroredMap/spike_clusters.npy'\n",
    "        spike_clusters_data = np.load(spike_clusters_file)\n",
    "        spike_channels_data = np.copy(spike_clusters_data)\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "        channel_maps_data = np.load(channel_maps_file)\n",
    "    elif kilosortver == 4:\n",
    "        channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "        # channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch_mirroredMap/channel_map.npy'\n",
    "        channel_maps_data = np.load(channel_maps_file)\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "        channel_pos_data = np.load(channel_pos_file)\n",
    "    elif kilosortver == 4:\n",
    "        channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "        # channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch_mirroredMap/channel_positions.npy'\n",
    "        channel_pos_data = np.load(channel_pos_file)\n",
    "    #\n",
    "    if kilosortver == 2:\n",
    "        clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "        clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "    elif kilosortver == 4:\n",
    "        clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "        # clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch_mirroredMap/cluster_info.tsv'\n",
    "        clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "    #\n",
    "    # only get the spikes that are manually checked\n",
    "    try:\n",
    "        good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "    except:\n",
    "        good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "    #\n",
    "    clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "    #\n",
    "    spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "    #\n",
    "    nclusters = np.shape(clusters_info_data)[0]\n",
    "    #\n",
    "    for icluster in np.arange(0,nclusters,1):\n",
    "        try:\n",
    "            cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "        except:\n",
    "            cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "        spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "    #\n",
    "    # \n",
    "    # get the channel to depth information, change 2 shanks to 1 shank \n",
    "    try:\n",
    "        channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "        # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "        # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "        channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "    except:\n",
    "        channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "        # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "        channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "        channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29b9f9",
   "metadata": {},
   "source": [
    "#### plot the mean LFP across all channels to check the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10498666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean LFP across channels (axis=0 means averaging across rows, i.e., channels)\n",
    "mean_lfp = np.nanmean(lfp_filt_data, axis=0)\n",
    "\n",
    "# Create a time axis based on the sampling rate\n",
    "time_axis = np.arange(mean_lfp.shape[0]) / fs_lfp  # fs_lfp is the sampling rate (1000 Hz)\n",
    "\n",
    "# Plot the mean LFP trace\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(time_axis, mean_lfp, color='black', linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Mean LFP (µV)\")\n",
    "plt.title(\"Mean LFP Across Channels Over Entire Recording\")\n",
    "\n",
    "# Improve visualization\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac4e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5587e5",
   "metadata": {},
   "source": [
    "#### define and remove bad channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute variance of each channel\n",
    "channel_variances = np.nanvar(lfp_filt_data, axis=1)\n",
    "\n",
    "# Define thresholds for bad channels\n",
    "low_threshold = np.percentile(channel_variances, 5)  # Bottom 5% (flat signals)\n",
    "high_threshold = np.percentile(channel_variances, 93)  # Top 5% (too noisy)\n",
    "\n",
    "# Identify bad channels\n",
    "# bad_channels = np.where((channel_variances < low_threshold) | (channel_variances > high_threshold))[0]\n",
    "bad_channels = np.where((channel_variances > high_threshold))[0]\n",
    "\n",
    "# Replace bad channels with NaN\n",
    "# lfp_filt_data[bad_channels, :] = np.nan\n",
    "\n",
    "# Print bad channels\n",
    "print(\"Identified bad channels:\", bad_channels)\n",
    "\n",
    "# Plot channel variances for inspection\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(channel_variances, marker='o', linestyle='-', color='b')\n",
    "plt.axhline(low_threshold, color='r', linestyle='--', label='Low Variance Threshold')\n",
    "plt.axhline(high_threshold, color='r', linestyle='--', label='High Variance Threshold')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.title(\"LFP Channel Variance Analysis; bad channels\"+str(bad_channels))\n",
    "plt.legend()\n",
    "\n",
    "fig_name = neural_data_folder+neural_record_condition+'/'+neural_record_condition+'_channels_variance_for_badones.pdf'\n",
    "plt.savefig(fig_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f702aa",
   "metadata": {},
   "source": [
    "#### match the LFP signals to the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25663e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort indices based on the descending order of channel_to_depth[1]\n",
    "sorted_indices = np.argsort(channel_to_depth[1])[::-1]\n",
    "\n",
    "# Reorder channel_to_depth[0] based on sorted indices\n",
    "channel_to_depth_sorted = channel_to_depth[:, sorted_indices]\n",
    "\n",
    "lfp_data_mapmatch_1shank = lfp_filt_data[channel_to_depth_sorted[0].astype(int), :]\n",
    "\n",
    "# lfp match to the channel maps\n",
    "lfp_data_mapmatch = lfp_filt_data[chan_map['chanMap'].flatten()-1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc497023",
   "metadata": {},
   "source": [
    "### plot the correlation confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify bad channels (fully NaN)\n",
    "bad_channels = np.all(np.isnan(lfp_filt_data), axis=1)\n",
    "\n",
    "# Keep only good channels\n",
    "good_channels = ~bad_channels\n",
    "lfp_good = lfp_filt_data[good_channels, :]\n",
    "\n",
    "# Find time points where all good channels have valid data (no NaNs)\n",
    "valid_timepoints = np.all(~np.isnan(lfp_good), axis=0)\n",
    "\n",
    "# Keep only valid time points\n",
    "lfp_clean = lfp_good[:, valid_timepoints]\n",
    "\n",
    "# Compute correlation matrix on the cleaned LFP data\n",
    "corr_matrix_good = np.corrcoef(lfp_clean)\n",
    "\n",
    "# Create a full correlation matrix with NaNs for bad channels\n",
    "num_channels = lfp_filt_data.shape[0]\n",
    "corr_matrix_full = np.full((num_channels, num_channels), np.nan)\n",
    "corr_matrix_full[np.ix_(good_channels, good_channels)] = corr_matrix_good\n",
    "\n",
    "# Mask NaNs for better visualization\n",
    "masked_corr = np.ma.masked_invalid(corr_matrix_full)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(masked_corr, cmap=\"jet\", aspect=\"auto\")\n",
    "\n",
    "# Set colorbar limits based on valid values\n",
    "valid_corr_values = corr_matrix_good[~np.isnan(corr_matrix_good)]\n",
    "im.set_clim(np.min(valid_corr_values), np.max(valid_corr_values))\n",
    "# im.set_clim(0.8, 1)\n",
    "\n",
    "plt.colorbar(label=\"Correlation Coefficient\")\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Channel ID\")\n",
    "plt.title(\"Channel-to-Channel LFP Correlation Matrix (Filtered for Valid Periods)\")\n",
    "\n",
    "fig_name = neural_data_folder+neural_record_condition+'/'+neural_record_condition+'_ch_to_ch_LFP_corr_noMapMatch.pdf'\n",
    "plt.savefig(fig_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a05bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify bad channels (fully NaN)\n",
    "bad_channels = np.all(np.isnan(lfp_data_mapmatch), axis=1)\n",
    "\n",
    "# Keep only good channels\n",
    "good_channels = ~bad_channels\n",
    "lfp_good = lfp_data_mapmatch[good_channels, :]\n",
    "\n",
    "# Find time points where all good channels have valid data (no NaNs)\n",
    "valid_timepoints = np.all(~np.isnan(lfp_good), axis=0)\n",
    "\n",
    "# Keep only valid time points\n",
    "lfp_clean = lfp_good[:, valid_timepoints]\n",
    "\n",
    "# Compute correlation matrix on the cleaned LFP data\n",
    "corr_matrix_good = np.corrcoef(lfp_clean)\n",
    "\n",
    "# Create a full correlation matrix with NaNs for bad channels\n",
    "num_channels = lfp_data_mapmatch.shape[0]\n",
    "corr_matrix_full = np.full((num_channels, num_channels), np.nan)\n",
    "corr_matrix_full[np.ix_(good_channels, good_channels)] = corr_matrix_good\n",
    "\n",
    "# Mask NaNs for better visualization\n",
    "masked_corr = np.ma.masked_invalid(corr_matrix_full)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(masked_corr, cmap=\"jet\", aspect=\"auto\")\n",
    "\n",
    "# Set colorbar limits based on valid values\n",
    "valid_corr_values = corr_matrix_good[~np.isnan(corr_matrix_good)]\n",
    "im.set_clim(np.min(valid_corr_values), np.max(valid_corr_values))\n",
    "# im.set_clim(0.8, 1)\n",
    "\n",
    "plt.colorbar(label=\"Correlation Coefficient\")\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Channel ID\")\n",
    "plt.title(\"Channel-to-Channel LFP Correlation Matrix (Bad Channels Removed); match the channel map\")\n",
    "\n",
    "\n",
    "fig_name = neural_data_folder+neural_record_condition+'/'+neural_record_condition+'_ch_to_ch_LFP_corr_withMapMatch_mirroredMap.pdf'\n",
    "plt.savefig(fig_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aed5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify bad channels (fully NaN)\n",
    "bad_channels = np.all(np.isnan(lfp_data_mapmatch_1shank), axis=1)\n",
    "\n",
    "# Keep only good channels\n",
    "good_channels = ~bad_channels\n",
    "lfp_good = lfp_data_mapmatch_1shank[good_channels, :]\n",
    "\n",
    "# Find time points where all good channels have valid data (no NaNs)\n",
    "valid_timepoints = np.all(~np.isnan(lfp_good), axis=0)\n",
    "\n",
    "# Keep only valid time points\n",
    "lfp_clean = lfp_good[:, valid_timepoints]\n",
    "\n",
    "# Compute correlation matrix on the cleaned LFP data\n",
    "corr_matrix_good = np.corrcoef(lfp_clean)\n",
    "\n",
    "# Create a full correlation matrix with NaNs for bad channels\n",
    "num_channels = lfp_data_mapmatch_1shank.shape[0]\n",
    "corr_matrix_full = np.full((num_channels, num_channels), np.nan)\n",
    "corr_matrix_full[np.ix_(good_channels, good_channels)] = corr_matrix_good\n",
    "\n",
    "# Mask NaNs for better visualization\n",
    "masked_corr = np.ma.masked_invalid(corr_matrix_full)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(masked_corr, cmap=\"jet\", aspect=\"auto\")\n",
    "\n",
    "# Set colorbar limits based on valid values\n",
    "valid_corr_values = corr_matrix_good[~np.isnan(corr_matrix_good)]\n",
    "im.set_clim(np.min(valid_corr_values), np.max(valid_corr_values))\n",
    "# im.set_clim(0.6, 0.7)\n",
    "\n",
    "plt.colorbar(label=\"Correlation Coefficient\")\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Channel ID\")\n",
    "plt.title(\"Channel-to-Channel LFP Correlation Matrix (Bad Channels Removed); match the channel map and combine to 1 shank\")\n",
    "\n",
    "fig_name = neural_data_folder+neural_record_condition+'/'+neural_record_condition+'_ch_to_ch_LFP_corr_withMapMatch1shank_mirroredMap.pdf'\n",
    "plt.savefig(fig_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496276ec",
   "metadata": {},
   "source": [
    "### plot the beta and gamma power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c414f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "fs = 1000  # Sampling rate in Hz\n",
    "nperseg = 500  # Segment length for Welch’s method\n",
    "\n",
    "# lfp_tgt = lfp_filt_data\n",
    "lfp_tgt = lfp_data_mapmatch_1shank\n",
    "# lfp_tgt = lfp_data_mapmatch\n",
    "\n",
    "lfp_tgt = lfp_tgt[:,~np.isnan(np.sum(lfp_tgt,axis=0))]\n",
    "\n",
    "# Compute PSD using Welch’s method\n",
    "freqs, psd = signal.welch(lfp_tgt, fs=fs, nperseg=nperseg, axis=1)  # Shape (num_channels, freq_bins)\n",
    "\n",
    "# Normalize by max power across channels for each frequency\n",
    "relative_power = psd / np.nanmax(psd, axis=0, keepdims=True)  # Shape (num_channels, freq_bins)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(relative_power, aspect='auto', cmap='jet', extent=[freqs[0], freqs[-1], 0, lfp_tgt.shape[0]], origin='lower')\n",
    "\n",
    "# Set axis labels\n",
    "plt.colorbar(label=\"Relative Power (0-1)\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Channel ID\")\n",
    "plt.title(\"Relative Power Across Channels and Frequencies\")\n",
    "\n",
    "# Save figure as PDF\n",
    "fig_name = neural_data_folder+neural_record_condition+'/'+neural_record_condition+'_lfp_power_spectrum_heatmap_withMapMatch1shank_mirroredMap.pdf'\n",
    "plt.savefig(fig_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d73d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a934f",
   "metadata": {},
   "source": [
    "### plot the behavioral events aligned mean LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e69409",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # Define parameters\n",
    "    time_window = 3  # Time range before and after pull event (3s each)\n",
    "    samples_window = time_window * fs_lfp  # 3000 samples per side (total 6000)\n",
    "    time_axis = np.linspace(-3, 3, 2 * samples_window)  # Time from -3s to +3s\n",
    "\n",
    "    # Initialize array to store LFP segments (shape: channels × time)\n",
    "    lfp_avg_matrix = np.zeros((64, 2 * samples_window))\n",
    "\n",
    "    # Loop over channels\n",
    "    for ch in range(64):\n",
    "        lfp_segments = []  # Store LFP trials for averaging\n",
    "\n",
    "        for pull_time in pulltimes2:\n",
    "            # Find the index of the pull event in LFP_timewins_aligned\n",
    "            pull_index = np.argmin(np.abs(LFP_timewins_aligned - pull_time))\n",
    "\n",
    "            # Ensure window is within the data range\n",
    "            start_idx = pull_index - samples_window\n",
    "            end_idx = pull_index + samples_window\n",
    "\n",
    "            if start_idx >= 0 and end_idx < lfp_data_mapmatch_1shank.shape[1]:\n",
    "                # Extract LFP segment and store\n",
    "                lfp_segments.append(lfp_data_mapmatch_1shank[ch, start_idx:end_idx])\n",
    "\n",
    "        # Compute the mean LFP across all pull1 events for this channel\n",
    "        if lfp_segments:\n",
    "            lfp_avg_matrix[ch, :] = np.nanmean(np.array(lfp_segments), axis=0)\n",
    "        else:\n",
    "            lfp_avg_matrix[ch, :] = np.nan  # Assign NaN if no valid data\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot each channel's mean LFP trace with an offset for visibility\n",
    "    offset = 20  # Increase the offset value to separate traces more\n",
    "    for ch in range(64):\n",
    "        plt.plot(time_axis, lfp_avg_matrix[ch, :] + ch * offset, label=f\"Channel {ch+1}\")  # Add an offset for separation\n",
    "\n",
    "    # Add vertical line for the pull event\n",
    "    plt.axvline(0, color='red', linestyle='--', label=\"Pull Event\")\n",
    "\n",
    "    # Labels and Title\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"LFP Signal (with offset per channel)\")\n",
    "    plt.title(\"Mean LFP Trace for Each Channel (with offsets)\")\n",
    "\n",
    "    # Remove legend if not needed\n",
    "    # plt.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd905dd9",
   "metadata": {},
   "source": [
    "### plot the behavioral events aligned mean LFP CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # Define parameters\n",
    "    time_window = 3  # Time range before and after pull event (3s each)\n",
    "    samples_window = time_window * fs_lfp  # 3000 samples per side (total 6000)\n",
    "    time_axis = np.linspace(-3, 3, 2 * samples_window)  # Time from -3s to +3s\n",
    "\n",
    "    # Initialize array to store LFP segments (shape: channels × time)\n",
    "    lfp_avg_matrix = np.zeros((64, 2 * samples_window))\n",
    "\n",
    "    # Loop over channels to compute LFP segments\n",
    "    for ch in range(64):\n",
    "        lfp_segments = []  # Store LFP trials for averaging\n",
    "\n",
    "        for pull_time in pulltimes1:\n",
    "            # Find the index of the pull event in LFP_timewins_aligned\n",
    "            pull_index = np.argmin(np.abs(LFP_timewins_aligned - pull_time))\n",
    "\n",
    "            # Ensure window is within the data range\n",
    "            start_idx = pull_index - samples_window\n",
    "            end_idx = pull_index + samples_window\n",
    "\n",
    "            if start_idx >= 0 and end_idx < lfp_data_mapmatch_1shank.shape[1]:\n",
    "                # Extract LFP segment\n",
    "                lfp_segment = lfp_data_mapmatch_1shank[ch, start_idx:end_idx]\n",
    "                lfp_segments.append(lfp_segment)\n",
    "\n",
    "        # Compute the mean LFP across all pull1 events for this channel\n",
    "        if lfp_segments:\n",
    "            lfp_avg_matrix[ch, :] = np.nanmean(np.array(lfp_segments), axis=0)\n",
    "        else:\n",
    "            lfp_avg_matrix[ch, :] = np.nan  # Assign NaN if no valid data\n",
    "\n",
    "    # Calculate the CSD by taking the second spatial derivative of the LFP\n",
    "    # Use finite differences: CSD = LFP(i+1) - 2*LFP(i) + LFP(i-1)\n",
    "    # Skip the first and last channel to avoid boundary issues\n",
    "    csd_matrix = np.zeros_like(lfp_avg_matrix)\n",
    "\n",
    "    # Compute the CSD for each time point across all channels\n",
    "    for t in range(lfp_avg_matrix.shape[1]):  # Loop over time points\n",
    "        for ch in range(1, 63):  # Loop over channels (skip first and last)\n",
    "            csd_matrix[ch, t] = lfp_avg_matrix[ch + 1, t] - 2 * lfp_avg_matrix[ch, t] + lfp_avg_matrix[ch - 1, t]\n",
    "\n",
    "    # Plot the CSD as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create a heatmap of the CSD\n",
    "    plt.imshow(csd_matrix[1:63, :], aspect='auto', cmap='jet', extent=[time_axis[0], time_axis[-1], 64, 1])\n",
    "    plt.colorbar(label=\"CSD (µA/cm²)\")\n",
    "    plt.axvline(0, color='red', linestyle='--')  # Mark pull event at t=0\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Channel\")\n",
    "    plt.title(\"CSD Heatmap (-3s to +3s around Pull1)\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d678e4c",
   "metadata": {},
   "source": [
    "### plot the behavioral events aligned mean LFP aligned at a chosen spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be867c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    spike_time_tgt = spike_time_data[spike_channels_data==37]\n",
    "    \n",
    "    # Define parameters\n",
    "    time_window = 0.1  # Time range before and after spike (0.1s each)\n",
    "    samples_window = int(time_window * fs_lfp)  # 100 samples per side (total 200)\n",
    "    time_axis = np.linspace(-0.1, 0.1, 2 * samples_window)  # Time from -0.1s to +0.1s\n",
    "\n",
    "    # Select only 200 random spike timestamps\n",
    "    num_spikes = min(200, len(spike_time_tgt))\n",
    "    selected_spikes = np.random.choice(spike_time_tgt, num_spikes, replace=False)\n",
    "\n",
    "    # Initialize array to store LFP segments (shape: channels × time)\n",
    "    lfp_avg_matrix = np.zeros((64, 2 * samples_window))\n",
    "\n",
    "    # Extract LFP segments aligned to selected spikes\n",
    "    for ch in range(64):\n",
    "        lfp_segments = []  \n",
    "\n",
    "        for spike_time in selected_spikes:\n",
    "            # Find the index of the spike event in LFP_timewins_aligned\n",
    "            spike_index = np.argmin(np.abs(LFP_timewins_aligned - spike_time))\n",
    "\n",
    "            # Ensure window is within data range\n",
    "            start_idx = spike_index - samples_window\n",
    "            end_idx = spike_index + samples_window\n",
    "\n",
    "            if start_idx >= 0 and end_idx < lfp_data_mapmatch_1shank.shape[1]:\n",
    "                # Extract LFP segment and store\n",
    "                lfp_segments.append(lfp_data_mapmatch_1shank[ch, start_idx:end_idx])\n",
    "\n",
    "        # Compute the mean LFP across selected spike events for this channel\n",
    "        if lfp_segments:\n",
    "            lfp_avg_matrix[ch, :] = np.nanmean(np.array(lfp_segments), axis=0)\n",
    "        else:\n",
    "            lfp_avg_matrix[ch, :] = np.nan  # Assign NaN if no valid data\n",
    "\n",
    "    # Compute CSD (Second Spatial Derivative)\n",
    "    csd_matrix = np.zeros_like(lfp_avg_matrix)\n",
    "    for ch in range(1, 63):  # Exclude first and last channels\n",
    "        csd_matrix[ch, :] = lfp_avg_matrix[ch - 1, :] - 2 * lfp_avg_matrix[ch, :] + lfp_avg_matrix[ch + 1, :]\n",
    "\n",
    "    # --- PLOTTING ---\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 12), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "    # Plot LFP traces with offset\n",
    "    offset = 0.8  \n",
    "    for ch in range(64):\n",
    "        axes[0].plot(time_axis, lfp_avg_matrix[ch, :] + ch * offset)  \n",
    "\n",
    "    axes[0].axvline(0, color='red', linestyle='--')  # Spike event marker\n",
    "    axes[0].set_xlabel(\"Time (s)\")\n",
    "    axes[0].set_ylabel(\"LFP Signal (with offset per channel)\")\n",
    "    axes[0].set_title(f\"LFP Aligned to {num_spikes} Randomly Selected Spike Events\")\n",
    "\n",
    "    # Plot CSD as a heatmap\n",
    "    im = axes[1].imshow(csd_matrix, aspect='auto', extent=[-0.1, 0.1, 64, 1], cmap='jet', interpolation='none')\n",
    "    axes[1].axvline(0, color='red', linestyle='--')\n",
    "    axes[1].set_xlabel(\"Time (s)\")\n",
    "    axes[1].set_ylabel(\"Channel\")\n",
    "    axes[1].set_title(\"CSD Aligned to Spikes\")\n",
    "\n",
    "    # Add colorbar for CSD\n",
    "    fig.colorbar(im, ax=axes[1], label=\"CSD (µV/mm²)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e43b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b6bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6001d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
