{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### the following detailed analysis focused on pull related behavioral events\n",
    "#### the pull action start events are defined based on the movement onset before each pull\n",
    "#### capture the entire section of neural activity from Pull Start/Onset to pull actions\n",
    "#### include multiple ways to defined trial start; such as based on speed, or pca that consider multiple continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1786d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note:\n",
    "# need to use pyddm environment to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace\n",
    "\n",
    "\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR_PullStartToPull_variedSection import plot_bhv_events_aligned_FR_PullStartToPull_variedSection\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace\n",
    "\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545792d",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d40abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "from ana_functions.get_pull_infos import get_pull_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# if use onset of the first increase after min\n",
    "doOnsetAfterMin = 1\n",
    "if not doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = ''\n",
    "elif doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = 'PullOnsetAfterMin_'\n",
    "\n",
    "# if use a hmm based method to find the trial start\n",
    "doHMMmethod = 0\n",
    "if doHMMmethod:\n",
    "    doOnsetAfterMin_suffix = 'HMMmethods_'\n",
    "\n",
    "    \n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     # '20231101_Dodson_withGinger_SR',\n",
    "                                     # '20231107_Dodson_withGinger_SR',\n",
    "                                     # '20231122_Dodson_withGinger_SR',\n",
    "                                     # '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      # \"20231101_SR\",\n",
    "                      # \"20231107_SR\",\n",
    "                      # \"20231122_SR\",\n",
    "                      # \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 # 0.00,   \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         # 2, \n",
    "                         # 2, \n",
    "                         # 4, \n",
    "                         # 4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorders = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorders = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "pull1_intv_all_dates = np.zeros((ndates,1))\n",
    "pull2_intv_all_dates = np.zeros((ndates,1))\n",
    "pull1_minintv_all_dates = np.zeros((ndates,1))\n",
    "pull2_minintv_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_infos_all_dates = dict.fromkeys(dates_list, []) # keep some useful information about pulls - time from last reward, number of preceding failed pull etc\n",
    "\n",
    "pull_rts_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_pullstartTopull_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_infos_all_dates  = pickle.load(f)  \n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_rts_all_dates  = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "      \n",
    "    with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstartTopull_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "        \n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # dummpy\n",
    "    \n",
    "    print('all data from all dates are loaded; pull start focus')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        #\n",
    "        pull_rts_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        \n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        #\n",
    "        pulltime1 = np.array(bhv_data[(bhv_data['behavior_events']==1)]['time_points'])\n",
    "        pulltime2 = np.array(bhv_data[(bhv_data['behavior_events']==2)]['time_points'])\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        \n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        #\n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        #\n",
    "        ind_good_pull1_succ = time_point_pull1_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_succ = time_point_pull1_succ[ind_good_pull1_succ]\n",
    "        ind_good_pull2_succ = time_point_pull2_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_succ = time_point_pull2_succ[ind_good_pull2_succ]\n",
    "        #\n",
    "        ind_good_pull1_fail = time_point_pull1_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_fail = time_point_pull1_fail[ind_good_pull1_fail]\n",
    "        ind_good_pull2_fail = time_point_pull2_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_fail = time_point_pull2_fail[ind_good_pull2_fail]\n",
    "        \n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        pull_infos_all_dates[date_tgt] = pull_infos\n",
    "        \n",
    "            \n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "        # define variables and use them to find the 'onset' of pull decision\n",
    "        print('use the gaze vector speed and face mass speed to define the start of the pull decision')\n",
    "        #\n",
    "        gausKernelsize = 16\n",
    "        #\n",
    "        # clean the data\n",
    "        time_point_pull1_temp = np.array(time_point_pull1)+session_start_time\n",
    "        time_point_pull1_temp = time_point_pull1_temp[time_point_pull1_temp<totalsess_time]\n",
    "        time_point_pull2_temp = np.array(time_point_pull2)+session_start_time\n",
    "        time_point_pull2_temp = time_point_pull2_temp[time_point_pull2_temp<totalsess_time]\n",
    "        #\n",
    "        # organize the data into a time series\n",
    "        pull1_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull1_data[np.round(time_point_pull1_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        pull2_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull2_data[np.round(time_point_pull2_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        facemass1 = output_key_locations['facemass_loc_all_merge']['dodson'].transpose()\n",
    "        facemass1 = np.hstack((facemass1,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass1[:,1:]-facemass1[:,:-1])\n",
    "        speed1_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed1_data = scipy.ndimage.gaussian_filter1d(speed1_data,gausKernelsize)\n",
    "        #\n",
    "        facemass2 = output_key_locations['facemass_loc_all_merge']['scorch'].transpose()\n",
    "        facemass2 = np.hstack((facemass2,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass2[:,1:]-facemass2[:,:-1])\n",
    "        speed2_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed2_data = scipy.ndimage.gaussian_filter1d(speed2_data,gausKernelsize)\n",
    "        #\n",
    "        gazevect1 = np.array(output_allvectors['head_vect_all_merge']['dodson']).transpose()\n",
    "        gazevect1 = np.hstack((gazevect1, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect1[:, 1:]\n",
    "        at0 = gazevect1[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed1_data = np.full(nframes, np.nan)\n",
    "        eps = 1e-10\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed1_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed1_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed1_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed1_data[~nans])\n",
    "        anglespeed1_data = scipy.ndimage.gaussian_filter1d(anglespeed1_data, gausKernelsize)\n",
    "        #\n",
    "        gazevect2 = np.array(output_allvectors['head_vect_all_merge']['scorch']).transpose()\n",
    "        gazevect2 = np.hstack((gazevect2, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect2[:, 1:]\n",
    "        at0 = gazevect2[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed2_data = np.full(nframes, np.nan)\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed2_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed2_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed2_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed2_data[~nans])\n",
    "        anglespeed2_data = scipy.ndimage.gaussian_filter1d(anglespeed2_data, gausKernelsize)\n",
    "        \n",
    "        #\n",
    "        # use one of the two methods; not the HMM based method\n",
    "        if not doHMMmethod:\n",
    "            if not doOnsetAfterMin:\n",
    "                # find the transitional time point of angle speed and speed in IPI\n",
    "                speed1_increase = find_sharp_increases_withinIPI(pull1_data,speed1_data,session_start_time,fps)\n",
    "                anglespeed1_increase = find_sharp_increases_withinIPI(pull1_data,anglespeed1_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull1_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull1_data, speed1_data, anglespeed1_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "                #\n",
    "                speed2_increase = find_sharp_increases_withinIPI(pull2_data,speed2_data,session_start_time,fps)\n",
    "                anglespeed2_increase = find_sharp_increases_withinIPI(pull2_data,anglespeed2_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull2_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull2_data, speed2_data, anglespeed2_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "            #\n",
    "            elif doOnsetAfterMin:\n",
    "                # find the transitional time point of angle speed and speed in IPI\n",
    "                speed1_increase = find_rising_onset_after_min_withinIPI(pull1_data,speed1_data,session_start_time,fps)\n",
    "                anglespeed1_increase = find_rising_onset_after_min_withinIPI(pull1_data,anglespeed1_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull1_action_onset_frames = find_rising_onset_after_min_dual_speed(pull1_data, speed1_data, anglespeed1_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "                #\n",
    "                speed2_increase = find_rising_onset_after_min_withinIPI(pull2_data,speed2_data,session_start_time,fps)\n",
    "                anglespeed2_increase = find_rising_onset_after_min_withinIPI(pull2_data,anglespeed2_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull2_action_onset_frames = find_rising_onset_after_min_dual_speed(pull2_data, speed2_data, anglespeed2_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "        #\n",
    "        elif doHMMmethod:\n",
    "            n_states = 3\n",
    "            \n",
    "            pull1_action_onset_framepoints, _ = get_trial_start_frames_from_HMM(speed1_data, anglespeed1_data, pull1_data, \n",
    "                                                                                fps, session_start_time, n_states)\n",
    "            pull1_action_onset_frames = np.isin(np.arange(len(pull1_data)), pull1_action_onset_framepoints).astype(int)\n",
    "            #\n",
    "            pull2_action_onset_framepoints, _ = get_trial_start_frames_from_HMM(speed2_data, anglespeed2_data, pull2_data, \n",
    "                                                                                fps, session_start_time, n_states)\n",
    "            pull2_action_onset_frames = np.isin(np.arange(len(pull2_data)), pull2_action_onset_framepoints).astype(int)\n",
    "\n",
    "            \n",
    "        #\n",
    "        # store the pull reaction time information\n",
    "        # temporary fix\n",
    "        try:\n",
    "            pull_data_points = np.where(pull1_data)[0]\n",
    "            pullonset_data_points = np.where(pull1_action_onset_frames)[0]\n",
    "            pull1_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "            pull_rts_all_dates[date_tgt][animal1] = pull1_rt\n",
    "            #\n",
    "            pull_data_points = np.where(pull2_data)[0]\n",
    "            pullonset_data_points = np.where(pull2_action_onset_frames)[0]\n",
    "            pull2_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "            pull_rts_all_dates[date_tgt][animal2] = pull2_rt\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # replace time_point_pull_xxx to the pull onset\n",
    "        time_point_pull1 = np.array(np.round(time_point_pull1,1))\n",
    "        time_point_pull2 = np.array(np.round(time_point_pull2,1))\n",
    "        time_point_pull1_succ = np.array(time_point_pull1_succ)\n",
    "        time_point_pull2_succ = np.array(time_point_pull2_succ)\n",
    "        time_point_pull1_fail = np.array(time_point_pull1_fail)\n",
    "        time_point_pull2_fail = np.array(time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1_succ_idx = np.isin(time_point_pull1,time_point_pull1_succ)\n",
    "        time_point_pull2_succ_idx = np.isin(time_point_pull2,time_point_pull2_succ)\n",
    "        time_point_pull1_fail_idx = np.isin(time_point_pull1,time_point_pull1_fail)\n",
    "        time_point_pull2_fail_idx = np.isin(time_point_pull2,time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1 = np.where(pull1_action_onset_frames)[0]/fps - session_start_time\n",
    "        time_point_pull2 = np.where(pull2_action_onset_frames)[0]/fps - session_start_time\n",
    "        #\n",
    "        time_point_pull1_succ = time_point_pull1[time_point_pull1_succ_idx]\n",
    "        time_point_pull2_succ = time_point_pull2[time_point_pull2_succ_idx]\n",
    "        time_point_pull1_fail = time_point_pull1[time_point_pull1_fail_idx]\n",
    "        time_point_pull2_fail = time_point_pull2[time_point_pull2_fail_idx]\n",
    "        #\n",
    "        pull1_rt_succ = pull1_rt[time_point_pull1_succ_idx]\n",
    "        pull2_rt_succ = pull2_rt[time_point_pull2_succ_idx]\n",
    "        pull1_rt_fail = pull1_rt[time_point_pull1_fail_idx]\n",
    "        pull2_rt_fail = pull2_rt[time_point_pull2_fail_idx]\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            print('plot self pull start triggered bhv variables')\n",
    "            \n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+cameraID+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "                        \n",
    "            min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "\n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                    pull1_rt, pull2_rt, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                        pull1_rt_succ, pull2_rt_succ, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpullstartTopull_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                        pull1_rt_fail, pull2_rt_fail, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpullstartTopull_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 0:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate; pull start focus')\n",
    "                #\n",
    "                #\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR_PullStartToPull_variedSection(\n",
    "                                           animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           pull1_rt, pull2_rt,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_rts_all_dates, f)  \n",
    "        \n",
    "        with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstartTopull_trig_events_all_dates, f) \n",
    "\n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstartTopull_trig_events_all_dates, f) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doOnsetAfterMin_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_rts_all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull1_num_all_dates[np.isin(dates_list,'20250423')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simple sanity check plot\n",
    "if 0:\n",
    "    date_toplot = '20240808'\n",
    "    event_id = 16\n",
    "    #\n",
    "    a1 = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_toplot]['kanga pull']['8']['FR_allevents'][event_id]\n",
    "    a2 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'socialgaze_prob')][event_id]\n",
    "    a3 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'selfpull_prob')][event_id]\n",
    "    a4 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'mass_move_speed')][event_id]/500\n",
    "    a5 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'gaze_angle_speed')][event_id]\n",
    "\n",
    "    plt.plot(a1)\n",
    "    plt.plot(a2)\n",
    "    plt.plot(a4)\n",
    "    plt.plot(a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pullstartTopull_trig_events_all_dates[date_toplot].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72093762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_trig_events_summary.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "## Organize the data\n",
    "### put all the target data together for further analysis\n",
    "### also organize and save the data for the hddm analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "# pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "pull_trig_events_tgtname = 'socialgaze_prob' # for testing if individual trial different was from gaze start time\n",
    "# pull_trig_events_tgtname = 'othergaze_prob' # if to test things aligned to partner's pull (in that case, the subject's gaze becomes othergaze) \n",
    "\n",
    "\n",
    "# Keep these as additional controls\n",
    "pull_trig_otherpull_name = 'otherpull_prob'\n",
    "pull_trig_selfpull_name = 'selfpull_prob'\n",
    "pull_trig_selfspeed_name = 'mass_move_speed'\n",
    "pull_trig_otherspeed_name = 'other_mass_move_speed'\n",
    "pull_trig_otherleverspeed_name = 'other_lever_speed'\n",
    "pull_trig_selfPC1_name = 'self_PC1'\n",
    "pull_trig_otherPC1_name = 'other_PC1'\n",
    "pull_time_pre_reward_name = 'time_from_last_reward'\n",
    "prefail_pull_num_name = 'num_preceding_failpull'\n",
    "pull_rt_name = 'pull_rt'\n",
    "\n",
    "#\n",
    "animal_to_ana = 'dodson'\n",
    "# animal_to_ana = 'kanga'\n",
    "\n",
    "#\n",
    "# conditions_to_ana = np.unique(task_conditions)\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho',]\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'SR', 'SR_withDodson' ]\n",
    "# \n",
    "# for dodson only\n",
    "conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                      'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                       'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "#\n",
    "# conditions_to_ana = ['MC', \n",
    "#               'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', 'SR', 'SR_withGingerNew', 'SR_withKanga',\n",
    "#              'SR_withKoala',  ]\n",
    "\n",
    "condition_name = 'allMC'\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                                    'succrate','clusterID',\n",
    "                                                                    'channelID','FR_ievent'])\n",
    "\n",
    "try:\n",
    "\n",
    "    # dummy \n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_RawFullDatas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\\\n",
    "                  animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates_df = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "        \n",
    "        # only analyze the targeted conditions\n",
    "        if not np.isin(task_condition,conditions_to_ana):\n",
    "            continue\n",
    "    \n",
    "        succrate = succ_rate_all_dates[idate]\n",
    "        \n",
    "        bhv_types = list(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "    \n",
    "        for ibhv_type in bhv_types:\n",
    "    \n",
    "            clusterIDs = list(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "    \n",
    "            ibhv_type_split = ibhv_type.split()\n",
    "            if np.shape(ibhv_type_split)[0]==3:\n",
    "                ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "    \n",
    "            # only analyze targeted action animal\n",
    "            if not np.isin(ibhv_type_split[0], animal_to_ana):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # load the pull_trig_continuous_events\n",
    "            try:\n",
    "                pull_trig_events_tgt = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_otherpull = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherpull_name)]\n",
    "                pull_trig_selfpull = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfpull_name)]\n",
    "                pull_trig_selfspeed = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfspeed_name)]\n",
    "                pull_trig_otherspeed = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherspeed_name)]\n",
    "                pull_trig_selfPC1 = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfPC1_name)]\n",
    "                pull_trig_otherPC1 = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherPC1_name)]\n",
    "                \n",
    "                #\n",
    "                pull_trig_prerewardtime = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],pull_time_pre_reward_name)]))\n",
    "                prefail_pull_num = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],prefail_pull_num_name)]))\n",
    "                #\n",
    "                pull_rt = np.array(list(pull_rts_all_dates[date_tgt][ibhv_type_split[0]]))\n",
    "                #\n",
    "                pull_outcome = np.hstack([1-(prefail_pull_num[1:]>0).astype(int),np.nan])\n",
    "                \n",
    "                # get the other animal's speed related to their lever\n",
    "                pull_trig_otherleverdist = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],'otherani_otherlever_dist')]\n",
    "                pull_trig_otherleverspeed = [(np.diff(trace)) for trace in pull_trig_otherleverdist]\n",
    "\n",
    "                \n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_otherpull = np.nan\n",
    "                pull_trig_selfpull = np.nan\n",
    "                pull_trig_selfspeed = np.nan\n",
    "                pull_trig_otherspeed = np.nan\n",
    "                pull_trig_selfPC1 = np.nan\n",
    "                pull_trig_otherPC1 = np.nan\n",
    "                #\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                prefail_pull_num = np.nan\n",
    "                pull_rt = np.nan\n",
    "            \n",
    "                \n",
    "            for iclusterID in clusterIDs:   \n",
    "    \n",
    "                ichannelID = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_allevents = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "    \n",
    "                #\n",
    "                nevents = np.shape([len(x) for x in pull_trig_events_tgt])[0]\n",
    "                \n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "                \n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type_split[0],\n",
    "                                                                                        'bhv_name': ibhv_type_split[1],\n",
    "                                                                                        'bhv_id':ievent,\n",
    "                                                                                        'succrate':succrate[0],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_ievent':iFR_allevents[ievent],\n",
    "                                                                                         pull_trig_events_tgtname:pull_trig_events_tgt[ievent],                          \n",
    "                                                                                         pull_trig_otherpull_name:pull_trig_otherpull[ievent],                          \n",
    "                                                                                         pull_trig_selfpull_name:pull_trig_selfpull[ievent],\n",
    "                                                                                         pull_trig_otherspeed_name:pull_trig_otherspeed[ievent],                          \n",
    "                                                                                         pull_trig_selfspeed_name:pull_trig_selfspeed[ievent],\n",
    "                                                                                         pull_trig_otherPC1_name:pull_trig_otherPC1[ievent],                          \n",
    "                                                                                         pull_trig_selfPC1_name:pull_trig_selfPC1[ievent],\n",
    "                                                                                                                      \n",
    "                                                                                         pull_time_pre_reward_name:pull_trig_prerewardtime[ievent],\n",
    "                                                                                         prefail_pull_num_name:prefail_pull_num[ievent],\n",
    "                                                                                         pull_rt_name:pull_rt[ievent],\n",
    "                                                                                             \n",
    "                                                                                         pull_trig_otherleverspeed_name: pull_trig_otherleverspeed[ievent],                            \n",
    "                                                                                         \n",
    "                                                                                         'pull_outcome': pull_outcome[ievent],\n",
    "                                                                                        }, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # save the data with other HDDM dataframes for the modeling\n",
    "    savedata = 1\n",
    "    \n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "            '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/hddm_RawFullDatas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\\\n",
    "                  animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_allevents_all_dates_df, f) \n",
    "\n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28680cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a plot of the distribution of the reaction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_rt = np.array(bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'])\n",
    "\n",
    "data = {'pull_rt': pull_rt}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# --- 1. Outlier Removal using IQR ---\n",
    "Q1 = df['pull_rt'].quantile(0.25)\n",
    "Q3 = df['pull_rt'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to create a new one without outliers\n",
    "df_filtered = df[(df['pull_rt'] >= lower_bound) & (df['pull_rt'] <= upper_bound)]\n",
    "\n",
    "# --- Plotting Code ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use seaborn.kdeplot to create a smooth distribution plot\n",
    "seaborn.kdeplot(\n",
    "    data=df_filtered,\n",
    "    x='pull_rt',\n",
    "    fill=True,  # Fills the area under the curve\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Distribution of Pull Reaction Time (pull_rt)', fontsize=16)\n",
    "plt.xlabel('Pull Reaction Time (s)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "\n",
    "# --- 3. Add Mean and STD Annotations ---\n",
    "# Calculate mean and std from the filtered data\n",
    "mean_rt = df_filtered['pull_rt'].mean()\n",
    "std_rt = df_filtered['pull_rt'].std()\n",
    "\n",
    "# Add vertical lines for mean and std\n",
    "plt.axvline(mean_rt, color='red', linestyle='--', label=f'Mean: {mean_rt:.2f}s')\n",
    "plt.axvline(mean_rt + std_rt, color='black', linestyle=':', label=f'Std Dev: {std_rt:.2f}s')\n",
    "plt.axvline(mean_rt - std_rt, color='black', linestyle=':')\n",
    "\n",
    "# Create the text string for the annotation\n",
    "stats_text = f\"Mean: {mean_rt:.2f}\\nStd Dev: {std_rt:.2f}\"\n",
    "\n",
    "# Add the text box to the plot\n",
    "# transform=plt.gca().transAxes places the text relative to the axes (0,0 is bottom-left, 1,1 is top-right)\n",
    "plt.text(0.75, 0.9, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv/\" + \\\n",
    "                    cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + \"/bhvvariables_summary_fig/\"\n",
    "\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "\n",
    "    plt.savefig(figsavefolder + animal_to_ana + '_in_' + condition_name +\n",
    "                '_pull_responseTime_distribution.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original dataframe for reference \n",
    "bhvevents_aligned_FR_allevents_all_dates_df_origin = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbb68c-ee11-412f-bd35-6d96b1c60fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df_origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22792829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some new columns\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['previous_pull_outcome'] = (\n",
    "        bhvevents_aligned_FR_allevents_all_dates_df['num_preceding_failpull'] == 0\n",
    "    ).astype(int)\n",
    "\n",
    "# Compute AUC for social gaze probability\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].apply(\n",
    "#     lambda x: np.trapz(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    "# )\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].apply(\n",
    "    lambda x: np.nansum(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = \\\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc']/bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']\n",
    "\n",
    "# Mean and STD for mass_move_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_std'] = bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for self_PC1\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['self_PC1_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['self_PC1'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['self_PC1_std'] = bhvevents_aligned_FR_allevents_all_dates_df['self_PC1'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_mass_move_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_lever_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_PC1\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_PC1_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_PC1'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_PC1_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_PC1'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# firing rate mean and slope before pull (0.85s before)\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "# Compute mean firing rate\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['FR_ievent'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# \n",
    "# Compute the slope, in two ways\n",
    "#\n",
    "# Make a clean copy to avoid SettingWithCopyWarning\n",
    "df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "\n",
    "# === Fixed slope before 0.85s from pull ===\n",
    "pull_margin_frames = int(0.85 * fps)\n",
    "\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        try:\n",
    "            slope, *_ = linregress(x, y)\n",
    "            return abs(slope)\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "df.loc[:, 'fr_slope'] = df['FR_ievent'].apply(compute_fr_slope)\n",
    "\n",
    "# === Peak-based slope calculation: use shortest trial ===\n",
    "\n",
    "# Prepare new columns for slopes and peak times\n",
    "df['fr_slope_peakbased'] = np.nan\n",
    "df['fr_peak_time'] = np.nan\n",
    "\n",
    "# Group by neuron\n",
    "grouped = df.groupby(['dates', 'clusterID'])\n",
    "\n",
    "for (date, cluster_id), group in tqdm(grouped, desc=\"Processing neurons\"):\n",
    "\n",
    "    traces = group['FR_ievent'].dropna().tolist()\n",
    "    if len(traces) < 2:\n",
    "        continue\n",
    "\n",
    "    # Align traces by pull (end), pad shorter trials with nan at the front\n",
    "    min_len = min(len(trace) for trace in traces)\n",
    "    aligned = [trace[-min_len:] if len(trace) >= min_len else\n",
    "               np.pad(trace, (min_len - len(trace), 0), constant_values=np.nan)\n",
    "               for trace in traces]\n",
    "\n",
    "    stacked = np.stack(aligned)  # shape: (n_trials, min_len)\n",
    "    mean_trace = np.nanmean(stacked, axis=0)\n",
    "\n",
    "    # Compute slope of mean trace (full)\n",
    "    x_full = np.arange(min_len)\n",
    "    slope_full = linregress(x_full, mean_trace).slope\n",
    "\n",
    "    # Find peak index based on slope sign\n",
    "    if slope_full >= 0:\n",
    "        peak_idx = np.nanargmax(mean_trace)\n",
    "    else:\n",
    "        peak_idx = np.nanargmin(mean_trace)\n",
    "\n",
    "    # Convert peak index to peak time relative to pull (end-aligned)\n",
    "    peak_time = (peak_idx - (min_len - 1)) / fps\n",
    "\n",
    "    # Save peak_time for all trials of this neuron\n",
    "    df.loc[(df['dates'] == date) & (df['clusterID'] == cluster_id), 'fr_peak_time'] = peak_time\n",
    "\n",
    "    # For each trial, calculate slope from trial start to the peak index (if peak_idx within trial length)\n",
    "    for idx, row in group.iterrows():\n",
    "        fr_trace = row['FR_ievent']\n",
    "        trial_len = len(fr_trace)\n",
    "        \n",
    "        # Align trial trace to end, pad front if needed\n",
    "        if trial_len < min_len:\n",
    "            padded_trace = np.pad(fr_trace, (min_len - trial_len, 0), constant_values=np.nan)\n",
    "        else:\n",
    "            padded_trace = fr_trace[-min_len:]\n",
    "\n",
    "        # Make sure peak_idx is within trial length\n",
    "        if peak_idx < len(padded_trace):\n",
    "            y = padded_trace[:peak_idx+1]\n",
    "            x = np.arange(len(y))\n",
    "            if np.isnan(y).all() or len(y) < 2:\n",
    "                slope = np.nan\n",
    "            else:\n",
    "                slope = linregress(x, y).slope\n",
    "            # Optional: store absolute slope if you want\n",
    "            slope = abs(slope) if not np.isnan(slope) else np.nan\n",
    "\n",
    "            df.loc[idx, 'fr_slope_peakbased'] = slope\n",
    "        else:\n",
    "            df.loc[idx, 'fr_slope_peakbased'] = np.nan\n",
    "\n",
    "# Save back to original dataframe\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_slope_peakbased'] = df['fr_slope_peakbased']\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_peak_time'] = df['fr_peak_time']\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_slope'] = df['fr_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4c4c3-6166-4206-b956-8850c27f6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_allevents_all_dates_df['fr_slope']\n",
    "yyy = bhvevents_aligned_FR_allevents_all_dates_df['fr_slope_peakbased']\n",
    "# plt.plot(xxx,yyy,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e81a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# organize the data in order to do HDDM - average across neurons for the unique trial\n",
    "##########\n",
    "\n",
    "# Step 1: Group by unique trial (date, bhv_id) and average the FR_ievent across neurons\n",
    "averaged_df = bhvevents_aligned_FR_allevents_all_dates_df.groupby(['dates', 'bhv_id'])['FR_ievent'].apply(\n",
    "    lambda traces: np.mean(np.stack(traces.to_numpy()), axis=0)\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Rename the averaged firing rate column\n",
    "averaged_df = averaged_df.rename(columns={'FR_ievent': 'FR_ievent_avg'})\n",
    "\n",
    "# Step 3: Select representative behavioral columns to merge back (drop duplicates so one per trial)\n",
    "representative_cols = [\n",
    "    'dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'bhv_id', 'mass_move_speed',\n",
    "       'num_preceding_failpull', 'other_mass_move_speed', 'otherpull_prob',\n",
    "       'pull_rt', 'selfpull_prob', 'socialgaze_prob', 'time_from_last_reward',\n",
    "       'previous_pull_outcome', 'pull_outcome', 'socialgaze_auc', 'mass_move_speed_mean',\n",
    "       'mass_move_speed_std', 'other_mass_move_speed_mean',\n",
    "       'other_mass_move_speed_std','self_PC1_mean','self_PC1_std',\n",
    "       'other_PC1_mean', 'other_PC1_std', 'other_PC1','self_PC1',\n",
    "       'other_lever_speed_mean',\n",
    "       'other_lever_speed_std',\n",
    "       'other_lever_speed',\n",
    "]\n",
    "\n",
    "# Get one row per (dates, bhv_id) combination\n",
    "behavior_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])[representative_cols]\n",
    "\n",
    "# Step 4: Merge firing rate and behavioral data\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = pd.merge(averaged_df, behavior_df, \n",
    "                                                                    on=['dates', 'bhv_id'], how='left')\n",
    "\n",
    "# Step 5\n",
    "# Compute mean firing rate\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['fr_mean'] = \\\n",
    "      bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].apply(\n",
    "    lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Compute slope of firing rate before 0.85s prior to pull\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        slope, _, _, _, _ = linregress(x, y)\n",
    "        #\n",
    "        if slope<0:\n",
    "            slope = -slope\n",
    "        \n",
    "        return slope\n",
    "    return np.nan\n",
    "\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['fr_slope'] = \\\n",
    "     bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].apply(compute_fr_slope)\n",
    "\n",
    "\n",
    "# save the data with other HDDM dataframes for the modeling\n",
    "savedata = 1\n",
    "\n",
    "if savedata:\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_datas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\n",
    "              animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        pickle.dump(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# load the HDDM data to get the drift diffusion v for further correlation\n",
    "##########\n",
    "\n",
    "if 1:\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_traces_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_traces = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_stats_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_stats = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_nogaze_traces_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_nogaze_stats_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "    \n",
    "    v_inter = hddm_model_fitted_stats['mean']['v_Intercept']\n",
    "    v_self_gaze_auc = hddm_model_fitted_stats['mean']['v_self_gaze_auc']\n",
    "    v_partner_mean_speed = hddm_model_fitted_stats['mean']['v_partner_mean_speed']\n",
    "    v_self_mean_speed = hddm_model_fitted_stats['mean']['v_self_mean_speed']\n",
    "    v_partner_speed_std = hddm_model_fitted_stats['mean']['v_partner_speed_std']\n",
    "    v_self_speed_std = hddm_model_fitted_stats['mean']['v_self_speed_std']\n",
    "\n",
    "    #\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df['predicted_v'] = v_inter + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] * v_self_gaze_auc + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_mean'] * v_partner_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_std'] * v_partner_speed_std + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_mean'] * v_self_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_std'] * v_self_speed_std \n",
    "\n",
    "    #\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['predicted_v'] = v_inter + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['socialgaze_auc'] * v_self_gaze_auc + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed_mean'] * v_partner_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed_std'] * v_partner_speed_std + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed_mean'] * v_self_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed_std'] * v_self_speed_std \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some criteria to remove trials\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['previous_pull_outcome']==1]\n",
    "\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']>3]\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']<20]\n",
    "\n",
    "# Remove outlier pull_rt values\n",
    "# method 1\n",
    "if 0:\n",
    "    # Compute IQR bounds\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    q1 = bhv_unique_df['pull_rt'].quantile(0.25)\n",
    "    q3 = bhv_unique_df['pull_rt'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    #\n",
    "    # Filter out outliers\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "# method 2    \n",
    "if 1:\n",
    "    # Symmetrical trimming: keep central 90% of pull_rt\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    lower_bound = bhv_unique_df['pull_rt'].quantile(0.05)\n",
    "    # lower_bound = 4\n",
    "    upper_bound = bhv_unique_df['pull_rt'].quantile(0.95)\n",
    "\n",
    "    # Filter the dataframe\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b923d5-396c-4dca-b998-e1ca1f8c0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at the successful pull\n",
    "if 0:\n",
    "    ind_ = bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome']==1\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[ind_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']\n",
    "yyy = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc']\n",
    "plt.plot(xxx,yyy,'.')\n",
    "st.pearsonr(xxx,yyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0747346",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_allevents_all_dates_df\n",
    "print(np.sum(bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome']))\n",
    "print(np.shape(bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same clearup for the merged dataframe\n",
    "# Remove outlier pull_rt values\n",
    "# method 1\n",
    "if 0:\n",
    "    # Compute IQR bounds\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    q1 = bhv_unique_df['pull_rt'].quantile(0.25)\n",
    "    q3 = bhv_unique_df['pull_rt'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    #\n",
    "    # Filter out outliers\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "# method 2    \n",
    "if 1:\n",
    "    # Symmetrical trimming: keep central 90% of pull_rt\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    lower_bound = bhv_unique_df['pull_rt'].quantile(0.05)\n",
    "    # lower_bound = 4\n",
    "    upper_bound = bhv_unique_df['pull_rt'].quantile(0.95)\n",
    "\n",
    "    # Filter the dataframe\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[\n",
    "        (bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7b2f3-29c3-4c68-92b4-401269a672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at the successful pull\n",
    "if 1:\n",
    "    ind_ = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_outcome']==1\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[ind_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7839fc-be9e-4fc6-b30c-b19dde280098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be6083-4219-4bac-a8cb-d5a41512cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = 1749\n",
    "if 0:\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].loc[ind_])\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['socialgaze_prob'].loc[ind_]*5)\n",
    "if 0:\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['FR_ievent'].loc[ind_])\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].loc[ind_]*5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b4af3-dbf7-411e-ba8b-c0d8d919cc5d",
   "metadata": {},
   "source": [
    "#### trial wise correlation between FR trace and other continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec81fb8-6388-4f7d-9faa-8048091ef518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.stats import f_oneway, ttest_rel\n",
    "    import itertools\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    betas = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        y = df.iloc[i]['FR_ievent_avg']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['mass_move_speed'],\n",
    "            'partner': df.iloc[i]['other_mass_move_speed'],\n",
    "            'selfpull': df.iloc[i]['selfpull_prob'],\n",
    "            'socialgaze': df.iloc[i]['socialgaze_prob']\n",
    "        }\n",
    "    \n",
    "        if any(len(y) != len(x) for x in X_vars.values()):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            # Univariate regressions\n",
    "            for varname, x in X_vars.items():\n",
    "                model_uni = LinearRegression().fit(np.array(x).reshape(-1, 1), y)\n",
    "                r2_uni[varname].append(model_uni.score(np.array(x).reshape(-1, 1), y))\n",
    "            \n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'partner', 'selfpull', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "    \n",
    "            for idx, var in enumerate(['mass', 'partner', 'selfpull', 'socialgaze']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "            # Leave-one-out regressions\n",
    "            for idx, var in enumerate(['mass', 'partner', 'selfpull', 'socialgaze']):\n",
    "                X_reduced = np.delete(X_full, idx, axis=1)\n",
    "                X_reduced_std = StandardScaler().fit_transform(X_reduced)\n",
    "                model_reduced = LinearRegression().fit(X_reduced_std, y_std)\n",
    "                r2_reduced = model_reduced.score(X_reduced_std, y_std)\n",
    "                delta_r2[var].append(r2_full_val - r2_reduced)\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # ---------- Step 3: Format for plotting ----------\n",
    "    df_r2_uni = pd.DataFrame([\n",
    "        {'Variable': var, 'R2': r2} for var, values in r2_uni.items() for r2 in values\n",
    "    ])\n",
    "    \n",
    "    df_delta_r2 = pd.DataFrame([\n",
    "        {'Variable': var, 'Delta_R2': delta} for var, values in delta_r2.items() for delta in values\n",
    "    ])\n",
    "    \n",
    "    df_betas = pd.DataFrame([\n",
    "        {'Variable': var, 'Beta': beta} for var, values in betas.items() for beta in values\n",
    "    ])\n",
    "    \n",
    "    # ---------- Step 4: Statistical tests ----------\n",
    "    anova_r2 = f_oneway(*[df_r2_uni[df_r2_uni['Variable'] == v]['R2'] for v in df_r2_uni['Variable'].unique()])\n",
    "    anova_delta = f_oneway(*[df_delta_r2[df_delta_r2['Variable'] == v]['Delta_R2'] for v in df_delta_r2['Variable'].unique()])\n",
    "    anova_beta = f_oneway(*[df_betas[df_betas['Variable'] == v]['Beta'] for v in df_betas['Variable'].unique()])\n",
    "    \n",
    "    pairwise_results = []\n",
    "    variables = ['mass', 'partner', 'selfpull', 'socialgaze']\n",
    "    for v1, v2 in itertools.combinations(variables, 2):\n",
    "        x = df_delta_r2[df_delta_r2['Variable'] == v1]['Delta_R2'].dropna()\n",
    "        y = df_delta_r2[df_delta_r2['Variable'] == v2]['Delta_R2'].dropna()\n",
    "        min_len = min(len(x), len(y))\n",
    "        t_stat, p_val = ttest_rel(x[:min_len], y[:min_len])\n",
    "        pairwise_results.append({'Var1': v1, 'Var2': v2, 'T-stat': t_stat, 'P-value': p_val})\n",
    "    pairwise_df = pd.DataFrame(pairwise_results)\n",
    "    \n",
    "    # ---------- Step 5: Plotting ----------\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    sns.violinplot(data=df_r2_uni, x='Variable', y='R2', inner='box', ax=axes[0])\n",
    "    axes[0].set_title('Univariate R² per Variable')\n",
    "    \n",
    "    sns.violinplot(data=df_delta_r2, x='Variable', y='Delta_R2',  inner='box', ax=axes[1])\n",
    "    axes[1].set_title('ΔR² (Unique Contribution) per Variable')\n",
    "    \n",
    "    sns.violinplot(data=df_betas, x='Variable', y='Beta', inner='box', ax=axes[2])\n",
    "    axes[2].set_title('Standardized Beta Coefficients')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"regression_violin_plots.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ---------- Optional: Print stats ----------\n",
    "    print(\"ANOVA R²:\", anova_r2)\n",
    "    print(\"ANOVA ΔR²:\", anova_delta)\n",
    "    print(\"ANOVA Betas:\", anova_beta)\n",
    "    print(\"Pairwise ΔR² comparisons:\\n\", pairwise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ac641-c582-4d39-97f4-f592f0db7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test if the mean FR across neurons in each trial is just tracking the self movement, \n",
    "    # or after considering the confound of self movement, it still encode social gaze\n",
    "    \n",
    "    # nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    # df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_all_dates_df)\n",
    "    df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    \n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'socialgaze']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'socialgaze']}\n",
    "    betas = {var: [] for var in ['mass',  'socialgaze']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # y = df.iloc[i]['FR_ievent_avg']\n",
    "        y = df.iloc[i]['FR_ievent']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['mass_move_speed'],\n",
    "            'socialgaze': df.iloc[i]['socialgaze_prob']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "        \n",
    "            for idx, var in enumerate(['mass', 'socialgaze']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f90f4d-cddd-4a5b-bcc9-87163c515700",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test if the social gaze prob in each trial is just tracking the self movement, \n",
    "    # or after considering the confound of self movement, it still encode parnter movement\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "\n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'partner']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'partner']}\n",
    "    betas = {var: [] for var in ['mass',  'partner']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        y = df.iloc[i]['socialgaze_prob']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['mass_move_speed'],\n",
    "            'partner': df.iloc[i]['other_mass_move_speed']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'partner']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "        \n",
    "            for idx, var in enumerate(['mass', 'partner']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7caf5d-f0cf-4565-adad-f38b2879779b",
   "metadata": {},
   "source": [
    "### Use multi-variable regression to define and label neurons that encode socialgaze, even after control the self movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb907f-b8cf-4de8-a48d-4539546d349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import f_oneway, ttest_rel\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#\n",
    "dates_toana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['dates'])\n",
    "ndates_toana = np.shape(dates_toana)[0]\n",
    "\n",
    "neuronEncodeSocialGaze_summary_df = pd.DataFrame(columns=['date','clusterID','neuronEncodeSocialGaze'])\n",
    "\n",
    "for idate in np.arange(0, ndates_toana,1):\n",
    "    date_toana = dates_toana[idate]\n",
    "\n",
    "    ind_idate = np.isin(bhvevents_aligned_FR_allevents_all_dates_df['dates'],date_toana)\n",
    "\n",
    "    df_allneurons_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_idate]\n",
    "\n",
    "    neurons_toana = np.unique(df_allneurons_tgt['clusterID'])\n",
    "    nneurons_toana = np.shape(neurons_toana)[0]\n",
    "\n",
    "    for ineuron in np.arange(0,nneurons_toana,1):\n",
    "        neuron_toana = neurons_toana[ineuron]\n",
    "\n",
    "        ind_ineuron = np.isin(df_allneurons_tgt['clusterID'],neuron_toana)\n",
    "\n",
    "        df_ineuron_tgt = df_allneurons_tgt[ind_ineuron]\n",
    "\n",
    "        # get the FR traces\n",
    "        FR_allevents = np.array(df_ineuron_tgt['FR_ievent'])\n",
    "        nFR_allevents = np.shape(FR_allevents)[0]\n",
    "\n",
    "        # get the socialgaze and self movement traces\n",
    "        selfspeed_allevents = np.array(df_ineuron_tgt['mass_move_speed'])\n",
    "        socialgaze_allevents = np.array(df_ineuron_tgt['socialgaze_prob'])\n",
    "        nbhv_allevents = np.shape(selfspeed_allevents)[0]\n",
    "\n",
    "        # make sure the fr and bhv traces has the same size\n",
    "        if nFR_allevents < nbhv_allevents:\n",
    "            selfspeed_allevents = selfspeed_allevents[0:nFR_allevents]\n",
    "            socialgaze_allevents = socialgaze_allevents[0:nFR_allevents]\n",
    "            nevents = nFR_allevents\n",
    "        elif nFR_allevents > nbhv_allevents:\n",
    "            FR_allevents = FR_allevents[0:nbhv_allevents]\n",
    "            nevents = nbhv_allevents\n",
    "        else:\n",
    "            nevents = nFR_allevents\n",
    "\n",
    "        # remove trial that does not match\n",
    "        for ievent in np.arange(0,nevents,1):\n",
    "            ntimepoint_FR = np.shape(FR_allevents[ievent])[0]\n",
    "            ntimepoint_speed = np.shape(selfspeed_allevents[ievent])[0]\n",
    "            ntimepoint_gaze = np.shape(socialgaze_allevents[ievent])[0]\n",
    "            \n",
    "            if (ntimepoint_speed != ntimepoint_FR) |\\\n",
    "               (ntimepoint_gaze != ntimepoint_FR) |\\\n",
    "               (ntimepoint_gaze != ntimepoint_speed):\n",
    "                FR_allevents[ievent] = []\n",
    "                selfspeed_allevents[ievent] = []\n",
    "                socialgaze_allevents[ievent] = []\n",
    "        \n",
    "        # conbine all trials\n",
    "        FR_allevents_flat = np.concatenate(FR_allevents)\n",
    "        selfspeed_allevents_flat = np.concatenate(selfspeed_allevents)\n",
    "        socialgaze_allevents_flat = np.concatenate(socialgaze_allevents)\n",
    "        #\n",
    "        min_len = np.min([len(FR_allevents_flat),\n",
    "                          len(selfspeed_allevents_flat),\n",
    "                          len(socialgaze_allevents_flat)])\n",
    "        FR_allevents_flat = FR_allevents_flat[0:min_len]\n",
    "        selfspeed_allevents_flat = selfspeed_allevents_flat[0:min_len]\n",
    "        socialgaze_allevents_flat = socialgaze_allevents_flat[0:min_len]\n",
    "        \n",
    "        # multi variable regression\n",
    "        y = FR_allevents_flat\n",
    "        X_vars = {\n",
    "            'mass': selfspeed_allevents_flat,\n",
    "            'socialgaze': socialgaze_allevents_flat\n",
    "        }\n",
    "\n",
    "        # Full multivariate regression\n",
    "        try:\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            # Create a clean DataFrame for statsmodels\n",
    "            df = pd.DataFrame(X_std, columns=['mass', 'gaze'])\n",
    "            df['fr'] = y_std # Use the standardized firing rate \n",
    "            # Add a constant (intercept) to the predictors\n",
    "            df = sm.add_constant(df)\n",
    "            # Fit the Ordinary Least Squares (OLS) model\n",
    "            model_sm = sm.OLS(df['fr'], df[['const', 'mass', 'gaze']])\n",
    "            results = model_sm.fit()\n",
    "    \n",
    "            if results.pvalues['gaze'] < 0.01:\n",
    "                neuronEncodeSocialGaze = True\n",
    "            else:\n",
    "                neuronEncodeSocialGaze = False\n",
    "    \n",
    "            neuronEncodeSocialGaze_summary_df = neuronEncodeSocialGaze_summary_df.append({'date':date_toana,\n",
    "                                                                                          'clusterID':neuron_toana,\n",
    "                                                                                          'neuronEncodeSocialGaze':neuronEncodeSocialGaze,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829f313-a110-4eb4-8752-aefb8f5660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SocialGazeEncodeNeuronRatio = np.sum(neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze'])/\\\n",
    "                              len(neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze'])\n",
    "\n",
    "print('SocialGazeEncodeNeuronRatio: '+str(SocialGazeEncodeNeuronRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db24362-5a64-4cd3-bd73-23a68113c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronEncodeSocialGaze_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3352e",
   "metadata": {},
   "source": [
    "### correlation among variables - across all trial, without carefully consider mixed effect, for general trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'other_lever_speed_mean',\n",
    "    'other_lever_speed_std',\n",
    "    'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std',\n",
    "    # 'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "doSocialGazeEncodeNeuron = 1\n",
    "#\n",
    "if doSocialGazeEncodeNeuron:\n",
    "    goodneuron_df = neuronEncodeSocialGaze_summary_df[neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze']]\n",
    "    #\n",
    "    df1 = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    df2 = goodneuron_df\n",
    "    #\n",
    "    # 1. Rename the 'date' column in the second DataFrame to match the first\n",
    "    df2_renamed = df2.rename(columns={'date': 'dates'})\n",
    "    # 2. Perform the inner merge\n",
    "    # This keeps only the rows from df1 that have a matching ('dates', 'clusterID') pair in df2_renamed\n",
    "    filtered_df = pd.merge(df1, df2_renamed[['dates', 'clusterID']], on=['dates', 'clusterID'], how='inner')\n",
    "    #\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = filtered_df[columns_of_interest].dropna()\n",
    "\n",
    "else:\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = bhvevents_aligned_FR_allevents_all_dates_df[columns_of_interest].dropna()\n",
    "    # df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(df_clean[columns_of_interest[i]], df_clean[columns_of_interest[j]])\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "\n",
    "# FDR correction\n",
    "pvals_flat = pval_matrix[np.triu_indices(n, k=1)]\n",
    "_, pvals_corrected, _, _ = multipletests(pvals_flat, method='bonferroni')\n",
    "\n",
    "# Map corrected p-values back into full matrix\n",
    "pval_corrected_matrix = np.ones_like(pval_matrix)\n",
    "pval_corrected_matrix[np.triu_indices(n, k=1)] = pvals_corrected\n",
    "i_lower = np.tril_indices_from(pval_corrected_matrix, -1)\n",
    "pval_corrected_matrix[i_lower] = pval_corrected_matrix.T[i_lower]\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_corrected_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix with FDR-corrected Significance (p < 0.01)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acd272-8210-4383-bcdd-10668a42201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df_clean[columns_of_interest[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9381054-109e-489b-8fb1-e846734a341e",
   "metadata": {},
   "source": [
    "### correlation among variables - run each session each neuron separately, the number plotted is the mean correlation coeffient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041bb11-139c-4362-9320-b264a02405f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "doSocialGazeEncodeNeuron = 1\n",
    "#\n",
    "if doSocialGazeEncodeNeuron:\n",
    "    goodneuron_df = neuronEncodeSocialGaze_summary_df[neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze']]\n",
    "    #\n",
    "    df1 = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    df2 = goodneuron_df\n",
    "    #\n",
    "    # 1. Rename the 'date' column in the second DataFrame to match the first\n",
    "    df2_renamed = df2.rename(columns={'date': 'dates'})\n",
    "    # 2. Perform the inner merge\n",
    "    # This keeps only the rows from df1 that have a matching ('dates', 'clusterID') pair in df2_renamed\n",
    "    filtered_df = pd.merge(df1, df2_renamed[['dates', 'clusterID']], on=['dates', 'clusterID'], how='inner')\n",
    "    #\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = filtered_df[columns_of_interest+['dates','clusterID']].dropna()\n",
    "\n",
    "else:\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = bhvevents_aligned_FR_allevents_all_dates_df[columns_of_interest+['dates','clusterID']].dropna()\n",
    "    # df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "dates_toana = np.unique(df_clean['dates'])\n",
    "ndates_toana = np.shape(dates_toana)[0]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "\n",
    "            corrs_temp = []\n",
    "            pvals_temp = []\n",
    "            \n",
    "            for idate in np.arange(0,ndates_toana,1):\n",
    "                \n",
    "                date_toana = dates_toana[idate]\n",
    "                \n",
    "                df_clean_idate = df_clean[np.isin(df_clean['dates'],date_toana)]\n",
    "            \n",
    "                neurons_toana = np.unique(df_clean_idate['clusterID'])\n",
    "                nneurons_toana = np.shape(neurons_toana)[0]\n",
    "            \n",
    "                for ineuron in np.arange(0,nneurons_toana,1):\n",
    "                    neuron_toana = neurons_toana[ineuron]\n",
    "\n",
    "                    try:\n",
    "                        df_clean_ineuron = df_clean_idate[np.isin(df_clean_idate['clusterID'],neuron_toana)]\n",
    "            \n",
    "                        r, p = pearsonr(df_clean_ineuron[columns_of_interest[i]], df_clean_ineuron[columns_of_interest[j]])\n",
    "                    \n",
    "                    except:\n",
    "                        r = np.nan\n",
    "                        p = np.nan\n",
    "                    \n",
    "                    corrs_temp.append(r)\n",
    "                    pvals_temp.append(p)\n",
    "\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = np.nanmean(np.unique(corrs_temp))\n",
    "            \n",
    "            corrs_temp = np.array(corrs_temp)\n",
    "            corrs_temp = corrs_temp[~np.isnan(corrs_temp)]\n",
    "            _,pttest = st.ttest_1samp(np.unique(corrs_temp),0)\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = pttest\n",
    "\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix across individual neurons (p < 0.01 means ttest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a5eed-b9ac-4628-8f8f-99d71f1812f9",
   "metadata": {},
   "source": [
    "### examine the percent of neurons that significantly encode each variables, looking at the FR slope and FR mean separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9d3ac-4428-4967-8142-34cd73eeda9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Variables to correlate with fr_mean and fr_slope\n",
    "behavior_vars = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'predicted_v',\n",
    "]\n",
    "\n",
    "neural_metrics = ['fr_mean', 'fr_slope']\n",
    "# neural_metrics = ['fr_mean', 'fr_slope','fr_slope_peakbased']\n",
    "\n",
    "# Store percent significance for heatmap\n",
    "results = []\n",
    "\n",
    "grouped = bhvevents_aligned_FR_allevents_all_dates_df.groupby(['dates', 'clusterID', 'condition'])\n",
    "\n",
    "for (date, cluster_id, condition), group in grouped:\n",
    "    for fr_type in neural_metrics:\n",
    "        df = group[behavior_vars + [fr_type]].dropna()\n",
    "        if len(df) >= len(behavior_vars) + 2:  # need more trials than predictors\n",
    "            X = sm.add_constant(df[behavior_vars])\n",
    "            y = df[fr_type]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            for var in behavior_vars:\n",
    "                results.append({\n",
    "                    'date': date,\n",
    "                    'clusterID': cluster_id,\n",
    "                    'condition': condition,\n",
    "                    'fr_metric': fr_type,\n",
    "                    'predictor': var,\n",
    "                    'beta': model.params.get(var, np.nan),\n",
    "                    'pval': model.pvalues.get(var, np.nan),\n",
    "                    'significant': model.pvalues.get(var, np.nan) < 0.05\n",
    "                })\n",
    "\n",
    "regression_results_df = pd.DataFrame(results)\n",
    "\n",
    "# do the heatmap plotting\n",
    "# --- Step 1: Compute % of significant neurons per predictor ---\n",
    "summary = (\n",
    "    regression_results_df.groupby(['fr_metric', 'predictor'])['significant']\n",
    "    .mean()\n",
    "    .unstack(0) * 100  # convert to percentage\n",
    ")\n",
    "\n",
    "# --- Step 2: Compute % with any significant beta ---\n",
    "any_sig_summary = (\n",
    "    regression_results_df\n",
    "    .groupby(['fr_metric', 'date', 'clusterID'])['significant']\n",
    "    .any()\n",
    "    .groupby(['fr_metric'])\n",
    "    .mean()\n",
    "    .to_frame()\n",
    "    .T * 100  # convert to percentage\n",
    ")\n",
    "any_sig_summary.index = ['any_predictor']\n",
    "\n",
    "# --- Step 3: Combine ---\n",
    "summary_with_total = pd.concat([summary, any_sig_summary])\n",
    "\n",
    "# --- Step 4: Round for annotation ---\n",
    "annot_vals = summary_with_total.round(1).astype(str) + '%'\n",
    "\n",
    "# --- Step 5: Plot ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(summary_with_total, annot=annot_vals, fmt='', cmap='YlOrRd',\n",
    "            cbar_kws={'label': '% of Neurons (p < 0.05)'}, linewidths=0.5)\n",
    "\n",
    "plt.title('Percent of Neurons Significantly Encoding Each Variable')\n",
    "plt.ylabel('Behavioral Predictor')\n",
    "plt.xlabel('Neural Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20bde7-9d78-42be-84f2-07869a65ef3a",
   "metadata": {},
   "source": [
    "### correlation among behavioral and neural variables, and this time only consider significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d6f41-0db7-4d11-b32f-2e4d1652da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "neural_features = ['fr_slope']  # 'fr_mean', 'fr_slope', 'fr_slope_peakbased'\n",
    "# neural_features = ['fr_mean']\n",
    "# predictors_to_check = [ 'predicted_v', ] # define which behavioral predictors to check significance for\n",
    "predictors_to_check =  np.unique(regression_results_df['predictor'])\n",
    "\n",
    "regression_results_df_totest = regression_results_df.copy()\n",
    "\n",
    "ind_good = (np.isin(regression_results_df_totest['fr_metric'],neural_features)) &\\\n",
    "           (np.isin(regression_results_df_totest['predictor'],predictors_to_check))\n",
    "regression_results_df_totest = regression_results_df_totest[ind_good]\n",
    "\n",
    "sig_neurons = (\n",
    "    regression_results_df_totest[regression_results_df_totest['significant']]\n",
    "    .groupby(['date', 'clusterID'])\n",
    "    .size()\n",
    "    .reset_index()[['date', 'clusterID']]\n",
    ")\n",
    "\n",
    "sig_neuron_keys = set(tuple(x) for x in sig_neurons.to_numpy())\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Filter the original DataFrames\n",
    "# -----------------------------\n",
    "bhvevents_filtered_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df[['dates', 'clusterID']].apply(tuple, axis=1).isin(sig_neuron_keys)\n",
    "].copy()\n",
    "\n",
    "##########\n",
    "# organize the data in order to do HDDM - average across neurons for the unique trial\n",
    "##########\n",
    "\n",
    "# Step 1: Group by unique trial (date, bhv_id) and average the FR_ievent across neurons\n",
    "averaged_df = bhvevents_filtered_df.groupby(['dates', 'bhv_id'])['FR_ievent'].apply(\n",
    "    lambda traces: np.mean(np.stack(traces.to_numpy()), axis=0)\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Rename the averaged firing rate column\n",
    "averaged_df = averaged_df.rename(columns={'FR_ievent': 'FR_ievent_avg'})\n",
    "\n",
    "# Step 3: Select representative behavioral columns to merge back (drop duplicates so one per trial)\n",
    "representative_cols = [\n",
    "    'dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'bhv_id', 'mass_move_speed',\n",
    "       'num_preceding_failpull', 'other_mass_move_speed', 'otherpull_prob',\n",
    "       'pull_rt', 'pull_outcome', 'selfpull_prob', 'socialgaze_prob', 'time_from_last_reward',\n",
    "       'previous_pull_outcome', 'socialgaze_auc', 'mass_move_speed_mean',\n",
    "       'mass_move_speed_std', 'other_mass_move_speed_mean',\n",
    "       'other_mass_move_speed_std', 'predicted_v',\n",
    "]\n",
    "\n",
    "# Get one row per (dates, bhv_id) combination\n",
    "behavior_df = bhvevents_filtered_df.drop_duplicates(subset=['dates', 'bhv_id'])[representative_cols]\n",
    "\n",
    "# Step 4: Merge firing rate and behavioral data\n",
    "bhvevents_filtered_mergedneurons_df = pd.merge(averaged_df, behavior_df, \n",
    "                                                on=['dates', 'bhv_id'], how='left')\n",
    "\n",
    "# Step 5\n",
    "# Compute mean firing rate\n",
    "bhvevents_filtered_mergedneurons_df['fr_mean'] = \\\n",
    "      bhvevents_filtered_mergedneurons_df['FR_ievent_avg'].apply(\n",
    "    lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Compute slope of firing rate before 0.85s prior to pull\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        slope, _, _, _, _ = linregress(x, y)\n",
    "        #\n",
    "        if slope<0:\n",
    "            slope = -slope\n",
    "        \n",
    "        return slope\n",
    "    return np.nan\n",
    "\n",
    "bhvevents_filtered_mergedneurons_df['fr_slope'] = \\\n",
    "     bhvevents_filtered_mergedneurons_df['FR_ievent_avg'].apply(compute_fr_slope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38453c46-3f5a-4ca5-85e1-6b2784a012e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    # 'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_clean = bhvevents_filtered_df[columns_of_interest].dropna()\n",
    "# df_clean = bhvevents_filtered_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(df_clean[columns_of_interest[i]], df_clean[columns_of_interest[j]])\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "\n",
    "# FDR correction\n",
    "pvals_flat = pval_matrix[np.triu_indices(n, k=1)]\n",
    "_, pvals_corrected, _, _ = multipletests(pvals_flat, method='bonferroni')\n",
    "\n",
    "# Map corrected p-values back into full matrix\n",
    "pval_corrected_matrix = np.ones_like(pval_matrix)\n",
    "pval_corrected_matrix[np.triu_indices(n, k=1)] = pvals_corrected\n",
    "i_lower = np.tril_indices_from(pval_corrected_matrix, -1)\n",
    "pval_corrected_matrix[i_lower] = pval_corrected_matrix.T[i_lower]\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_corrected_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix with FDR-corrected Significance (p < 0.01) \\n only significant neurons')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3fa36-3ce4-408f-ad82-82a4e6b70d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a05a598c",
   "metadata": {},
   "source": [
    "### rescale all trials with different temporal scale to the same so it's easy to average across trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef268626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def resample_time_series_column(df, column_name, target_len):\n",
    "    \"\"\"\n",
    "    Interpolates time series in a dataframe column to a fixed length.\n",
    "    - df: DataFrame with a column containing lists/arrays of time series\n",
    "    - column_name: name of the column to interpolate (e.g., 'FR_ievent')\n",
    "    - target_len: desired length after resampling\n",
    "    Returns:\n",
    "        New column with interpolated time series.\n",
    "    \"\"\"\n",
    "    resampled_col = []\n",
    "    for ts in df[column_name]:\n",
    "        ts = np.array(ts)\n",
    "        orig_len = len(ts)\n",
    "        if orig_len < 2:\n",
    "            resampled_col.append(np.full(target_len, np.nan))  # skip or fill if too short\n",
    "            continue\n",
    "        x_orig = np.linspace(0, 1, orig_len)\n",
    "        x_new = np.linspace(0, 1, target_len)\n",
    "        interp_fn = interp1d(x_orig, ts, kind='linear', fill_value=\"extrapolate\")\n",
    "        ts_resampled = interp_fn(x_new)\n",
    "        resampled_col.append(ts_resampled)\n",
    "    return resampled_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b14c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this to each column to normalize\n",
    "# df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df = bhvevents_filtered_mergedneurons_df.copy()\n",
    "\n",
    "try:\n",
    "    df = df.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df = df\n",
    "\n",
    "target_len = 4*fps  # choose based on average trial length (pull_rt length)\n",
    "\n",
    "for col in ['FR_ievent', 'socialgaze_prob', 'otherpull_prob', \n",
    "            'selfpull_prob','mass_move_speed','other_mass_move_speed']:\n",
    "    df[f'{col}_resampled'] = resample_time_series_column(df, col, target_len=target_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average trials in each quantile of the target variable\n",
    "\n",
    "quantile_tgt_name = 'pull_rt' # 'pull_rt' or 'socialgaze_auc' or 'other_mass_move_speed_mean'\n",
    "\n",
    "df[quantile_tgt_name+'_bin'] = pd.qcut(df[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Time axis\n",
    "time = np.linspace(0, 1, 120)\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over bins\n",
    "for label in ['low', 'medium', 'high']:\n",
    "        \n",
    "    group = df[df[quantile_tgt_name+'_bin'] == label]['FR_ievent_resampled'].dropna()\n",
    "\n",
    "    # Convert list of arrays to 2D matrix\n",
    "    stacked = np.stack(group.values)\n",
    "    \n",
    "    # Compute mean and SEM\n",
    "    mean_trace = np.nanmean(stacked, axis=0)\n",
    "    error_trace = sem(stacked, axis=0, nan_policy='omit')\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error_trace, mean_trace + error_trace, alpha=0.3)\n",
    "\n",
    "# Finalize plot\n",
    "plt.xlabel('Normalized Time (0 to 1)')\n",
    "plt.ylabel('Firing Rate (resampled)')\n",
    "plt.title('Firing Rate by '+quantile_tgt_name+' Quantiles')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838f563",
   "metadata": {},
   "source": [
    "### padding with NaN at the end of each trial to align at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda2390-2aaf-4344-bf1b-d1a180eced44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df_clean = bhvevents_filtered_mergedneurons_df.copy()\n",
    "\n",
    "try:\n",
    "    df_clean = df_clean.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df_clean = df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine max trial length\n",
    "\n",
    "quantile_tgt_name = 'socialgaze_auc' # 'pull_rt' or 'socialgaze_auc' or 'other_mass_move_speed_mean'\n",
    "\n",
    "cont_vari_name = 'FR_ievent' # FR_ievent; socialgaze_prob; mass_move_speed\n",
    "\n",
    "target_len = df_clean[cont_vari_name].apply(len).max()\n",
    "\n",
    "def pad_align_start(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([ts, np.full(pad_len, np.nan)])\n",
    "    else:\n",
    "        return ts[:target_len]\n",
    "\n",
    "#\n",
    "df_clean[cont_vari_name+'_aligned'] = df_clean[cont_vari_name].apply(lambda ts: pad_align_start(ts, target_len))\n",
    "\n",
    "df_clean[quantile_tgt_name+'_bin'] = pd.qcut(df_clean[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = np.arange(target_len) / fps  # since we're aligned to trial start\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for label in ['low', 'medium', 'high']:\n",
    "    group = df_clean[df_clean[quantile_tgt_name+'_bin'] == label][cont_vari_name+'_aligned'].dropna()\n",
    "    traces = np.stack(group.values)\n",
    "\n",
    "    mean_trace = np.nanmean(traces, axis=0)\n",
    "    error = sem(traces, axis=0, nan_policy='omit')\n",
    "\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error, mean_trace + error, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (s, aligned to trial start)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.title(cont_vari_name+' Aligned to Trial Start by '+quantile_tgt_name+' Quantile')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfc0b0-465b-4704-8d8b-484959110ad2",
   "metadata": {},
   "source": [
    "### padding with NaN from the beginning to align everything to the pull event at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c043f2-c127-46e7-a569-737a83ecfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set target length to max length in FR_ievent\n",
    "\n",
    "quantile_tgt_name = 'socialgaze_auc' # 'pull_rt' or 'socialgaze_auc' or 'other_mass_move_speed_mean'\n",
    "\n",
    "cont_vari_name = 'FR_ievent' # FR_ievent; socialgaze_prob; mass_move_speed\n",
    "\n",
    "# Compute the maximum length of trials\n",
    "target_len = df_clean[cont_vari_name].apply(len).max()\n",
    "\n",
    "def pad_align_end(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([np.full(pad_len, np.nan), ts])\n",
    "    else:\n",
    "        return ts[-target_len:]  # just in case\n",
    "\n",
    "# apply the padding\n",
    "df_clean[cont_vari_name+'_aligned'] = df_clean[cont_vari_name].apply(lambda ts: pad_align_end(ts, target_len))\n",
    "\n",
    "# Step 4: Bin pull_rt into quantiles\n",
    "df_clean[quantile_tgt_name+'_bin'] = pd.qcut(df_clean[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6638f0f-99b3-40a8-835d-c58bbbc6bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the plotting\n",
    "time = np.linspace(-target_len / fps, 0, target_len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for label in ['low', 'medium', 'high']:\n",
    "    group = df_clean[df_clean[quantile_tgt_name+'_bin'] == label][cont_vari_name+'_aligned'].dropna()\n",
    "    traces = np.stack(group.values)\n",
    "\n",
    "    mean_trace = np.nanmean(traces, axis=0)\n",
    "    error = sem(traces, axis=0, nan_policy='omit')\n",
    "\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error, mean_trace + error, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (s, aligned to pull)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.title('Aligned '+cont_vari_name+' by '+quantile_tgt_name+' Quantile')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0b912-5884-4263-b91e-425d8ec30f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfe33-19d7-4351-bff0-d3bd53de1cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506d7a1-5e1e-4abe-b416-0ebe75530901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb6844-b9fc-4adc-a050-a98da178b76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46978d28-da6b-4366-ae71-bfdf53577c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0db6b6-e5eb-4dda-b5a4-c02c68bd5dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57403280-29f8-402a-916a-6a758d216386",
   "metadata": {},
   "source": [
    "### Use svm or other method to separate sucessful pull and failed pull;\n",
    "#### training based on FR or some behabioral variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# df_clean = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df_clean = bhvevents_filtered_mergedneurons_df.copy()\n",
    "# df_clean = bhvevents_filtered_df.copy()\n",
    "\n",
    "\n",
    "# svm_tgt_name = 'FR_ievent' # FR_ievent; socialgaze_prob;\n",
    "svm_tgt_name = 'socialgaze_prob' # FR_ievent; socialgaze_prob;\n",
    "\n",
    "try:\n",
    "    df_clean = df_clean.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df_clean = df_clean\n",
    "\n",
    "# Compute the maximum length of trials\n",
    "target_len = df_clean[svm_tgt_name].apply(len).max()\n",
    "\n",
    "def pad_align_end(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([np.full(pad_len, np.nan), ts])\n",
    "    else:\n",
    "        return ts[-target_len:]  # just in case\n",
    "\n",
    "# apply the padding\n",
    "df_clean[svm_tgt_name+'_aligned'] = df_clean[svm_tgt_name].apply(lambda ts: pad_align_end(ts, target_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_iterations = 100\n",
    "n_trials_per_class = 300\n",
    "\n",
    "# Extract\n",
    "X_full = df_clean[f'{svm_tgt_name}_aligned'].tolist()\n",
    "y_full = df_clean['pull_outcome'].values\n",
    "\n",
    "# Preprocess into 2D array (trials x time)\n",
    "max_len = max(len(x) for x in X_full)\n",
    "X = np.array([np.pad(x, (max_len - len(x), 0), constant_values=np.nan) for x in X_full])  # pad front\n",
    "y = np.array(y_full)\n",
    "\n",
    "# Collect AUCs over time\n",
    "n_timepoints = X.shape[1]\n",
    "auc_over_time = np.zeros((n_iterations, n_timepoints))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    idx_success = np.random.choice(np.where(y == 1)[0], size=n_trials_per_class, replace=True)\n",
    "    idx_fail = np.random.choice(np.where(y == 0)[0], size=n_trials_per_class, replace=True)\n",
    "    idx_sample = np.concatenate([idx_success, idx_fail])\n",
    "    \n",
    "    X_sample = X[idx_sample]\n",
    "    y_sample = y[idx_sample]\n",
    "\n",
    "    # Shuffle the samples\n",
    "    shuffle_idx = np.random.permutation(len(y_sample))\n",
    "    X_sample = X_sample[shuffle_idx]\n",
    "    y_sample = y_sample[shuffle_idx]\n",
    "\n",
    "    # Train/test split (70/30)\n",
    "    split_idx = int(0.7 * len(y_sample))\n",
    "    X_train, X_test = X_sample[:split_idx], X_sample[split_idx:]\n",
    "    y_train, y_test = y_sample[:split_idx], y_sample[split_idx:]\n",
    "\n",
    "    # Time-point-wise SVM\n",
    "    for t in range(n_timepoints):\n",
    "        # Drop NaNs for this time point\n",
    "        train_valid = ~np.isnan(X_train[:, t])\n",
    "        test_valid = ~np.isnan(X_test[:, t])\n",
    "\n",
    "        try:\n",
    "            if np.sum(train_valid) > 10 and np.sum(test_valid) > 10:\n",
    "                clf = SVC(kernel='linear', probability=True)\n",
    "                clf.fit(X_train[train_valid, t].reshape(-1, 1), y_train[train_valid])\n",
    "                y_proba = clf.predict_proba(X_test[test_valid, t].reshape(-1, 1))[:, 1]\n",
    "                auc = roc_auc_score(y_test[test_valid], y_proba)\n",
    "                auc_over_time[i, t] = auc\n",
    "            else:\n",
    "                auc_over_time[i, t] = np.nan\n",
    "        except:\n",
    "                auc_over_time[i, t] = np.nan\n",
    "\n",
    "\n",
    "# do some plotting\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "n_timepoints = auc_over_time.shape[1]\n",
    "time_axis = (np.arange(n_timepoints) - n_timepoints) / fps  # negative to 0\n",
    "\n",
    "# Compute mean and stderr\n",
    "mean_auc = np.nanmean(auc_over_time, axis=0)\n",
    "stderr_auc = np.nanstd(auc_over_time, axis=0) / np.sqrt(n_iterations)\n",
    "\n",
    "# Perform one-sample t-test against chance level (0.5)\n",
    "t_vals, p_vals = ttest_1samp(auc_over_time, popmean=0.5, axis=0, nan_policy='omit')\n",
    "\n",
    "significant = p_vals < 0.05\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_axis, mean_auc, label='Mean AUC')\n",
    "plt.fill_between(time_axis, mean_auc - stderr_auc, mean_auc + stderr_auc, alpha=0.3)\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "plt.scatter(time_axis[significant], mean_auc[significant], color='red', s=20, label='p < 0.05 (FDR)')\n",
    "plt.title(f'SVM Decoding: Predicting Pull Outcome from {svm_tgt_name}_aligned')\n",
    "plt.xlabel('Time (s, aligned to pull)')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee19f-a861-46b1-a470-895d542c40b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d734ce-fd42-434d-a65e-ebb2ea39b9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20414058-86a4-4267-b249-713b223c9090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697abb96-f779-47fc-baf6-f386196b3b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba5ade-bb09-4b20-8fae-72c3f604b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15422f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
