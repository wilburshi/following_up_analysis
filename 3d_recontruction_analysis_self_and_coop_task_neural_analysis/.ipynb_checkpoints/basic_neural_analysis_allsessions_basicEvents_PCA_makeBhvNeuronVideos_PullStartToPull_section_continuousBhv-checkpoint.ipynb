{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### the following detailed analysis focused on pull related behavioral events\n",
    "#### the pull action start events are defined based on the movement onset before each pull\n",
    "#### capture the entire section of neural activity from Pull Start/Onset to pull actions\n",
    "#### include multiple ways to defined trial start; such as based on speed, or pca that consider multiple continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1786d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note:\n",
    "# need to use pyddm environment to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace\n",
    "\n",
    "\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR_PullStartToPull_variedSection import plot_bhv_events_aligned_FR_PullStartToPull_variedSection\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace\n",
    "\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545792d",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d40abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "from ana_functions.get_pull_infos import get_pull_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# if use onset of the first increase after min\n",
    "doOnsetAfterMin = 1\n",
    "if not doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = ''\n",
    "elif doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = 'PullOnsetAfterMin_'\n",
    "\n",
    "# if use a hmm based method to find the trial start\n",
    "doHMMmethod = 0\n",
    "if doHMMmethod:\n",
    "    doOnsetAfterMin_suffix = 'HMMmethods_'\n",
    "\n",
    "    \n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     # '20231101_Dodson_withGinger_SR',\n",
    "                                     # '20231107_Dodson_withGinger_SR',\n",
    "                                     # '20231122_Dodson_withGinger_SR',\n",
    "                                     # '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      # \"20231101_SR\",\n",
    "                      # \"20231107_SR\",\n",
    "                      # \"20231122_SR\",\n",
    "                      # \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 # 0.00,   \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         # 2, \n",
    "                         # 2, \n",
    "                         # 4, \n",
    "                         # 4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorders = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorders = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "pull1_intv_all_dates = np.zeros((ndates,1))\n",
    "pull2_intv_all_dates = np.zeros((ndates,1))\n",
    "pull1_minintv_all_dates = np.zeros((ndates,1))\n",
    "pull2_minintv_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_infos_all_dates = dict.fromkeys(dates_list, []) # keep some useful information about pulls - time from last reward, number of preceding failed pull etc\n",
    "\n",
    "pull_rts_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_pullstartTopull_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_infos_all_dates  = pickle.load(f)  \n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_rts_all_dates  = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "      \n",
    "    with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstartTopull_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "        \n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # dummpy\n",
    "    \n",
    "    print('all data from all dates are loaded; pull start focus')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        #\n",
    "        pull_rts_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        \n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        #\n",
    "        pulltime1 = np.array(bhv_data[(bhv_data['behavior_events']==1)]['time_points'])\n",
    "        pulltime2 = np.array(bhv_data[(bhv_data['behavior_events']==2)]['time_points'])\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        \n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        #\n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        #\n",
    "        ind_good_pull1_succ = time_point_pull1_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_succ = time_point_pull1_succ[ind_good_pull1_succ]\n",
    "        ind_good_pull2_succ = time_point_pull2_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_succ = time_point_pull2_succ[ind_good_pull2_succ]\n",
    "        #\n",
    "        ind_good_pull1_fail = time_point_pull1_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_fail = time_point_pull1_fail[ind_good_pull1_fail]\n",
    "        ind_good_pull2_fail = time_point_pull2_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_fail = time_point_pull2_fail[ind_good_pull2_fail]\n",
    "        \n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        pull_infos_all_dates[date_tgt] = pull_infos\n",
    "        \n",
    "            \n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "        # define variables and use them to find the 'onset' of pull decision\n",
    "        print('use the gaze vector speed and face mass speed to define the start of the pull decision')\n",
    "        #\n",
    "        gausKernelsize = 16\n",
    "        #\n",
    "        # clean the data\n",
    "        time_point_pull1_temp = np.array(time_point_pull1)+session_start_time\n",
    "        time_point_pull1_temp = time_point_pull1_temp[time_point_pull1_temp<totalsess_time]\n",
    "        time_point_pull2_temp = np.array(time_point_pull2)+session_start_time\n",
    "        time_point_pull2_temp = time_point_pull2_temp[time_point_pull2_temp<totalsess_time]\n",
    "        #\n",
    "        # organize the data into a time series\n",
    "        pull1_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull1_data[np.round(time_point_pull1_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        pull2_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull2_data[np.round(time_point_pull2_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        facemass1 = output_key_locations['facemass_loc_all_merge']['dodson'].transpose()\n",
    "        facemass1 = np.hstack((facemass1,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass1[:,1:]-facemass1[:,:-1])\n",
    "        speed1_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed1_data = scipy.ndimage.gaussian_filter1d(speed1_data,gausKernelsize)\n",
    "        #\n",
    "        facemass2 = output_key_locations['facemass_loc_all_merge']['scorch'].transpose()\n",
    "        facemass2 = np.hstack((facemass2,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass2[:,1:]-facemass2[:,:-1])\n",
    "        speed2_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed2_data = scipy.ndimage.gaussian_filter1d(speed2_data,gausKernelsize)\n",
    "        #\n",
    "        gazevect1 = np.array(output_allvectors['head_vect_all_merge']['dodson']).transpose()\n",
    "        gazevect1 = np.hstack((gazevect1, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect1[:, 1:]\n",
    "        at0 = gazevect1[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed1_data = np.full(nframes, np.nan)\n",
    "        eps = 1e-10\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed1_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed1_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed1_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed1_data[~nans])\n",
    "        anglespeed1_data = scipy.ndimage.gaussian_filter1d(anglespeed1_data, gausKernelsize)\n",
    "        #\n",
    "        gazevect2 = np.array(output_allvectors['head_vect_all_merge']['scorch']).transpose()\n",
    "        gazevect2 = np.hstack((gazevect2, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect2[:, 1:]\n",
    "        at0 = gazevect2[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed2_data = np.full(nframes, np.nan)\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed2_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed2_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed2_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed2_data[~nans])\n",
    "        anglespeed2_data = scipy.ndimage.gaussian_filter1d(anglespeed2_data, gausKernelsize)\n",
    "        \n",
    "        #\n",
    "        # use one of the two methods; not the HMM based method\n",
    "        if not doHMMmethod:\n",
    "            if not doOnsetAfterMin:\n",
    "                # find the transitional time point of angle speed and speed in IPI\n",
    "                speed1_increase = find_sharp_increases_withinIPI(pull1_data,speed1_data,session_start_time,fps)\n",
    "                anglespeed1_increase = find_sharp_increases_withinIPI(pull1_data,anglespeed1_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull1_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull1_data, speed1_data, anglespeed1_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "                #\n",
    "                speed2_increase = find_sharp_increases_withinIPI(pull2_data,speed2_data,session_start_time,fps)\n",
    "                anglespeed2_increase = find_sharp_increases_withinIPI(pull2_data,anglespeed2_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull2_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull2_data, speed2_data, anglespeed2_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "            #\n",
    "            elif doOnsetAfterMin:\n",
    "                # find the transitional time point of angle speed and speed in IPI\n",
    "                speed1_increase = find_rising_onset_after_min_withinIPI(pull1_data,speed1_data,session_start_time,fps)\n",
    "                anglespeed1_increase = find_rising_onset_after_min_withinIPI(pull1_data,anglespeed1_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull1_action_onset_frames = find_rising_onset_after_min_dual_speed(pull1_data, speed1_data, anglespeed1_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "                #\n",
    "                speed2_increase = find_rising_onset_after_min_withinIPI(pull2_data,speed2_data,session_start_time,fps)\n",
    "                anglespeed2_increase = find_rising_onset_after_min_withinIPI(pull2_data,anglespeed2_data,session_start_time,fps)\n",
    "                # find the transitional time point using both angle speed and mass speed in IPI\n",
    "                pull2_action_onset_frames = find_rising_onset_after_min_dual_speed(pull2_data, speed2_data, anglespeed2_data, \n",
    "                                                                                      session_start_time, fps)\n",
    "        #\n",
    "        elif doHMMmethod:\n",
    "            n_states = 3\n",
    "            \n",
    "            pull1_action_onset_framepoints, _ = get_trial_start_frames_from_HMM(speed1_data, anglespeed1_data, pull1_data, \n",
    "                                                                                fps, session_start_time, n_states)\n",
    "            pull1_action_onset_frames = np.isin(np.arange(len(pull1_data)), pull1_action_onset_framepoints).astype(int)\n",
    "            #\n",
    "            pull2_action_onset_framepoints, _ = get_trial_start_frames_from_HMM(speed2_data, anglespeed2_data, pull2_data, \n",
    "                                                                                fps, session_start_time, n_states)\n",
    "            pull2_action_onset_frames = np.isin(np.arange(len(pull2_data)), pull2_action_onset_framepoints).astype(int)\n",
    "\n",
    "            \n",
    "        #\n",
    "        # store the pull reaction time information\n",
    "        # temporary fix\n",
    "        try:\n",
    "            pull_data_points = np.where(pull1_data)[0]\n",
    "            pullonset_data_points = np.where(pull1_action_onset_frames)[0]\n",
    "            pull1_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "            pull_rts_all_dates[date_tgt][animal1] = pull1_rt\n",
    "            #\n",
    "            pull_data_points = np.where(pull2_data)[0]\n",
    "            pullonset_data_points = np.where(pull2_action_onset_frames)[0]\n",
    "            pull2_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "            pull_rts_all_dates[date_tgt][animal2] = pull2_rt\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # replace time_point_pull_xxx to the pull onset\n",
    "        time_point_pull1 = np.array(np.round(time_point_pull1,1))\n",
    "        time_point_pull2 = np.array(np.round(time_point_pull2,1))\n",
    "        time_point_pull1_succ = np.array(time_point_pull1_succ)\n",
    "        time_point_pull2_succ = np.array(time_point_pull2_succ)\n",
    "        time_point_pull1_fail = np.array(time_point_pull1_fail)\n",
    "        time_point_pull2_fail = np.array(time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1_succ_idx = np.isin(time_point_pull1,time_point_pull1_succ)\n",
    "        time_point_pull2_succ_idx = np.isin(time_point_pull2,time_point_pull2_succ)\n",
    "        time_point_pull1_fail_idx = np.isin(time_point_pull1,time_point_pull1_fail)\n",
    "        time_point_pull2_fail_idx = np.isin(time_point_pull2,time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1 = np.where(pull1_action_onset_frames)[0]/fps - session_start_time\n",
    "        time_point_pull2 = np.where(pull2_action_onset_frames)[0]/fps - session_start_time\n",
    "        #\n",
    "        time_point_pull1_succ = time_point_pull1[time_point_pull1_succ_idx]\n",
    "        time_point_pull2_succ = time_point_pull2[time_point_pull2_succ_idx]\n",
    "        time_point_pull1_fail = time_point_pull1[time_point_pull1_fail_idx]\n",
    "        time_point_pull2_fail = time_point_pull2[time_point_pull2_fail_idx]\n",
    "        #\n",
    "        pull1_rt_succ = pull1_rt[time_point_pull1_succ_idx]\n",
    "        pull2_rt_succ = pull2_rt[time_point_pull2_succ_idx]\n",
    "        pull1_rt_fail = pull1_rt[time_point_pull1_fail_idx]\n",
    "        pull2_rt_fail = pull2_rt[time_point_pull2_fail_idx]\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            print('plot self pull start triggered bhv variables')\n",
    "            \n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+cameraID+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "                        \n",
    "            min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "\n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                    pull1_rt, pull2_rt, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                        pull1_rt_succ, pull2_rt_succ, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpullstartTopull_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                        pull1_rt_fail, pull2_rt_fail, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpullstartTopull_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 0:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate; pull start focus')\n",
    "                #\n",
    "                #\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR_PullStartToPull_variedSection(\n",
    "                                           animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           pull1_rt, pull2_rt,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_rts_all_dates, f)  \n",
    "        \n",
    "        with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstartTopull_trig_events_all_dates, f) \n",
    "\n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstartTopull_trig_events_all_dates, f) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doOnsetAfterMin_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_rts_all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull1_num_all_dates[np.isin(dates_list,'20250423')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simple sanity check plot\n",
    "if 0:\n",
    "    date_toplot = '20240808'\n",
    "    event_id = 16\n",
    "    #\n",
    "    a1 = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_toplot]['kanga pull']['8']['FR_allevents'][event_id]\n",
    "    a2 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'socialgaze_prob')][event_id]\n",
    "    a3 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'selfpull_prob')][event_id]\n",
    "    a4 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'mass_move_speed')][event_id]/500\n",
    "    a5 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'gaze_angle_speed')][event_id]\n",
    "\n",
    "    plt.plot(a1)\n",
    "    plt.plot(a2)\n",
    "    plt.plot(a4)\n",
    "    plt.plot(a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pullstartTopull_trig_events_all_dates[date_toplot].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72093762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_trig_events_summary.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "## Organize the data\n",
    "### put all the target data together for further analysis\n",
    "### also organize and save the data for the hddm analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "# pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "pull_trig_events_tgtname = 'socialgaze_prob' # for testing if individual trial different was from gaze start time\n",
    "# pull_trig_events_tgtname = 'othergaze_prob' # if to test things aligned to partner's pull (in that case, the subject's gaze becomes othergaze) \n",
    "\n",
    "\n",
    "# Keep these as additional controls\n",
    "pull_trig_otherpull_name = 'otherpull_prob'\n",
    "pull_trig_selfpull_name = 'selfpull_prob'\n",
    "pull_trig_selfspeed_name = 'mass_move_speed'\n",
    "pull_trig_otherspeed_name = 'other_mass_move_speed'\n",
    "pull_trig_otherleverspeed_name = 'other_lever_speed'\n",
    "pull_trig_selfPC1_name = 'self_PC1'\n",
    "pull_trig_otherPC1_name = 'other_PC1'\n",
    "pull_time_pre_reward_name = 'time_from_last_reward'\n",
    "prefail_pull_num_name = 'num_preceding_failpull'\n",
    "pull_rt_name = 'pull_rt'\n",
    "\n",
    "#\n",
    "animal_to_ana = 'dodson'\n",
    "# animal_to_ana = 'kanga'\n",
    "\n",
    "#\n",
    "# conditions_to_ana = np.unique(task_conditions)\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho',]\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'SR', 'SR_withDodson' ]\n",
    "# \n",
    "# for dodson only\n",
    "conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                      'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                       'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "#\n",
    "# conditions_to_ana = ['MC', \n",
    "#               'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', 'SR', 'SR_withGingerNew', 'SR_withKanga',\n",
    "#              'SR_withKoala',  ]\n",
    "\n",
    "condition_name = 'allMC'\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                                    'succrate','clusterID',\n",
    "                                                                    'channelID','FR_ievent'])\n",
    "\n",
    "try:\n",
    "\n",
    "    # dummy \n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_RawFullDatas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\\\n",
    "                  animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates_df = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "        \n",
    "        # only analyze the targeted conditions\n",
    "        if not np.isin(task_condition,conditions_to_ana):\n",
    "            continue\n",
    "    \n",
    "        succrate = succ_rate_all_dates[idate]\n",
    "        \n",
    "        bhv_types = list(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "    \n",
    "        for ibhv_type in bhv_types:\n",
    "    \n",
    "            clusterIDs = list(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "    \n",
    "            ibhv_type_split = ibhv_type.split()\n",
    "            if np.shape(ibhv_type_split)[0]==3:\n",
    "                ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "    \n",
    "            # only analyze targeted action animal\n",
    "            if not np.isin(ibhv_type_split[0], animal_to_ana):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # load the pull_trig_continuous_events\n",
    "            try:\n",
    "                pull_trig_events_tgt = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_otherpull = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherpull_name)]\n",
    "                pull_trig_selfpull = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfpull_name)]\n",
    "                pull_trig_selfspeed = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfspeed_name)]\n",
    "                pull_trig_otherspeed = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherspeed_name)]\n",
    "                pull_trig_selfPC1 = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfPC1_name)]\n",
    "                pull_trig_otherPC1 = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherPC1_name)]\n",
    "                \n",
    "                #\n",
    "                pull_trig_prerewardtime = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],pull_time_pre_reward_name)]))\n",
    "                prefail_pull_num = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],prefail_pull_num_name)]))\n",
    "                #\n",
    "                pull_rt = np.array(list(pull_rts_all_dates[date_tgt][ibhv_type_split[0]]))\n",
    "                #\n",
    "                pull_outcome = np.hstack([1-(prefail_pull_num[1:]>0).astype(int),np.nan])\n",
    "                \n",
    "                # get the other animal's speed related to their lever\n",
    "                pull_trig_otherleverdist = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],'otherani_otherlever_dist')]\n",
    "                pull_trig_otherleverspeed = [(np.diff(trace)) for trace in pull_trig_otherleverdist]\n",
    "\n",
    "                \n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_otherpull = np.nan\n",
    "                pull_trig_selfpull = np.nan\n",
    "                pull_trig_selfspeed = np.nan\n",
    "                pull_trig_otherspeed = np.nan\n",
    "                pull_trig_selfPC1 = np.nan\n",
    "                pull_trig_otherPC1 = np.nan\n",
    "                #\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                prefail_pull_num = np.nan\n",
    "                pull_rt = np.nan\n",
    "            \n",
    "                \n",
    "            for iclusterID in clusterIDs:   \n",
    "    \n",
    "                ichannelID = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_allevents = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "    \n",
    "                #\n",
    "                nevents = np.shape([len(x) for x in pull_trig_events_tgt])[0]\n",
    "                \n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "                \n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type_split[0],\n",
    "                                                                                        'bhv_name': ibhv_type_split[1],\n",
    "                                                                                        'bhv_id':ievent,\n",
    "                                                                                        'succrate':succrate[0],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_ievent':iFR_allevents[ievent],\n",
    "                                                                                         pull_trig_events_tgtname:pull_trig_events_tgt[ievent],                          \n",
    "                                                                                         pull_trig_otherpull_name:pull_trig_otherpull[ievent],                          \n",
    "                                                                                         pull_trig_selfpull_name:pull_trig_selfpull[ievent],\n",
    "                                                                                         pull_trig_otherspeed_name:pull_trig_otherspeed[ievent],                          \n",
    "                                                                                         pull_trig_selfspeed_name:pull_trig_selfspeed[ievent],\n",
    "                                                                                         pull_trig_otherPC1_name:pull_trig_otherPC1[ievent],                          \n",
    "                                                                                         pull_trig_selfPC1_name:pull_trig_selfPC1[ievent],\n",
    "                                                                                                                      \n",
    "                                                                                         pull_time_pre_reward_name:pull_trig_prerewardtime[ievent],\n",
    "                                                                                         prefail_pull_num_name:prefail_pull_num[ievent],\n",
    "                                                                                         pull_rt_name:pull_rt[ievent],\n",
    "                                                                                             \n",
    "                                                                                         pull_trig_otherleverspeed_name: pull_trig_otherleverspeed[ievent],                            \n",
    "                                                                                         \n",
    "                                                                                         'pull_outcome': pull_outcome[ievent],\n",
    "                                                                                        }, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # save the data with other HDDM dataframes for the modeling\n",
    "    savedata = 1\n",
    "    \n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "            '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/hddm_RawFullDatas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\\\n",
    "                  animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_allevents_all_dates_df, f) \n",
    "\n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28680cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a plot of the distribution of the reaction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_rt = np.array(bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'])\n",
    "\n",
    "data = {'pull_rt': pull_rt}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# --- 1. Outlier Removal using IQR ---\n",
    "Q1 = df['pull_rt'].quantile(0.25)\n",
    "Q3 = df['pull_rt'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to create a new one without outliers\n",
    "df_filtered = df[(df['pull_rt'] >= lower_bound) & (df['pull_rt'] <= upper_bound)]\n",
    "\n",
    "# --- Plotting Code ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use seaborn.kdeplot to create a smooth distribution plot\n",
    "seaborn.kdeplot(\n",
    "    data=df_filtered,\n",
    "    x='pull_rt',\n",
    "    fill=True,  # Fills the area under the curve\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Distribution of Pull Reaction Time (pull_rt)', fontsize=16)\n",
    "plt.xlabel('Pull Reaction Time (s)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "\n",
    "# --- 3. Add Mean and STD Annotations ---\n",
    "# Calculate mean and std from the filtered data\n",
    "mean_rt = df_filtered['pull_rt'].mean()\n",
    "std_rt = df_filtered['pull_rt'].std()\n",
    "\n",
    "# Add vertical lines for mean and std\n",
    "plt.axvline(mean_rt, color='red', linestyle='--', label=f'Mean: {mean_rt:.2f}s')\n",
    "plt.axvline(mean_rt + std_rt, color='black', linestyle=':', label=f'Std Dev: {std_rt:.2f}s')\n",
    "plt.axvline(mean_rt - std_rt, color='black', linestyle=':')\n",
    "\n",
    "# Create the text string for the annotation\n",
    "stats_text = f\"Mean: {mean_rt:.2f}\\nStd Dev: {std_rt:.2f}\"\n",
    "\n",
    "# Add the text box to the plot\n",
    "# transform=plt.gca().transAxes places the text relative to the axes (0,0 is bottom-left, 1,1 is top-right)\n",
    "plt.text(0.75, 0.9, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv/\" + \\\n",
    "                    cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + \"/bhvvariables_summary_fig/\"\n",
    "\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "\n",
    "    plt.savefig(figsavefolder + animal_to_ana + '_in_' + condition_name +\n",
    "                '_pull_responseTime_distribution.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original dataframe for reference \n",
    "bhvevents_aligned_FR_allevents_all_dates_df_origin = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbb68c-ee11-412f-bd35-6d96b1c60fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df_origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22792829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some new columns\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['previous_pull_outcome'] = (\n",
    "        bhvevents_aligned_FR_allevents_all_dates_df['num_preceding_failpull'] == 0\n",
    "    ).astype(int)\n",
    "\n",
    "# Compute AUC for social gaze probability\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].apply(\n",
    "#     lambda x: np.trapz(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    "# )\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].apply(\n",
    "    lambda x: np.nansum(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = \\\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc']/bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']\n",
    "\n",
    "# Mean and STD for mass_move_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_std'] = bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for self_PC1\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['self_PC1_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['self_PC1'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['self_PC1_std'] = bhvevents_aligned_FR_allevents_all_dates_df['self_PC1'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_mass_move_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_lever_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_lever_speed'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_PC1\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_PC1_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_PC1'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_PC1_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_PC1'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# firing rate mean and slope before pull (0.85s before)\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "# Compute mean firing rate\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['FR_ievent'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# \n",
    "# Compute the slope, in two ways\n",
    "#\n",
    "# Make a clean copy to avoid SettingWithCopyWarning\n",
    "df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "\n",
    "# === Fixed slope before 0.85s from pull ===\n",
    "pull_margin_frames = int(0.85 * fps)\n",
    "\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        try:\n",
    "            slope, *_ = linregress(x, y)\n",
    "            return abs(slope)\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "df.loc[:, 'fr_slope'] = df['FR_ievent'].apply(compute_fr_slope)\n",
    "\n",
    "# === Peak-based slope calculation: use shortest trial ===\n",
    "\n",
    "# Prepare new columns for slopes and peak times\n",
    "df['fr_slope_peakbased'] = np.nan\n",
    "df['fr_peak_time'] = np.nan\n",
    "\n",
    "# Group by neuron\n",
    "grouped = df.groupby(['dates', 'clusterID'])\n",
    "\n",
    "for (date, cluster_id), group in tqdm(grouped, desc=\"Processing neurons\"):\n",
    "\n",
    "    traces = group['FR_ievent'].dropna().tolist()\n",
    "    if len(traces) < 2:\n",
    "        continue\n",
    "\n",
    "    # Align traces by pull (end), pad shorter trials with nan at the front\n",
    "    min_len = min(len(trace) for trace in traces)\n",
    "    aligned = [trace[-min_len:] if len(trace) >= min_len else\n",
    "               np.pad(trace, (min_len - len(trace), 0), constant_values=np.nan)\n",
    "               for trace in traces]\n",
    "\n",
    "    stacked = np.stack(aligned)  # shape: (n_trials, min_len)\n",
    "    mean_trace = np.nanmean(stacked, axis=0)\n",
    "\n",
    "    # Compute slope of mean trace (full)\n",
    "    x_full = np.arange(min_len)\n",
    "    slope_full = linregress(x_full, mean_trace).slope\n",
    "\n",
    "    # Find peak index based on slope sign\n",
    "    if slope_full >= 0:\n",
    "        peak_idx = np.nanargmax(mean_trace)\n",
    "    else:\n",
    "        peak_idx = np.nanargmin(mean_trace)\n",
    "\n",
    "    # Convert peak index to peak time relative to pull (end-aligned)\n",
    "    peak_time = (peak_idx - (min_len - 1)) / fps\n",
    "\n",
    "    # Save peak_time for all trials of this neuron\n",
    "    df.loc[(df['dates'] == date) & (df['clusterID'] == cluster_id), 'fr_peak_time'] = peak_time\n",
    "\n",
    "    # For each trial, calculate slope from trial start to the peak index (if peak_idx within trial length)\n",
    "    for idx, row in group.iterrows():\n",
    "        fr_trace = row['FR_ievent']\n",
    "        trial_len = len(fr_trace)\n",
    "        \n",
    "        # Align trial trace to end, pad front if needed\n",
    "        if trial_len < min_len:\n",
    "            padded_trace = np.pad(fr_trace, (min_len - trial_len, 0), constant_values=np.nan)\n",
    "        else:\n",
    "            padded_trace = fr_trace[-min_len:]\n",
    "\n",
    "        # Make sure peak_idx is within trial length\n",
    "        if peak_idx < len(padded_trace):\n",
    "            y = padded_trace[:peak_idx+1]\n",
    "            x = np.arange(len(y))\n",
    "            if np.isnan(y).all() or len(y) < 2:\n",
    "                slope = np.nan\n",
    "            else:\n",
    "                slope = linregress(x, y).slope\n",
    "            # Optional: store absolute slope if you want\n",
    "            slope = abs(slope) if not np.isnan(slope) else np.nan\n",
    "\n",
    "            df.loc[idx, 'fr_slope_peakbased'] = slope\n",
    "        else:\n",
    "            df.loc[idx, 'fr_slope_peakbased'] = np.nan\n",
    "\n",
    "# Save back to original dataframe\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_slope_peakbased'] = df['fr_slope_peakbased']\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_peak_time'] = df['fr_peak_time']\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_slope'] = df['fr_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4c4c3-6166-4206-b956-8850c27f6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_allevents_all_dates_df['fr_slope']\n",
    "yyy = bhvevents_aligned_FR_allevents_all_dates_df['fr_slope_peakbased']\n",
    "# plt.plot(xxx,yyy,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e81a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# organize the data in order to do HDDM - average across neurons for the unique trial\n",
    "##########\n",
    "\n",
    "# Step 1: Group by unique trial (date, bhv_id) and average the FR_ievent across neurons\n",
    "averaged_df = bhvevents_aligned_FR_allevents_all_dates_df.groupby(['dates', 'bhv_id'])['FR_ievent'].apply(\n",
    "    lambda traces: np.mean(np.stack(traces.to_numpy()), axis=0)\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Rename the averaged firing rate column\n",
    "averaged_df = averaged_df.rename(columns={'FR_ievent': 'FR_ievent_avg'})\n",
    "\n",
    "# Step 3: Select representative behavioral columns to merge back (drop duplicates so one per trial)\n",
    "representative_cols = [\n",
    "    'dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'bhv_id', 'mass_move_speed',\n",
    "       'num_preceding_failpull', 'other_mass_move_speed', 'otherpull_prob',\n",
    "       'pull_rt', 'selfpull_prob', 'socialgaze_prob', 'time_from_last_reward',\n",
    "       'previous_pull_outcome', 'pull_outcome', 'socialgaze_auc', 'mass_move_speed_mean',\n",
    "       'mass_move_speed_std', 'other_mass_move_speed_mean',\n",
    "       'other_mass_move_speed_std','self_PC1_mean','self_PC1_std',\n",
    "       'other_PC1_mean', 'other_PC1_std', 'other_PC1','self_PC1',\n",
    "       'other_lever_speed_mean',\n",
    "       'other_lever_speed_std',\n",
    "       'other_lever_speed',\n",
    "]\n",
    "\n",
    "# Get one row per (dates, bhv_id) combination\n",
    "behavior_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])[representative_cols]\n",
    "\n",
    "# Step 4: Merge firing rate and behavioral data\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = pd.merge(averaged_df, behavior_df, \n",
    "                                                                    on=['dates', 'bhv_id'], how='left')\n",
    "\n",
    "# Step 5\n",
    "# Compute mean firing rate\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['fr_mean'] = \\\n",
    "      bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].apply(\n",
    "    lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Compute slope of firing rate before 0.85s prior to pull\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        slope, _, _, _, _ = linregress(x, y)\n",
    "        #\n",
    "        if slope<0:\n",
    "            slope = -slope\n",
    "        \n",
    "        return slope\n",
    "    return np.nan\n",
    "\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['fr_slope'] = \\\n",
    "     bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].apply(compute_fr_slope)\n",
    "\n",
    "\n",
    "# save the data with other HDDM dataframes for the modeling\n",
    "savedata = 1\n",
    "\n",
    "if savedata:\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_datas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\n",
    "              animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        pickle.dump(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# load the HDDM data to get the drift diffusion v for further correlation\n",
    "##########\n",
    "\n",
    "if 1:\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_traces_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_traces = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_stats_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_stats = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_nogaze_traces_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_nogaze_stats_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "    \n",
    "    v_inter = hddm_model_fitted_stats['mean']['v_Intercept']\n",
    "    v_self_gaze_auc = hddm_model_fitted_stats['mean']['v_self_gaze_auc']\n",
    "    v_partner_mean_speed = hddm_model_fitted_stats['mean']['v_partner_mean_speed']\n",
    "    v_self_mean_speed = hddm_model_fitted_stats['mean']['v_self_mean_speed']\n",
    "    v_partner_speed_std = hddm_model_fitted_stats['mean']['v_partner_speed_std']\n",
    "    v_self_speed_std = hddm_model_fitted_stats['mean']['v_self_speed_std']\n",
    "\n",
    "    #\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df['predicted_v'] = v_inter + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] * v_self_gaze_auc + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_mean'] * v_partner_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_std'] * v_partner_speed_std + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_mean'] * v_self_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_std'] * v_self_speed_std \n",
    "\n",
    "    #\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['predicted_v'] = v_inter + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['socialgaze_auc'] * v_self_gaze_auc + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed_mean'] * v_partner_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed_std'] * v_partner_speed_std + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed_mean'] * v_self_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed_std'] * v_self_speed_std \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some criteria to remove trials\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['previous_pull_outcome']==1]\n",
    "\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']>3]\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']<20]\n",
    "\n",
    "# Remove outlier pull_rt values\n",
    "# method 1\n",
    "if 0:\n",
    "    # Compute IQR bounds\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    q1 = bhv_unique_df['pull_rt'].quantile(0.25)\n",
    "    q3 = bhv_unique_df['pull_rt'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    #\n",
    "    # Filter out outliers\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "# method 2    \n",
    "if 1:\n",
    "    # Symmetrical trimming: keep central 90% of pull_rt\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    lower_bound = bhv_unique_df['pull_rt'].quantile(0.05)\n",
    "    # lower_bound = 4\n",
    "    upper_bound = bhv_unique_df['pull_rt'].quantile(0.95)\n",
    "\n",
    "    # Filter the dataframe\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b923d5-396c-4dca-b998-e1ca1f8c0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at the successful pull\n",
    "if 0:\n",
    "    ind_ = bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome']==1\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[ind_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']\n",
    "yyy = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc']\n",
    "plt.plot(xxx,yyy,'.')\n",
    "st.pearsonr(xxx,yyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0747346",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_allevents_all_dates_df\n",
    "print(np.sum(bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome']))\n",
    "print(np.shape(bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same clearup for the merged dataframe\n",
    "# Remove outlier pull_rt values\n",
    "# method 1\n",
    "if 0:\n",
    "    # Compute IQR bounds\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    q1 = bhv_unique_df['pull_rt'].quantile(0.25)\n",
    "    q3 = bhv_unique_df['pull_rt'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    #\n",
    "    # Filter out outliers\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "# method 2    \n",
    "if 1:\n",
    "    # Symmetrical trimming: keep central 90% of pull_rt\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    lower_bound = bhv_unique_df['pull_rt'].quantile(0.05)\n",
    "    # lower_bound = 4\n",
    "    upper_bound = bhv_unique_df['pull_rt'].quantile(0.95)\n",
    "\n",
    "    # Filter the dataframe\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[\n",
    "        (bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7b2f3-29c3-4c68-92b4-401269a672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at the successful pull\n",
    "if 1:\n",
    "    ind_ = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_outcome']==1\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[ind_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7839fc-be9e-4fc6-b30c-b19dde280098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be6083-4219-4bac-a8cb-d5a41512cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = 1749\n",
    "if 0:\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].loc[ind_])\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['socialgaze_prob'].loc[ind_]*5)\n",
    "if 0:\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['FR_ievent'].loc[ind_])\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].loc[ind_]*5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b4af3-dbf7-411e-ba8b-c0d8d919cc5d",
   "metadata": {},
   "source": [
    "#### trial wise correlation between FR trace and other continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec81fb8-6388-4f7d-9faa-8048091ef518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.stats import f_oneway, ttest_rel\n",
    "    import itertools\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    betas = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        y = df.iloc[i]['FR_ievent_avg']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['mass_move_speed'],\n",
    "            'partner': df.iloc[i]['other_mass_move_speed'],\n",
    "            'selfpull': df.iloc[i]['selfpull_prob'],\n",
    "            'socialgaze': df.iloc[i]['socialgaze_prob']\n",
    "        }\n",
    "    \n",
    "        if any(len(y) != len(x) for x in X_vars.values()):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            # Univariate regressions\n",
    "            for varname, x in X_vars.items():\n",
    "                model_uni = LinearRegression().fit(np.array(x).reshape(-1, 1), y)\n",
    "                r2_uni[varname].append(model_uni.score(np.array(x).reshape(-1, 1), y))\n",
    "            \n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'partner', 'selfpull', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "    \n",
    "            for idx, var in enumerate(['mass', 'partner', 'selfpull', 'socialgaze']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "            # Leave-one-out regressions\n",
    "            for idx, var in enumerate(['mass', 'partner', 'selfpull', 'socialgaze']):\n",
    "                X_reduced = np.delete(X_full, idx, axis=1)\n",
    "                X_reduced_std = StandardScaler().fit_transform(X_reduced)\n",
    "                model_reduced = LinearRegression().fit(X_reduced_std, y_std)\n",
    "                r2_reduced = model_reduced.score(X_reduced_std, y_std)\n",
    "                delta_r2[var].append(r2_full_val - r2_reduced)\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # ---------- Step 3: Format for plotting ----------\n",
    "    df_r2_uni = pd.DataFrame([\n",
    "        {'Variable': var, 'R2': r2} for var, values in r2_uni.items() for r2 in values\n",
    "    ])\n",
    "    \n",
    "    df_delta_r2 = pd.DataFrame([\n",
    "        {'Variable': var, 'Delta_R2': delta} for var, values in delta_r2.items() for delta in values\n",
    "    ])\n",
    "    \n",
    "    df_betas = pd.DataFrame([\n",
    "        {'Variable': var, 'Beta': beta} for var, values in betas.items() for beta in values\n",
    "    ])\n",
    "    \n",
    "    # ---------- Step 4: Statistical tests ----------\n",
    "    anova_r2 = f_oneway(*[df_r2_uni[df_r2_uni['Variable'] == v]['R2'] for v in df_r2_uni['Variable'].unique()])\n",
    "    anova_delta = f_oneway(*[df_delta_r2[df_delta_r2['Variable'] == v]['Delta_R2'] for v in df_delta_r2['Variable'].unique()])\n",
    "    anova_beta = f_oneway(*[df_betas[df_betas['Variable'] == v]['Beta'] for v in df_betas['Variable'].unique()])\n",
    "    \n",
    "    pairwise_results = []\n",
    "    variables = ['mass', 'partner', 'selfpull', 'socialgaze']\n",
    "    for v1, v2 in itertools.combinations(variables, 2):\n",
    "        x = df_delta_r2[df_delta_r2['Variable'] == v1]['Delta_R2'].dropna()\n",
    "        y = df_delta_r2[df_delta_r2['Variable'] == v2]['Delta_R2'].dropna()\n",
    "        min_len = min(len(x), len(y))\n",
    "        t_stat, p_val = ttest_rel(x[:min_len], y[:min_len])\n",
    "        pairwise_results.append({'Var1': v1, 'Var2': v2, 'T-stat': t_stat, 'P-value': p_val})\n",
    "    pairwise_df = pd.DataFrame(pairwise_results)\n",
    "    \n",
    "    # ---------- Step 5: Plotting ----------\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    sns.violinplot(data=df_r2_uni, x='Variable', y='R2', inner='box', ax=axes[0])\n",
    "    axes[0].set_title('Univariate R per Variable')\n",
    "    \n",
    "    sns.violinplot(data=df_delta_r2, x='Variable', y='Delta_R2',  inner='box', ax=axes[1])\n",
    "    axes[1].set_title('R (Unique Contribution) per Variable')\n",
    "    \n",
    "    sns.violinplot(data=df_betas, x='Variable', y='Beta', inner='box', ax=axes[2])\n",
    "    axes[2].set_title('Standardized Beta Coefficients')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"regression_violin_plots.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ---------- Optional: Print stats ----------\n",
    "    print(\"ANOVA R:\", anova_r2)\n",
    "    print(\"ANOVA R:\", anova_delta)\n",
    "    print(\"ANOVA Betas:\", anova_beta)\n",
    "    print(\"Pairwise R comparisons:\\n\", pairwise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ac641-c582-4d39-97f4-f592f0db7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test if the mean FR across neurons in each trial is just tracking the self movement, \n",
    "    # or after considering the confound of self movement, it still encode social gaze\n",
    "    \n",
    "    # nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    # df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_all_dates_df)\n",
    "    df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    \n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'socialgaze']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'socialgaze']}\n",
    "    betas = {var: [] for var in ['mass',  'socialgaze']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # y = df.iloc[i]['FR_ievent_avg']\n",
    "        y = df.iloc[i]['FR_ievent']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['mass_move_speed'],\n",
    "            'socialgaze': df.iloc[i]['socialgaze_prob']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "        \n",
    "            for idx, var in enumerate(['mass', 'socialgaze']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f90f4d-cddd-4a5b-bcc9-87163c515700",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test if the social gaze prob in each trial is just tracking the self movement, \n",
    "    # or after considering the confound of self movement, it still encode parnter movement\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "\n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'partner']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'partner']}\n",
    "    betas = {var: [] for var in ['mass',  'partner']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        y = df.iloc[i]['socialgaze_prob']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['mass_move_speed'],\n",
    "            'partner': df.iloc[i]['other_mass_move_speed']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'partner']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "        \n",
    "            for idx, var in enumerate(['mass', 'partner']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7caf5d-f0cf-4565-adad-f38b2879779b",
   "metadata": {},
   "source": [
    "### Use multi-variable regression to define and label neurons that encode socialgaze, even after control the self movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb907f-b8cf-4de8-a48d-4539546d349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import f_oneway, ttest_rel\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#\n",
    "dates_toana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['dates'])\n",
    "ndates_toana = np.shape(dates_toana)[0]\n",
    "\n",
    "neuronEncodeSocialGaze_summary_df = pd.DataFrame(columns=['date','clusterID','neuronEncodeSocialGaze'])\n",
    "\n",
    "for idate in np.arange(0, ndates_toana,1):\n",
    "    date_toana = dates_toana[idate]\n",
    "\n",
    "    ind_idate = np.isin(bhvevents_aligned_FR_allevents_all_dates_df['dates'],date_toana)\n",
    "\n",
    "    df_allneurons_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_idate]\n",
    "\n",
    "    neurons_toana = np.unique(df_allneurons_tgt['clusterID'])\n",
    "    nneurons_toana = np.shape(neurons_toana)[0]\n",
    "\n",
    "    for ineuron in np.arange(0,nneurons_toana,1):\n",
    "        neuron_toana = neurons_toana[ineuron]\n",
    "\n",
    "        ind_ineuron = np.isin(df_allneurons_tgt['clusterID'],neuron_toana)\n",
    "\n",
    "        df_ineuron_tgt = df_allneurons_tgt[ind_ineuron]\n",
    "\n",
    "        # get the FR traces\n",
    "        FR_allevents = np.array(df_ineuron_tgt['FR_ievent'])\n",
    "        nFR_allevents = np.shape(FR_allevents)[0]\n",
    "\n",
    "        # get the socialgaze and self movement traces\n",
    "        selfspeed_allevents = np.array(df_ineuron_tgt['mass_move_speed'])\n",
    "        socialgaze_allevents = np.array(df_ineuron_tgt['socialgaze_prob'])\n",
    "        nbhv_allevents = np.shape(selfspeed_allevents)[0]\n",
    "\n",
    "        # make sure the fr and bhv traces has the same size\n",
    "        if nFR_allevents < nbhv_allevents:\n",
    "            selfspeed_allevents = selfspeed_allevents[0:nFR_allevents]\n",
    "            socialgaze_allevents = socialgaze_allevents[0:nFR_allevents]\n",
    "            nevents = nFR_allevents\n",
    "        elif nFR_allevents > nbhv_allevents:\n",
    "            FR_allevents = FR_allevents[0:nbhv_allevents]\n",
    "            nevents = nbhv_allevents\n",
    "        else:\n",
    "            nevents = nFR_allevents\n",
    "\n",
    "        # remove trial that does not match\n",
    "        for ievent in np.arange(0,nevents,1):\n",
    "            ntimepoint_FR = np.shape(FR_allevents[ievent])[0]\n",
    "            ntimepoint_speed = np.shape(selfspeed_allevents[ievent])[0]\n",
    "            ntimepoint_gaze = np.shape(socialgaze_allevents[ievent])[0]\n",
    "            \n",
    "            if (ntimepoint_speed != ntimepoint_FR) |\\\n",
    "               (ntimepoint_gaze != ntimepoint_FR) |\\\n",
    "               (ntimepoint_gaze != ntimepoint_speed):\n",
    "                FR_allevents[ievent] = []\n",
    "                selfspeed_allevents[ievent] = []\n",
    "                socialgaze_allevents[ievent] = []\n",
    "        \n",
    "        # conbine all trials\n",
    "        FR_allevents_flat = np.concatenate(FR_allevents)\n",
    "        selfspeed_allevents_flat = np.concatenate(selfspeed_allevents)\n",
    "        socialgaze_allevents_flat = np.concatenate(socialgaze_allevents)\n",
    "        #\n",
    "        min_len = np.min([len(FR_allevents_flat),\n",
    "                          len(selfspeed_allevents_flat),\n",
    "                          len(socialgaze_allevents_flat)])\n",
    "        FR_allevents_flat = FR_allevents_flat[0:min_len]\n",
    "        selfspeed_allevents_flat = selfspeed_allevents_flat[0:min_len]\n",
    "        socialgaze_allevents_flat = socialgaze_allevents_flat[0:min_len]\n",
    "        \n",
    "        # multi variable regression\n",
    "        y = FR_allevents_flat\n",
    "        X_vars = {\n",
    "            'mass': selfspeed_allevents_flat,\n",
    "            'socialgaze': socialgaze_allevents_flat\n",
    "        }\n",
    "\n",
    "        # Full multivariate regression\n",
    "        try:\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            # Create a clean DataFrame for statsmodels\n",
    "            df = pd.DataFrame(X_std, columns=['mass', 'gaze'])\n",
    "            df['fr'] = y_std # Use the standardized firing rate \n",
    "            # Add a constant (intercept) to the predictors\n",
    "            df = sm.add_constant(df)\n",
    "            # Fit the Ordinary Least Squares (OLS) model\n",
    "            model_sm = sm.OLS(df['fr'], df[['const', 'mass', 'gaze']])\n",
    "            results = model_sm.fit()\n",
    "    \n",
    "            if results.pvalues['gaze'] < 0.01:\n",
    "                neuronEncodeSocialGaze = True\n",
    "            else:\n",
    "                neuronEncodeSocialGaze = False\n",
    "    \n",
    "            neuronEncodeSocialGaze_summary_df = neuronEncodeSocialGaze_summary_df.append({'date':date_toana,\n",
    "                                                                                          'clusterID':neuron_toana,\n",
    "                                                                                          'neuronEncodeSocialGaze':neuronEncodeSocialGaze,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829f313-a110-4eb4-8752-aefb8f5660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SocialGazeEncodeNeuronRatio = np.sum(neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze'])/\\\n",
    "                              len(neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze'])\n",
    "\n",
    "print('SocialGazeEncodeNeuronRatio: '+str(SocialGazeEncodeNeuronRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db24362-5a64-4cd3-bd73-23a68113c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronEncodeSocialGaze_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3352e",
   "metadata": {},
   "source": [
    "### correlation among variables - across all trial, without carefully consider mixed effect, for general trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'other_lever_speed_mean',\n",
    "    'other_lever_speed_std',\n",
    "    'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std',\n",
    "    # 'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "doSocialGazeEncodeNeuron = 1\n",
    "#\n",
    "if doSocialGazeEncodeNeuron:\n",
    "    goodneuron_df = neuronEncodeSocialGaze_summary_df[neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze']]\n",
    "    #\n",
    "    df1 = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    df2 = goodneuron_df\n",
    "    #\n",
    "    # 1. Rename the 'date' column in the second DataFrame to match the first\n",
    "    df2_renamed = df2.rename(columns={'date': 'dates'})\n",
    "    # 2. Perform the inner merge\n",
    "    # This keeps only the rows from df1 that have a matching ('dates', 'clusterID') pair in df2_renamed\n",
    "    filtered_df = pd.merge(df1, df2_renamed[['dates', 'clusterID']], on=['dates', 'clusterID'], how='inner')\n",
    "    #\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = filtered_df[columns_of_interest].dropna()\n",
    "\n",
    "else:\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = bhvevents_aligned_FR_allevents_all_dates_df[columns_of_interest].dropna()\n",
    "    # df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(df_clean[columns_of_interest[i]], df_clean[columns_of_interest[j]])\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "\n",
    "# FDR correction\n",
    "pvals_flat = pval_matrix[np.triu_indices(n, k=1)]\n",
    "_, pvals_corrected, _, _ = multipletests(pvals_flat, method='bonferroni')\n",
    "\n",
    "# Map corrected p-values back into full matrix\n",
    "pval_corrected_matrix = np.ones_like(pval_matrix)\n",
    "pval_corrected_matrix[np.triu_indices(n, k=1)] = pvals_corrected\n",
    "i_lower = np.tril_indices_from(pval_corrected_matrix, -1)\n",
    "pval_corrected_matrix[i_lower] = pval_corrected_matrix.T[i_lower]\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_corrected_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix with FDR-corrected Significance (p < 0.01)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acd272-8210-4383-bcdd-10668a42201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df_clean[columns_of_interest[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9381054-109e-489b-8fb1-e846734a341e",
   "metadata": {},
   "source": [
    "### correlation among variables - run each session each neuron separately, the number plotted is the mean correlation coeffient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041bb11-139c-4362-9320-b264a02405f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "doSocialGazeEncodeNeuron = 1\n",
    "#\n",
    "if doSocialGazeEncodeNeuron:\n",
    "    goodneuron_df = neuronEncodeSocialGaze_summary_df[neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze']]\n",
    "    #\n",
    "    df1 = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    df2 = goodneuron_df\n",
    "    #\n",
    "    # 1. Rename the 'date' column in the second DataFrame to match the first\n",
    "    df2_renamed = df2.rename(columns={'date': 'dates'})\n",
    "    # 2. Perform the inner merge\n",
    "    # This keeps only the rows from df1 that have a matching ('dates', 'clusterID') pair in df2_renamed\n",
    "    filtered_df = pd.merge(df1, df2_renamed[['dates', 'clusterID']], on=['dates', 'clusterID'], how='inner')\n",
    "    #\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = filtered_df[columns_of_interest+['dates','clusterID']].dropna()\n",
    "\n",
    "else:\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = bhvevents_aligned_FR_allevents_all_dates_df[columns_of_interest+['dates','clusterID']].dropna()\n",
    "    # df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "dates_toana = np.unique(df_clean['dates'])\n",
    "ndates_toana = np.shape(dates_toana)[0]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "\n",
    "            corrs_temp = []\n",
    "            pvals_temp = []\n",
    "            \n",
    "            for idate in np.arange(0,ndates_toana,1):\n",
    "                \n",
    "                date_toana = dates_toana[idate]\n",
    "                \n",
    "                df_clean_idate = df_clean[np.isin(df_clean['dates'],date_toana)]\n",
    "            \n",
    "                neurons_toana = np.unique(df_clean_idate['clusterID'])\n",
    "                nneurons_toana = np.shape(neurons_toana)[0]\n",
    "            \n",
    "                for ineuron in np.arange(0,nneurons_toana,1):\n",
    "                    neuron_toana = neurons_toana[ineuron]\n",
    "\n",
    "                    try:\n",
    "                        df_clean_ineuron = df_clean_idate[np.isin(df_clean_idate['clusterID'],neuron_toana)]\n",
    "            \n",
    "                        r, p = pearsonr(df_clean_ineuron[columns_of_interest[i]], df_clean_ineuron[columns_of_interest[j]])\n",
    "                    \n",
    "                    except:\n",
    "                        r = np.nan\n",
    "                        p = np.nan\n",
    "                    \n",
    "                    corrs_temp.append(r)\n",
    "                    pvals_temp.append(p)\n",
    "\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = np.nanmean(np.unique(corrs_temp))\n",
    "            \n",
    "            corrs_temp = np.array(corrs_temp)\n",
    "            corrs_temp = corrs_temp[~np.isnan(corrs_temp)]\n",
    "            _,pttest = st.ttest_1samp(np.unique(corrs_temp),0)\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = pttest\n",
    "\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix across individual neurons (p < 0.01 means ttest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a5eed-b9ac-4628-8f8f-99d71f1812f9",
   "metadata": {},
   "source": [
    "### examine the percent of neurons that significantly encode each variables, looking at the FR slope and FR mean separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9d3ac-4428-4967-8142-34cd73eeda9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Variables to correlate with fr_mean and fr_slope\n",
    "behavior_vars = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'predicted_v',\n",
    "]\n",
    "\n",
    "neural_metrics = ['fr_mean', 'fr_slope']\n",
    "# neural_metrics = ['fr_mean', 'fr_slope','fr_slope_peakbased']\n",
    "\n",
    "# Store percent significance for heatmap\n",
    "results = []\n",
    "\n",
    "grouped = bhvevents_aligned_FR_allevents_all_dates_df.groupby(['dates', 'clusterID', 'condition'])\n",
    "\n",
    "for (date, cluster_id, condition), group in grouped:\n",
    "    for fr_type in neural_metrics:\n",
    "        df = group[behavior_vars + [fr_type]].dropna()\n",
    "        if len(df) >= len(behavior_vars) + 2:  # need more trials than predictors\n",
    "            X = sm.add_constant(df[behavior_vars])\n",
    "            y = df[fr_type]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            for var in behavior_vars:\n",
    "                results.append({\n",
    "                    'date': date,\n",
    "                    'clusterID': cluster_id,\n",
    "                    'condition': condition,\n",
    "                    'fr_metric': fr_type,\n",
    "                    'predictor': var,\n",
    "                    'beta': model.params.get(var, np.nan),\n",
    "                    'pval': model.pvalues.get(var, np.nan),\n",
    "                    'significant': model.pvalues.get(var, np.nan) < 0.05\n",
    "                })\n",
    "\n",
    "regression_results_df = pd.DataFrame(results)\n",
    "\n",
    "# do the heatmap plotting\n",
    "# --- Step 1: Compute % of significant neurons per predictor ---\n",
    "summary = (\n",
    "    regression_results_df.groupby(['fr_metric', 'predictor'])['significant']\n",
    "    .mean()\n",
    "    .unstack(0) * 100  # convert to percentage\n",
    ")\n",
    "\n",
    "# --- Step 2: Compute % with any significant beta ---\n",
    "any_sig_summary = (\n",
    "    regression_results_df\n",
    "    .groupby(['fr_metric', 'date', 'clusterID'])['significant']\n",
    "    .any()\n",
    "    .groupby(['fr_metric'])\n",
    "    .mean()\n",
    "    .to_frame()\n",
    "    .T * 100  # convert to percentage\n",
    ")\n",
    "any_sig_summary.index = ['any_predictor']\n",
    "\n",
    "# --- Step 3: Combine ---\n",
    "summary_with_total = pd.concat([summary, any_sig_summary])\n",
    "\n",
    "# --- Step 4: Round for annotation ---\n",
    "annot_vals = summary_with_total.round(1).astype(str) + '%'\n",
    "\n",
    "# --- Step 5: Plot ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(summary_with_total, annot=annot_vals, fmt='', cmap='YlOrRd',\n",
    "            cbar_kws={'label': '% of Neurons (p < 0.05)'}, linewidths=0.5)\n",
    "\n",
    "plt.title('Percent of Neurons Significantly Encoding Each Variable')\n",
    "plt.ylabel('Behavioral Predictor')\n",
    "plt.xlabel('Neural Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20bde7-9d78-42be-84f2-07869a65ef3a",
   "metadata": {},
   "source": [
    "### correlation among behavioral and neural variables, and this time only consider significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d6f41-0db7-4d11-b32f-2e4d1652da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "neural_features = ['fr_slope']  # 'fr_mean', 'fr_slope', 'fr_slope_peakbased'\n",
    "# neural_features = ['fr_mean']\n",
    "# predictors_to_check = [ 'predicted_v', ] # define which behavioral predictors to check significance for\n",
    "predictors_to_check =  np.unique(regression_results_df['predictor'])\n",
    "\n",
    "regression_results_df_totest = regression_results_df.copy()\n",
    "\n",
    "ind_good = (np.isin(regression_results_df_totest['fr_metric'],neural_features)) &\\\n",
    "           (np.isin(regression_results_df_totest['predictor'],predictors_to_check))\n",
    "regression_results_df_totest = regression_results_df_totest[ind_good]\n",
    "\n",
    "sig_neurons = (\n",
    "    regression_results_df_totest[regression_results_df_totest['significant']]\n",
    "    .groupby(['date', 'clusterID'])\n",
    "    .size()\n",
    "    .reset_index()[['date', 'clusterID']]\n",
    ")\n",
    "\n",
    "sig_neuron_keys = set(tuple(x) for x in sig_neurons.to_numpy())\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Filter the original DataFrames\n",
    "# -----------------------------\n",
    "bhvevents_filtered_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df[['dates', 'clusterID']].apply(tuple, axis=1).isin(sig_neuron_keys)\n",
    "].copy()\n",
    "\n",
    "##########\n",
    "# organize the data in order to do HDDM - average across neurons for the unique trial\n",
    "##########\n",
    "\n",
    "# Step 1: Group by unique trial (date, bhv_id) and average the FR_ievent across neurons\n",
    "averaged_df = bhvevents_filtered_df.groupby(['dates', 'bhv_id'])['FR_ievent'].apply(\n",
    "    lambda traces: np.mean(np.stack(traces.to_numpy()), axis=0)\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Rename the averaged firing rate column\n",
    "averaged_df = averaged_df.rename(columns={'FR_ievent': 'FR_ievent_avg'})\n",
    "\n",
    "# Step 3: Select representative behavioral columns to merge back (drop duplicates so one per trial)\n",
    "representative_cols = [\n",
    "    'dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'bhv_id', 'mass_move_speed',\n",
    "       'num_preceding_failpull', 'other_mass_move_speed', 'otherpull_prob',\n",
    "       'pull_rt', 'pull_outcome', 'selfpull_prob', 'socialgaze_prob', 'time_from_last_reward',\n",
    "       'previous_pull_outcome', 'socialgaze_auc', 'mass_move_speed_mean',\n",
    "       'mass_move_speed_std', 'other_mass_move_speed_mean',\n",
    "       'other_mass_move_speed_std', 'predicted_v',\n",
    "]\n",
    "\n",
    "# Get one row per (dates, bhv_id) combination\n",
    "behavior_df = bhvevents_filtered_df.drop_duplicates(subset=['dates', 'bhv_id'])[representative_cols]\n",
    "\n",
    "# Step 4: Merge firing rate and behavioral data\n",
    "bhvevents_filtered_mergedneurons_df = pd.merge(averaged_df, behavior_df, \n",
    "                                                on=['dates', 'bhv_id'], how='left')\n",
    "\n",
    "# Step 5\n",
    "# Compute mean firing rate\n",
    "bhvevents_filtered_mergedneurons_df['fr_mean'] = \\\n",
    "      bhvevents_filtered_mergedneurons_df['FR_ievent_avg'].apply(\n",
    "    lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Compute slope of firing rate before 0.85s prior to pull\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        slope, _, _, _, _ = linregress(x, y)\n",
    "        #\n",
    "        if slope<0:\n",
    "            slope = -slope\n",
    "        \n",
    "        return slope\n",
    "    return np.nan\n",
    "\n",
    "bhvevents_filtered_mergedneurons_df['fr_slope'] = \\\n",
    "     bhvevents_filtered_mergedneurons_df['FR_ievent_avg'].apply(compute_fr_slope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38453c46-3f5a-4ca5-85e1-6b2784a012e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'mass_move_speed_mean',\n",
    "    'mass_move_speed_std',\n",
    "    'other_mass_move_speed_mean',\n",
    "    'other_mass_move_speed_std',\n",
    "    'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    # 'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_clean = bhvevents_filtered_df[columns_of_interest].dropna()\n",
    "# df_clean = bhvevents_filtered_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(df_clean[columns_of_interest[i]], df_clean[columns_of_interest[j]])\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "\n",
    "# FDR correction\n",
    "pvals_flat = pval_matrix[np.triu_indices(n, k=1)]\n",
    "_, pvals_corrected, _, _ = multipletests(pvals_flat, method='bonferroni')\n",
    "\n",
    "# Map corrected p-values back into full matrix\n",
    "pval_corrected_matrix = np.ones_like(pval_matrix)\n",
    "pval_corrected_matrix[np.triu_indices(n, k=1)] = pvals_corrected\n",
    "i_lower = np.tril_indices_from(pval_corrected_matrix, -1)\n",
    "pval_corrected_matrix[i_lower] = pval_corrected_matrix.T[i_lower]\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_corrected_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix with FDR-corrected Significance (p < 0.01) \\n only significant neurons')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3fa36-3ce4-408f-ad82-82a4e6b70d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a05a598c",
   "metadata": {},
   "source": [
    "### rescale all trials with different temporal scale to the same so it's easy to average across trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef268626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def resample_time_series_column(df, column_name, target_len):\n",
    "    \"\"\"\n",
    "    Interpolates time series in a dataframe column to a fixed length.\n",
    "    - df: DataFrame with a column containing lists/arrays of time series\n",
    "    - column_name: name of the column to interpolate (e.g., 'FR_ievent')\n",
    "    - target_len: desired length after resampling\n",
    "    Returns:\n",
    "        New column with interpolated time series.\n",
    "    \"\"\"\n",
    "    resampled_col = []\n",
    "    for ts in df[column_name]:\n",
    "        ts = np.array(ts)\n",
    "        orig_len = len(ts)\n",
    "        if orig_len < 2:\n",
    "            resampled_col.append(np.full(target_len, np.nan))  # skip or fill if too short\n",
    "            continue\n",
    "        x_orig = np.linspace(0, 1, orig_len)\n",
    "        x_new = np.linspace(0, 1, target_len)\n",
    "        interp_fn = interp1d(x_orig, ts, kind='linear', fill_value=\"extrapolate\")\n",
    "        ts_resampled = interp_fn(x_new)\n",
    "        resampled_col.append(ts_resampled)\n",
    "    return resampled_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b14c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this to each column to normalize\n",
    "# df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df = bhvevents_filtered_mergedneurons_df.copy()\n",
    "\n",
    "try:\n",
    "    df = df.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df = df\n",
    "\n",
    "target_len = 4*fps  # choose based on average trial length (pull_rt length)\n",
    "\n",
    "for col in ['FR_ievent', 'socialgaze_prob', 'otherpull_prob', \n",
    "            'selfpull_prob','mass_move_speed','other_mass_move_speed']:\n",
    "    df[f'{col}_resampled'] = resample_time_series_column(df, col, target_len=target_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average trials in each quantile of the target variable\n",
    "\n",
    "quantile_tgt_name = 'pull_rt' # 'pull_rt' or 'socialgaze_auc' or 'other_mass_move_speed_mean'\n",
    "\n",
    "df[quantile_tgt_name+'_bin'] = pd.qcut(df[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Time axis\n",
    "time = np.linspace(0, 1, 120)\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over bins\n",
    "for label in ['low', 'medium', 'high']:\n",
    "        \n",
    "    group = df[df[quantile_tgt_name+'_bin'] == label]['FR_ievent_resampled'].dropna()\n",
    "\n",
    "    # Convert list of arrays to 2D matrix\n",
    "    stacked = np.stack(group.values)\n",
    "    \n",
    "    # Compute mean and SEM\n",
    "    mean_trace = np.nanmean(stacked, axis=0)\n",
    "    error_trace = sem(stacked, axis=0, nan_policy='omit')\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error_trace, mean_trace + error_trace, alpha=0.3)\n",
    "\n",
    "# Finalize plot\n",
    "plt.xlabel('Normalized Time (0 to 1)')\n",
    "plt.ylabel('Firing Rate (resampled)')\n",
    "plt.title('Firing Rate by '+quantile_tgt_name+' Quantiles')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838f563",
   "metadata": {},
   "source": [
    "### padding with NaN at the end of each trial to align at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda2390-2aaf-4344-bf1b-d1a180eced44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df_clean = bhvevents_filtered_mergedneurons_df.copy()\n",
    "\n",
    "try:\n",
    "    df_clean = df_clean.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df_clean = df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine max trial length\n",
    "\n",
    "quantile_tgt_name = 'socialgaze_auc' # 'pull_rt' or 'socialgaze_auc' or 'other_mass_move_speed_mean'\n",
    "\n",
    "cont_vari_name = 'FR_ievent' # FR_ievent; socialgaze_prob; mass_move_speed\n",
    "\n",
    "target_len = df_clean[cont_vari_name].apply(len).max()\n",
    "\n",
    "def pad_align_start(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([ts, np.full(pad_len, np.nan)])\n",
    "    else:\n",
    "        return ts[:target_len]\n",
    "\n",
    "#\n",
    "df_clean[cont_vari_name+'_aligned'] = df_clean[cont_vari_name].apply(lambda ts: pad_align_start(ts, target_len))\n",
    "\n",
    "df_clean[quantile_tgt_name+'_bin'] = pd.qcut(df_clean[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = np.arange(target_len) / fps  # since we're aligned to trial start\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for label in ['low', 'medium', 'high']:\n",
    "    group = df_clean[df_clean[quantile_tgt_name+'_bin'] == label][cont_vari_name+'_aligned'].dropna()\n",
    "    traces = np.stack(group.values)\n",
    "\n",
    "    mean_trace = np.nanmean(traces, axis=0)\n",
    "    error = sem(traces, axis=0, nan_policy='omit')\n",
    "\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error, mean_trace + error, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (s, aligned to trial start)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.title(cont_vari_name+' Aligned to Trial Start by '+quantile_tgt_name+' Quantile')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfc0b0-465b-4704-8d8b-484959110ad2",
   "metadata": {},
   "source": [
    "### padding with NaN from the beginning to align everything to the pull event at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c043f2-c127-46e7-a569-737a83ecfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set target length to max length in FR_ievent\n",
    "\n",
    "quantile_tgt_name = 'socialgaze_auc' # 'pull_rt' or 'socialgaze_auc' or 'other_mass_move_speed_mean'\n",
    "\n",
    "cont_vari_name = 'FR_ievent' # FR_ievent; socialgaze_prob; mass_move_speed\n",
    "\n",
    "# Compute the maximum length of trials\n",
    "target_len = df_clean[cont_vari_name].apply(len).max()\n",
    "\n",
    "def pad_align_end(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([np.full(pad_len, np.nan), ts])\n",
    "    else:\n",
    "        return ts[-target_len:]  # just in case\n",
    "\n",
    "# apply the padding\n",
    "df_clean[cont_vari_name+'_aligned'] = df_clean[cont_vari_name].apply(lambda ts: pad_align_end(ts, target_len))\n",
    "\n",
    "# Step 4: Bin pull_rt into quantiles\n",
    "df_clean[quantile_tgt_name+'_bin'] = pd.qcut(df_clean[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6638f0f-99b3-40a8-835d-c58bbbc6bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the plotting\n",
    "time = np.linspace(-target_len / fps, 0, target_len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for label in ['low', 'medium', 'high']:\n",
    "    group = df_clean[df_clean[quantile_tgt_name+'_bin'] == label][cont_vari_name+'_aligned'].dropna()\n",
    "    traces = np.stack(group.values)\n",
    "\n",
    "    mean_trace = np.nanmean(traces, axis=0)\n",
    "    error = sem(traces, axis=0, nan_policy='omit')\n",
    "\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error, mean_trace + error, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (s, aligned to pull)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.title('Aligned '+cont_vari_name+' by '+quantile_tgt_name+' Quantile')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0b912-5884-4263-b91e-425d8ec30f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfe33-19d7-4351-bff0-d3bd53de1cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506d7a1-5e1e-4abe-b416-0ebe75530901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb6844-b9fc-4adc-a050-a98da178b76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46978d28-da6b-4366-ae71-bfdf53577c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0db6b6-e5eb-4dda-b5a4-c02c68bd5dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57403280-29f8-402a-916a-6a758d216386",
   "metadata": {},
   "source": [
    "### Use svm or other method to separate sucessful pull and failed pull;\n",
    "#### training based on FR or some behabioral variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# df_clean = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df_clean = bhvevents_filtered_mergedneurons_df.copy()\n",
    "# df_clean = bhvevents_filtered_df.copy()\n",
    "\n",
    "\n",
    "# svm_tgt_name = 'FR_ievent' # FR_ievent; socialgaze_prob;\n",
    "svm_tgt_name = 'socialgaze_prob' # FR_ievent; socialgaze_prob;\n",
    "\n",
    "try:\n",
    "    df_clean = df_clean.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df_clean = df_clean\n",
    "\n",
    "# Compute the maximum length of trials\n",
    "target_len = df_clean[svm_tgt_name].apply(len).max()\n",
    "\n",
    "def pad_align_end(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([np.full(pad_len, np.nan), ts])\n",
    "    else:\n",
    "        return ts[-target_len:]  # just in case\n",
    "\n",
    "# apply the padding\n",
    "df_clean[svm_tgt_name+'_aligned'] = df_clean[svm_tgt_name].apply(lambda ts: pad_align_end(ts, target_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_iterations = 100\n",
    "n_trials_per_class = 300\n",
    "\n",
    "# Extract\n",
    "X_full = df_clean[f'{svm_tgt_name}_aligned'].tolist()\n",
    "y_full = df_clean['pull_outcome'].values\n",
    "\n",
    "# Preprocess into 2D array (trials x time)\n",
    "max_len = max(len(x) for x in X_full)\n",
    "X = np.array([np.pad(x, (max_len - len(x), 0), constant_values=np.nan) for x in X_full])  # pad front\n",
    "y = np.array(y_full)\n",
    "\n",
    "# Collect AUCs over time\n",
    "n_timepoints = X.shape[1]\n",
    "auc_over_time = np.zeros((n_iterations, n_timepoints))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    idx_success = np.random.choice(np.where(y == 1)[0], size=n_trials_per_class, replace=True)\n",
    "    idx_fail = np.random.choice(np.where(y == 0)[0], size=n_trials_per_class, replace=True)\n",
    "    idx_sample = np.concatenate([idx_success, idx_fail])\n",
    "    \n",
    "    X_sample = X[idx_sample]\n",
    "    y_sample = y[idx_sample]\n",
    "\n",
    "    # Shuffle the samples\n",
    "    shuffle_idx = np.random.permutation(len(y_sample))\n",
    "    X_sample = X_sample[shuffle_idx]\n",
    "    y_sample = y_sample[shuffle_idx]\n",
    "\n",
    "    # Train/test split (70/30)\n",
    "    split_idx = int(0.7 * len(y_sample))\n",
    "    X_train, X_test = X_sample[:split_idx], X_sample[split_idx:]\n",
    "    y_train, y_test = y_sample[:split_idx], y_sample[split_idx:]\n",
    "\n",
    "    # Time-point-wise SVM\n",
    "    for t in range(n_timepoints):\n",
    "        # Drop NaNs for this time point\n",
    "        train_valid = ~np.isnan(X_train[:, t])\n",
    "        test_valid = ~np.isnan(X_test[:, t])\n",
    "\n",
    "        try:\n",
    "            if np.sum(train_valid) > 10 and np.sum(test_valid) > 10:\n",
    "                clf = SVC(kernel='linear', probability=True)\n",
    "                clf.fit(X_train[train_valid, t].reshape(-1, 1), y_train[train_valid])\n",
    "                y_proba = clf.predict_proba(X_test[test_valid, t].reshape(-1, 1))[:, 1]\n",
    "                auc = roc_auc_score(y_test[test_valid], y_proba)\n",
    "                auc_over_time[i, t] = auc\n",
    "            else:\n",
    "                auc_over_time[i, t] = np.nan\n",
    "        except:\n",
    "                auc_over_time[i, t] = np.nan\n",
    "\n",
    "\n",
    "# do some plotting\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "n_timepoints = auc_over_time.shape[1]\n",
    "time_axis = (np.arange(n_timepoints) - n_timepoints) / fps  # negative to 0\n",
    "\n",
    "# Compute mean and stderr\n",
    "mean_auc = np.nanmean(auc_over_time, axis=0)\n",
    "stderr_auc = np.nanstd(auc_over_time, axis=0) / np.sqrt(n_iterations)\n",
    "\n",
    "# Perform one-sample t-test against chance level (0.5)\n",
    "t_vals, p_vals = ttest_1samp(auc_over_time, popmean=0.5, axis=0, nan_policy='omit')\n",
    "\n",
    "significant = p_vals < 0.05\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_axis, mean_auc, label='Mean AUC')\n",
    "plt.fill_between(time_axis, mean_auc - stderr_auc, mean_auc + stderr_auc, alpha=0.3)\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "plt.scatter(time_axis[significant], mean_auc[significant], color='red', s=20, label='p < 0.05 (FDR)')\n",
    "plt.title(f'SVM Decoding: Predicting Pull Outcome from {svm_tgt_name}_aligned')\n",
    "plt.xlabel('Time (s, aligned to pull)')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee19f-a861-46b1-a470-895d542c40b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d734ce-fd42-434d-a65e-ebb2ea39b9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20414058-86a4-4267-b249-713b223c9090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697abb96-f779-47fc-baf6-f386196b3b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba5ade-bb09-4b20-8fae-72c3f604b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15422f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
