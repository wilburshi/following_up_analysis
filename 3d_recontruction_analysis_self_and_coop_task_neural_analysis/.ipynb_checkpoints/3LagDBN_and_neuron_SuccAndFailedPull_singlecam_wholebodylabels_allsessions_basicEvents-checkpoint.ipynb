{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### In this script, DBN is run on the all the sessions\n",
    "### In this script, DBN is run with 1s time bin, 3 time lag \n",
    "### In this script, the animal tracking is done with only one camera - camera 2 (middle)\n",
    "### In this script, DBN is run together with neural data and basic bhv variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 0.5*30 # second*30fps\n",
    "nframes = 5*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# only analyze the best (five) sessions for each conditions\n",
    "do_bestsession = 1\n",
    "if do_bestsession:\n",
    "    savefile_sufix = '_bestsessions'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if not do_bestsession:\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_SR', \n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20231101_SR\",\"20231101_MC\",\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,  0.00, \n",
    "                              ] # in second\n",
    "    elif do_bestsession:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_SR', \n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20231101_SR\",\"20231101_MC\"\n",
    "                     ]\n",
    "        session_start_times = [ \n",
    "                                0.00,  0.00,   \n",
    "                              ] # in second\n",
    "    \n",
    "    animal1_fixedorder = ['dodson']\n",
    "    animal2_fixedorder = ['ginger']\n",
    "\n",
    "    animal1_filename = \"Dodson\"\n",
    "    animal2_filename = \"Ginger\"\n",
    "    \n",
    "\n",
    "    \n",
    "#    \n",
    "# dates_list = [\"20221128\"]\n",
    "# session_start_times = [1.00] # in second\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# get this information using DLC animal tracking GUI, the results are stored: \n",
    "# /home/ws523/marmoset_tracking_DLCv2/marmoset_tracking_with_lever_tube-weikang-2023-04-13/labeled-data/\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/gibbs/pi/jadi/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        session_start_time = session_start_times[idate]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/gibbs/pi/jadi/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "        try: \n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "            video_file_original = camera12_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,date_tgt)\n",
    "            video_file_original = camera23_analyzed_path+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "        except:\n",
    "            bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial+1].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "        pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, 600, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, 600, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "        owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "        mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "        mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "\n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 1:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            tracking_video_singlecam_wholebody_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                              lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                              animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                              animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                              video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body)         \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 1:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aada694",
   "metadata": {},
   "source": [
    "#### redefine the tasktype and cooperation threshold to merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1548e",
   "metadata": {},
   "source": [
    "### prepare the input data for DBN\n",
    "#### for both neural and behavioral events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DBN related summarizing variables\n",
    "\n",
    "doBhvitv_timebin = 0 # 1: if use the mean bhv event interval for time bin\n",
    "\n",
    "pulltypes = ['succpull','failedpull']\n",
    "npulltypes = np.shape(pulltypes)[0]\n",
    "\n",
    "prepare_input_data = 1\n",
    "\n",
    "# DBN resolutions (make sure they are the same as in the later part of the code)\n",
    "totalsess_time = 600 # total session time in s\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "mergetempRos = 0\n",
    "\n",
    "# # train the dynamic bayesian network - Alec's model \n",
    "#   prepare the multi-session table; one time lag; multi time steps (temporal resolution) as separate files\n",
    "\n",
    "# prepare the DBN input data\n",
    "if prepare_input_data:\n",
    "    \n",
    "    DBN_input_data_alltypes = dict.fromkeys(pulltypes, [])\n",
    "    \n",
    "    for ipulltype in np.arange(0,npulltypes,1):\n",
    "        pulltype = pulltypes[ipulltype]\n",
    "        \n",
    "        DBN_input_data_alltypes[pulltype] = dict.fromkeys(dates_list, [])\n",
    "        \n",
    "        for idate in np.arange(0,ndates,1):\n",
    "\n",
    "            neural_record_condition = neural_record_conditions[idate]\n",
    "\n",
    "            date_tgt = dates_list[idate]\n",
    "            session_start_time = session_start_times[idate]\n",
    "\n",
    "            # # load spike sorting results\n",
    "            spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "            spike_time_data = np.load(spike_time_file)\n",
    "            #\n",
    "            spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "            spike_clusters_data = np.load(spike_clusters_file)\n",
    "\n",
    "            # load the behavioral data\n",
    "            try:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "                #\n",
    "                with open(ni_data_json[0]) as f:\n",
    "                    for line in f:\n",
    "                        ni_data=json.loads(line)           \n",
    "            except:\n",
    "                bhv_data_path = \"/home/ws523/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "                #\n",
    "                with open(ni_data_json[0]) as f:\n",
    "                    for line in f:\n",
    "                        ni_data=json.loads(line)\n",
    "\n",
    "            # separate successful and failed pulls\n",
    "            if pulltype == 'succpull':\n",
    "                trialnum_tgtpull = trial_record[trial_record['rewarded']>0]['trial_number'].reset_index(drop=True)\n",
    "            elif pulltype == 'failedpull':\n",
    "                trialnum_tgtpull = trial_record[trial_record['rewarded']==0]['trial_number'].reset_index(drop=True)\n",
    "            bhv_data = bhv_data[bhv_data['trial_number'].isin(trialnum_tgtpull)]\n",
    "            trial_record = trial_record[trial_record['trial_number'].isin(trialnum_tgtpull)]\n",
    "            bhv_data = bhv_data.reset_index(drop=True)\n",
    "            trial_record = trial_record.reset_index(drop=True)\n",
    "            \n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # clean up the trial_record\n",
    "            #try:\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            for itrial in trial_record['trial_number']:\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            for itrial in trial_record_clean['trial_number']:\n",
    "                ind = bhv_data[\"trial_number\"]==itrial\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + float(trial_record_clean[trial_record_clean['trial_number']==itrial]['trial_starttime'].iloc[0])\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "            #except:\n",
    "            #    trial_record_clean = trial_record\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "                \n",
    "\n",
    "            # session starting time compared with the neural recording\n",
    "            session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "            neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset    \n",
    "\n",
    "            # align the spike time to the session start, change the temporal resolution to 100ms\n",
    "            spike_time_point = np.round((spike_time_data/fs_spikes+neural_start_time_session_start_offset)*10)/10\n",
    "\n",
    "\n",
    "            # load behavioral event results\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            #\n",
    "            look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "            look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "            look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "            # change the unit to second\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']   \n",
    "\n",
    "\n",
    "            if mergetempRos:\n",
    "                temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "                # use bhv event to decide temporal resolution\n",
    "                #\n",
    "                #low_lim,up_lim,_ = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #temp_resolus = temp_resolus = np.arange(low_lim,up_lim,0.1)\n",
    "            #\n",
    "            if doBhvitv_timebin:\n",
    "                pull_other_intv_ii = pd.Series(bhv_intv_all_dates[date_tgt]['pull_other_pooled'])\n",
    "                # remove the interval that is too large\n",
    "                pull_other_intv_ii[pull_other_intv_ii>(np.nanmean(pull_other_intv_ii)+2*np.nanstd(pull_other_intv_ii))]= np.nan    \n",
    "                # pull_other_intv_ii[pull_other_intv_ii>10]= np.nan\n",
    "                temp_resolus = [np.nanmean(pull_other_intv_ii)]          \n",
    "            #\n",
    "            ntemp_reses = np.shape(temp_resolus)[0]           \n",
    "\n",
    "\n",
    "            # try different temporal resolutions\n",
    "            for temp_resolu in temp_resolus:\n",
    "                bhv_df = []\n",
    "\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, spike_time_point, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                else:\n",
    "                    bhv_df_itr,_,_ = train_DBN_multiLag_create_df_only(totalsess_time, session_start_time, temp_resolu, spike_time_point, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)     \n",
    "\n",
    "                if len(bhv_df)==0:\n",
    "                    bhv_df = bhv_df_itr\n",
    "                else:\n",
    "                    bhv_df = pd.concat([bhv_df,bhv_df_itr])                   \n",
    "                    bhv_df = bhv_df.reset_index(drop=True)        \n",
    "\n",
    "                DBN_input_data_alltypes[pulltype][date_tgt] = bhv_df\n",
    "            \n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "        if not mergetempRos:\n",
    "            if doBhvitv_timebin:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'wb') as f:\n",
    "                    pickle.dump(DBN_input_data_alltypes, f)\n",
    "        else:\n",
    "            with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'wb') as f:\n",
    "                pickle.dump(DBN_input_data_alltypes, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc973b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded1439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a743731",
   "metadata": {},
   "source": [
    "### run the DBN model on the combined session data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7d323",
   "metadata": {},
   "source": [
    "#### a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d13d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "pulltypes = ['succpull','failedpull']\n",
    "npulltypes = np.shape(pulltypes)[0]\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 1 # number of random starting points/graphs\n",
    "nbootstraps = 1\n",
    "\n",
    "if 0:\n",
    "\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "    # analyze successful pull and failed pull separately\n",
    "    for pulltype in pulltypes:\n",
    "        \n",
    "        # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "        for temp_resolu in temp_resolus:\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            if not mergetempRos:\n",
    "                if doBhvitv_timebin:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_alltypes = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_alltypes = pickle.load(f)\n",
    "\n",
    "            DBN_input_data_alltypes = DBN_input_data_alltypes[pulltype]\n",
    "\n",
    "            # only try three sample sizes\n",
    "            #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "            #- full row number of each session\n",
    "            if minmaxfullSampSize:\n",
    "                key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "                key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "                key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "                min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "                min_samplesize = int(min_samplesize/100)*100\n",
    "                max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "                max_samplesize = int(max_samplesize/100)*100\n",
    "                #samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "                #samplingsizes_name = ['min_row_number','max_row_number','full_row_number']\n",
    "                samplingsizes = [np.nan]\n",
    "                samplingsizes_name = ['full_row_number']\n",
    "                nsamplings = np.shape(samplingsizes)[0]\n",
    "                print(samplingsizes)\n",
    "\n",
    "            # try different down/re-sampling size\n",
    "            # for jj in np.arange(0,nsamplings,1):\n",
    "            for jj in np.arange(0,1,1):\n",
    "\n",
    "                isamplingsize = samplingsizes[jj]\n",
    "\n",
    "                DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "                DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "                DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "                DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "                weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "                weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "                sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "                # different individual sessions\n",
    "                ndates = np.shape(dates_list)[0]\n",
    "                for idate in np.arange(0,ndates,1):\n",
    "                    date_tgt = dates_list[idate]\n",
    "\n",
    "                    if samplingsizes_name[jj]=='full_row_number':\n",
    "                        isamplingsize = np.shape(DBN_input_data_alltypes[date_tgt])[0]\n",
    "\n",
    "                    try:\n",
    "                        bhv_df_all = DBN_input_data_alltypes[date_tgt]\n",
    "\n",
    "                        # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                        colnames = list(bhv_df_all.columns)\n",
    "                        eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\",\"spikes\"]\n",
    "                        nevents = np.size(eventnames)\n",
    "\n",
    "                        all_pops = list(bhv_df_all.columns)\n",
    "                        from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                        to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                        causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                        nFromNodes = np.shape(from_pops)[0]\n",
    "                        nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                        DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        score_randstart = np.zeros((num_starting_points))\n",
    "                        score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                        # step 1: randomize the starting point for num_starting_points times\n",
    "                        for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                            # try different down/re-sampling size\n",
    "                            bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                            aic = AicScore(bhv_df)\n",
    "\n",
    "                            #Anirban(Alec) shuffle, slow\n",
    "                            bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                            aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                            np.random.seed(istarting_points)\n",
    "                            random.seed(istarting_points)\n",
    "                            starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                            starting_graph = DAG()\n",
    "                            starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                            starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                            # step 2: add the shffled data results\n",
    "                            # shuffled bhv_df\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                        DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                        DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                        DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                        DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                        weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                        weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                        sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                        weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                        weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                        sig_edges_alltypes[date_tgt] = sig_edges\n",
    "\n",
    "                    except:\n",
    "                        DAGs_alltypes[date_tgt] = [] \n",
    "                        DAGs_shuffle_alltypes[date_tgt] = []\n",
    "\n",
    "                        DAGs_scores_alltypes[date_tgt] = []\n",
    "                        DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "\n",
    "                        weighted_graphs_alltypes[date_tgt] = []\n",
    "                        weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                        sig_edges_alltypes[date_tgt] = []\n",
    "\n",
    "                DAGscores_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "                DAGscores_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "                weighted_graphs_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "                sig_edges_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "        print(weighted_graphs_diffTempRo_diffSampSize)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f90ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d647783a",
   "metadata": {},
   "source": [
    "#### run on the entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBN on the large table with merged sessions\n",
    "\n",
    "pulltypes = ['succpull','failedpull']\n",
    "npulltypes = np.shape(pulltypes)[0]\n",
    "\n",
    "mergetempRos = 0 # 1: merge different time bins\n",
    "\n",
    "minmaxfullSampSize = 1 # 1: use the  min row number and max row number, or the full row for each session\n",
    "\n",
    "moreSampSize = 0 # 1: use more sample size (more than just minimal row number and max row number)\n",
    "\n",
    "num_starting_points = 100 # number of random starting points/graphs\n",
    "nbootstraps = 95\n",
    "\n",
    "try:\n",
    "    # dumpy\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "    if minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "            sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    if moreSampSize:\n",
    "        # different data (down/re)sampling numbers\n",
    "        samplingsizes = np.arange(1100,3000,100)\n",
    "        # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "        # samplingsizes = [100,500]\n",
    "        # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "        samplingsizes_name = list(map(str, samplingsizes))\n",
    "        nsamplings = np.shape(samplingsizes)[0]\n",
    "\n",
    "    weighted_graphs_diffTempRo_diffSampSize = {}\n",
    "    weighted_graphs_shuffled_diffTempRo_diffSampSize = {}\n",
    "    sig_edges_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_diffTempRo_diffSampSize = {}\n",
    "    DAGscores_shuffled_diffTempRo_diffSampSize = {}\n",
    "\n",
    "    totalsess_time = 600 # total session time in s\n",
    "    # temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "    ntemp_reses = np.shape(temp_resolus)[0]\n",
    "\n",
    "     # analyze successful pull and failed pull separately\n",
    "    for pulltype in pulltypes:\n",
    "        \n",
    "        # try different temporal resolutions, remember to use the same settings as in the previous ones\n",
    "        for temp_resolu in temp_resolus:\n",
    "\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            if not mergetempRos:\n",
    "                if doBhvitv_timebin:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'bhvItvTempReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_allsessions = pickle.load(f)\n",
    "                else:\n",
    "                    with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'sReSo.pkl', 'rb') as f:\n",
    "                        DBN_input_data_allsessions = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'//DBN_input_data_alltypes_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_mergeTempsReSo.pkl', 'rb') as f:\n",
    "                    DBN_input_data_allsessions = pickle.load(f)\n",
    "            #        \n",
    "            DBN_input_data_allsessions = DBN_input_data_allsessions[pulltype]\n",
    "\n",
    "            # only try three sample sizes\n",
    "            #- minimal row number (require data downsample) and maximal row number (require data upsample)\n",
    "            #- full row number of each session\n",
    "            if minmaxfullSampSize:\n",
    "                key_to_value_lengths = {k:len(v) for k, v in DBN_input_data_alltypes.items()}\n",
    "                key_to_value_lengths_array = np.fromiter(key_to_value_lengths.values(),dtype=float)\n",
    "                key_to_value_lengths_array[key_to_value_lengths_array==0]=np.nan\n",
    "                min_samplesize = np.nanmin(key_to_value_lengths_array)\n",
    "                min_samplesize = int(min_samplesize/100)*100\n",
    "                max_samplesize = np.nanmax(key_to_value_lengths_array)\n",
    "                max_samplesize = int(max_samplesize/100)*100\n",
    "                # samplingsizes = [min_samplesize,max_samplesize,np.nan]\n",
    "                # samplingsizes_name = ['min_row_number','max_row_number','full_row_number']   \n",
    "                samplingsizes = [np.nan]\n",
    "                samplingsizes_name = ['full_row_number']\n",
    "                nsamplings = np.shape(samplingsizes)[0]\n",
    "                print(samplingsizes)\n",
    "\n",
    "            # try different down/re-sampling size\n",
    "            for jj in np.arange(0,nsamplings,1):\n",
    "\n",
    "                isamplingsize = samplingsizes[jj]\n",
    "\n",
    "                DAGs_alltypes = dict.fromkeys(dates_list, [])\n",
    "                DAGs_shuffle_alltypes = dict.fromkeys(dates_list, [])\n",
    "                DAGs_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "                DAGs_shuffle_scores_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "                weighted_graphs_alltypes = dict.fromkeys(dates_list, [])\n",
    "                weighted_graphs_shuffled_alltypes = dict.fromkeys(dates_list, [])\n",
    "                sig_edges_alltypes = dict.fromkeys(dates_list, [])\n",
    "\n",
    "                # different individual sessions\n",
    "                ndates = np.shape(dates_list)[0]\n",
    "                for idate in np.arange(0,ndates,1):\n",
    "                    date_tgt = dates_list[idate]\n",
    "\n",
    "                    if samplingsizes_name[jj]=='full_row_number':\n",
    "                        isamplingsize = np.shape(DBN_input_data_allsessions[date_tgt])[0]\n",
    "\n",
    "                    try:\n",
    "                        bhv_df_all = DBN_input_data_allsessions[date_tgt]\n",
    "\n",
    "\n",
    "                        # define DBN graph structures; make sure they are the same as in the train_DBN_multiLag\n",
    "                        colnames = list(bhv_df_all.columns)\n",
    "                        eventnames = [\"pull1\",\"pull2\",\"owgaze1\",\"owgaze2\",\"spikes\"]\n",
    "                        nevents = np.size(eventnames)\n",
    "\n",
    "                        all_pops = list(bhv_df_all.columns)\n",
    "                        from_pops = [pop for pop in all_pops if not pop.endswith('t3')]\n",
    "                        to_pops = [pop for pop in all_pops if pop.endswith('t3')]\n",
    "                        causal_whitelist = [(from_pop,to_pop) for from_pop in from_pops for to_pop in to_pops]\n",
    "\n",
    "                        nFromNodes = np.shape(from_pops)[0]\n",
    "                        nToNodes = np.shape(to_pops)[0]\n",
    "\n",
    "                        DAGs_randstart = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        DAGs_randstart_shuffle = np.zeros((num_starting_points, nFromNodes, nToNodes))\n",
    "                        score_randstart = np.zeros((num_starting_points))\n",
    "                        score_randstart_shuffle = np.zeros((num_starting_points))\n",
    "\n",
    "                        # step 1: randomize the starting point for num_starting_points times\n",
    "                        for istarting_points in np.arange(0,num_starting_points,1):\n",
    "\n",
    "                            # try different down/re-sampling size\n",
    "                            bhv_df = bhv_df_all.sample(isamplingsize,replace = True, random_state = istarting_points) # take the subset for DBN training\n",
    "                            aic = AicScore(bhv_df)\n",
    "\n",
    "                            #Anirban(Alec) shuffle, slow\n",
    "                            bhv_df_shuffle, df_shufflekeys = EfficientShuffle(bhv_df,round(time()))\n",
    "                            aic_shuffle = AicScore(bhv_df_shuffle)\n",
    "\n",
    "                            np.random.seed(istarting_points)\n",
    "                            random.seed(istarting_points)\n",
    "                            starting_edges = random.sample(causal_whitelist, np.random.randint(1,len(causal_whitelist)))\n",
    "                            starting_graph = DAG()\n",
    "                            starting_graph.add_nodes_from(nodes=all_pops)\n",
    "                            starting_graph.add_edges_from(ebunch=starting_edges)\n",
    "\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart[istarting_points] = aic.score(best_model)\n",
    "\n",
    "                            # step 2: add the shffled data results\n",
    "                            # shuffled bhv_df\n",
    "                            best_model,edges,DAGs = train_DBN_multiLag_training_only(bhv_df_shuffle,starting_graph,colnames,eventnames,from_pops,to_pops)           \n",
    "                            DAGs[0][np.isnan(DAGs[0])]=0\n",
    "\n",
    "                            DAGs_randstart_shuffle[istarting_points,:,:] = DAGs[0]\n",
    "                            score_randstart_shuffle[istarting_points] = aic_shuffle.score(best_model)\n",
    "\n",
    "                        DAGs_alltypes[date_tgt] = DAGs_randstart \n",
    "                        DAGs_shuffle_alltypes[date_tgt] = DAGs_randstart_shuffle\n",
    "\n",
    "                        DAGs_scores_alltypes[date_tgt] = score_randstart\n",
    "                        DAGs_shuffle_scores_alltypes[date_tgt] = score_randstart_shuffle\n",
    "\n",
    "                        weighted_graphs = get_weighted_dags(DAGs_alltypes[date_tgt],nbootstraps)\n",
    "                        weighted_graphs_shuffled = get_weighted_dags(DAGs_shuffle_alltypes[date_tgt],nbootstraps)\n",
    "                        sig_edges = get_significant_edges(weighted_graphs,weighted_graphs_shuffled)\n",
    "\n",
    "                        weighted_graphs_alltypes[date_tgt] = weighted_graphs\n",
    "                        weighted_graphs_shuffled_alltypes[date_tgt] = weighted_graphs_shuffled\n",
    "                        sig_edges_alltypes[date_tgt] = sig_edges\n",
    "\n",
    "                    except:\n",
    "                        DAGs_alltypes[date_tgt] = [] \n",
    "                        DAGs_shuffle_alltypes[date_tgt] = []\n",
    "\n",
    "                        DAGs_scores_alltypes[date_tgt] = []\n",
    "                        DAGs_shuffle_scores_alltypes[date_tgt] = []\n",
    "\n",
    "                        weighted_graphs_alltypes[date_tgt] = []\n",
    "                        weighted_graphs_shuffled_alltypes[date_tgt] = []\n",
    "                        sig_edges_alltypes[date_tgt] = []\n",
    "\n",
    "                DAGscores_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_scores_alltypes\n",
    "                DAGscores_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = DAGs_shuffle_scores_alltypes\n",
    "\n",
    "                weighted_graphs_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_alltypes\n",
    "                weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = weighted_graphs_shuffled_alltypes\n",
    "                sig_edges_diffTempRo_diffSampSize[(pulltype,str(temp_resolu),samplingsizes_name[jj])] = sig_edges_alltypes\n",
    "\n",
    "            \n",
    "    # save data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "        os.makedirs(data_saved_subfolder)\n",
    "    if moreSampSize:  \n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n",
    "    elif minmaxfullSampSize:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)        \n",
    "    else:\n",
    "        with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(DAGscores_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(weighted_graphs_shuffled_diffTempRo_diffSampSize, f)\n",
    "        with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sig_edges_diffTempRo_diffSampSize, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7819edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886cc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45fc258d",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge with arrows; show the best time bin and row number; show the three time lag separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list_plot = [\"20231101_SR\",\"20231101_MC\",]\n",
    "ndates_plot = np.shape(dates_list_plot)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulltype_forplot = 'failedpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\",'OFCspikes']\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0],[-0.5,0.5],]\n",
    "eventname_locations = [[0,1],[1,1],[0,0],[1,0],[-0.5,0.5],]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65],[-0.15,1.0],[0.15,0.75]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75],[0.85,0.75],[1.15,1.00]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10],[-0.15,-.05],[0.15,0.00]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25],[0.85,0.00],[1.15,-.05]],\n",
    "                       [[-0.35,.65],[-0.25,.65],[-0.35,.25],[-0.35,.35],[-0.65,.50],[-0.2,0.45]],\n",
    "                       ]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50],[-0.2,-0.2],[1.00,-0.2]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50],[-1.0,-0.2],[0.20,-0.2]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00],[-0.2,0.20],[1.00,0.20]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45],[-1.0,0.20],[0.20,0.20]],\n",
    "                        [[0.20,0.20],[1.10,0.20],[0.20,-.20],[1.10,-.20],[  45, -90],[1.25,0.00]],\n",
    "                       ]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00','#BF3EFF',]\n",
    "eventnodes_shape = [\"o\",\"o\",\"^\",\"^\",\"*\",]\n",
    "    \n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,ndates_plot)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*ndates_plot)\n",
    "\n",
    "time_lags = ['t_-1.5','t_-1','t_-0.5']\n",
    "# fromRowIDs =[[0,1,2,3],[6,7,8,9],[12,13,14,15]]\n",
    "# toColID = [0,1,2,3]\n",
    "fromRowIDs =[[0,1,2,3,4,],[5,6,7,8,9,],[10,11,12,13,14,]]\n",
    "toColID = [0,1,2,3,4,]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]    \n",
    "\n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "    \n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_plot,1):\n",
    "\n",
    "        # try:\n",
    "\n",
    "            idate_typename = dates_list_plot[idate]\n",
    "\n",
    "            # load data accordingly\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            #\n",
    "            if moreSampSize:\n",
    "                with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            elif minmaxfullSampSize:\n",
    "                with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][idate_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][idate_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][idate_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "\n",
    "            #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "\n",
    "            sig_avg_dags = weighted_graphs_tgt.mean(axis = 0) * sig_edges_tgt\n",
    "            sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "            sig_avg_dags = sig_avg_dags[:,toColID]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,idate].set_title(idate_typename,fontsize=14)\n",
    "            axs[ilag*2+0,idate].set_xlim([-1,2])\n",
    "            axs[ilag*2+0,idate].set_ylim([-1,2])\n",
    "            axs[ilag*2+0,idate].set_xticks([])\n",
    "            axs[ilag*2+0,idate].set_xticklabels([])\n",
    "            axs[ilag*2+0,idate].set_yticks([])\n",
    "            axs[ilag*2+0,idate].set_yticklabels([])\n",
    "            axs[ilag*2+0,idate].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,idate].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,idate].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,idate].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,idate].axis('equal')\n",
    "\n",
    "\n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,idate].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                              eventnodes_shape[ieventnode],markersize=40,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                              markeredgecolor='none')              \n",
    "                axs[ilag*2+0,idate].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                                       eventnames[ieventnode],fontsize=15)\n",
    "\n",
    "                clmap = mpl.cm.get_cmap('Greens')\n",
    "\n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt>0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,idate].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,idate].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        # width=0.04*abs(edge_weight_tgt),\n",
    "                                                        head_width=0.08,\n",
    "                                                        width=0.04,   \n",
    "                                                        color = clmap(edge_weight_tgt))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      # 0.04*abs(edge_weight_tgt),\n",
    "                                                      0.04,\n",
    "                                                      color = clmap(edge_weight_tgt))\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap(edge_weight_tgt), \n",
    "                                    edgecolor=clmap(edge_weight_tgt)\n",
    "                                )\n",
    "                                axs[ilag*2+0,idate].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,idate].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap(edge_weight_tgt),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "                                elif ifromNode < 4:\n",
    "                                    axs[ilag*2+0,idate].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap(edge_weight_tgt),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = 0,1\n",
    "            import matplotlib as mpl\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,idate].pcolormesh(sig_avg_dags_df,cmap=\"Greens\",norm=norm)\n",
    "            #\n",
    "            if idate == ndates_plot-1:\n",
    "                cax = axs[ilag*2+1,idate].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,idate], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,idate].axis('equal')\n",
    "            axs[ilag*2+1,idate].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,idate].set_xticks(np.arange(0.5,5.5,1))\n",
    "            axs[ilag*2+1,idate].set_xticklabels(eventnames)\n",
    "            if idate == 0:\n",
    "                axs[ilag*2+1,idate].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,idate].set_yticks(np.arange(0.5,5.5,1))\n",
    "                axs[ilag*2+1,idate].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,idate].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,idate].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,idate].set_yticks([])\n",
    "                axs[ilag*2+1,idate].set_yticklabels([])\n",
    "\n",
    "        # except:\n",
    "        #      continue\n",
    "                \n",
    "savefigs = 1\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_neuron_SuccAndFailedPull_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+idate_typename[0:7]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if moreSampSize:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+panimal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows.pdf')\n",
    "    else:  \n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'.pdf')\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030609c",
   "metadata": {},
   "source": [
    "### plot graphs - show the edge differences, use one condition as the base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd31164",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list_plot = [\"20231101_SR\",\"20231101_MC\",]\n",
    "ndates_plot = np.shape(dates_list_plot)[0]\n",
    "\n",
    "basecondition = '20231101_SR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulltype_forplot = 'succpull' # 'succpull' or 'failedpull'\n",
    "\n",
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "else:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "\n",
    "# make sure these variables are consistent with the train_DBN_alec.py settings\n",
    "eventnames = [\"pull1\",\"pull2\",\"gaze1\",\"gaze2\",'OFCspikes']\n",
    "eventnode_locations = [[0,1],[1,1],[0,0],[1,0],[-0.5,0.5],]\n",
    "eventname_locations = [[0,1],[1,1],[0,0],[1,0],[-0.5,0.5],]\n",
    "# indicate where edge starts\n",
    "# for the self edge, it's the center of the self loop\n",
    "nodearrow_locations = [[[0.00,1.25],[0.25,1.10],[-.10,0.75],[0.15,0.65],[-0.15,1.0],[0.15,0.75]],\n",
    "                       [[0.75,1.00],[1.00,1.25],[0.85,0.65],[1.10,0.75],[0.85,0.75],[1.15,1.00]],\n",
    "                       [[0.00,0.25],[0.25,0.35],[0.00,-.25],[0.25,-.10],[-0.15,-.05],[0.15,0.00]],\n",
    "                       [[0.75,0.35],[1.00,0.25],[0.75,0.00],[1.00,-.25],[0.85,0.00],[1.15,-.05]],\n",
    "                       [[-0.35,.65],[-0.25,.65],[-0.35,.25],[-0.35,.35],[-0.65,.50],[-0.2,0.45]],\n",
    "                       ]\n",
    "# indicate where edge goes\n",
    "# for the self edge, it's the theta1 and theta2 (with fixed radius)\n",
    "nodearrow_directions = [[[ -45,-180],[0.50,0.00],[0.00,-.50],[0.50,-.50],[-0.2,-0.2],[1.00,-0.2]],\n",
    "                        [[-.50,0.00],[ -45,-180],[-.50,-.50],[0.00,-.50],[-1.0,-0.2],[0.20,-0.2]],\n",
    "                        [[0.00,0.50],[0.50,0.50],[ 180,  45],[0.50,0.00],[-0.2,0.20],[1.00,0.20]],\n",
    "                        [[-.50,0.50],[0.00,0.50],[-.50,0.00],[ 180,  45],[-1.0,0.20],[0.20,0.20]],\n",
    "                        [[0.20,0.20],[1.10,0.20],[0.20,-.20],[1.10,-.20],[  45, -90],[1.25,0.00]],\n",
    "                       ]\n",
    "\n",
    "nevents = np.size(eventnames)\n",
    "eventnodes_color = ['#BF3EFF','#FF7F00','#BF3EFF','#FF7F00','#BF3EFF',]\n",
    "eventnodes_shape = [\"o\",\"o\",\"^\",\"^\",\"*\",]\n",
    "    \n",
    "\n",
    "# different session conditions (aka DBN groups)\n",
    "# different time lags (t_-3, t_-2 and t_-1)\n",
    "fig, axs = plt.subplots(6,ndates_plot)\n",
    "fig.set_figheight(48)\n",
    "fig.set_figwidth(8*ndates_plot)\n",
    "\n",
    "time_lags = ['t_-1.5','t_-1','t_-0.5']\n",
    "# fromRowIDs =[[0,1,2,3],[6,7,8,9],[12,13,14,15]]\n",
    "# toColID = [0,1,2,3]\n",
    "fromRowIDs =[[0,1,2,3,4,],[5,6,7,8,9,],[10,11,12,13,14,]]\n",
    "toColID = [0,1,2,3,4,]\n",
    "ntime_lags = np.shape(time_lags)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]  \n",
    "\n",
    "# load data accordingly\n",
    "data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "#\n",
    "if moreSampSize:\n",
    "    with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "        DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "        DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "elif minmaxfullSampSize:\n",
    "    with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "        DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "        DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "else:\n",
    "    with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "        sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                    \n",
    "weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "#sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][basecondition]\n",
    "sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "# sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "\n",
    "weighted_graphs_base = weighted_graphs_tgt\n",
    "\n",
    "sig_edges_base = sig_edges_tgt\n",
    "\n",
    "sig_avg_dags_base =  weighted_graphs_base.mean(axis = 0) * sig_edges_base\n",
    "    \n",
    "for ilag in np.arange(0,ntime_lags,1):\n",
    "\n",
    "    time_lag_name = time_lags[ilag]\n",
    "    fromRowID = fromRowIDs[ilag]\n",
    "\n",
    "    for idate in np.arange(0,ndates_plot,1):\n",
    "            \n",
    "        # try:\n",
    "\n",
    "            idate_typename = dates_list_plot[idate]\n",
    "\n",
    "            # load data accordingly\n",
    "            data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_withNeuron_SuccAndFailedPull_allsessions'+savefile_sufix+'_3lags/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "            #\n",
    "            if moreSampSize:\n",
    "                with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_moreSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            elif minmaxfullSampSize:\n",
    "                with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'_minmaxfullSampSize.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "            else:\n",
    "                with open(data_saved_subfolder+'/DAGscores_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    DAGscores_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/DAGscores_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    DAGscores_shuffled_diffTempRo_diffSampSize = pickle.load(f) \n",
    "                with open(data_saved_subfolder+'/weighted_graphs_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/weighted_graphs_shuffled_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    weighted_graphs_shuffled_diffTempRo_diffSampSize = pickle.load(f)\n",
    "                with open(data_saved_subfolder+'/sig_edges_diffTempRo_diffSampSize_'+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pkl', 'rb') as f:\n",
    "                    sig_edges_diffTempRo_diffSampSize = pickle.load(f)\n",
    "\n",
    "            weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][idate_typename]\n",
    "            weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][idate_typename]\n",
    "            # sig_edges_tgt = sig_edges_diffTempRo_diffSampSize[(pulltype_forplot,str(temp_resolu),j_sampsize_name)][idate_typename]\n",
    "            sig_edges_tgt = get_significant_edges(weighted_graphs_tgt,weighted_graphs_shuffled_tgt)\n",
    "           \n",
    "            #sig_edges_tgt = sig_edges_tgt*((weighted_graphs_tgt.mean(axis=0)>0.5)*1)\n",
    "            \n",
    "            if 0:\n",
    "                weighted_graphs_delta = (weighted_graphs_tgt-weighted_graphs_base)\n",
    "                weighted_graphs_delta = weighted_graphs_delta.mean(axis=0)\n",
    "                #\n",
    "                sig_edges_delta = ((sig_edges_tgt+sig_edges_base)>0)*1\n",
    "            else:\n",
    "                weighted_graphs_delta,sig_edges_delta = Modulation_Index(weighted_graphs_base, weighted_graphs_tgt,\n",
    "                                                                         sig_edges_base, sig_edges_tgt, 8000)\n",
    "                weighted_graphs_delta = weighted_graphs_delta.mean(axis=0)\n",
    "                \n",
    "            sig_avg_dags = weighted_graphs_delta * sig_edges_delta\n",
    "            sig_avg_dags = sig_avg_dags[fromRowID,:]\n",
    "            sig_avg_dags = sig_avg_dags[:,toColID]\n",
    "\n",
    "            # plot\n",
    "            axs[ilag*2+0,idate].set_title(idate_typename,fontsize=18)\n",
    "            axs[ilag*2+0,idate].set_xlim([-1,2])\n",
    "            axs[ilag*2+0,idate].set_ylim([-1,2])\n",
    "            axs[ilag*2+0,idate].set_xticks([])\n",
    "            axs[ilag*2+0,idate].set_xticklabels([])\n",
    "            axs[ilag*2+0,idate].set_yticks([])\n",
    "            axs[ilag*2+0,idate].set_yticklabels([])\n",
    "            axs[ilag*2+0,idate].spines['top'].set_visible(False)\n",
    "            axs[ilag*2+0,idate].spines['right'].set_visible(False)\n",
    "            axs[ilag*2+0,idate].spines['bottom'].set_visible(False)\n",
    "            axs[ilag*2+0,idate].spines['left'].set_visible(False)\n",
    "            # axs[ilag*2+0,idate].axis('equal')\n",
    "\n",
    "            for ieventnode in np.arange(0,nevents,1):\n",
    "                # plot the event nodes\n",
    "                axs[ilag*2+0,idate].plot(eventnode_locations[ieventnode][0],eventnode_locations[ieventnode][1],\n",
    "                                              eventnodes_shape[ieventnode],markersize=45,markerfacecolor=eventnodes_color[ieventnode],\n",
    "                                              markeredgecolor='none')              \n",
    "                \n",
    "                axs[ilag*2+0,idate].text(eventname_locations[ieventnode][0],eventname_locations[ieventnode][1],\n",
    "                                       eventnames[ieventnode],fontsize=10)\n",
    "                \n",
    "                clmap = mpl.cm.get_cmap('bwr')\n",
    "                \n",
    "                # plot the event edges\n",
    "                for ifromNode in np.arange(0,nevents,1):\n",
    "                    for itoNode in np.arange(0,nevents,1):\n",
    "                        edge_weight_tgt = sig_avg_dags[ifromNode,itoNode]\n",
    "                        if edge_weight_tgt!=0:\n",
    "                            if not ifromNode == itoNode:\n",
    "                                #axs[ilag*2+0,idate].plot(eventnode_locations[ifromNode],eventnode_locations[itoNode],'k-',linewidth=edge_weight_tgt*3)\n",
    "                                axs[ilag*2+0,idate].arrow(nodearrow_locations[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                        nodearrow_directions[ifromNode][itoNode][1],\n",
    "                                                        # head_width=0.08*abs(edge_weight_tgt),\n",
    "                                                        # width=0.04*abs(edge_weight_tgt),\n",
    "                                                        head_width=0.08,\n",
    "                                                        width=0.04,       \n",
    "                                                        color = clmap((1+edge_weight_tgt)/2))\n",
    "                            if ifromNode == itoNode:\n",
    "                                ring = mpatches.Wedge(nodearrow_locations[ifromNode][itoNode],\n",
    "                                                      .1, nodearrow_directions[ifromNode][itoNode][0],\n",
    "                                                      nodearrow_directions[ifromNode][itoNode][1], \n",
    "                                                      # 0.04*abs(edge_weight_tgt)\n",
    "                                                      0.04\n",
    "                                                     )\n",
    "                                p = PatchCollection(\n",
    "                                    [ring], \n",
    "                                    facecolor=clmap((1+edge_weight_tgt)/2), \n",
    "                                    edgecolor=clmap((1+edge_weight_tgt)/2)\n",
    "                                )\n",
    "                                axs[ilag*2+0,idate].add_collection(p)\n",
    "                                # add arrow head\n",
    "                                if ifromNode < 2:\n",
    "                                    axs[ilag*2+0,idate].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,-0.05,color=clmap((1+edge_weight_tgt)/2),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "                                elif ifromNode < 4:\n",
    "                                    axs[ilag*2+0,idate].arrow(nodearrow_locations[ifromNode][itoNode][0]-0.1+0.02*edge_weight_tgt,\n",
    "                                                            nodearrow_locations[ifromNode][itoNode][1],\n",
    "                                                            0,0.02,color=clmap((1+edge_weight_tgt)/2),\n",
    "                                                            # head_width=0.08*edge_weight_tgt,width=0.04*edge_weight_tgt\n",
    "                                                            head_width=0.08,width=0.04      \n",
    "                                                            )\n",
    "\n",
    "            # heatmap for the weights\n",
    "            sig_avg_dags_df = pd.DataFrame(sig_avg_dags)\n",
    "            sig_avg_dags_df.columns = eventnames\n",
    "            sig_avg_dags_df.index = eventnames\n",
    "            vmin,vmax = -1,1\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            im = axs[ilag*2+1,idate].pcolormesh(sig_avg_dags_df,cmap=\"bwr\",norm=norm)\n",
    "            #-\n",
    "            if idate == ndates_plot-1:\n",
    "                cax = axs[ilag*2+1,idate].inset_axes([1.04, 0.2, 0.05, 0.8])\n",
    "                fig.colorbar(im, ax=axs[ilag*2+1,idate], cax=cax,label='edge confidence')\n",
    "\n",
    "            axs[ilag*2+1,idate].axis('equal')\n",
    "            axs[ilag*2+1,idate].set_xlabel('to Node',fontsize=14)\n",
    "            axs[ilag*2+1,idate].set_xticks(np.arange(0.5,5.5,1))\n",
    "            axs[ilag*2+1,idate].set_xticklabels(eventnames)\n",
    "            if idate == 0:\n",
    "                axs[ilag*2+1,idate].set_ylabel('from Node',fontsize=14)\n",
    "                axs[ilag*2+1,idate].set_yticks(np.arange(0.5,5.5,1))\n",
    "                axs[ilag*2+1,idate].set_yticklabels(eventnames)\n",
    "                axs[ilag*2+1,idate].text(-1.5,1,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "                axs[ilag*2+0,idate].text(-1.25,0,time_lag_name+' time lag',rotation=90,fontsize=20)\n",
    "            else:\n",
    "                axs[ilag*2+1,idate].set_yticks([])\n",
    "                axs[ilag*2+1,idate].set_yticklabels([])\n",
    "\n",
    "        # except:\n",
    "        #     continue\n",
    "    \n",
    "    \n",
    "savefigs = 1    \n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_neuron_SuccAndFailedPull_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+idate_typename[0:7]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if moreSampSize:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+str(j_sampsize_name)+'_rows_EdgeFifferenceFrom_'+basecondition+'AsBase.pdf')\n",
    "    else:\n",
    "        plt.savefig(figsavefolder+\"threeTimeLag_DAGs_\"+pulltype_forplot+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'_'+str(temp_resolu)+'_'+j_sampsize_name+'_EdgeFifferenceFrom_'+basecondition+'AsBase.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dabe20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03fb83aa",
   "metadata": {},
   "source": [
    "### plot the edges over time (session)\n",
    "#### mean edge weights of selected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30602b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100: self; 3: 3s coop; 2: 2s coop; 1.5: 1.5s coop; 1: 1s coop; -1: no-vision\n",
    "tasktypes_all_dates[tasktypes_all_dates==5] = -1 # change the task type code for no-vision\n",
    "coopthres_forsort = (tasktypes_all_dates-1)*coopthres_all_dates/2\n",
    "coopthres_forsort[coopthres_forsort==0] = 100 # get the cooperation threshold for sorting\n",
    "\n",
    "#\n",
    "# sort the data based on task type and dates\n",
    "sorting_df = pd.DataFrame({'dates': dates_list, 'coopthres': coopthres_forsort.ravel()}, columns=['dates', 'coopthres'])\n",
    "sorting_df = sorting_df.sort_values(by=['coopthres','dates'], ascending = [False, True])\n",
    "dates_list_sorted = np.array(dates_list)[sorting_df.index]\n",
    "ndates_sorted = np.shape(dates_list_sorted)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 0.5s time lag\n",
    "edges_target_names = [['0.5slag_pull2_pull1','0.5slag_pull1_pull2'],\n",
    "                      ['0.5slag_gaze1_pull1','0.5slag_gaze2_pull2'],\n",
    "                      ['0.5slag_pull2_gaze1','0.5slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[11,10],\n",
    "                [12,13],\n",
    "                [ 9, 8],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(10*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_sorted,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_sorted,1):\n",
    "        idate_name = dates_list_sorted[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_mean_forplot_all_dates,'ko',markersize=10)\n",
    "    #axs.flatten()[i_edge].plot(np.arange(0,ndates_sorted,1),edgeweight_shuffled_mean_forplot_all_dates,'bo',markersize=10)\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    axs.flatten()[i_edge].set_xlim([-0.5,ndates_sorted-0.5])\n",
    "    #\n",
    "    if i_edge > int(n_edges-1):\n",
    "        axs.flatten()[i_edge].set_xticks(np.arange(0,ndates_sorted,1))\n",
    "        axs.flatten()[i_edge].set_xticklabels(dates_list_sorted, rotation=90,fontsize=10)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    tasktypes = ['self','coop(3s)','coop(2s)','coop(1.5s)','coop(1s)','no-vision']\n",
    "    taskswitches = np.where(np.array(sorting_df['coopthres'])[1:]-np.array(sorting_df['coopthres'])[:-1]!=0)[0]+0.5\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].plot([taskswitch,taskswitch],[-0.1,1.1],'k--')\n",
    "    taskswitches = np.concatenate(([0],taskswitches))\n",
    "    for itaskswitch in np.arange(0,np.shape(taskswitches)[0],1):\n",
    "        taskswitch = taskswitches[itaskswitch]\n",
    "        axs.flatten()[i_edge].text(taskswitch+0.25,-0.05,tasktypes[itaskswitch],fontsize=10)\n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_neuron_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweight_acrossAllSessions_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5ffd",
   "metadata": {},
   "source": [
    "#### mean edge weights of selected edges v.s. other behavioral measures\n",
    "##### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the targeted dates\n",
    "sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)|(sorting_df['coopthres']==2)|(sorting_df['coopthres']==3)]\n",
    "# sorting_tgt_df = sorting_df[(sorting_df['coopthres']==1)]\n",
    "dates_list_tgt = sorting_tgt_df['dates']\n",
    "dates_list_tgt = np.array(dates_list_tgt)\n",
    "#\n",
    "ndates_tgt = np.shape(dates_list_tgt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these variables are the same as in the previous steps\n",
    "# temp_resolus = [0.5,1,1.5,2] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "temp_resolus = [0.5] # temporal resolution in the DBN model, eg: 0.5 means 500ms\n",
    "ntemp_reses = np.shape(temp_resolus)[0]\n",
    "#\n",
    "if moreSampSize:\n",
    "    # different data (down/re)sampling numbers\n",
    "    # samplingsizes = np.arange(1100,3000,100)\n",
    "    samplingsizes = [1100]\n",
    "    # samplingsizes = [100,500,1000,1500,2000,2500,3000]        \n",
    "    # samplingsizes = [100,500]\n",
    "    # samplingsizes_name = ['100','500','1000','1500','2000','2500','3000']\n",
    "    samplingsizes_name = list(map(str, samplingsizes))\n",
    "elif minmaxfullSampSize:\n",
    "    samplingsizes_name = ['full_row_number']   \n",
    "nsamplings = np.shape(samplingsizes_name)[0]\n",
    "\n",
    "temp_resolu = temp_resolus[0]\n",
    "j_sampsize_name = samplingsizes_name[0]   \n",
    "\n",
    "# 0.5s time lag\n",
    "edges_target_names = [['0.5slag_pull2_pull1','0.5slag_pull1_pull2'],\n",
    "                      ['0.5slag_gaze1_pull1','0.5slag_gaze2_pull2'],\n",
    "                      ['0.5slag_pull2_gaze1','0.5slag_pull1_gaze2'],]\n",
    "fromNodesIDs = [[11,10],\n",
    "                [12,13],\n",
    "                [11,10],]\n",
    "toNodesIDs = [[0,1],\n",
    "              [0,1],\n",
    "              [2,3]]\n",
    "\n",
    "#\n",
    "xplottype = 'succrate' # 'succrate', 'meangazenum'\n",
    "xplotlabel = 'successful rate' # 'successful rate', 'mean gaze number'\n",
    "# xplottype = 'meangazenum' # 'succrate', 'meangazenum'\n",
    "# xplotlabel = 'mean gaze number' # 'successful rate', 'mean gaze number'\n",
    "\n",
    "n_edges = np.shape(np.array(edges_target_names).flatten())[0]\n",
    "\n",
    "# figure initiate\n",
    "fig, axs = plt.subplots(int(np.ceil(n_edges/2)),2)\n",
    "fig.set_figheight(5*np.ceil(n_edges/2))\n",
    "fig.set_figwidth(5*2)\n",
    "\n",
    "#\n",
    "for i_edge in np.arange(0,n_edges,1):\n",
    "    #\n",
    "    edgeweight_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_mean_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    edgeweight_shuffled_std_forplot_all_dates = np.zeros((ndates_tgt,1))\n",
    "    \n",
    "    edge_tgt_name = np.array(edges_target_names).flatten()[i_edge]\n",
    "    fromNodesID = np.array(fromNodesIDs).flatten()[i_edge]\n",
    "    toNodesID = np.array(toNodesIDs).flatten()[i_edge]\n",
    "    \n",
    "    for idate in np.arange(0,ndates_tgt,1):\n",
    "        idate_name = dates_list_tgt[idate]\n",
    "        \n",
    "        weighted_graphs_tgt = weighted_graphs_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "        weighted_graphs_shuffled_tgt = weighted_graphs_shuffled_diffTempRo_diffSampSize[(str(temp_resolu),j_sampsize_name)][idate_name]\n",
    "    \n",
    "        edgeweight_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_mean_forplot_all_dates[idate] = np.nanmean(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_tgt[:,fromNodesID,toNodesID])\n",
    "        edgeweight_shuffled_std_forplot_all_dates[idate] = np.nanstd(weighted_graphs_shuffled_tgt[:,fromNodesID,toNodesID])\n",
    "        \n",
    "      \n",
    "    # plot \n",
    "    if xplottype == 'succrate':\n",
    "        xxx = succ_rate_all_dates[sorting_tgt_df.index]\n",
    "    elif xplottype == 'meangazenum':   \n",
    "        xxx = gazemean_num_all_dates[sorting_tgt_df.index]\n",
    "    #     \n",
    "    yyy = edgeweight_mean_forplot_all_dates\n",
    "    #\n",
    "    rr_spe,pp_spe = scipy.stats.spearmanr(xxx, yyy)\n",
    "    slope, intercept, rr_reg, pp_reg, std_err = st.linregress(xxx.astype(float).T[0], yyy.astype(float).T[0])\n",
    "    #\n",
    "    axs.flatten()[i_edge].plot(xxx,yyy,'bo',markersize=8)\n",
    "    axs.flatten()[i_edge].plot(np.array([xxx.min(),xxx.max()]),np.array([xxx.min(),xxx.max()])*slope+intercept,'k-')\n",
    "    #\n",
    "    axs.flatten()[i_edge].set_title(edge_tgt_name,fontsize=16)\n",
    "    axs.flatten()[i_edge].set_ylabel('mean edge weight',fontsize=13)\n",
    "    axs.flatten()[i_edge].set_ylim([-0.1,1.1])\n",
    "    #\n",
    "    if i_edge > int(n_edges-3):\n",
    "        axs.flatten()[i_edge].set_xlabel(xplotlabel,fontsize=13)\n",
    "    else:\n",
    "        axs.flatten()[i_edge].set_xticklabels('')\n",
    "    #\n",
    "    # axs.flatten()[i_edge].text(xxx.min(),1.0,'spearman r='+\"{:.2f}\".format(rr_spe),fontsize=10)\n",
    "    # axs.flatten()[i_edge].text(xxx.min(),0.9,'spearman p='+\"{:.2f}\".format(pp_spe),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),1.0,'regression r='+\"{:.2f}\".format(rr_reg),fontsize=10)\n",
    "    axs.flatten()[i_edge].text(xxx.min(),0.9,'regression p='+\"{:.2f}\".format(pp_reg),fontsize=10)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "savefigs = 0\n",
    "if savefigs:\n",
    "    figsavefolder = data_saved_folder+'figs_for_3LagDBN_and_neuron_bhv_singlecam_wholebodylabels_allsessions_basicEvents/'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    plt.savefig(figsavefolder+\"edgeweights_vs_\"+xplottype+\"_\"+animal1_fixedorder[0]+animal2_fixedorder[0]+'.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02539e5",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "####  mean edge weights of selected edges v.s. other behavioral measures\n",
    "##### only the cooperation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399f140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f503348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c034114a",
   "metadata": {},
   "source": [
    "## Plots that include all pairs\n",
    "####  plot the coorelation between pull time, and social gaze time\n",
    "#### pull <-> pull; with animal gaze -> pull; across animal pull -> gaze; with animal pull -> gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55324239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659e669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0321a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
