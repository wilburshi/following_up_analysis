{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### analyze the spike triggered pull and gaze ditribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multitest import multipletests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            \n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "            \n",
    "                            4,\n",
    "                            4,    \n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                            4,\n",
    "                       ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "                              ] # in second\n",
    "        kilosortvers = [\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "            \n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                        ]\n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger',\n",
    "                               'koala','koala','koala',\n",
    "                               'vermelho','vermelho','vermelho',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Koala\",\"Koala\",\"Koala\",\n",
    "                             \"Vermelho\",\"Vermelho\",\"Vermelho\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0:\n",
    "    neural_record_conditions = ['20240509_Kanga_MC']\n",
    "    dates_list = [\"20240509\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC']\n",
    "    session_start_times = [36.0] # in second\n",
    "    kilosortvers = [4]\n",
    "    animal1_fixedorders = ['dannon']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dannon\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0:\n",
    "    neural_record_conditions = ['20240531_Dodson_MC_and_SR']\n",
    "    dates_list = [\"20240531\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_and_SR']\n",
    "    session_start_times = [0.0] # in second\n",
    "    kilosortvers = [4]\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['ginger']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Ginger\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "spike_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvevents_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "strategy_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "strategy_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        spike_trig_events_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        strategy_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "\n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "        pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "\n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "  \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "            \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "                \n",
    "        # # plot behavioral events\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "                plot_bhv_events(date_tgt,animal1, animal2, session_start_time, totalsess_time, time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "        else:\n",
    "                plot_bhv_events(date_tgt,animal2, animal1, session_start_time, totalsess_time, time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1)\n",
    "        #\n",
    "        # save behavioral events plot\n",
    "        if 0:\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            plt.savefig(data_saved_folder+\"/bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/'+date_tgt+\"_\"+cameraID_short+\".pdf\")\n",
    "\n",
    "        #\n",
    "        owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "        owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "        mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "        mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "\n",
    "     \n",
    "        \n",
    "        # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "        # could be used for define time bin for DBN\n",
    "        if 1:\n",
    "            _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                         oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "            #\n",
    "            pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "            bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                            'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 1:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 0: \n",
    "                print('plot event aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_average_all,bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_aligned_FR_all_dates[date_tgt] = bhvevents_aligned_FR_average_all\n",
    "                bhvevents_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "            \n",
    "            # the three strategy aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot strategy aligned firing rate')\n",
    "                #\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                stg_twins = 1.5 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                strategy_aligned_FR_average_all,strategy_aligned_FR_allevents_all = plot_strategy_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,stg_twins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                strategy_aligned_FR_all_dates[date_tgt] = strategy_aligned_FR_average_all\n",
    "                strategy_aligned_FR_allevents_all_dates[date_tgt] = strategy_aligned_FR_allevents_all\n",
    "                \n",
    "            \n",
    "            #\n",
    "            # Run PCA analysis\n",
    "            FR_zscore_allch_np_merged = np.array(pd.DataFrame(FR_zscore_allch).T)\n",
    "            FR_zscore_allch_np_merged = FR_zscore_allch_np_merged[~np.isnan(np.sum(FR_zscore_allch_np_merged,axis=1)),:]\n",
    "            # # run PCA on the entire session\n",
    "            pca = PCA(n_components=3)\n",
    "            FR_zscore_allch_PCs = pca.fit_transform(FR_zscore_allch_np_merged.T)\n",
    "            #\n",
    "            # # run PCA around the -PCAtwins to PCAtwins for each behavioral events\n",
    "            PCAtwins = 4 # 5 second\n",
    "            gaze_thresold = 0.5 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "            savefigs = 0 \n",
    "            if 0:\n",
    "                PCA_around_bhv_events(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                              savefigs,data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            if 0:\n",
    "                if (np.isin(animal1, ['dodson'])) | (np.isin(animal2, ['kanga'])):\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                                      oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "                elif (np.isin(animal2, ['dodson'])) | (np.isin(animal1, ['kanga'])):\n",
    "                    time_point_pulls_succfail_rev = time_point_pulls_succfail.copy()\n",
    "                    time_point_pulls_succfail_rev['pull1_succ'] = time_point_pulls_succfail['pull2_succ']\n",
    "                    time_point_pulls_succfail_rev['pull1_fail'] = time_point_pulls_succfail['pull2_fail']\n",
    "                    time_point_pulls_succfail_rev['pull2_succ'] = time_point_pulls_succfail['pull1_succ']\n",
    "                    time_point_pulls_succfail_rev['pull2_fail'] = time_point_pulls_succfail['pull1_fail']\n",
    "                    PCA_around_bhv_events_video(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull2,time_point_pull1,time_point_pulls_succfail_rev, \n",
    "                                      oneway_gaze2,oneway_gaze1,mutual_gaze2,mutual_gaze1,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                                      data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # do the spike triggered average of different bhv variables, for the single camera tracking, look at the pulling and social gaze actions\n",
    "            # the goal is to get a sense for glm\n",
    "            if 0: \n",
    "                print('plot spike triggered bhv variables')\n",
    "\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                do_shuffle = 0\n",
    "                #\n",
    "                min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "                #\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                \n",
    "                gaze_thresold = 0.2\n",
    "                \n",
    "                stg_twins = 3 # 3s, the behavioral event interval used to define strategy, consistent with DBN 3s time lags\n",
    "                #\n",
    "                spike_trig_average_all =  plot_spike_triggered_singlecam_bhvevent(date_tgt,savefig,save_path, animal1, animal2, session_start_time,min_length, trig_twins,\n",
    "                                                                              stg_twins, time_point_pull1, time_point_pull2, time_point_pulls_succfail,\n",
    "                                                                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,animalnames_videotrack,\n",
    "                                                                              spike_clusters_data, spike_time_data,spike_channels_data,do_shuffle)\n",
    "\n",
    "                spike_trig_events_all_dates[date_tgt] = spike_trig_average_all\n",
    "\n",
    "            \n",
    "        # load filtered lfp\n",
    "        if 0:\n",
    "            print('load LFP data for '+neural_record_condition)\n",
    "            lfp_filt_filename = neural_data_folder+neural_record_condition+'/lfp_filt_subsample.txt' # already downsample to 30Hz\n",
    "            lfp_filt_data_df = genfromtxt(lfp_filt_filename, delimiter=',')\n",
    "            # aligned to the session start\n",
    "            lfp_filt_sess_aligned=lfp_filt_data_df[:,int(-neural_start_time_session_start_offset*30):]\n",
    "            # normalize the activity to 0 - 1\n",
    "            lfp_filt_sess_aligned = (lfp_filt_sess_aligned-np.min(lfp_filt_sess_aligned))/(np.max(lfp_filt_sess_aligned)-np.min(lfp_filt_sess_aligned))\n",
    "\n",
    "        \n",
    "        # plot the tracking demo video\n",
    "        if 0: \n",
    "            print('make the demo videos')\n",
    "            if 0:\n",
    "                # all the bhv traces in the same panel\n",
    "                tracking_video_singlecam_wholebody_withNeuron_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                  lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                  animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                  animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                                  video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body,\n",
    "                                                  spike_time_data,lfp_filt_sess_aligned,spike_channels_data,channel_to_depth)\n",
    "            if 1:\n",
    "                # all the bhv traces are in separate panels\n",
    "                tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                 lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                 animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                 animal1_filename,animal2_filename,session_start_time,fps,nframes,cameraID,\n",
    "                                                 video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body,\n",
    "                                                 spike_time_data,lfp_filt_sess_aligned,spike_channels_data,channel_to_depth)\n",
    "        \n",
    "        # plot the example frame from the tracking demo video\n",
    "        if 0: \n",
    "            print('print the example frame from the demo videos')\n",
    "            if 1:\n",
    "                example_frame = 60*30+1\n",
    "                start_frame = 55*30\n",
    "                # all the bhv traces are in separate panels\n",
    "                tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo(bodyparts_locs_camI,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                 lever_locs_camI,tube_locs_camI,time_point_pull1,time_point_pull2,\n",
    "                                                 animalnames_videotrack,bodypartnames_videotrack,date_tgt,\n",
    "                                                 animal1_filename,animal2_filename,session_start_time,fps,start_frame,example_frame,cameraID,\n",
    "                                                 video_file_original,sqr_thres_tubelever,sqr_thres_face,sqr_thres_body,\n",
    "                                                 spike_time_data,lfp_filt_sess_aligned,spike_channels_data,channel_to_depth)\n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt+\"/\"\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                if savefig:\n",
    "                    plt.savefig(save_path+'singlecam_wholebody_tracking_withNeuron_sepbhv_demo_oneframe.pdf')\n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(spike_trig_events_all_dates, f)  \n",
    "    \n",
    "        with open(data_saved_subfolder+'/bhvevents_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/bhvevents_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_allevents_all_dates, f) \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/strategy_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(strategy_aligned_FR_allevents_all_dates, f) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4d7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf70aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6a57",
   "metadata": {},
   "source": [
    "### plot \n",
    "#### plot the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44922ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fig, axs = plt.subplots(12,1)\n",
    "    fig.set_figheight(20*1)\n",
    "    fig.set_figwidth(5*3)\n",
    "\n",
    "    x_lims = [0,totalsess_time_forFR]\n",
    "    PC1min = np.min(FR_zscore_allch_PCs[:,0])\n",
    "    PC1max = np.max(FR_zscore_allch_PCs[:,0])\n",
    "    PC2min = np.min(FR_zscore_allch_PCs[:,1])\n",
    "    PC2max = np.max(FR_zscore_allch_PCs[:,1])\n",
    "    PC3min = np.min(FR_zscore_allch_PCs[:,2])\n",
    "    PC3max = np.max(FR_zscore_allch_PCs[:,2])\n",
    "\n",
    "    for iplotype in np.arange(0,4,1):\n",
    "\n",
    "        if iplotype == 0:\n",
    "            eventplot  = np.array(time_point_pull1)\n",
    "            eventplotname = 'animal1_pull'\n",
    "        elif iplotype == 1:\n",
    "            eventplot  = np.array(time_point_pull2)\n",
    "            eventplotname = 'animal2_pull'\n",
    "        elif iplotype == 2:\n",
    "            eventplot  = np.hstack([oneway_gaze1,mutual_gaze1])\n",
    "            eventplotname = 'animal1_gaze'\n",
    "        elif iplotype == 3:\n",
    "            eventplot  = np.hstack([oneway_gaze2,mutual_gaze2])\n",
    "            eventplotname = 'animal2_gaze'\n",
    "\n",
    "        # plot 1\n",
    "        nevents = np.shape(eventplot)[0]\n",
    "        for ievent in np.arange(0,nevents,1):\n",
    "            axs[0+3*iplotype].plot([eventplot[ievent],eventplot[ievent]],[PC1min,PC1max],'k-')\n",
    "        axs[0+3*iplotype].set_xlim(x_lims[0],x_lims[1])\n",
    "        axs[0+3*iplotype].set_ylim(PC1min,PC1max)\n",
    "        #\n",
    "        axs[0+3*iplotype].plot(FR_timepoint_allch,FR_zscore_allch_PCs[:,0])\n",
    "        axs[0+3*iplotype].set_xlim(x_lims[0],x_lims[1])\n",
    "        axs[0+3*iplotype].set_ylabel('PC1\\n'+eventplotname)\n",
    "\n",
    "        # plot 2\n",
    "        nevents = np.shape(eventplot)[0]\n",
    "        for ievent in np.arange(0,nevents,1):\n",
    "            axs[1+3*iplotype].plot([eventplot[ievent],eventplot[ievent]],[PC2min,PC2max],'k-')\n",
    "        axs[1+3*iplotype].set_xlim(x_lims[0],x_lims[1])\n",
    "        axs[1+3*iplotype].set_ylim(PC2min,PC2max)\n",
    "        #\n",
    "        axs[1+3*iplotype].plot(FR_timepoint_allch,FR_zscore_allch_PCs[:,1])\n",
    "        axs[1+3*iplotype].set_xlim(x_lims[0],x_lims[1])\n",
    "        axs[1+3*iplotype].set_ylabel('PC2\\n'+eventplotname)\n",
    "\n",
    "        # plot 3\n",
    "        nevents = np.shape(eventplot)[0]\n",
    "        for ievent in np.arange(0,nevents,1):\n",
    "            axs[2+3*iplotype].plot([eventplot[ievent],eventplot[ievent]],[PC3min,PC3max],'k-')\n",
    "        axs[2+3*iplotype].set_xlim(x_lims[0],x_lims[1])\n",
    "        axs[2+3*iplotype].set_ylim(PC3min,PC3max)\n",
    "        #\n",
    "        axs[2+3*iplotype].plot(FR_timepoint_allch,FR_zscore_allch_PCs[:,2])\n",
    "        axs[2+3*iplotype].set_xlim(x_lims[0],x_lims[1])\n",
    "        axs[2+3*iplotype].set_ylabel('PC3\\n'+eventplotname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e471a",
   "metadata": {},
   "source": [
    "#### analyze the bhv aligned firing rate across all dates\n",
    "#### plot the tsne or PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "\n",
    "    bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(bhvevents_aligned_FR_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    if 0:\n",
    "        # normalize FR_average for each unit\n",
    "        nspikeunits = np.shape(bhvevents_aligned_FR_all_dates_df)[0]\n",
    "        for ispikeunit in np.arange(0,nspikeunits,1):\n",
    "            stevent = bhvevents_aligned_FR_all_dates_df['FR_average'][ispikeunit]\n",
    "            stevent_norm = (stevent-np.nanmin(stevent))/(np.nanmax(stevent)-np.nanmin(stevent))\n",
    "            bhvevents_aligned_FR_all_dates_df['FR_average'][ispikeunit] = stevent_norm            \n",
    "\n",
    "    # only focus on the certain act animal and certain bhv_name\n",
    "    # act_animals_all = ['kanga']\n",
    "    # bhv_names_all = ['leverpull_prob']\n",
    "    act_animals_all = np.unique(bhvevents_aligned_FR_all_dates_df['act_animal'])\n",
    "    bhv_names_all = np.unique(bhvevents_aligned_FR_all_dates_df['bhv_name'])\n",
    "    #\n",
    "    nact_animals = np.shape(act_animals_all)[0]\n",
    "    nbhv_names = np.shape(bhv_names_all)[0]\n",
    "\n",
    "    # set for plot\n",
    "    # plot all units\n",
    "    fig1, axs1 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig1.set_figheight(6*nact_animals)\n",
    "    fig1.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different days\n",
    "    fig2, axs2 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig2.set_figheight(6*nact_animals)\n",
    "    fig2.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but seprate different channels\n",
    "    fig3, axs3 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig3.set_figheight(4*nact_animals)\n",
    "    fig3.set_figwidth(4*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different conditions\n",
    "    fig4, axs4 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig4.set_figheight(6*nact_animals)\n",
    "    fig4.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # spike triggered average for different task conditions\n",
    "    # # to be save, prepare for five conditions\n",
    "    fig6, axs6 = plt.subplots(nact_animals*5,nbhv_names)\n",
    "    fig6.set_figheight(6*nact_animals*5)\n",
    "    fig6.set_figwidth(6*nbhv_names)\n",
    "    # fig6, axs6 = plt.subplots(nact_animals,nbhv_names)\n",
    "    # fig6.set_figheight(6*nact_animals)\n",
    "    # fig6.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different k-mean cluster\n",
    "    fig5, axs5 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig5.set_figheight(6*nact_animals)\n",
    "    fig5.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # spike triggered average for different k-mean cluster\n",
    "    # to be save, prepare for 14 clusters\n",
    "    fig7, axs7 = plt.subplots(nact_animals*14,nbhv_names)\n",
    "    fig7.set_figheight(6*nact_animals*14)\n",
    "    fig7.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # stacked bar plot to show the cluster distribution of each conditions\n",
    "    fig8, axs8 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig8.set_figheight(6*nact_animals)\n",
    "    fig8.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    #\n",
    "    for ianimal in np.arange(0,nact_animals,1):\n",
    "\n",
    "        act_animal = act_animals_all[ianimal]\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal)&(bhvevents_aligned_FR_all_dates_df['bhv_name']==bhv_name)\n",
    "\n",
    "            bhvevents_aligned_FR_tgt = np.vstack(list(bhvevents_aligned_FR_all_dates_df[ind]['FR_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(bhvevents_aligned_FR_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            bhvevents_aligned_FR_pca = pca.fit_transform(bhvevents_aligned_FR_tgt)\n",
    "            tsne = TSNE(n_components=2, random_state=0)\n",
    "            bhvevents_aligned_FR_tsne = tsne.fit_transform(bhvevents_aligned_FR_tgt)\n",
    "            #\n",
    "            range_n_clusters = np.arange(2,8,1)\n",
    "            silhouette_avg_all = np.ones(np.shape(range_n_clusters))*np.nan\n",
    "            nkmeancls = np.shape(range_n_clusters)[0]\n",
    "            #\n",
    "            for ikmeancl in np.arange(0,nkmeancls,1):\n",
    "                n_clusters = range_n_clusters[ikmeancl]\n",
    "                #\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                # cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_tgt)\n",
    "                if doPCA:\n",
    "                    cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_pca)\n",
    "                if doTSNE:\n",
    "                    cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_tgt)\n",
    "                    # cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_tsne)\n",
    "                #\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                # silhouette_avg = silhouette_score(bhvevents_aligned_FR_tgt, cluster_labels)\n",
    "                if doPCA:\n",
    "                    silhouette_avg = silhouette_score(bhvevents_aligned_FR_pca, cluster_labels)\n",
    "                if doTSNE:\n",
    "                    silhouette_avg = silhouette_score(bhvevents_aligned_FR_tgt, cluster_labels)\n",
    "                    # silhouette_avg = silhouette_score(bhvevents_aligned_FR_tsne, cluster_labels)\n",
    "                #\n",
    "                silhouette_avg_all[ikmeancl] = silhouette_avg\n",
    "            #\n",
    "            best_k_num = range_n_clusters[silhouette_avg_all==np.nanmax(silhouette_avg_all)][0]\n",
    "            #\n",
    "            clusterer = KMeans(n_clusters=best_k_num, random_state=0)\n",
    "            # kmean_cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_tgt)\n",
    "            if doPCA:\n",
    "                kmean_cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_pca)\n",
    "            if doTSNE:\n",
    "                kmean_cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_tgt)\n",
    "                # kmean_cluster_labels = clusterer.fit_predict(bhvevents_aligned_FR_tsne)\n",
    "\n",
    "\n",
    "            # run PCA and TSNE     \n",
    "            pca = PCA(n_components=2)\n",
    "            tsne = TSNE(n_components=2, random_state=0)\n",
    "            #\n",
    "            bhvevents_aligned_FR_pca = pca.fit_transform(bhvevents_aligned_FR_tgt)\n",
    "            bhvevents_aligned_FR_tsne = tsne.fit_transform(bhvevents_aligned_FR_tgt)\n",
    "\n",
    "            # plot all units\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs1[ianimal,ibhvname].plot(bhvevents_aligned_FR_tsne[:,0],bhvevents_aligned_FR_tsne[:,1],'.')\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs1[ianimal,ibhvname].plot(bhvevents_aligned_FR_pca[:,0],bhvevents_aligned_FR_pca[:,1],'.')\n",
    "\n",
    "            axs1[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs1[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs1[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different dates\n",
    "            dates_forplot = np.unique(bhvevents_aligned_FR_all_dates_df[ind]['dates'])\n",
    "            for idate_forplot in dates_forplot:\n",
    "                ind_idate = list(bhvevents_aligned_FR_all_dates_df[ind]['dates']==idate_forplot)\n",
    "                ind_idate = list(np.array(ind_idate)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs2[ianimal,ibhvname].plot(bhvevents_aligned_FR_tsne[ind_idate,0],bhvevents_aligned_FR_tsne[ind_idate,1],\n",
    "                                            '.',label=idate_forplot)\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs2[ianimal,ibhvname].plot(bhvevents_aligned_FR_pca[ind_idate,0],bhvevents_aligned_FR_pca[ind_idate,1],\n",
    "                                            '.',label=idate_forplot)\n",
    "                #\n",
    "            axs2[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs2[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs2[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs2[ianimal,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different channels\n",
    "            chs_forplot = np.unique(bhvevents_aligned_FR_all_dates_df[ind]['channelID'])\n",
    "            for ich_forplot in chs_forplot:\n",
    "                ind_ich = list(bhvevents_aligned_FR_all_dates_df[ind]['channelID']==ich_forplot)\n",
    "                ind_ich = list(np.array(ind_ich)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs3[ianimal,ibhvname].plot(bhvevents_aligned_FR_tsne[ind_ich,0],bhvevents_aligned_FR_tsne[ind_ich,1],\n",
    "                                            '.',label=str(ich_forplot))\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs3[ianimal,ibhvname].plot(bhvevents_aligned_FR_pca[ind_ich,0],bhvevents_aligned_FR_pca[ind_ich,1],\n",
    "                                            '.',label=str(ich_forplot))\n",
    "                #\n",
    "            axs3[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs3[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs3[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs3[ianimal,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different task conditions\n",
    "            cons_forplot = np.unique(bhvevents_aligned_FR_all_dates_df[ind]['condition'])\n",
    "            for icon_forplot in cons_forplot:\n",
    "                ind_icon = list(bhvevents_aligned_FR_all_dates_df[ind]['condition']==icon_forplot)\n",
    "                ind_icon = list(np.array(ind_icon)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs4[ianimal,ibhvname].plot(bhvevents_aligned_FR_tsne[ind_icon,0],bhvevents_aligned_FR_tsne[ind_icon,1],\n",
    "                                            '.',label=icon_forplot)\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs4[ianimal,ibhvname].plot(bhvevents_aligned_FR_pca[ind_icon,0],bhvevents_aligned_FR_pca[ind_icon,1],\n",
    "                                            '.',label=icon_forplot)\n",
    "                #\n",
    "            axs4[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs4[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs4[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs4[ianimal,ibhvname].legend()\n",
    "\n",
    "            # plot the mean spike trigger average trace across neurons in each condition\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "            #\n",
    "            cons_forplot = np.unique(bhvevents_aligned_FR_all_dates_df[ind]['condition'])\n",
    "            icon_ind = 0\n",
    "            for icon_forplot in cons_forplot:\n",
    "                ind_icon = list(bhvevents_aligned_FR_all_dates_df[ind]['condition']==icon_forplot)\n",
    "                ind_icon = list(np.array(ind_icon)[~ind_nan])\n",
    "                #\n",
    "                mean_trig_trace_icon = np.nanmean(bhvevents_aligned_FR_tgt[ind_icon,:],axis=0)\n",
    "                std_trig_trace_icon = np.nanstd(bhvevents_aligned_FR_tgt[ind_icon,:],axis=0)\n",
    "                sem_trig_trace_icon = np.nanstd(bhvevents_aligned_FR_tgt[ind_icon,:],axis=0)/np.sqrt(np.shape(bhvevents_aligned_FR_tgt[ind_icon,:])[0])\n",
    "                itv95_trig_trace_icon = 1.96*sem_trig_trace_icon\n",
    "                #\n",
    "                if 1:\n",
    "                # plot each trace in a seperate traces\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                               color='#E0E0E0',ecolor='#EEEEEE',label=icon_forplot)\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                                                                  np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xlabel('time (s)')\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].legend()\n",
    "                if 0:\n",
    "                    axs6[ianimal,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                    label=icon_forplot)\n",
    "                    # axs6[ianimal,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                    #                                               np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "                    axs6[ianimal,ibhvname].plot([0,0],[0,0.1],'--k') \n",
    "                    axs6[ianimal,ibhvname].set_xlabel('time (s)')\n",
    "                    axs6[ianimal,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                    axs6[ianimal,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                    axs6[ianimal,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                    axs6[ianimal,ibhvname].legend()\n",
    "                #\n",
    "                icon_ind = icon_ind + 1\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different k-mean clusters\n",
    "            kms_forplot = np.unique(kmean_cluster_labels)\n",
    "            for ikm_forplot in kms_forplot:\n",
    "                ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs5[ianimal,ibhvname].plot(bhvevents_aligned_FR_tsne[ind_ikm,0],bhvevents_aligned_FR_tsne[ind_ikm,1],\n",
    "                                            '.',label=str(ikm_forplot))\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs5[ianimal,ibhvname].plot(bhvevents_aligned_FR_pca[ind_ikm,0],bhvevents_aligned_FR_pca[ind_ikm,1],\n",
    "                                            '.',label=str(ikm_forplot))\n",
    "                #\n",
    "            axs5[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs5[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs5[ianimal,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "            axs5[ianimal,ibhvname].legend()\n",
    "\n",
    "            # plot the mean spike trigger average trace across neurons in each cluster\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "            #\n",
    "            kms_forplot = np.unique(kmean_cluster_labels)\n",
    "            for ikm_forplot in kms_forplot:\n",
    "                ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "                #\n",
    "                mean_trig_trace_ikm = np.nanmean(bhvevents_aligned_FR_tgt[ind_ikm,:],axis=0)\n",
    "                std_trig_trace_ikm = np.nanstd(bhvevents_aligned_FR_tgt[ind_ikm,:],axis=0)\n",
    "                sem_trig_trace_ikm = np.nanstd(bhvevents_aligned_FR_tgt[ind_ikm,:],axis=0)/np.sqrt(np.shape(bhvevents_aligned_FR_tgt[ind_ikm,:])[0])\n",
    "                itv95_trig_trace_ikm = 1.96*sem_trig_trace_ikm\n",
    "                #\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].errorbar(xxx_forplot,mean_trig_trace_ikm,yerr=itv95_trig_trace_ikm,\n",
    "                                                              color='#E0E0E0',ecolor='#EEEEEE',label='cluster#'+str(ikm_forplot))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_ikm-itv95_trig_trace_ikm),\n",
    "                                                                 np.nanmax(mean_trig_trace_ikm+itv95_trig_trace_ikm)],'--k')\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xlabel('time (s)')\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # stacked bar plot to show the cluster distribution of each conditions\n",
    "            df = pd.DataFrame({'cond':np.array(bhvevents_aligned_FR_all_dates_df[ind]['condition'])[~ind_nan],\n",
    "                               'cluID':kmean_cluster_labels})\n",
    "            (df.groupby('cond')['cluID'].value_counts(normalize=True)\n",
    "               .unstack('cluID').plot.bar(stacked=True, ax=axs8[ianimal,ibhvname]))\n",
    "            axs8[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/bhvAlignedFRAver_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doTSNE:\n",
    "            fig1.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "            fig2.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "            fig3.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "            fig4.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "            fig5.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig6.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')        \n",
    "            fig7.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig8.savefig(figsavefolder+'bhv_aligned_FR_tsne_clusters_kmeanclusters_propotion_each_condition'+savefile_sufix+'.pdf')\n",
    "\n",
    "        if doPCA:\n",
    "            fig1.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "            fig2.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "            fig3.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "            fig4.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "            fig5.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig6.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')                           \n",
    "            fig7.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig8.savefig(figsavefolder+'bhv_aligned_FR_pca_clusters_kmeanclusters_propotion_each_condition'+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f94da",
   "metadata": {},
   "source": [
    "#### analyze the spike triggered behavioral variables across all dates\n",
    "#### plot the tsne or PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a556411",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "\n",
    "    spike_trig_events_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','st_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        act_animals = list(spike_trig_events_all_dates[date_tgt].keys())\n",
    "\n",
    "        for iact_animal in act_animals:\n",
    "\n",
    "            bhv_types = list(spike_trig_events_all_dates[date_tgt][iact_animal].keys())\n",
    "\n",
    "            for ibhv_type in bhv_types:\n",
    "\n",
    "                clusterIDs = list(spike_trig_events_all_dates[date_tgt][iact_animal][ibhv_type].keys())\n",
    "\n",
    "                for iclusterID in clusterIDs:\n",
    "\n",
    "                    ichannelID = spike_trig_events_all_dates[date_tgt][iact_animal][ibhv_type][iclusterID]['ch']\n",
    "                    ist_average = spike_trig_events_all_dates[date_tgt][iact_animal][ibhv_type][iclusterID]['st_average']\n",
    "\n",
    "                    spike_trig_events_all_dates_df = spike_trig_events_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                            'condition':task_condition,\n",
    "                                                                                            'act_animal': iact_animal, \n",
    "                                                                                            'bhv_name': ibhv_type,\n",
    "                                                                                            'clusterID':iclusterID,\n",
    "                                                                                            'channelID':ichannelID,\n",
    "                                                                                            'st_average':ist_average,\n",
    "                                                                                           }, ignore_index=True)\n",
    "    if 0:\n",
    "        # normalize st_average for each unit\n",
    "        nspikeunits = np.shape(spike_trig_events_all_dates_df)[0]\n",
    "        for ispikeunit in np.arange(0,nspikeunits,1):\n",
    "            stevent = spike_trig_events_all_dates_df['st_average'][ispikeunit]\n",
    "            stevent_norm = (stevent-np.nanmin(stevent))/(np.nanmax(stevent)-np.nanmin(stevent))\n",
    "            spike_trig_events_all_dates_df['st_average'][ispikeunit] = stevent_norm            \n",
    "\n",
    "    # only focus on the certain act animal and certain bhv_name\n",
    "    # act_animals_all = ['kanga']\n",
    "    # bhv_names_all = ['leverpull_prob']\n",
    "    act_animals_all = np.unique(spike_trig_events_all_dates_df['act_animal'])\n",
    "    bhv_names_all = np.unique(spike_trig_events_all_dates_df['bhv_name'])\n",
    "    #\n",
    "    nact_animals = np.shape(act_animals_all)[0]\n",
    "    nbhv_names = np.shape(bhv_names_all)[0]\n",
    "\n",
    "    # set for plot\n",
    "    # plot all units\n",
    "    fig1, axs1 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig1.set_figheight(6*nact_animals)\n",
    "    fig1.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different days\n",
    "    fig2, axs2 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig2.set_figheight(6*nact_animals)\n",
    "    fig2.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but seprate different channels\n",
    "    fig3, axs3 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig3.set_figheight(4*nact_animals)\n",
    "    fig3.set_figwidth(4*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different conditions\n",
    "    fig4, axs4 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig4.set_figheight(6*nact_animals)\n",
    "    fig4.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # spike triggered average for different task conditions\n",
    "    # # to be save, prepare for five conditions\n",
    "    fig6, axs6 = plt.subplots(nact_animals*5,nbhv_names)\n",
    "    fig6.set_figheight(6*nact_animals*5)\n",
    "    fig6.set_figwidth(6*nbhv_names)\n",
    "    # fig6, axs6 = plt.subplots(nact_animals,nbhv_names)\n",
    "    # fig6.set_figheight(6*nact_animals)\n",
    "    # fig6.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different k-mean cluster\n",
    "    fig5, axs5 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig5.set_figheight(6*nact_animals)\n",
    "    fig5.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # spike triggered average for different k-mean cluster\n",
    "    # to be save, prepare for 14 clusters\n",
    "    fig7, axs7 = plt.subplots(nact_animals*14,nbhv_names)\n",
    "    fig7.set_figheight(6*nact_animals*14)\n",
    "    fig7.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # stacked bar plot to show the cluster distribution of each conditions\n",
    "    fig8, axs8 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig8.set_figheight(6*nact_animals)\n",
    "    fig8.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    #\n",
    "    for ianimal in np.arange(0,nact_animals,1):\n",
    "\n",
    "        act_animal = act_animals_all[ianimal]\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (spike_trig_events_all_dates_df['act_animal']==act_animal)&(spike_trig_events_all_dates_df['bhv_name']==bhv_name)\n",
    "\n",
    "            spike_trig_events_tgt = np.vstack(list(spike_trig_events_all_dates_df[ind]['st_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(spike_trig_events_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            spike_trig_events_tgt = spike_trig_events_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            spike_trig_events_pca = pca.fit_transform(spike_trig_events_tgt)\n",
    "            tsne = TSNE(n_components=2, random_state=0)\n",
    "            spike_trig_events_tsne = tsne.fit_transform(spike_trig_events_tgt)\n",
    "            #\n",
    "            range_n_clusters = np.arange(2,8,1)\n",
    "            silhouette_avg_all = np.ones(np.shape(range_n_clusters))*np.nan\n",
    "            nkmeancls = np.shape(range_n_clusters)[0]\n",
    "            #\n",
    "            for ikmeancl in np.arange(0,nkmeancls,1):\n",
    "                n_clusters = range_n_clusters[ikmeancl]\n",
    "                #\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                # cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "                if doPCA:\n",
    "                    cluster_labels = clusterer.fit_predict(spike_trig_events_pca)\n",
    "                if doTSNE:\n",
    "                    cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "                    # cluster_labels = clusterer.fit_predict(spike_trig_events_tsne)\n",
    "                #\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                # silhouette_avg = silhouette_score(spike_trig_events_tgt, cluster_labels)\n",
    "                if doPCA:\n",
    "                    silhouette_avg = silhouette_score(spike_trig_events_pca, cluster_labels)\n",
    "                if doTSNE:\n",
    "                    silhouette_avg = silhouette_score(spike_trig_events_tgt, cluster_labels)\n",
    "                    # silhouette_avg = silhouette_score(spike_trig_events_tsne, cluster_labels)\n",
    "                #\n",
    "                silhouette_avg_all[ikmeancl] = silhouette_avg\n",
    "            #\n",
    "            best_k_num = range_n_clusters[silhouette_avg_all==np.nanmax(silhouette_avg_all)][0]\n",
    "            #\n",
    "            clusterer = KMeans(n_clusters=best_k_num, random_state=0)\n",
    "            # kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "            if doPCA:\n",
    "                kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_pca)\n",
    "            if doTSNE:\n",
    "                kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "                # kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_tsne)\n",
    "\n",
    "\n",
    "            # run PCA and TSNE     \n",
    "            pca = PCA(n_components=2)\n",
    "            tsne = TSNE(n_components=2, random_state=0)\n",
    "            #\n",
    "            spike_trig_events_pca = pca.fit_transform(spike_trig_events_tgt)\n",
    "            spike_trig_events_tsne = tsne.fit_transform(spike_trig_events_tgt)\n",
    "\n",
    "            # plot all units\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs1[ianimal,ibhvname].plot(spike_trig_events_tsne[:,0],spike_trig_events_tsne[:,1],'.')\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs1[ianimal,ibhvname].plot(spike_trig_events_pca[:,0],spike_trig_events_pca[:,1],'.')\n",
    "\n",
    "            axs1[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs1[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs1[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different dates\n",
    "            dates_forplot = np.unique(spike_trig_events_all_dates_df[ind]['dates'])\n",
    "            for idate_forplot in dates_forplot:\n",
    "                ind_idate = list(spike_trig_events_all_dates_df[ind]['dates']==idate_forplot)\n",
    "                ind_idate = list(np.array(ind_idate)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs2[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_idate,0],spike_trig_events_tsne[ind_idate,1],\n",
    "                                            '.',label=idate_forplot)\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs2[ianimal,ibhvname].plot(spike_trig_events_pca[ind_idate,0],spike_trig_events_pca[ind_idate,1],\n",
    "                                            '.',label=idate_forplot)\n",
    "                #\n",
    "            axs2[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs2[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs2[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs2[ianimal,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different channels\n",
    "            chs_forplot = np.unique(spike_trig_events_all_dates_df[ind]['channelID'])\n",
    "            for ich_forplot in chs_forplot:\n",
    "                ind_ich = list(spike_trig_events_all_dates_df[ind]['channelID']==ich_forplot)\n",
    "                ind_ich = list(np.array(ind_ich)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs3[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_ich,0],spike_trig_events_tsne[ind_ich,1],\n",
    "                                            '.',label=str(ich_forplot))\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs3[ianimal,ibhvname].plot(spike_trig_events_pca[ind_ich,0],spike_trig_events_pca[ind_ich,1],\n",
    "                                            '.',label=str(ich_forplot))\n",
    "                #\n",
    "            axs3[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs3[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs3[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs3[ianimal,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different task conditions\n",
    "            cons_forplot = np.unique(spike_trig_events_all_dates_df[ind]['condition'])\n",
    "            for icon_forplot in cons_forplot:\n",
    "                ind_icon = list(spike_trig_events_all_dates_df[ind]['condition']==icon_forplot)\n",
    "                ind_icon = list(np.array(ind_icon)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs4[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_icon,0],spike_trig_events_tsne[ind_icon,1],\n",
    "                                            '.',label=icon_forplot)\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs4[ianimal,ibhvname].plot(spike_trig_events_pca[ind_icon,0],spike_trig_events_pca[ind_icon,1],\n",
    "                                            '.',label=icon_forplot)\n",
    "                #\n",
    "            axs4[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs4[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs4[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs4[ianimal,ibhvname].legend()\n",
    "\n",
    "            # plot the mean spike trigger average trace across neurons in each condition\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "            #\n",
    "            cons_forplot = np.unique(spike_trig_events_all_dates_df[ind]['condition'])\n",
    "            icon_ind = 0\n",
    "            for icon_forplot in cons_forplot:\n",
    "                ind_icon = list(spike_trig_events_all_dates_df[ind]['condition']==icon_forplot)\n",
    "                ind_icon = list(np.array(ind_icon)[~ind_nan])\n",
    "                #\n",
    "                mean_trig_trace_icon = np.nanmean(spike_trig_events_tgt[ind_icon,:],axis=0)\n",
    "                std_trig_trace_icon = np.nanstd(spike_trig_events_tgt[ind_icon,:],axis=0)\n",
    "                sem_trig_trace_icon = np.nanstd(spike_trig_events_tgt[ind_icon,:],axis=0)/np.sqrt(np.shape(spike_trig_events_tgt[ind_icon,:])[0])\n",
    "                itv95_trig_trace_icon = 1.96*sem_trig_trace_icon\n",
    "                #\n",
    "                if 1:\n",
    "                # plot each trace in a seperate traces\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                               color='#E0E0E0',ecolor='#EEEEEE',label=icon_forplot)\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                                                                  np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xlabel('time (s)')\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].legend()\n",
    "                if 0:\n",
    "                    axs6[ianimal,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                    label=icon_forplot)\n",
    "                    # axs6[ianimal,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                    #                                               np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "                    axs6[ianimal,ibhvname].plot([0,0],[0,0.1],'--k') \n",
    "                    axs6[ianimal,ibhvname].set_xlabel('time (s)')\n",
    "                    axs6[ianimal,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                    axs6[ianimal,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                    axs6[ianimal,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                    axs6[ianimal,ibhvname].legend()\n",
    "                #\n",
    "                icon_ind = icon_ind + 1\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different k-mean clusters\n",
    "            kms_forplot = np.unique(kmean_cluster_labels)\n",
    "            for ikm_forplot in kms_forplot:\n",
    "                ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs5[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_ikm,0],spike_trig_events_tsne[ind_ikm,1],\n",
    "                                            '.',label=str(ikm_forplot))\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs5[ianimal,ibhvname].plot(spike_trig_events_pca[ind_ikm,0],spike_trig_events_pca[ind_ikm,1],\n",
    "                                            '.',label=str(ikm_forplot))\n",
    "                #\n",
    "            axs5[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs5[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs5[ianimal,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "            axs5[ianimal,ibhvname].legend()\n",
    "\n",
    "            # plot the mean spike trigger average trace across neurons in each cluster\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "            #\n",
    "            kms_forplot = np.unique(kmean_cluster_labels)\n",
    "            for ikm_forplot in kms_forplot:\n",
    "                ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "                #\n",
    "                mean_trig_trace_ikm = np.nanmean(spike_trig_events_tgt[ind_ikm,:],axis=0)\n",
    "                std_trig_trace_ikm = np.nanstd(spike_trig_events_tgt[ind_ikm,:],axis=0)\n",
    "                sem_trig_trace_ikm = np.nanstd(spike_trig_events_tgt[ind_ikm,:],axis=0)/np.sqrt(np.shape(spike_trig_events_tgt[ind_ikm,:])[0])\n",
    "                itv95_trig_trace_ikm = 1.96*sem_trig_trace_ikm\n",
    "                #\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].errorbar(xxx_forplot,mean_trig_trace_ikm,yerr=itv95_trig_trace_ikm,\n",
    "                                                              color='#E0E0E0',ecolor='#EEEEEE',label='cluster#'+str(ikm_forplot))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_ikm-itv95_trig_trace_ikm),\n",
    "                                                                 np.nanmax(mean_trig_trace_ikm+itv95_trig_trace_ikm)],'--k')\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xlabel('time (s)')\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # stacked bar plot to show the cluster distribution of each conditions\n",
    "            df = pd.DataFrame({'cond':np.array(spike_trig_events_all_dates_df[ind]['condition'])[~ind_nan],\n",
    "                               'cluID':kmean_cluster_labels})\n",
    "            (df.groupby('cond')['cluID'].value_counts(normalize=True)\n",
    "               .unstack('cluID').plot.bar(stacked=True, ax=axs8[ianimal,ibhvname]))\n",
    "            axs8[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/spikeTrigAver_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doTSNE:\n",
    "            fig1.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "            fig2.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "            fig3.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "            fig4.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "            fig5.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig6.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')        \n",
    "            fig7.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig8.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_kmeanclusters_propotion_each_condition'+savefile_sufix+'.pdf')\n",
    "\n",
    "        if doPCA:\n",
    "            fig1.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "            fig2.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "            fig3.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "            fig4.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "            fig5.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig6.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')                           \n",
    "            fig7.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig8.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_kmeanclusters_propotion_each_condition'+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb775e",
   "metadata": {},
   "source": [
    "#### analyze the stretagy aligned firing rate across all dates\n",
    "#### plot the tsne or PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "\n",
    "    strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(strategy_aligned_FR_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(strategy_aligned_FR_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    if 0:\n",
    "        # normalize FR_average for each unit\n",
    "        nspikeunits = np.shape(strategy_aligned_FR_all_dates_df)[0]\n",
    "        for ispikeunit in np.arange(0,nspikeunits,1):\n",
    "            stevent = strategy_aligned_FR_all_dates_df['FR_average'][ispikeunit]\n",
    "            stevent_norm = (stevent-np.nanmin(stevent))/(np.nanmax(stevent)-np.nanmin(stevent))\n",
    "            strategy_aligned_FR_all_dates_df['FR_average'][ispikeunit] = stevent_norm            \n",
    "\n",
    "    # only focus on the certain act animal and certain bhv_name\n",
    "    # act_animals_all = ['kanga']\n",
    "    # bhv_names_all = ['leverpull_prob']\n",
    "    act_animals_all = np.unique(strategy_aligned_FR_all_dates_df['act_animal'])\n",
    "    bhv_names_all = np.unique(strategy_aligned_FR_all_dates_df['bhv_name'])\n",
    "    #\n",
    "    nact_animals = np.shape(act_animals_all)[0]\n",
    "    nbhv_names = np.shape(bhv_names_all)[0]\n",
    "\n",
    "    # set for plot\n",
    "    # plot all units\n",
    "    fig1, axs1 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig1.set_figheight(6*nact_animals)\n",
    "    fig1.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different days\n",
    "    fig2, axs2 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig2.set_figheight(6*nact_animals)\n",
    "    fig2.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but seprate different channels\n",
    "    fig3, axs3 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig3.set_figheight(4*nact_animals)\n",
    "    fig3.set_figwidth(4*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different conditions\n",
    "    fig4, axs4 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig4.set_figheight(6*nact_animals)\n",
    "    fig4.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # spike triggered average for different task conditions\n",
    "    # # to be save, prepare for five conditions\n",
    "    fig6, axs6 = plt.subplots(nact_animals*5,nbhv_names)\n",
    "    fig6.set_figheight(6*nact_animals*5)\n",
    "    fig6.set_figwidth(6*nbhv_names)\n",
    "    # fig6, axs6 = plt.subplots(nact_animals,nbhv_names)\n",
    "    # fig6.set_figheight(6*nact_animals)\n",
    "    # fig6.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # plot all units but separate different k-mean cluster\n",
    "    fig5, axs5 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig5.set_figheight(6*nact_animals)\n",
    "    fig5.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # spike triggered average for different k-mean cluster\n",
    "    # to be save, prepare for 14 clusters\n",
    "    fig7, axs7 = plt.subplots(nact_animals*14,nbhv_names)\n",
    "    fig7.set_figheight(6*nact_animals*14)\n",
    "    fig7.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    # stacked bar plot to show the cluster distribution of each conditions\n",
    "    fig8, axs8 = plt.subplots(nact_animals,nbhv_names)\n",
    "    fig8.set_figheight(6*nact_animals)\n",
    "    fig8.set_figwidth(6*nbhv_names)\n",
    "\n",
    "    #\n",
    "    for ianimal in np.arange(0,nact_animals,1):\n",
    "\n",
    "        act_animal = act_animals_all[ianimal]\n",
    "\n",
    "        for ibhvname in np.arange(0,nbhv_names,1):\n",
    "\n",
    "            bhv_name = bhv_names_all[ibhvname]\n",
    "\n",
    "            ind = (strategy_aligned_FR_all_dates_df['act_animal']==act_animal)&(strategy_aligned_FR_all_dates_df['bhv_name']==bhv_name)\n",
    "\n",
    "            strategy_aligned_FR_tgt = np.vstack(list(strategy_aligned_FR_all_dates_df[ind]['FR_average']))\n",
    "\n",
    "            ind_nan = np.isnan(np.sum(strategy_aligned_FR_tgt,axis=1)) # exist because of failed pull in SR\n",
    "            strategy_aligned_FR_tgt = strategy_aligned_FR_tgt[~ind_nan,:]\n",
    "\n",
    "            # k means clustering\n",
    "            # run clustering on the 15 or 2 dimension PC space (for doPCA), or the whole dataset or 2 dimension (for doTSNE)\n",
    "            pca = PCA(n_components=10)\n",
    "            strategy_aligned_FR_pca = pca.fit_transform(strategy_aligned_FR_tgt)\n",
    "            tsne = TSNE(n_components=2, random_state=0)\n",
    "            strategy_aligned_FR_tsne = tsne.fit_transform(strategy_aligned_FR_tgt)\n",
    "            #\n",
    "            range_n_clusters = np.arange(2,15,1)\n",
    "            silhouette_avg_all = np.ones(np.shape(range_n_clusters))*np.nan\n",
    "            nkmeancls = np.shape(range_n_clusters)[0]\n",
    "            #\n",
    "            for ikmeancl in np.arange(0,nkmeancls,1):\n",
    "                n_clusters = range_n_clusters[ikmeancl]\n",
    "                #\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                # cluster_labels = clusterer.fit_predict(strategy_aligned_FR_tgt)\n",
    "                if doPCA:\n",
    "                    cluster_labels = clusterer.fit_predict(strategy_aligned_FR_pca)\n",
    "                if doTSNE:\n",
    "                    cluster_labels = clusterer.fit_predict(strategy_aligned_FR_tgt)\n",
    "                    # cluster_labels = clusterer.fit_predict(strategy_aligned_FR_tsne)\n",
    "                #\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                # silhouette_avg = silhouette_score(strategy_aligned_FR_tgt, cluster_labels)\n",
    "                if doPCA:\n",
    "                    silhouette_avg = silhouette_score(strategy_aligned_FR_pca, cluster_labels)\n",
    "                if doTSNE:\n",
    "                    silhouette_avg = silhouette_score(strategy_aligned_FR_tgt, cluster_labels)\n",
    "                    # silhouette_avg = silhouette_score(strategy_aligned_FR_tsne, cluster_labels)\n",
    "                #\n",
    "                silhouette_avg_all[ikmeancl] = silhouette_avg\n",
    "            #\n",
    "            best_k_num = range_n_clusters[silhouette_avg_all==np.nanmax(silhouette_avg_all)][0]\n",
    "            #\n",
    "            clusterer = KMeans(n_clusters=best_k_num, random_state=0)\n",
    "            # kmean_cluster_labels = clusterer.fit_predict(strategy_aligned_FR_tgt)\n",
    "            if doPCA:\n",
    "                kmean_cluster_labels = clusterer.fit_predict(strategy_aligned_FR_pca)\n",
    "            if doTSNE:\n",
    "                kmean_cluster_labels = clusterer.fit_predict(strategy_aligned_FR_tgt)\n",
    "                # kmean_cluster_labels = clusterer.fit_predict(strategy_aligned_FR_tsne)\n",
    "\n",
    "\n",
    "            # run PCA and TSNE     \n",
    "            pca = PCA(n_components=2)\n",
    "            tsne = TSNE(n_components=2, random_state=0)\n",
    "            #\n",
    "            strategy_aligned_FR_pca = pca.fit_transform(strategy_aligned_FR_tgt)\n",
    "            strategy_aligned_FR_tsne = tsne.fit_transform(strategy_aligned_FR_tgt)\n",
    "\n",
    "            # plot all units\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs1[ianimal,ibhvname].plot(strategy_aligned_FR_tsne[:,0],strategy_aligned_FR_tsne[:,1],'.')\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs1[ianimal,ibhvname].plot(strategy_aligned_FR_pca[:,0],strategy_aligned_FR_pca[:,1],'.')\n",
    "\n",
    "            axs1[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs1[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs1[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different dates\n",
    "            dates_forplot = np.unique(strategy_aligned_FR_all_dates_df[ind]['dates'])\n",
    "            for idate_forplot in dates_forplot:\n",
    "                ind_idate = list(strategy_aligned_FR_all_dates_df[ind]['dates']==idate_forplot)\n",
    "                ind_idate = list(np.array(ind_idate)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs2[ianimal,ibhvname].plot(strategy_aligned_FR_tsne[ind_idate,0],strategy_aligned_FR_tsne[ind_idate,1],\n",
    "                                            '.',label=idate_forplot)\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs2[ianimal,ibhvname].plot(strategy_aligned_FR_pca[ind_idate,0],strategy_aligned_FR_pca[ind_idate,1],\n",
    "                                            '.',label=idate_forplot)\n",
    "                #\n",
    "            axs2[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs2[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs2[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs2[ianimal,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different channels\n",
    "            chs_forplot = np.unique(strategy_aligned_FR_all_dates_df[ind]['channelID'])\n",
    "            for ich_forplot in chs_forplot:\n",
    "                ind_ich = list(strategy_aligned_FR_all_dates_df[ind]['channelID']==ich_forplot)\n",
    "                ind_ich = list(np.array(ind_ich)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs3[ianimal,ibhvname].plot(strategy_aligned_FR_tsne[ind_ich,0],strategy_aligned_FR_tsne[ind_ich,1],\n",
    "                                            '.',label=str(ich_forplot))\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs3[ianimal,ibhvname].plot(strategy_aligned_FR_pca[ind_ich,0],strategy_aligned_FR_pca[ind_ich,1],\n",
    "                                            '.',label=str(ich_forplot))\n",
    "                #\n",
    "            axs3[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs3[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs3[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs3[ianimal,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different task conditions\n",
    "            cons_forplot = np.unique(strategy_aligned_FR_all_dates_df[ind]['condition'])\n",
    "            for icon_forplot in cons_forplot:\n",
    "                ind_icon = list(strategy_aligned_FR_all_dates_df[ind]['condition']==icon_forplot)\n",
    "                ind_icon = list(np.array(ind_icon)[~ind_nan])\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs4[ianimal,ibhvname].plot(strategy_aligned_FR_tsne[ind_icon,0],strategy_aligned_FR_tsne[ind_icon,1],\n",
    "                                            '.',label=icon_forplot)\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs4[ianimal,ibhvname].plot(strategy_aligned_FR_pca[ind_icon,0],strategy_aligned_FR_pca[ind_icon,1],\n",
    "                                            '.',label=icon_forplot)\n",
    "                #\n",
    "            axs4[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs4[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs4[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "            axs4[ianimal,ibhvname].legend()\n",
    "\n",
    "            # plot the mean spike trigger average trace across neurons in each condition\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "            #\n",
    "            cons_forplot = np.unique(strategy_aligned_FR_all_dates_df[ind]['condition'])\n",
    "            icon_ind = 0\n",
    "            for icon_forplot in cons_forplot:\n",
    "                ind_icon = list(strategy_aligned_FR_all_dates_df[ind]['condition']==icon_forplot)\n",
    "                ind_icon = list(np.array(ind_icon)[~ind_nan])\n",
    "                #\n",
    "                mean_trig_trace_icon = np.nanmean(strategy_aligned_FR_tgt[ind_icon,:],axis=0)\n",
    "                std_trig_trace_icon = np.nanstd(strategy_aligned_FR_tgt[ind_icon,:],axis=0)\n",
    "                sem_trig_trace_icon = np.nanstd(strategy_aligned_FR_tgt[ind_icon,:],axis=0)/np.sqrt(np.shape(strategy_aligned_FR_tgt[ind_icon,:])[0])\n",
    "                itv95_trig_trace_icon = 1.96*sem_trig_trace_icon\n",
    "                #\n",
    "                if 1:\n",
    "                # plot each trace in a seperate traces\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                               color='#E0E0E0',ecolor='#EEEEEE',label=icon_forplot)\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                                                                  np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xlabel('time (s)')\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                    axs6[ianimal*5+icon_ind,ibhvname].legend()\n",
    "                if 0:\n",
    "                    axs6[ianimal,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                    label=icon_forplot)\n",
    "                    # axs6[ianimal,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                    #                                               np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "                    axs6[ianimal,ibhvname].plot([0,0],[0,0.1],'--k') \n",
    "                    axs6[ianimal,ibhvname].set_xlabel('time (s)')\n",
    "                    axs6[ianimal,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                    axs6[ianimal,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                    axs6[ianimal,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                    axs6[ianimal,ibhvname].legend()\n",
    "                #\n",
    "                icon_ind = icon_ind + 1\n",
    "\n",
    "\n",
    "            # plot all units, but seprate different k-mean clusters\n",
    "            kms_forplot = np.unique(kmean_cluster_labels)\n",
    "            for ikm_forplot in kms_forplot:\n",
    "                ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "                #\n",
    "                # plot the tsne\n",
    "                if doTSNE:\n",
    "                    axs5[ianimal,ibhvname].plot(strategy_aligned_FR_tsne[ind_ikm,0],strategy_aligned_FR_tsne[ind_ikm,1],\n",
    "                                            '.',label=str(ikm_forplot))\n",
    "                # plot the pca\n",
    "                if doPCA:\n",
    "                    axs5[ianimal,ibhvname].plot(strategy_aligned_FR_pca[ind_ikm,0],strategy_aligned_FR_pca[ind_ikm,1],\n",
    "                                            '.',label=str(ikm_forplot))\n",
    "                #\n",
    "            axs5[ianimal,ibhvname].set_xticklabels([])\n",
    "            axs5[ianimal,ibhvname].set_yticklabels([])\n",
    "            axs5[ianimal,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "            axs5[ianimal,ibhvname].legend()\n",
    "\n",
    "            # plot the mean spike trigger average trace across neurons in each cluster\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "            #\n",
    "            kms_forplot = np.unique(kmean_cluster_labels)\n",
    "            for ikm_forplot in kms_forplot:\n",
    "                ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "                #\n",
    "                mean_trig_trace_ikm = np.nanmean(strategy_aligned_FR_tgt[ind_ikm,:],axis=0)\n",
    "                std_trig_trace_ikm = np.nanstd(strategy_aligned_FR_tgt[ind_ikm,:],axis=0)\n",
    "                sem_trig_trace_ikm = np.nanstd(strategy_aligned_FR_tgt[ind_ikm,:],axis=0)/np.sqrt(np.shape(strategy_aligned_FR_tgt[ind_ikm,:])[0])\n",
    "                itv95_trig_trace_ikm = 1.96*sem_trig_trace_ikm\n",
    "                #\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].errorbar(xxx_forplot,mean_trig_trace_ikm,yerr=itv95_trig_trace_ikm,\n",
    "                                                              color='#E0E0E0',ecolor='#EEEEEE',label='cluster#'+str(ikm_forplot))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_ikm-itv95_trig_trace_ikm),\n",
    "                                                                 np.nanmax(mean_trig_trace_ikm+itv95_trig_trace_ikm)],'--k')\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xlabel('time (s)')\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].set_title(act_animal+'; '+bhv_name)\n",
    "                axs7[ianimal*14+ikm_forplot,ibhvname].legend()\n",
    "\n",
    "\n",
    "            # stacked bar plot to show the cluster distribution of each conditions\n",
    "            df = pd.DataFrame({'cond':np.array(strategy_aligned_FR_all_dates_df[ind]['condition'])[~ind_nan],\n",
    "                               'cluID':kmean_cluster_labels})\n",
    "            (df.groupby('cond')['cluID'].value_counts(normalize=True)\n",
    "               .unstack('cluID').plot.bar(stacked=True, ax=axs8[ianimal,ibhvname]))\n",
    "            axs8[ianimal,ibhvname].set_title(act_animal+';'+bhv_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/bhvAlignedFRAver_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        if doTSNE:\n",
    "            fig1.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "            fig2.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "            fig3.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "            fig4.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "            fig5.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig6.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')        \n",
    "            fig7.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig8.savefig(figsavefolder+'stretagy_aligned_FR_tsne_clusters_kmeanclusters_propotion_each_condition'+savefile_sufix+'.pdf')\n",
    "\n",
    "        if doPCA:\n",
    "            fig1.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "            fig2.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "            fig3.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "            fig4.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "            fig5.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig6.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')                           \n",
    "            fig7.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "            fig8.savefig(figsavefolder+'stretagy_aligned_FR_pca_clusters_kmeanclusters_propotion_each_condition'+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048173b",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different bhv events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_allevents'])\n",
    "    bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                #\n",
    "                ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "    act_animals_to_ana = ['kanga']\n",
    "    # act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['pull','gaze']\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        # ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = bhvevents_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                # ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                # bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # PCA_dataset = np.hstack(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(bhvevents_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # run PCA\n",
    "                pca = PCA(n_components=3)\n",
    "                pca.fit(PCA_dataset)\n",
    "                PCA_dataset_proj = pca.transform(PCA_dataset)\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'bhvevent_aligned_PCspace_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'bhvevent_aligned_PCspace_trajectory_allconditions'+savefile_sufix+'.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8837a",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### use CCA to align across different conditions (use SR as the base line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735830c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:   \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cross_decomposition import CCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_allevents'])\n",
    "    bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                #\n",
    "                ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "    act_animals_to_ana = ['kanga']\n",
    "    # act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['pull','gaze']\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    # \n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','PCs'])\n",
    "\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        # ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = bhvevents_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                # ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                # bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # PCA_dataset = np.hstack(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(bhvevents_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # run PCA\n",
    "                pca = PCA(n_components=10)\n",
    "                pca.fit(PCA_dataset)\n",
    "                PCA_dataset_proj = pca.transform(PCA_dataset)\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                #\n",
    "                FRPCA_all_dates_sum_df = FRPCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                        'act_animal':act_animal_ana,\n",
    "                                                                        'bhv_name': bhvname_ana,\n",
    "                                                                        'PCs':PCA_dataset_proj,\n",
    "                                                                       }, ignore_index=True)\n",
    "\n",
    "\n",
    "    cond_base = 'SR'\n",
    "    ind_condbase = FRPCA_all_dates_sum_df['condition']==cond_base   \n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[0]\n",
    "                FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[0]\n",
    "\n",
    "                # Step 2: Apply CCA\n",
    "                cca = CCA(n_components=8)  # Match PCA dimensions\n",
    "                U1, U2 = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRCCA_all_dates_base = U1[:, :top_k]\n",
    "                FRCCA_all_dates_tgt = U2[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    savefig = 0\n",
    "    \n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'bhvevent_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'bhvevent_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcdbb6",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### use CCA to align across different conditions (use each condition as the baseline and then average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd51f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cross_decomposition import CCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_allevents'])\n",
    "    bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                #\n",
    "                ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "    act_animals_to_ana = ['kanga']\n",
    "    # act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['pull','gaze']\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nbhvnames_to_ana,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nbhvnames_to_ana)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "\n",
    "    # Step 1 - run PCA separately\n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','PCs','iteration'])\n",
    "\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        # ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = bhvevents_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                # ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                # bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # PCA_dataset = np.hstack(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(bhvevents_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "\n",
    "                # run PCA\n",
    "                # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "                niters = 100\n",
    "                unitsamplesizes = 50\n",
    "                #\n",
    "                nunits = np.shape(PCA_dataset)[1]\n",
    "                ntimesteps = np.shape(PCA_dataset)[0]\n",
    "                #\n",
    "                PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "                #\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "                    PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),niters)]\n",
    "                    #\n",
    "                    pca = PCA(n_components=10)\n",
    "                    pca.fit(PCA_dataset_sample)\n",
    "                    PCA_dataset_proj_iiter = pca.transform(PCA_dataset_sample)\n",
    "\n",
    "                    #\n",
    "                    FRPCA_all_dates_sum_df = FRPCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                            'act_animal':act_animal_ana,\n",
    "                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                            'PCs':PCA_dataset_proj_iiter,\n",
    "                                                                            'iteration':iiter,\n",
    "                                                                           }, ignore_index=True)\n",
    "\n",
    "\n",
    "    # step 2: run CCA\n",
    "    FRCCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                   'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRPCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[iiter]\n",
    "                        FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                        # Step 2: Apply CCA\n",
    "                        cca = CCA(n_components=10)  # Match PCA dimensions\n",
    "                        U1_ibase, U2_ibase = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "\n",
    "                        #\n",
    "                        FRCCA_all_dates_sum_df = FRCCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'CCAs':U2_ibase,\n",
    "                                                                                'base_condition':cond_base,\n",
    "                                                                                'iteration':iiter,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "                        if (icond_base == 0) & (iiter == 0):\n",
    "                            U1_allbase = U1_ibase\n",
    "                            U2_allbase = U2_ibase\n",
    "                        else:\n",
    "                            U1_allbase = U1_allbase + U1_ibase\n",
    "                            U2_allbase = U2_allbase + U2_ibase\n",
    "                        #\n",
    "                        U1 = U1_allbase / nconds_to_ana * niters\n",
    "                        U2 = U2_allbase / nconds_to_ana * niters\n",
    "\n",
    "\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRCCA_all_dates_base = U1[:, :top_k]\n",
    "                FRCCA_all_dates_tgt = U2[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    # step 3\n",
    "    FRCCA_value_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                         'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRCCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        # ax4 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRCCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRCCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRCCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRCCA_all_dates_base = np.array(FRCCA_all_dates_sum_df[ind_base]['CCAs'])[iiter]\n",
    "                        FRCCA_all_dates_tgt = np.array(FRCCA_all_dates_sum_df[ind_ana]['CCAs'])[iiter]\n",
    "\n",
    "                        cca = CCA(n_components=3)  # Match PCA dimensions\n",
    "                        U1,U2 = cca.fit_transform(FRCCA_all_dates_base, FRCCA_all_dates_tgt)\n",
    "                        cca_score = np.nanmean([np.corrcoef(U1[:, i], U2[:, i])[0, 1] for i in range(3)])\n",
    "\n",
    "                        #\n",
    "                        FRCCA_value_all_dates_sum_df = FRCCA_value_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                            'act_animal':act_animal_ana,\n",
    "                                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                                            'CCAs':cca_score,\n",
    "                                                                                            'base_condition':cond_base,\n",
    "                                                                                            'iteration':iiter,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n",
    "                # for plot\n",
    "                ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana\n",
    "                ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "                ind_bhv_plot = FRCCA_value_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_plot = ind_animal_plot & ind_bhv_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                seaborn.boxplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_condition\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "\n",
    "                # Formatting\n",
    "                axs3[ibhvname_ana,icond_ana].set_xlabel(\"Compared Condition\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_title(act_animal_ana+' '+bhvname_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs3[ibhvname_ana,icond_ana].set_xticklabels(axs3[ibhvname_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "                # Adjust layout to fit everything nicely\n",
    "                fig3.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'bhvevent_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'bhvevent_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'bhvevent_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'_CCAscores.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec72496",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### run PCA for all bhv events together combined\n",
    "#### use CCA to align across different conditions (use each condition as the baseline and then average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_allevents'])\n",
    "bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "       \n",
    "    bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "\n",
    "        clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type.split()[0],\n",
    "                                                                                    'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_allevents':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "            \n",
    "            #\n",
    "            ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type.split()[0],\n",
    "                                                                                    'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_average':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "            \n",
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "# act_animals_to_ana = ['kanga']\n",
    "act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['pull','gaze']\n",
    "# bhv_names_to_ana = ['pull','gaze','partner pull','partner gaze']\n",
    "# bhv_names_to_ana = ['succpull','failpull','gaze','partner succpull','partner failpull','partner gaze']\n",
    "bhv_names_to_ana = ['succpull','failpull','gaze',]\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "# Step 1 - run PCA separately\n",
    "# save the simple PCA data\n",
    "FRPCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','PCs','iteration'])\n",
    "\n",
    "for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "    cond_ana = conditions_to_ana[icond_ana]\n",
    "    # ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "    ind_cond = bhvevents_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "         \n",
    "    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "        \n",
    "        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "\n",
    "        if ' ' in bhvname_ana:\n",
    "            bhvname_action = bhvname_ana.split()[1]\n",
    "            # ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_action\n",
    "            ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_action\n",
    "            #\n",
    "            # SR does NOT have failed pull, use all pull instead\n",
    "            if ((cond_ana == 'SR')|(cond_ana == 'SR_withKoala')|(cond_ana == 'SR_withGingerNew')) & (bhvname_action == 'failpull'):\n",
    "                ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']=='pull'\n",
    "        \n",
    "        else:\n",
    "            # ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "            ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "            #\n",
    "            # SR does NOT have failed pull, use all pull instead\n",
    "            if ((cond_ana == 'SR')|(cond_ana == 'SR_withKoala')|(cond_ana == 'SR_withGingerNew')) & (bhvname_ana == 'failpull'):\n",
    "                ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']=='pull'\n",
    "            \n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]        \n",
    "\n",
    "            if ' ' in bhvname_ana:\n",
    "                # ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']!=act_animal_ana\n",
    "                ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']!=act_animal_ana\n",
    "            else:\n",
    "                # ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "                ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "       \n",
    "            ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "            \n",
    "            # bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "            bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "            \n",
    "            # PCA_dataset = np.hstack(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "            PCA_dataset_ibv = np.array(list(bhvevents_aligned_FR_tgt['FR_average']))\n",
    "            \n",
    "            # combine all bhv for running PCA in the same neural space\n",
    "            if ibhvname_ana == 0:\n",
    "                PCA_dataset = PCA_dataset_ibv\n",
    "                timepointnums = np.shape(PCA_dataset_ibv)[1]\n",
    "            else:\n",
    "                PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "                \n",
    "    # remove nan raw from the data set\n",
    "    ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "    PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "    PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "\n",
    "    # run PCA\n",
    "    # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "    niters = 50\n",
    "    unitsamplesizes = 35\n",
    "    #\n",
    "    nunits = np.shape(PCA_dataset)[1]\n",
    "    ntimesteps = np.shape(PCA_dataset)[0]\n",
    "    #\n",
    "    PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "    #\n",
    "    for iiter in np.arange(0,niters,1):\n",
    "        PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),unitsamplesizes)]\n",
    "        #\n",
    "        pca = PCA(n_components=10)\n",
    "        pca.fit(PCA_dataset_sample)\n",
    "        PCA_dataset_proj_iiter = pca.transform(PCA_dataset_sample)\n",
    "\n",
    "        for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "            bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "            #\n",
    "            FRPCA_all_dates_sum_df = FRPCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                    'act_animal':act_animal_ana,\n",
    "                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                    # 'PCs':PCA_dataset_proj_iiter[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:],\n",
    "                                                                    'PCs':PCA_dataset_proj_iiter,\n",
    "                                                                    'iteration':iiter,\n",
    "                                                                   }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3892890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCA comparision across task condition for each behavioral events\n",
    "if 0:    \n",
    "    # step 2: run CCA\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nbhvnames_to_ana,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nbhvnames_to_ana)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    FRCCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                   'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRPCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[iiter]\n",
    "                        FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                        # Step 2: Apply CCA\n",
    "                        cca = CCA(n_components=10)  # Match PCA dimensions\n",
    "                        U1_ibase, U2_ibase = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "\n",
    "                        U1_ibase = U1_ibase[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "                        U2_ibase = U2_ibase[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "\n",
    "                        #\n",
    "                        FRCCA_all_dates_sum_df = FRCCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'CCAs':U2_ibase,\n",
    "                                                                                'base_condition':cond_base,\n",
    "                                                                                'iteration':iiter,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "                        if (icond_base == 0) & (iiter == 0):\n",
    "                            U1_allbase = U1_ibase\n",
    "                            U2_allbase = U2_ibase\n",
    "                        else:\n",
    "                            U1_allbase = U1_allbase + U1_ibase\n",
    "                            U2_allbase = U2_allbase + U2_ibase\n",
    "                        #\n",
    "                        U1 = U1_allbase / nconds_to_ana * niters\n",
    "                        U2 = U2_allbase / nconds_to_ana * niters\n",
    "\n",
    "\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRCCA_all_dates_base = U1[:, :top_k]\n",
    "                FRCCA_all_dates_tgt = U2[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    # step 3\n",
    "    FRCCA_value_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                         'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRCCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        # ax4 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRCCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRCCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRCCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRCCA_all_dates_base = np.array(FRCCA_all_dates_sum_df[ind_base]['CCAs'])[np.random.choice(range(0, niters, 1))]\n",
    "                        FRCCA_all_dates_tgt = np.array(FRCCA_all_dates_sum_df[ind_ana]['CCAs'])[np.random.choice(range(0, niters, 1))]\n",
    "\n",
    "                        cca = CCA(n_components=3)  # Match PCA dimensions\n",
    "                        U1,U2 = cca.fit_transform(FRCCA_all_dates_base, FRCCA_all_dates_tgt)\n",
    "                        cca_score = np.nanmean([np.corrcoef(U1[:, i], U2[:, i])[0, 1] for i in range(3)])\n",
    "\n",
    "                        #\n",
    "                        FRCCA_value_all_dates_sum_df = FRCCA_value_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                            'act_animal':act_animal_ana,\n",
    "                                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                                            'CCAs':cca_score,\n",
    "                                                                                            'base_condition':cond_base,\n",
    "                                                                                            'iteration':iiter,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n",
    "                # for plot\n",
    "                ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana\n",
    "                ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "                ind_bhv_plot = FRCCA_value_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_plot = ind_animal_plot & ind_bhv_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                seaborn.boxplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_condition\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "\n",
    "                # Formatting\n",
    "                axs3[ibhvname_ana,icond_ana].set_xlabel(\"Compared Condition\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_title(act_animal_ana+' '+bhvname_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs3[ibhvname_ana,icond_ana].set_xticklabels(axs3[ibhvname_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "                # Perform ANOVA\n",
    "                df = FRCCA_value_plot\n",
    "                anova_pval = st.f_oneway(*[df[df[\"base_condition\"] == cond][\"CCAs\"] for cond in conditions_to_ana]).pvalue\n",
    "\n",
    "                # Perform post hoc Tukey's HSD test\n",
    "                tukey = pairwise_tukeyhsd(df[\"CCAs\"], df[\"base_condition\"], alpha=0.05)\n",
    "\n",
    "                # Extract raw p-values from Tukey's test\n",
    "                raw_pvals = np.array([row[3] for row in tukey.summary().data[1:]])\n",
    "\n",
    "                # Apply Benjamini-Hochberg correction (FDR)\n",
    "                # _, adj_pvals, _, _ = multipletests(raw_pvals, method='fdr_bh')\n",
    "                adj_pvals = raw_pvals*nconds_to_ana*(nconds_to_ana-1)/2 # *np.sqrt(niters)\n",
    "\n",
    "                # Extract significant pairs after FDR correction\n",
    "                sig_pairs = [(row[0], row[1], adj_p) for row, adj_p in zip(tukey.summary().data[1:], adj_pvals) if adj_p < 0.05]\n",
    "\n",
    "                # Add asterisks for significant comparisons\n",
    "                y_max = df[\"CCAs\"].max() + 0.002  # Base y position for annotations\n",
    "                y_step = 0.002  # Step to avoid overlap\n",
    "\n",
    "                for i, (g1, g2, pval) in enumerate(sig_pairs):\n",
    "                    x1, x2 = list(conditions_to_ana).index(g1), list(conditions_to_ana).index(g2)\n",
    "                    significance = \"*\" if pval >= 0.01 else \"**\" if pval >= 0.001 else \"***\"\n",
    "\n",
    "                    # Plot the significance line\n",
    "                    axs3[ibhvname_ana,icond_ana].plot([x1, x1, x2, x2], [y_max, y_max + y_step, y_max + y_step, y_max], color=\"black\")\n",
    "\n",
    "                    # Add the significance label\n",
    "                    axs3[ibhvname_ana,icond_ana].text((x1 + x2) / 2, y_max + y_step * 1.2, significance, ha='center', fontsize=14, color=\"red\")\n",
    "\n",
    "                    y_max += y_step * 2  # Move y position up for next annotation\n",
    "\n",
    "\n",
    "\n",
    "                # Adjust layout to fit everything nicely\n",
    "                fig3.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'bhvevent_aligned_PCspace_allbhvTogether_CCAaligned_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'bhvevent_aligned_PCspace_allbhvTogether_CCAaligned_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'bhvevent_aligned_PCspace_allbhvTogether_CCAaligned_trajectory_allconditions'+savefile_sufix+'_CCAscores.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot in each condition comparing within itself, but across different bhv\n",
    "# no need to run CCA to align, because all bhv in the same pc space\n",
    "if 0:    \n",
    "    # step 2: run CCA\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nbhvnames_to_ana,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nbhvnames_to_ana)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                    FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                    U1_ibase = FRPCA_all_dates_tgt[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "\n",
    "                \n",
    "                    if (iiter == 0):\n",
    "                        U1_allbase = U1_ibase\n",
    "                    else:\n",
    "                        U1_allbase = U1_allbase + U1_ibase\n",
    "                    #\n",
    "                    U1 = U1_allbase / niters\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRPCA_all_dates_plot = U1[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6),\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6),\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    # step 3\n",
    "    FRCCA_value_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                         'base_bhv','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        # ax4 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for ibhv_base in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhv_base = bhv_names_to_ana[ibhv_base]\n",
    "                    ind_bhvbase = FRPCA_all_dates_sum_df['bhv_name']==bhv_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhvbase & ind_cond\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[np.random.choice(range(0, niters, 1))]\n",
    "                        FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[np.random.choice(range(0, niters, 1))]\n",
    "\n",
    "                        FRPCA_all_dates_tgt = FRPCA_all_dates_tgt[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "                        FRPCA_all_dates_base = FRPCA_all_dates_base[timepointnums*ibhv_base:timepointnums*(ibhv_base+1),:]                       \n",
    "                        \n",
    "                        cca = CCA(n_components=3)  # Match PCA dimensions\n",
    "                        U1,U2 = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "                        cca_score = np.nanmean([np.corrcoef(U1[:, i], U2[:, i])[0, 1] for i in range(3)])\n",
    "\n",
    "                        #\n",
    "                        FRCCA_value_all_dates_sum_df = FRCCA_value_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                            'act_animal':act_animal_ana,\n",
    "                                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                                            'CCAs':cca_score,\n",
    "                                                                                            'base_bhv':bhv_base,\n",
    "                                                                                            'iteration':iiter,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n",
    "                # for plot\n",
    "                ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana\n",
    "                ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "                ind_bhv_plot = FRCCA_value_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_plot = ind_animal_plot & ind_bhv_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                seaborn.boxplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_bhv\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "\n",
    "                # Formatting\n",
    "                axs3[ibhvname_ana,icond_ana].set_xlabel(\"Compared behavior\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_title(act_animal_ana+' '+bhvname_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs3[ibhvname_ana,icond_ana].set_xticklabels(axs3[ibhvname_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "                # Perform ANOVA\n",
    "                df = FRCCA_value_plot\n",
    "                anova_pval = st.f_oneway(*[df[df[\"base_bhv\"] == bhv][\"CCAs\"] for bhv in bhv_names_to_ana]).pvalue\n",
    "\n",
    "                # Perform post hoc Tukey's HSD test\n",
    "                tukey = pairwise_tukeyhsd(df[\"CCAs\"], df[\"base_bhv\"], alpha=0.05)\n",
    "\n",
    "                # Extract raw p-values from Tukey's test\n",
    "                raw_pvals = np.array([row[3] for row in tukey.summary().data[1:]])\n",
    "\n",
    "                # Apply Benjamini-Hochberg correction (FDR)\n",
    "                # _, adj_pvals, _, _ = multipletests(raw_pvals, method='fdr_bh')\n",
    "                adj_pvals = raw_pvals*nbhvnames_to_ana*(nbhvnames_to_ana-1)/2 # *np.sqrt(niters)\n",
    "\n",
    "                # Extract significant pairs after FDR correction\n",
    "                sig_pairs = [(row[0], row[1], adj_p) for row, adj_p in zip(tukey.summary().data[1:], adj_pvals) if adj_p < 0.05]\n",
    "\n",
    "                # Add asterisks for significant comparisons\n",
    "                y_max = df[\"CCAs\"].max() + 0.002  # Base y position for annotations\n",
    "                y_step = 0.002  # Step to avoid overlap\n",
    "\n",
    "                for i, (g1, g2, pval) in enumerate(sig_pairs):\n",
    "                    x1, x2 = list(bhv_names_to_ana).index(g1), list(bhv_names_to_ana).index(g2)\n",
    "                    significance = \"*\" if pval >= 0.01 else \"**\" if pval >= 0.001 else \"***\"\n",
    "\n",
    "                    # Plot the significance line\n",
    "                    axs3[ibhvname_ana,icond_ana].plot([x1, x1, x2, x2], [y_max, y_max + y_step, y_max + y_step, y_max], color=\"black\")\n",
    "\n",
    "                    # Add the significance label\n",
    "                    axs3[ibhvname_ana,icond_ana].text((x1 + x2) / 2, y_max + y_step * 1.2, significance, ha='center', fontsize=14, color=\"red\")\n",
    "\n",
    "                    y_max += y_step * 2  # Move y position up for next annotation\n",
    "\n",
    "\n",
    "\n",
    "                # Adjust layout to fit everything nicely\n",
    "                fig3.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'bhvevent_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'bhvevent_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'bhvevent_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_CCAscores.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16718715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and plot the PC trajectories' length curvature and tortuosity\n",
    "# no need to run CCA to align, because all bhv in the same pc space\n",
    "if 0:    \n",
    "\n",
    "    plottypes = ['PCtort','PCcurv','PClength']\n",
    "    nplottypes = np.shape(plottypes)[0]\n",
    "    \n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nplottypes,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nplottypes)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    FRPCAfeatures_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal',\n",
    "                                                         'bhv_name','PClength','PCcurv','PCtort',\n",
    "                                                         'iteration'])\n",
    "    \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                    FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                    FRPCA_ievent_toana = FRPCA_all_dates_tgt[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "                    \n",
    "                    # # only the top 3 pcs\n",
    "                    # FRPCA_ievent_toana = FRPCA_ievent_toana[:,0:3]\n",
    "                    \n",
    "                    # smooth the pc trajectory\n",
    "                    FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                             arr=FRPCA_ievent_toana, sigma=6)\n",
    "\n",
    "                    # calculate the length, curvature and tortuosity\n",
    "                    PC_traj = FRPCA_ievent_toana  # Shape (240, 3)\n",
    "\n",
    "                    # Compute differences between consecutive points\n",
    "                    diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                    # Compute segment lengths\n",
    "                    segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                    total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                    # Compute curvature\n",
    "                    # First derivatives\n",
    "                    dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                    dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                    dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                    dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                    # Second derivatives\n",
    "                    d2X_dt2 = np.gradient(dX_dt)\n",
    "                    d2Y_dt2 = np.gradient(dY_dt)\n",
    "                    d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                    d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                    # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                    cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                    curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                    # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                    euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                    tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "                    \n",
    "                    #\n",
    "                    FRPCAfeatures_all_dates_sum_df = FRPCAfeatures_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                            'act_animal':act_animal_ana,\n",
    "                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                            'iteration':iiter,\n",
    "                                                                            'PClength':total_length,\n",
    "                                                                            'PCcurv':np.nanmean(curvature),\n",
    "                                                                            'PCtort':tortuosity,\n",
    "                                                                           }, ignore_index=True)\n",
    "\n",
    "        # plot\n",
    "        for iplottype in np.arange(0,nplottypes,1):\n",
    "            \n",
    "            plottype = plottypes[iplottype]\n",
    "            \n",
    "            seaborn.violinplot(ax=axs3[iplottype,icond_ana],data=FRPCAfeatures_all_dates_sum_df,\n",
    "                               x = 'bhv_name',y=plottype)\n",
    "            \n",
    "            axs3[iplottype,icond_ana].set_title(act_animal_ana+' in '+cond_ana)\n",
    "            \n",
    "            # Perform ANOVA\n",
    "            df = FRPCAfeatures_all_dates_sum_df\n",
    "            anova_pval = st.f_oneway(*[df[df[\"bhv_name\"] == bhv][plottype] for bhv in bhv_names_to_ana]).pvalue\n",
    "\n",
    "            # Perform post hoc Tukey's HSD test\n",
    "            tukey = pairwise_tukeyhsd(df[plottype], df[\"bhv_name\"], alpha=0.05)\n",
    "\n",
    "            # Extract raw p-values from Tukey's test\n",
    "            raw_pvals = np.array([row[3] for row in tukey.summary().data[1:]])\n",
    "\n",
    "            # Apply Benjamini-Hochberg correction (FDR)\n",
    "            # _, adj_pvals, _, _ = multipletests(raw_pvals, method='fdr_bh')\n",
    "            adj_pvals = raw_pvals*nbhvnames_to_ana*(nbhvnames_to_ana-1)/2 # *np.sqrt(niters)\n",
    "\n",
    "            # Extract significant pairs after FDR correction\n",
    "            sig_pairs = [(row[0], row[1], adj_p) for row, adj_p in zip(tukey.summary().data[1:], adj_pvals) if adj_p < 0.05]\n",
    "\n",
    "            # Add asterisks for significant comparisons\n",
    "            y_max = df[plottype].max() + 0.002  # Base y position for annotations\n",
    "            y_step = 1  # Step to avoid overlap\n",
    "\n",
    "            for i, (g1, g2, pval) in enumerate(sig_pairs):\n",
    "                x1, x2 = list(bhv_names_to_ana).index(g1), list(bhv_names_to_ana).index(g2)\n",
    "                significance = \"*\" if pval >= 0.01 else \"**\" if pval >= 0.001 else \"***\"\n",
    "\n",
    "                # Plot the significance line\n",
    "                axs3[iplottype,icond_ana].plot([x1, x1, x2, x2], [y_max, y_max + y_step, y_max + y_step, y_max], color=\"black\")\n",
    "\n",
    "                # Add the significance label\n",
    "                axs3[iplottype,icond_ana].text((x1 + x2) / 2, y_max + y_step * 1.2, significance, ha='center', fontsize=14, color=\"red\")\n",
    "\n",
    "                y_max += y_step * 2  # Move y position up for next annotation\n",
    "\n",
    "        # Adjust layout to fit everything nicely\n",
    "        fig3.tight_layout()\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig3.savefig(figsavefolder+'bhvevents_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_trajectoryFeatures.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193cfff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce491311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876d574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7c88eb",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately each condition\n",
    "#### for the activity aligned at the different bhv events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507082c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_allevents'])\n",
    "    bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                #\n",
    "                ichannelID = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = bhvevents_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "    act_animals_to_ana = ['kanga']\n",
    "    # act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['pull','gaze']\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "    # \n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond_allevents = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = bhvevents_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal_allevents = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal = bhvevents_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv_allevents = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = bhvevents_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_allevents = ind_animal_allevents & ind_bhv_allevents & ind_cond_allevents\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana_allevents]\n",
    "                bhvevents_aligned_FR_tgt = bhvevents_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # separate for each dates\n",
    "                dates_to_ana = np.unique(bhvevents_aligned_FR_tgt['dates'])\n",
    "                ndates_ana = np.shape(dates_to_ana)[0]\n",
    "\n",
    "                for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                    date_ana = dates_to_ana[idate_ana]\n",
    "                    ind_date_allevents = bhvevents_aligned_FR_allevents_tgt['dates']==date_ana\n",
    "                    ind_date = bhvevents_aligned_FR_tgt['dates']==date_ana\n",
    "\n",
    "                    # get the PCA training data set\n",
    "                    PCA_dataset = np.hstack(list(bhvevents_aligned_FR_allevents_tgt[ind_date_allevents]['FR_allevents']))\n",
    "                    #\n",
    "                    ncells = np.shape(bhvevents_aligned_FR_allevents_tgt[ind_date_allevents])[0]\n",
    "                    PCA_dataset_train_pre_df = pd.DataFrame(columns=['clusterID','channelID','FR_pooled'])\n",
    "                    PCA_dataset_train_pre_df['clusterID'] = bhvevents_aligned_FR_allevents_tgt[ind_date_allevents]['clusterID']\n",
    "                    PCA_dataset_train_pre_df['channelID'] = bhvevents_aligned_FR_allevents_tgt[ind_date_allevents]['channelID']\n",
    "                    for icell in np.arange(0,ncells,1):\n",
    "                        FR_ravel = np.ravel(bhvevents_aligned_FR_allevents_tgt[ind_date_allevents]['FR_allevents'].iloc[icell])\n",
    "                        PCA_dataset_train_pre_df['FR_pooled'].iloc[icell] = FR_ravel\n",
    "                    PCA_dataset_train = np.array(list(PCA_dataset_train_pre_df['FR_pooled']))\n",
    "                    # remove nan raw from the data set\n",
    "                    ind_nan = np.isnan(np.sum(PCA_dataset_train,axis=0))\n",
    "                    PCA_dataset_train = PCA_dataset_train[:,~ind_nan]\n",
    "\n",
    "                    # get the PCA test dataset\n",
    "                    PCA_dataset_test = np.array(list(bhvevents_aligned_FR_tgt[ind_date]['FR_average']))\n",
    "                    # remove nan raw from the data set\n",
    "                    ind_nan = np.isnan(np.sum(PCA_dataset_test,axis=0))\n",
    "                    PCA_dataset_test = PCA_dataset_test[:,~ind_nan]\n",
    "\n",
    "                    # run PCA\n",
    "                    pca = PCA(n_components=3)\n",
    "                    pca.fit(PCA_dataset_train.transpose())\n",
    "                    PCA_dataset_train_proj = pca.transform(PCA_dataset_train.transpose())\n",
    "                    PCA_dataset_proj = pca.transform(PCA_dataset_test.transpose())\n",
    "\n",
    "                    trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                    xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                    # plot PC1\n",
    "                    axs1[0,icond_ana].plot( xxx_forplot,PCA_dataset_proj[:,0],label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                    axs1[1,icond_ana].plot( xxx_forplot,PCA_dataset_proj[:,1],label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                    axs1[2,icond_ana].plot( xxx_forplot,PCA_dataset_proj[:,2],label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156dd4a1",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "    strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            ibhv_type_split = ibhv_type.split()\n",
    "            if np.shape(ibhv_type_split)[0]==3:\n",
    "                ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                #\n",
    "                ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type_split[0],\n",
    "                                                                                        'bhv_name': ibhv_type_split[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(strategy_aligned_FR_all_dates_df['act_animal'])\n",
    "    act_animals_to_ana = ['kanga']\n",
    "    # act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(strategy_aligned_FR_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(strategy_aligned_FR_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        # ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = strategy_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal = strategy_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                # ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = strategy_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                # strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                strategy_aligned_FR_tgt = strategy_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # PCA_dataset = np.hstack(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(strategy_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # run PCA\n",
    "                # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "                niters = 100\n",
    "                unitsamplesizes = 100\n",
    "                #\n",
    "                nunits = np.shape(PCA_dataset)[1]\n",
    "                ntimesteps = np.shape(PCA_dataset)[0]\n",
    "                #\n",
    "                PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "                #\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "                    PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),niters)]\n",
    "                    #\n",
    "                    pca = PCA(n_components=3)\n",
    "                    pca.fit(PCA_dataset_sample)\n",
    "                    PCA_dataset_proj_allsamples[iiter,:,:] = pca.transform(PCA_dataset_sample)\n",
    "                #\n",
    "                PCA_dataset_proj = np.nanmean(PCA_dataset_proj_allsamples,axis=0)\n",
    "\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[0],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(PCA_dataset_proj[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb9fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d23d93",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### use CCA to align across different conditions (use each condition as the baseline and then average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cross_decomposition import CCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    doPCA = 1\n",
    "    doTSNE = 0\n",
    "    strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            ibhv_type_split = ibhv_type.split()\n",
    "            if np.shape(ibhv_type_split)[0]==3:\n",
    "                ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                #\n",
    "                ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type_split[0],\n",
    "                                                                                        'bhv_name': ibhv_type_split[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(strategy_aligned_FR_all_dates_df['act_animal'])\n",
    "    act_animals_to_ana = ['kanga']\n",
    "    # act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(strategy_aligned_FR_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(strategy_aligned_FR_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nbhvnames_to_ana,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nbhvnames_to_ana)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "\n",
    "    # Step 1 - run PCA separately\n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','PCs','iteration'])\n",
    "\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        # ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = strategy_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            # ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            ind_animal = strategy_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                # ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = strategy_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                # strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                strategy_aligned_FR_tgt = strategy_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # PCA_dataset = np.hstack(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "                PCA_dataset = np.array(list(strategy_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "\n",
    "                # run PCA\n",
    "                # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "                niters = 100\n",
    "                unitsamplesizes = 50\n",
    "                #\n",
    "                nunits = np.shape(PCA_dataset)[1]\n",
    "                ntimesteps = np.shape(PCA_dataset)[0]\n",
    "                #\n",
    "                PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "                #\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "                    PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),niters)]\n",
    "                    #\n",
    "                    pca = PCA(n_components=10)\n",
    "                    pca.fit(PCA_dataset_sample)\n",
    "                    PCA_dataset_proj_iiter = pca.transform(PCA_dataset_sample)\n",
    "\n",
    "                    #\n",
    "                    FRPCA_all_dates_sum_df = FRPCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                            'act_animal':act_animal_ana,\n",
    "                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                            'PCs':PCA_dataset_proj_iiter,\n",
    "                                                                            'iteration':iiter,\n",
    "                                                                           }, ignore_index=True)\n",
    "\n",
    "\n",
    "    # step 2: run CCA\n",
    "    FRCCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                   'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRPCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[iiter]\n",
    "                        FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                        # Step 2: Apply CCA\n",
    "                        cca = CCA(n_components=10)  # Match PCA dimensions\n",
    "                        U1_ibase, U2_ibase = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "\n",
    "                        #\n",
    "                        FRCCA_all_dates_sum_df = FRCCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'CCAs':U2_ibase,\n",
    "                                                                                'base_condition':cond_base,\n",
    "                                                                                'iteration':iiter,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "                        if (icond_base == 0) & (iiter == 0):\n",
    "                            U1_allbase = U1_ibase\n",
    "                            U2_allbase = U2_ibase\n",
    "                        else:\n",
    "                            U1_allbase = U1_allbase + U1_ibase\n",
    "                            U2_allbase = U2_allbase + U2_ibase\n",
    "                        #\n",
    "                        U1 = U1_allbase / nconds_to_ana * niters\n",
    "                        U2 = U2_allbase / nconds_to_ana * niters\n",
    "\n",
    "\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRCCA_all_dates_base = U1[:, :top_k]\n",
    "                FRCCA_all_dates_tgt = U2[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    # step 3\n",
    "    FRCCA_value_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                         'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRCCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        # ax4 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRCCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRCCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRCCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRCCA_all_dates_base = np.array(FRCCA_all_dates_sum_df[ind_base]['CCAs'])[iiter]\n",
    "                        FRCCA_all_dates_tgt = np.array(FRCCA_all_dates_sum_df[ind_ana]['CCAs'])[iiter]\n",
    "\n",
    "                        cca = CCA(n_components=3)  # Match PCA dimensions\n",
    "                        U1,U2 = cca.fit_transform(FRCCA_all_dates_base, FRCCA_all_dates_tgt)\n",
    "                        cca_score = np.nanmean([np.corrcoef(U1[:, i], U2[:, i])[0, 1] for i in range(3)])\n",
    "\n",
    "                        #\n",
    "                        FRCCA_value_all_dates_sum_df = FRCCA_value_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                            'act_animal':act_animal_ana,\n",
    "                                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                                            'CCAs':cca_score,\n",
    "                                                                                            'base_condition':cond_base,\n",
    "                                                                                            'iteration':iiter,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n",
    "                # for plot\n",
    "                ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana\n",
    "                ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "                ind_bhv_plot = FRCCA_value_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_plot = ind_animal_plot & ind_bhv_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                seaborn.boxplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_condition\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "\n",
    "                # Formatting\n",
    "                axs3[ibhvname_ana,icond_ana].set_xlabel(\"Compared Condition\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_title(act_animal_ana+' '+bhvname_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs3[ibhvname_ana,icond_ana].set_xticklabels(axs3[ibhvname_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "                # Adjust layout to fit everything nicely\n",
    "                fig3.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'strategy_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'strategy_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'strategy_aligned_PCspace_CCAaligned_trajectory_allconditions'+savefile_sufix+'_CCAscores.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04d3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "330d8a05",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, pool sessions from the same condition together\n",
    "#### for the activity aligned at the different strategies\n",
    "#### run PCA for all strategies together combined\n",
    "#### use CCA to align across different conditions (use each condition as the baseline and then average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "\n",
    "    bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "\n",
    "        clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if np.shape(ibhv_type_split)[0]==3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "\n",
    "            #\n",
    "            ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_average':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "# act_animals_to_ana = np.unique(strategy_aligned_FR_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(strategy_aligned_FR_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "bhv_names_to_ana = ['gaze_lead_pull', 'not_gaze_lead_pull','social_attention', 'not_social_attention']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "conditions_to_ana = np.unique(strategy_aligned_FR_all_dates_df['condition'])\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "# Step 1 - run PCA separately\n",
    "# save the simple PCA data\n",
    "FRPCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','PCs','iteration'])\n",
    "\n",
    "\n",
    "for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "    cond_ana = conditions_to_ana[icond_ana]\n",
    "    # ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "    ind_cond = strategy_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "    for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "        act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "        # ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "        ind_animal = strategy_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "        for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "            bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "            # ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "            ind_bhv = strategy_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "            ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "            # strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "            strategy_aligned_FR_tgt = strategy_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "            # PCA_dataset = np.hstack(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))\n",
    "            PCA_dataset_ibv = np.array(list(strategy_aligned_FR_tgt['FR_average']))\n",
    "\n",
    "            # combine all bhv for running PCA in the same neural space\n",
    "            if ibhvname_ana == 0:\n",
    "                PCA_dataset = PCA_dataset_ibv\n",
    "                timepointnums = np.shape(PCA_dataset_ibv)[1]\n",
    "            else:\n",
    "                PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "            \n",
    "        # remove nan raw from the data set\n",
    "        # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "        # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "        ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "        PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "        PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "\n",
    "        # run PCA\n",
    "        # newly added, randomly sample 100 \"neuron\" units and run PCA for 100 (niters) iterations\n",
    "        niters = 50\n",
    "        unitsamplesizes = 35\n",
    "        #\n",
    "        nunits = np.shape(PCA_dataset)[1]\n",
    "        ntimesteps = np.shape(PCA_dataset)[0]\n",
    "        #\n",
    "        PCA_dataset_proj_allsamples = np.ones((niters,ntimesteps,3))*np.nan\n",
    "        #\n",
    "        for iiter in np.arange(0,niters,1):\n",
    "            PCA_dataset_sample = PCA_dataset[:,np.random.choice(range(nunits),unitsamplesizes)]\n",
    "            #\n",
    "            pca = PCA(n_components=10)\n",
    "            pca.fit(PCA_dataset_sample)\n",
    "            PCA_dataset_proj_iiter = pca.transform(PCA_dataset_sample)\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                #\n",
    "                FRPCA_all_dates_sum_df = FRPCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                        'act_animal':act_animal_ana,\n",
    "                                                                        'bhv_name': bhvname_ana,\n",
    "                                                                         # 'PCs':PCA_dataset_proj_iiter[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:],\n",
    "                                                                        'PCs':PCA_dataset_proj_iiter,\n",
    "                                                                        'iteration':iiter,\n",
    "                                                                       }, ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCA comparision across task condition for each behavioral events\n",
    "if 0:    \n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nbhvnames_to_ana,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nbhvnames_to_ana)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    # step 2: run CCA\n",
    "    FRCCA_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                   'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRPCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[iiter]\n",
    "                        FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                        # Step 2: Apply CCA\n",
    "                        cca = CCA(n_components=10)  # Match PCA dimensions\n",
    "                        U1_ibase, U2_ibase = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "\n",
    "                        # keep this if do CCA on the concatanated PCA data\n",
    "                        U1_ibase = U1_ibase[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "                        U2_ibase = U2_ibase[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "\n",
    "                        #\n",
    "                        FRCCA_all_dates_sum_df = FRCCA_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'CCAs':U2_ibase,\n",
    "                                                                                'base_condition':cond_base,\n",
    "                                                                                'iteration':iiter,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "                        if (icond_base == 0) & (iiter == 0):\n",
    "                            U1_allbase = U1_ibase\n",
    "                            U2_allbase = U2_ibase\n",
    "                        else:\n",
    "                            U1_allbase = U1_allbase + U1_ibase\n",
    "                            U2_allbase = U2_allbase + U2_ibase\n",
    "                        #\n",
    "                        U1 = U1_allbase / nconds_to_ana * niters\n",
    "                        U2 = U2_allbase / nconds_to_ana * niters\n",
    "\n",
    "\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRCCA_all_dates_base = U1[:, :top_k]\n",
    "                FRCCA_all_dates_tgt = U2[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6),\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRCCA_all_dates_tgt[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRCCA_all_dates_tgt[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    # step 3\n",
    "    FRCCA_value_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal','bhv_name','CCAs',\n",
    "                                                         'base_condition','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRCCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        # ax4 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRCCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRCCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for icond_base in np.arange(0,nconds_to_ana,1):\n",
    "                    cond_base = conditions_to_ana[icond_base]\n",
    "                    ind_condbase = FRCCA_all_dates_sum_df['condition']==cond_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhv & ind_condbase\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRCCA_all_dates_base = np.array(FRCCA_all_dates_sum_df[ind_base]['CCAs'])[np.random.choice(range(0, niters, 1))]\n",
    "                        FRCCA_all_dates_tgt = np.array(FRCCA_all_dates_sum_df[ind_ana]['CCAs'])[np.random.choice(range(0, niters, 1))]\n",
    "\n",
    "                        cca = CCA(n_components=3)  # Match PCA dimensions\n",
    "                        U1,U2 = cca.fit_transform(FRCCA_all_dates_base, FRCCA_all_dates_tgt)\n",
    "                        cca_score = np.nanmean([np.corrcoef(U1[:, i], U2[:, i])[0, 1] for i in range(3)])\n",
    "\n",
    "                        #\n",
    "                        FRCCA_value_all_dates_sum_df = FRCCA_value_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                            'act_animal':act_animal_ana,\n",
    "                                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                                            'CCAs':cca_score,\n",
    "                                                                                            'base_condition':cond_base,\n",
    "                                                                                            'iteration':iiter,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n",
    "                # for plot\n",
    "                ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana\n",
    "                ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "                ind_bhv_plot = FRCCA_value_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_plot = ind_animal_plot & ind_bhv_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                seaborn.boxplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_condition\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "\n",
    "                # Formatting\n",
    "                axs3[ibhvname_ana,icond_ana].set_xlabel(\"Compared Condition\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_title(act_animal_ana+' '+bhvname_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs3[ibhvname_ana,icond_ana].set_xticklabels(axs3[ibhvname_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "                # Perform ANOVA\n",
    "                df = FRCCA_value_plot\n",
    "                anova_pval = st.f_oneway(*[df[df[\"base_condition\"] == cond][\"CCAs\"] for cond in conditions_to_ana]).pvalue\n",
    "\n",
    "                # Perform post hoc Tukey's HSD test\n",
    "                tukey = pairwise_tukeyhsd(df[\"CCAs\"], df[\"base_condition\"], alpha=0.05)\n",
    "\n",
    "                # Extract raw p-values from Tukey's test\n",
    "                raw_pvals = np.array([row[3] for row in tukey.summary().data[1:]])\n",
    "\n",
    "                # Apply Benjamini-Hochberg correction (FDR)\n",
    "                # _, adj_pvals, _, _ = multipletests(raw_pvals, method='fdr_bh')\n",
    "                adj_pvals = raw_pvals*nconds_to_ana*(nconds_to_ana-1)/2 # *niters\n",
    "\n",
    "                # Extract significant pairs after FDR correction\n",
    "                sig_pairs = [(row[0], row[1], adj_p) for row, adj_p in zip(tukey.summary().data[1:], adj_pvals) if adj_p < 0.05]\n",
    "\n",
    "                # Add asterisks for significant comparisons\n",
    "                y_max = df[\"CCAs\"].max() + 0.002  # Base y position for annotations\n",
    "                y_step = 0.002  # Step to avoid overlap\n",
    "\n",
    "                for i, (g1, g2, pval) in enumerate(sig_pairs):\n",
    "                    x1, x2 = list(conditions_to_ana).index(g1), list(conditions_to_ana).index(g2)\n",
    "                    significance = \"*\" if pval >= 0.01 else \"**\" if pval >= 0.001 else \"***\"\n",
    "\n",
    "                    # Plot the significance line\n",
    "                    axs3[ibhvname_ana,icond_ana].plot([x1, x1, x2, x2], [y_max, y_max + y_step, y_max + y_step, y_max], color=\"black\")\n",
    "\n",
    "                    # Add the significance label\n",
    "                    axs3[ibhvname_ana,icond_ana].text((x1 + x2) / 2, y_max + y_step * 1.2, significance, ha='center', fontsize=14, color=\"red\")\n",
    "\n",
    "                    y_max += y_step * 2  # Move y position up for next annotation\n",
    "\n",
    "                # Adjust layout to fit everything nicely\n",
    "                fig3.tight_layout()           \n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'strategy_aligned_PCspace_allStrTogether_CCAaligned_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'strategy_aligned_PCspace_allStrTogether_CCAaligned_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'strategy_aligned_PCspace_allStrTogether_CCAaligned_trajectory_allconditions'+savefile_sufix+'_CCAscores.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot in each condition comparing within itself, but across different bhv\n",
    "# no need to run CCA to align, because all bhv in the same pc space\n",
    "if 1:    \n",
    "    # step 2: run CCA\n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    #\n",
    "    # 3d figure\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nbhvnames_to_ana,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nbhvnames_to_ana)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "    \n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                    FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                    U1_ibase = FRPCA_all_dates_tgt[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "\n",
    "                \n",
    "                    if (iiter == 0):\n",
    "                        U1_allbase = U1_ibase\n",
    "                    else:\n",
    "                        U1_allbase = U1_allbase + U1_ibase\n",
    "                    #\n",
    "                    U1 = U1_allbase / niters\n",
    "\n",
    "                # Step 3: Select top k aligned dimensions based on correlation\n",
    "                top_k = 3  # Choose a smaller aligned space\n",
    "                FRPCA_all_dates_plot = U1[:, :top_k]\n",
    "\n",
    "                trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                # plot PC1\n",
    "                axs1[0,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[1,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                axs1[2,icond_ana].plot( xxx_forplot,gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6),\n",
    "                                       label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                # plot the 3d trojactory\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6),\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6),\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6),\n",
    "                         label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                # start of time window\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6)[0],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6)[0],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6)[0],\n",
    "                         'o',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # action time\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6)[np.where(xxx_forplot==0)[0][0]],\n",
    "                         '>',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "                # end of time window\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_all_dates_plot[:,0], 6)[-1],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,1], 6)[-1],\n",
    "                         gaussian_filter1d(FRPCA_all_dates_plot[:,2], 6)[-1],\n",
    "                         's',markersize = 9, color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')    \n",
    "        ax2.set_title(cond_ana)\n",
    "        ax2.legend()    \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "\n",
    "\n",
    "    # step 3\n",
    "    FRCCA_value_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal',\n",
    "                                                         'action_name','comparison_type',\n",
    "                                                         'bhv_name','CCAs',\n",
    "                                                         'base_bhv','iteration'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        # ax4 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                for ibhv_base in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhv_base = bhv_names_to_ana[ibhv_base]\n",
    "                    ind_bhvbase = FRPCA_all_dates_sum_df['bhv_name']==bhv_base  \n",
    "\n",
    "                    ind_base = ind_animal & ind_bhvbase & ind_cond\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                    for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                        FRPCA_all_dates_base = np.array(FRPCA_all_dates_sum_df[ind_base]['PCs'])[np.random.choice(range(0, niters, 1))]\n",
    "                        FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[np.random.choice(range(0, niters, 1))]\n",
    "\n",
    "                        FRPCA_all_dates_tgt = FRPCA_all_dates_tgt[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "                        FRPCA_all_dates_base = FRPCA_all_dates_base[timepointnums*ibhv_base:timepointnums*(ibhv_base+1),:]                       \n",
    "                        \n",
    "                        cca = CCA(n_components=3)  # Match PCA dimensions\n",
    "                        U1,U2 = cca.fit_transform(FRPCA_all_dates_base, FRPCA_all_dates_tgt)\n",
    "                        cca_score = np.nanmean([np.corrcoef(U1[:, i], U2[:, i])[0, 1] for i in range(3)])\n",
    "\n",
    "                        if 'pull' in bhvname_ana:\n",
    "                            action_name = 'pull'\n",
    "                            if bhvname_ana == bhv_base:\n",
    "                                comp_type = 'within_strategy'\n",
    "                            elif 'pull' in bhv_base:\n",
    "                                comp_type = 'across_strategy'\n",
    "                            else:\n",
    "                                comp_type = 'across_action'\n",
    "                            \n",
    "                        else:\n",
    "                            action_name = 'gaze'\n",
    "                            if bhvname_ana == bhv_base:\n",
    "                                comp_type = 'within_strategy'\n",
    "                            elif 'pull' not in bhv_base:\n",
    "                                comp_type = 'across_strategy'\n",
    "                            else:\n",
    "                                comp_type = 'across_action'\n",
    "                        \n",
    "                        #\n",
    "                        FRCCA_value_all_dates_sum_df = FRCCA_value_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                                            'act_animal':act_animal_ana,\n",
    "                                                                                            'action_name':action_name,\n",
    "                                                                                            'comparison_type':comp_type,\n",
    "                                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                                            'CCAs':cca_score,\n",
    "                                                                                            'base_bhv':bhv_base,\n",
    "                                                                                            'iteration':iiter,\n",
    "                                                                                           }, ignore_index=True)\n",
    "\n",
    "                # for plot\n",
    "                ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana\n",
    "                ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "                ind_bhv_plot = FRCCA_value_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_plot = ind_animal_plot & ind_bhv_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                # seaborn.boxplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_bhv\", y=\"CCAs\",\n",
    "                #                    data=FRCCA_value_plot)\n",
    "                seaborn.violinplot(ax=axs3[ibhvname_ana,icond_ana],x=\"base_bhv\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "\n",
    "                \n",
    "                # Formatting\n",
    "                axs3[ibhvname_ana,icond_ana].set_xlabel(\"Compared behavior\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs3[ibhvname_ana,icond_ana].set_title(act_animal_ana+' '+bhvname_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs3[ibhvname_ana,icond_ana].set_xticklabels(axs3[ibhvname_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "                # Perform ANOVA\n",
    "                df = FRCCA_value_plot\n",
    "                anova_pval = st.f_oneway(*[df[df[\"base_bhv\"] == bhv][\"CCAs\"] for bhv in bhv_names_to_ana]).pvalue\n",
    "\n",
    "                # Perform post hoc Tukey's HSD test\n",
    "                tukey = pairwise_tukeyhsd(df[\"CCAs\"], df[\"base_bhv\"], alpha=0.05)\n",
    "\n",
    "                # Extract raw p-values from Tukey's test\n",
    "                raw_pvals = np.array([row[3] for row in tukey.summary().data[1:]])\n",
    "\n",
    "                # Apply Benjamini-Hochberg correction (FDR)\n",
    "                # _, adj_pvals, _, _ = multipletests(raw_pvals, method='fdr_bh')\n",
    "                adj_pvals = raw_pvals*nbhvnames_to_ana*(nbhvnames_to_ana-1)/2 # *np.sqrt(niters)\n",
    "\n",
    "                # Extract significant pairs after FDR correction\n",
    "                sig_pairs = [(row[0], row[1], adj_p) for row, adj_p in zip(tukey.summary().data[1:], adj_pvals) if adj_p < 0.05]\n",
    "\n",
    "                # Add asterisks for significant comparisons\n",
    "                y_max = df[\"CCAs\"].max() + 0.002  # Base y position for annotations\n",
    "                y_step = 0.002  # Step to avoid overlap\n",
    "\n",
    "                for i, (g1, g2, pval) in enumerate(sig_pairs):\n",
    "                    x1, x2 = list(bhv_names_to_ana).index(g1), list(bhv_names_to_ana).index(g2)\n",
    "                    significance = \"*\" if pval >= 0.01 else \"**\" if pval >= 0.001 else \"***\"\n",
    "\n",
    "                    # Plot the significance line\n",
    "                    axs3[ibhvname_ana,icond_ana].plot([x1, x1, x2, x2], [y_max, y_max + y_step, y_max + y_step, y_max], color=\"black\")\n",
    "\n",
    "                    # Add the significance label\n",
    "                    axs3[ibhvname_ana,icond_ana].text((x1 + x2) / 2, y_max + y_step * 1.2, significance, ha='center', fontsize=14, color=\"red\")\n",
    "\n",
    "                    y_max += y_step * 2  # Move y position up for next annotation\n",
    "\n",
    "\n",
    "\n",
    "                # Adjust layout to fit everything nicely\n",
    "                fig3.tight_layout()\n",
    "\n",
    "    \n",
    "    # step 4; plot with combining conditions, to show the CCA of the same action within or across stretagies\n",
    "    #\n",
    "    # figures; pool across conditions; looking at the distance within and across strategies of the same action\n",
    "    actions_ana = ['pull','gaze']\n",
    "    nactions_ana = np.shape(actions_ana)[0]\n",
    "    fig4, axs4 = plt.subplots(nactions_ana,nconds_to_ana)\n",
    "    fig4.set_figheight(6*nactions_ana)\n",
    "    fig4.set_figwidth(6*nconds_to_ana)\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond_plot = FRCCA_value_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal_plot = FRCCA_value_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for iaction_ana in np.arange(0,nactions_ana,1):\n",
    "                action_ana = actions_ana[iaction_ana]\n",
    "                ind_act_plot = FRCCA_value_all_dates_sum_df['action_name']==action_ana\n",
    "                \n",
    "                ind_ana_plot = ind_animal_plot & ind_act_plot & ind_cond_plot\n",
    "\n",
    "                FRCCA_value_plot = FRCCA_value_all_dates_sum_df[ind_ana_plot]\n",
    "\n",
    "                seaborn.violinplot(ax=axs4[iaction_ana,icond_ana],x=\"comparison_type\", y=\"CCAs\",\n",
    "                                   data=FRCCA_value_plot)\n",
    "                \n",
    "                # Formatting\n",
    "                axs4[iaction_ana,icond_ana].set_xlabel(\"Compared behavior\")\n",
    "                axs4[iaction_ana,icond_ana].set_ylabel(\"CCA Score\")\n",
    "                axs4[iaction_ana,icond_ana].set_title(act_animal_ana+' '+action_ana+' in '+cond_ana)\n",
    "\n",
    "                # Rotate x-axis labels by 45 degrees\n",
    "                axs4[iaction_ana,icond_ana].set_xticklabels(axs4[iaction_ana,icond_ana].get_xticklabels(), \n",
    "                                                             rotation=45)\n",
    "\n",
    "    # Adjust layout to fit everything nicely\n",
    "    fig4.tight_layout()    \n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'strategy_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'strategy_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'strategy_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_CCAscores.pdf')\n",
    "        fig4.savefig(figsavefolder+'strategy_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_CCAscores_pooledAction.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and plot the PC trajectories' length curvature and tortuosity\n",
    "# no need to run CCA to align, because all bhv in the same pc space\n",
    "if 1:    \n",
    "\n",
    "    plottypes = ['PCtort','PCcurv','PClength','PCsmoothness']\n",
    "    nplottypes = np.shape(plottypes)[0]\n",
    "    #\n",
    "    # figures \n",
    "    fig3, axs3 = plt.subplots(nplottypes,nconds_to_ana)\n",
    "    fig3.set_figheight(6*nplottypes)\n",
    "    fig3.set_figwidth(6*nconds_to_ana)\n",
    "    \n",
    "    plottracetypes = ['PCspeed_trace','PCcurv_trace']\n",
    "    nplotracetypes = np.shape(plottracetypes)[0]\n",
    "    #\n",
    "    # figures\n",
    "    fig4, axs4 = plt.subplots(nplotracetypes,nconds_to_ana)\n",
    "    fig4.set_figheight(3*nplottypes)\n",
    "    fig4.set_figwidth(6*nconds_to_ana)\n",
    "    \n",
    "    #\n",
    "    FRPCAfeatures_all_dates_sum_df = pd.DataFrame(columns=['condition','act_animal',\n",
    "                                                         'bhv_name','PClength','PCcurv','PCtort',\n",
    "                                                         'iteration'])\n",
    "    \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_dates_sum_df['condition']==cond_ana    \n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]                   \n",
    "            ind_animal = FRPCA_all_dates_sum_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_dates_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                for iiter in np.arange(0,niters,1):\n",
    "\n",
    "                    FRPCA_all_dates_tgt = np.array(FRPCA_all_dates_sum_df[ind_ana]['PCs'])[iiter]\n",
    "\n",
    "                    FRPCA_ievent_toana = FRPCA_all_dates_tgt[timepointnums*ibhvname_ana:timepointnums*(ibhvname_ana+1),:]\n",
    "                    \n",
    "                    # # only the top 3 pcs\n",
    "                    # FRPCA_ievent_toana = FRPCA_ievent_toana[:,0:3]\n",
    "                    \n",
    "                    # smooth the pc trajectory\n",
    "                    FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                             arr=FRPCA_ievent_toana, sigma=6)\n",
    "\n",
    "                    # calculate the length, curvature and tortuosity\n",
    "                    PC_traj = FRPCA_ievent_toana  # Shape (240, 3)\n",
    "\n",
    "                    # Compute differences between consecutive points\n",
    "                    diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                    # Compute segment lengths\n",
    "                    segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                    total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                    # Compute curvature\n",
    "                    # First derivatives\n",
    "                    dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                    dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                    dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                    dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                    # Second derivatives\n",
    "                    d2X_dt2 = np.gradient(dX_dt)\n",
    "                    d2Y_dt2 = np.gradient(dY_dt)\n",
    "                    d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                    d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                    # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                    cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                    curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                    # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                    euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                    tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "                    \n",
    "                    # Compute speed \n",
    "                    dt = 1.0 / fps  # Time between frames\n",
    "                    # Velocity: first derivative of position\n",
    "                    velocity = np.gradient(PC_traj, axis=0) / dt\n",
    "                    # Speed: magnitude of velocity\n",
    "                    speed = np.linalg.norm(velocity, axis=1)\n",
    "                    \n",
    "                    # Compute Smoothness - A simple way to compute trajectory smoothness is to look at the jerk \n",
    "                    #  the third derivative of position (how quickly acceleration changes), \n",
    "                    # which reflects sudden directional/velocity shifts\n",
    "                    # Acceleration: second derivative\n",
    "                    acceleration = np.gradient(velocity, axis=0) / dt\n",
    "                    # Jerk: third derivative\n",
    "                    jerk = np.gradient(acceleration, axis=0) / dt\n",
    "                    # Smoothness metric: integrated squared jerk over time\n",
    "                    squared_jerk = np.linalg.norm(jerk, axis=1) ** 2\n",
    "                    smoothness = np.sum(squared_jerk) * dt\n",
    "\n",
    "                    #\n",
    "                    FRPCAfeatures_all_dates_sum_df = FRPCAfeatures_all_dates_sum_df.append({'condition':cond_ana,\n",
    "                                                                            'act_animal':act_animal_ana,\n",
    "                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                            'iteration':iiter,\n",
    "                                                                            'PClength':total_length,\n",
    "                                                                            'PCcurv':np.nanmean(curvature),\n",
    "                                                                            'PCtort':tortuosity,\n",
    "                                                                            'PCcurv_trace':curvature,\n",
    "                                                                            'PCsmoothness':smoothness,\n",
    "                                                                            'PCspeed_trace':speed,                \n",
    "                                                                           }, ignore_index=True)\n",
    "\n",
    "        # plot\n",
    "        for iplottype in np.arange(0,nplottypes,1):\n",
    "            \n",
    "            plottype = plottypes[iplottype]\n",
    "            \n",
    "            seaborn.violinplot(ax=axs3[iplottype,icond_ana],data=FRPCAfeatures_all_dates_sum_df,\n",
    "                               x = 'bhv_name',y=plottype)\n",
    "            \n",
    "            axs3[iplottype,icond_ana].set_title(act_animal_ana+' in '+cond_ana)\n",
    "            \n",
    "            # Perform ANOVA\n",
    "            df = FRPCAfeatures_all_dates_sum_df\n",
    "            anova_pval = st.f_oneway(*[df[df[\"bhv_name\"] == bhv][plottype] for bhv in bhv_names_to_ana]).pvalue\n",
    "\n",
    "            # Perform post hoc Tukey's HSD test\n",
    "            tukey = pairwise_tukeyhsd(df[plottype], df[\"bhv_name\"], alpha=0.05)\n",
    "\n",
    "            # Extract raw p-values from Tukey's test\n",
    "            raw_pvals = np.array([row[3] for row in tukey.summary().data[1:]])\n",
    "\n",
    "            # Apply Benjamini-Hochberg correction (FDR)\n",
    "            # _, adj_pvals, _, _ = multipletests(raw_pvals, method='fdr_bh')\n",
    "            adj_pvals = raw_pvals*nbhvnames_to_ana*(nbhvnames_to_ana-1)/2 # *np.sqrt(niters)\n",
    "\n",
    "            # Extract significant pairs after FDR correction\n",
    "            sig_pairs = [(row[0], row[1], adj_p) for row, adj_p in zip(tukey.summary().data[1:], adj_pvals) if adj_p < 0.05]\n",
    "\n",
    "            # Add asterisks for significant comparisons\n",
    "            y_max = df[plottype].max() + 0.002  # Base y position for annotations\n",
    "            y_step = 1  # Step to avoid overlap\n",
    "\n",
    "            for i, (g1, g2, pval) in enumerate(sig_pairs):\n",
    "                x1, x2 = list(bhv_names_to_ana).index(g1), list(bhv_names_to_ana).index(g2)\n",
    "                significance = \"*\" if pval >= 0.01 else \"**\" if pval >= 0.001 else \"***\"\n",
    "\n",
    "                # Plot the significance line\n",
    "                axs3[iplottype,icond_ana].plot([x1, x1, x2, x2], [y_max, y_max + y_step, y_max + y_step, y_max], color=\"black\")\n",
    "\n",
    "                # Add the significance label\n",
    "                axs3[iplottype,icond_ana].text((x1 + x2) / 2, y_max + y_step * 1.2, significance, ha='center', fontsize=14, color=\"red\")\n",
    "\n",
    "                y_max += y_step * 2  # Move y position up for next annotation\n",
    "\n",
    "        # Adjust layout to fit everything nicely\n",
    "        fig3.tight_layout()\n",
    "\n",
    "    \n",
    "    # plot the speed and curvature trace\n",
    "    # Convert string-lists to real lists if necessary\n",
    "    for col in plottracetypes:\n",
    "        if isinstance(FRPCAfeatures_all_dates_sum_df[col].iloc[0], str):\n",
    "            FRPCAfeatures_all_dates_sum_df[col] = FRPCAfeatures_all_dates_sum_df[col].apply(eval)\n",
    "\n",
    "    # Get unique conditions and behavior types\n",
    "    conditions = FRPCAfeatures_all_dates_sum_df['condition'].unique()\n",
    "    nconds_to_ana = len(conditions)\n",
    "    bhv_names = FRPCAfeatures_all_dates_sum_df['bhv_name'].unique()\n",
    "\n",
    "    # Loop through trace types (rows) and conditions (columns)\n",
    "    for i, trace_type in enumerate(plottracetypes):\n",
    "        max_len = max(FRPCAfeatures_all_dates_sum_df[trace_type].apply(len))  # For consistent x-axis\n",
    "\n",
    "        for j, cond in enumerate(conditions):\n",
    "            ax = axs4[i, j]\n",
    "            df_cond = FRPCAfeatures_all_dates_sum_df[FRPCAfeatures_all_dates_sum_df['condition'] == cond]\n",
    "\n",
    "            for bhv in bhv_names:\n",
    "                df_bhv = df_cond[df_cond['bhv_name'] == bhv]\n",
    "\n",
    "                if df_bhv.empty:\n",
    "                    continue  # Skip if no data for this bhv_name under this condition\n",
    "\n",
    "                # Smooth each trace first, then pad\n",
    "                smoothed_traces = []\n",
    "                for trace in df_bhv[trace_type]:\n",
    "                    trace = np.array(trace)\n",
    "                    smoothed = gaussian_filter1d(trace, sigma=6)\n",
    "                    padded = np.pad(smoothed, (0, max_len - len(smoothed)), constant_values=np.nan)\n",
    "                    smoothed_traces.append(padded)\n",
    "                #\n",
    "                traces = np.array(smoothed_traces)\n",
    "\n",
    "                # Compute mean and SEM\n",
    "                mean_trace = np.nanmean(traces, axis=0)\n",
    "                sem_trace = np.nanstd(traces, axis=0) / np.sqrt(np.sum(~np.isnan(traces), axis=0))\n",
    "\n",
    "                # x = np.arange(len(mean_trace))\n",
    "                try:\n",
    "                    x = np.arange(-4,4,1/30)\n",
    "                    ax.plot(x, mean_trace, label=bhv)\n",
    "                    ax.fill_between(x, mean_trace - sem_trace, mean_trace + sem_trace, alpha=0.3)\n",
    "                except:\n",
    "                    x = np.arange(-4+1/30,4,1/30)\n",
    "                    ax.plot(x, mean_trace, label=bhv)\n",
    "                    ax.fill_between(x, mean_trace - sem_trace, mean_trace + sem_trace, alpha=0.3)\n",
    "\n",
    "            # Axis titles and labels\n",
    "            ax.set_title(f'{trace_type}  {cond}', fontsize=10)\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            ax.set_ylabel(trace_type)\n",
    "            ax.legend()\n",
    "            #if i == 0 and j == nconds_to_ana - 1:\n",
    "            #     ax.legend(title='Behavior', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Average Traces with SEM by Condition and Behavior', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig3.savefig(figsavefolder+'strategy_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_trajectoryFeatures.pdf')\n",
    "        fig4.savefig(figsavefolder+'strategy_aligned_PCspace_allbhvTogether_DiffBhvSameCond_trajectory_allconditions'+savefile_sufix+'_trajectoryFeatureTraces.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4673a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72082b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e94e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e268f394",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately for each condition\n",
    "#### for the activity aligned at the different strategies\n",
    "#### this is for summarizing the firing and do some basic plotting to get a general sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f831047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "strategy_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_allevents'])\n",
    "strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_average'])\n",
    "\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "\n",
    "    bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "\n",
    "        clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if np.shape(ibhv_type_split)[0]==3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "            \n",
    "\n",
    "            ichannelID = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            strategy_aligned_FR_allevents_all_dates_df = strategy_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_allevents':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            #\n",
    "            ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_average':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ce021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "# act_animals_to_ana = ['kanga']\n",
    "act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "bhv_names_to_ana = ['gaze_lead_pull', 'not_gaze_lead_pull','social_attention', 'not_social_attention']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "# conditions_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['condition'])\n",
    "conditions_to_ana = ['MC']\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "\n",
    "# load the data \n",
    "for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "    cond_ana = conditions_to_ana[icond_ana]\n",
    "    ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "    for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "        act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "        ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "        \n",
    "        # get the dates\n",
    "        dates_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "        ndates_ana = np.shape(dates_ana)[0]\n",
    "        \n",
    "        for idate_ana in np.arange(0,ndates_ana,1):\n",
    "            date_ana = dates_ana[idate_ana]\n",
    "            ind_date = strategy_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "           \n",
    "            # get the neurons \n",
    "            neurons_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "            nneurons = np.shape(neurons_ana)[0]\n",
    "            \n",
    "            # Determine subplot grid (5 columns, dynamic rows)\n",
    "            ncols = 5\n",
    "            nrows = int(np.ceil(nneurons / ncols))\n",
    "        \n",
    "            fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "            axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "        \n",
    "            for ineuron in np.arange(0,nneurons,1):\n",
    "                clusterID_ineuron = neurons_ana[ineuron]\n",
    "                ind_neuron = strategy_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "                \n",
    "                ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "                    \n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                    strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "            \n",
    "                    \n",
    "                    FRs_allevents_ineuron = np.array(strategy_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "                    \n",
    "                    nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "                    \n",
    "                    FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "                    \n",
    "                    # Compute mean and SEM while ignoring NaNs\n",
    "                    mean_trace = np.nanmean(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                    std_trace = np.nanstd(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                    sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                    # Plot the results\n",
    "                    time = np.arange(-4,4,1/30)  # Assuming time is just indices\n",
    "\n",
    "                    # Plot each behavior as a separate trace\n",
    "                    ax.plot(time, mean_trace, label=bhvname_ana+'(n='+str(nevents)+')', \n",
    "                            color=bhvname_clrs[ibhvname_ana])\n",
    "                    ax.fill_between(time, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                    color=bhvname_clrs[ibhvname_ana], alpha=0.3)\n",
    "\n",
    "                ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                ax.set_xlabel(\"Time (s)\")\n",
    "                ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                ax.legend()\n",
    "               \n",
    "            # Hide empty subplots if nneurons < total grid size\n",
    "            for i in range(nneurons, len(axes)):\n",
    "                fig.delaxes(axes[i])\n",
    "\n",
    "            # Figure title\n",
    "            fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)\n",
    "\n",
    "            plt.show()\n",
    "                   \n",
    "            savefig = 1\n",
    "            if savefig:\n",
    "                figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/StrategyAlignedFRs_allcells/\"\n",
    "                figsavefolder = figsavefolder+act_animal_ana+\"/\"+cond_ana+\"/\"+date_ana+\"/\"\n",
    "\n",
    "                if not os.path.exists(figsavefolder):\n",
    "                    os.makedirs(figsavefolder)\n",
    "\n",
    "                fig.savefig(figsavefolder+'strategy_aligned_FRs_trace_allcells_summary.pdf')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a17563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75bb1c01",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately for each condition\n",
    "#### for the activity aligned at the different strategies\n",
    "#### run PCA for all strategies together combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "strategy_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_allevents'])\n",
    "strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','FR_average'])\n",
    "\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "\n",
    "    bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "\n",
    "        clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if np.shape(ibhv_type_split)[0]==3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "\n",
    "        for iclusterID in clusterIDs:\n",
    "            \n",
    "\n",
    "            ichannelID = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            strategy_aligned_FR_allevents_all_dates_df = strategy_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_allevents':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "\n",
    "            #\n",
    "            ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                    'condition':task_condition,\n",
    "                                                                                    'act_animal':ibhv_type_split[0],\n",
    "                                                                                    'bhv_name': ibhv_type_split[1],\n",
    "                                                                                    'clusterID':iclusterID,\n",
    "                                                                                    'channelID':ichannelID,\n",
    "                                                                                    'FR_average':iFR_average,\n",
    "                                                                                   }, ignore_index=True)\n",
    "            \n",
    "# act_animals_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "# act_animals_to_ana = ['kanga']\n",
    "act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', 'not_social_attention']\n",
    "bhv_names_to_ana = ['gaze_lead_pull', 'not_gaze_lead_pull','social_attention', 'not_social_attention']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "# conditions_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['condition'])\n",
    "conditions_to_ana = ['MC']\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "# Step 1 - run PCA separately\n",
    "# save the simple PCA data\n",
    "FRPCA_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','act_animal',\n",
    "                                                            'bhv_name','bhv_id','PCs',])\n",
    "\n",
    "\n",
    "for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "    cond_ana = conditions_to_ana[icond_ana]\n",
    "    ind_cond = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "    for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "        act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "        ind_animal = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "        \n",
    "        # get the dates\n",
    "        dates_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "        ndates_ana = np.shape(dates_ana)[0]\n",
    "        \n",
    "        for idate_ana in np.arange(0,ndates_ana,1):\n",
    "            date_ana = dates_ana[idate_ana]\n",
    "            ind_date = strategy_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "            \n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                # to better combine different bhv events, choose the same amount\n",
    "                nbhv_topick = 50\n",
    "                \n",
    "                # Convert list of arrays into a single NumPy array \n",
    "                data_array = np.array(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                valid_bhvs = ~np.any(np.isnan(data_array), axis=(0, 1))  # Shape (144,)\n",
    "                data_array = data_array[:, :, valid_bhvs]\n",
    "                \n",
    "                nneurons = np.shape(data_array)[0]\n",
    "                timepointnums = np.shape(data_array)[1]\n",
    "                mbhv_total = np.shape(data_array)[2]\n",
    "                \n",
    "                # Randomly select bhv events with replacement, once for all neurons\n",
    "                selected_bhvs = np.random.choice(mbhv_total, nbhv_topick, replace=True)\n",
    "                sampled_data = data_array[:, :, selected_bhvs]\n",
    "\n",
    "                # Reshape to (4, 4800) by flattening the last two dimensions\n",
    "                final_array = sampled_data.reshape(nneurons, -1)\n",
    "                \n",
    "                PCA_dataset_ibv = final_array\n",
    "                \n",
    "                # combine all bhv for running PCA in the same neural space\n",
    "                if ibhvname_ana == 0:\n",
    "                    PCA_dataset = PCA_dataset_ibv\n",
    "                else:\n",
    "                    PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "\n",
    "            # remove nan raw from the data set\n",
    "            # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "            # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "            ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "            PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "            PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "            # Run PCA on this concatenated data \n",
    "            pca = PCA(n_components=3)\n",
    "            pca.fit(PCA_dataset)\n",
    "            \n",
    "            totalneuronNum = np.shape(PCA_dataset)[1]\n",
    "                \n",
    "            # project on the individual events\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                \n",
    "                # Convert list of arrays into a single NumPy array \n",
    "                data_array = np.array(list(strategy_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                for ibhv in np.arange(0,mbhv_total,1):\n",
    "                    \n",
    "                    data_ibhv = data_array[:,:,ibhv]\n",
    "                    \n",
    "                    try:\n",
    "                        PCA_proj_ibhv = pca.transform(np.transpose(data_ibhv))\n",
    "                    except:\n",
    "                        PCA_proj_ibhv = np.full((timepointnums, 3), np.nan)\n",
    "                \n",
    "                    FRPCA_all_sessions_allevents_sum_df = FRPCA_all_sessions_allevents_sum_df.append({'condition':cond_ana,\n",
    "                                                                            'act_animal':act_animal_ana,\n",
    "                                                                            'bhv_name': bhvname_ana,\n",
    "                                                                            'session':date_ana,\n",
    "                                                                            'bhv_id':ibhv,\n",
    "                                                                            'PCs':PCA_proj_ibhv,\n",
    "                                                                            'neuronNumBeforePCA':totalneuronNum,\n",
    "                                                                           }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c1794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some plotting\n",
    "# only plot a few trace\n",
    "if 1:\n",
    "    fig2 = plt.figure(figsize=(6,6))\n",
    "    ax2 = fig2.add_subplot(1,1,1,projection = '3d')\n",
    "\n",
    "    condition_toplot = 'MC'\n",
    "    ind_cond_toplot = FRPCA_all_sessions_allevents_sum_df['condition']==condition_toplot\n",
    "\n",
    "    sessions_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df['session'])\n",
    "    session_toplot = sessions_toplot[3]\n",
    "    ind_sess_toplot = FRPCA_all_sessions_allevents_sum_df['session']==session_toplot\n",
    "\n",
    "    animals_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df['act_animal'])\n",
    "    animal_toplot = 'dodson'\n",
    "    # animal_toplot = 'kanga'\n",
    "    ind_animal_toplot = FRPCA_all_sessions_allevents_sum_df['act_animal']==animal_toplot\n",
    "\n",
    "    # bhvs_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df['bhv_name'])\n",
    "    # bhvs_toplot = ['gaze_lead_pull','synced_pull']\n",
    "    bhvs_toplot = ['gaze_lead_pull','not_gaze_lead_pull']\n",
    "    bhvs_trace_colors = ['#2e62c5','#00adee']\n",
    "    nbhvs_toplot = np.shape(bhvs_toplot)[0]\n",
    "\n",
    "    for ibhv_toplot in np.arange(0,nbhvs_toplot,1):\n",
    "\n",
    "        bhv_toplot = bhvs_toplot[ibhv_toplot]\n",
    "        ind_bhv_toplot = FRPCA_all_sessions_allevents_sum_df['bhv_name']==bhv_toplot\n",
    "\n",
    "        ind_toplot = ind_cond_toplot & ind_sess_toplot & ind_bhv_toplot & ind_animal_toplot\n",
    "        FRPCA_allevents_toplot = FRPCA_all_sessions_allevents_sum_df[ind_toplot]\n",
    "\n",
    "        nevents_toplot = 10\n",
    "        events_id_toplot = np.random.choice(FRPCA_allevents_toplot['bhv_id'], size=nevents_toplot, replace=False)\n",
    "\n",
    "        for ievent_toplot in np.arange(0,nevents_toplot,1):\n",
    "\n",
    "            ind_event_toplot = FRPCA_allevents_toplot['bhv_id'] == events_id_toplot[ievent_toplot]\n",
    "            FRPCA_ievent_toplot = FRPCA_allevents_toplot[ind_event_toplot]['PCs']\n",
    "            FRPCA_ievent_toplot = np.array(FRPCA_ievent_toplot)[0]\n",
    "\n",
    "            # plot 3d PC1,2,3 trace\n",
    "            if ievent_toplot == 0:\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_ievent_toplot[:,0], 6),\n",
    "                         gaussian_filter1d(FRPCA_ievent_toplot[:,1], 6),\n",
    "                         gaussian_filter1d(FRPCA_ievent_toplot[:,2], 6),\n",
    "                         color = bhvs_trace_colors[ibhv_toplot],label = bhv_toplot,\n",
    "                        )\n",
    "            else:\n",
    "                ax2.plot(gaussian_filter1d(FRPCA_ievent_toplot[:,0], 6),\n",
    "                         gaussian_filter1d(FRPCA_ievent_toplot[:,1], 6),\n",
    "                         gaussian_filter1d(FRPCA_ievent_toplot[:,2], 6),\n",
    "                         color = bhvs_trace_colors[ibhv_toplot],\n",
    "                        )\n",
    "\n",
    "    ax2.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each PCA trace, calculate the length, curvature, and/or tortusity for comparison later\n",
    "if 1:\n",
    "    \n",
    "    import statsmodels.formula.api as smf\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    FRPCAfeatures_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','act_animal',\n",
    "                                                                        'bhv_name','bhv_id',\n",
    "                                                                        'PClength','PCcurv','PCtort'])\n",
    "    \n",
    "    condition_toana = 'MC'\n",
    "    ind_cond_toana = FRPCA_all_sessions_allevents_sum_df['condition']==condition_toana\n",
    "\n",
    "    sessions_toana = np.unique(FRPCA_all_sessions_allevents_sum_df['session'])\n",
    "    nsessions_toana = np.shape(sessions_toana)[0]\n",
    "    # session_toana = sessions_toana[3]\n",
    "    # ind_sess_toana = FRPCA_all_sessions_allevents_sum_df['session']==session_toana\n",
    "\n",
    "    animals_toana = np.unique(FRPCA_all_sessions_allevents_sum_df['act_animal'])\n",
    "    animal_toana = 'dodson'\n",
    "    # animal_toana = 'kanga'\n",
    "    ind_animal_toana = FRPCA_all_sessions_allevents_sum_df['act_animal']==animal_toana\n",
    "\n",
    "    # bhvs_toana = np.unique(FRPCA_all_sessions_allevents_sum_df['bhv_name'])\n",
    "    bhvs_toana = ['gaze_lead_pull','not_gaze_lead_pull']\n",
    "    # bhvs_toana = ['social_attention','not_social_attention']\n",
    "    nbhvs_toana = np.shape(bhvs_toana)[0]\n",
    "    \n",
    "    # figures \n",
    "    fig5, axs5 = plt.subplots(3,nsessions_toana)\n",
    "    fig5.set_figheight(6*3)\n",
    "    fig5.set_figwidth(6*nsessions_toana)\n",
    "    \n",
    "    # figures \n",
    "    fig7, axs7 = plt.subplots(1,3)\n",
    "    fig7.set_figheight(6)\n",
    "    fig7.set_figwidth(6*3)\n",
    "    \n",
    "    for ibhv_toana in np.arange(0,nbhvs_toana,1):\n",
    "\n",
    "        bhv_toana = bhvs_toana[ibhv_toana]\n",
    "        ind_bhv_toana = FRPCA_all_sessions_allevents_sum_df['bhv_name']==bhv_toana\n",
    "\n",
    "        \n",
    "        for isession_toana in np.arange(0,nsessions_toana,1):\n",
    "            session_toana = sessions_toana[isession_toana]\n",
    "            ind_sess_toana = FRPCA_all_sessions_allevents_sum_df['session']==session_toana     \n",
    "        \n",
    "            ind_toana = ind_cond_toana & ind_sess_toana & ind_bhv_toana & ind_animal_toana\n",
    "            FRPCA_allevents_toana = FRPCA_all_sessions_allevents_sum_df[ind_toana]\n",
    "\n",
    "            bhv_ids = np.array(FRPCA_allevents_toana['bhv_id'])\n",
    "            nbhvevents = np.shape(bhv_ids)[0]\n",
    "            \n",
    "            for ibhv_id in np.arange(0,nbhvevents,1):\n",
    "                \n",
    "                bhv_id = bhv_ids[ibhv_id]\n",
    "                ind_bhvid = FRPCA_allevents_toana['bhv_id'] == bhv_id\n",
    "                \n",
    "                FRPCA_ievent_toana = np.array(FRPCA_allevents_toana[ind_bhvid]['PCs'])[0]\n",
    "                \n",
    "                # smooth the pc trajectory\n",
    "                FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                         arr=FRPCA_ievent_toana, sigma=6)\n",
    "                \n",
    "                # calculate the length, curvature and tortuosity\n",
    "                PC_traj = FRPCA_ievent_toana  # Shape (240, 3)\n",
    "\n",
    "                # Compute differences between consecutive points\n",
    "                diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                # Compute segment lengths\n",
    "                segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                # Compute curvature\n",
    "                # First derivatives\n",
    "                dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                # Second derivatives\n",
    "                d2X_dt2 = np.gradient(dX_dt)\n",
    "                d2Y_dt2 = np.gradient(dY_dt)\n",
    "                d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "\n",
    "                FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df.append({'condition':cond_ana,\n",
    "                                                                            'act_animal':animal_toana,\n",
    "                                                                            'bhv_name': bhv_toana,\n",
    "                                                                            'session':session_toana,\n",
    "                                                                            'bhv_id':ibhv_id,\n",
    "                                                                            'PClength':total_length,\n",
    "                                                                            'PCcurv':np.nanmean(curvature),\n",
    "                                                                            'PCtort':tortuosity,\n",
    "                                                                           }, ignore_index=True)\n",
    "                \n",
    "    # plot\n",
    "    plottypes = ['PCtort','PCcurv','PClength']\n",
    "    nplottypes = np.shape(plottypes)[0]\n",
    "        \n",
    "    # plot each session separately\n",
    "    for isession_toplot in np.arange(0,nsessions_toana,1):\n",
    "        session_toplot = sessions_toana[isession_toplot]\n",
    "        ind_sess_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['session']==session_toplot\n",
    "        \n",
    "        FRPCAfeatures_isession_toplot = FRPCAfeatures_all_sessions_allevents_sum_df[ind_sess_toplot]\n",
    " \n",
    "        for iplottype in np.arange(0,nplottypes,1):\n",
    "        \n",
    "            plottype = plottypes[iplottype]\n",
    "            \n",
    "            # plot \n",
    "            seaborn.violinplot(ax=axs5[iplottype,isession_toplot],data=FRPCAfeatures_isession_toplot,\n",
    "                           x='bhv_name',y=plottype)        \n",
    "            # Drop NaN values \n",
    "            df_anova = FRPCAfeatures_isession_toplot.dropna(subset=[plottype])\n",
    "            # Run ANOVA\n",
    "            model = smf.ols(plottype+' ~ C(bhv_name)', data=df_anova).fit()\n",
    "            anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "            # Extract and print p-value\n",
    "            p_value = anova_table[\"PR(>F)\"][0]\n",
    "            p_value_text = \"anova \"+f\"p = {p_value:.4f}\"\n",
    "            #\n",
    "            # Add text annotation to the plot\n",
    "            axs5[iplottype, isession_toplot].text(0.5, 0.9, p_value_text, \n",
    "                                  transform=axs5[iplottype, isession_toplot].transAxes,\n",
    "                                  ha=\"center\", fontsize=12, color=\"red\", fontweight=\"bold\")\n",
    "\n",
    "\n",
    "    # plot pooled sesison\n",
    "    for iplottype in np.arange(0,nplottypes,1):\n",
    "        \n",
    "        plottype = plottypes[iplottype]\n",
    "\n",
    "        # plot \n",
    "        seaborn.violinplot(ax=axs7[iplottype],data=FRPCAfeatures_all_sessions_allevents_sum_df,\n",
    "                       x='bhv_name',y=plottype)        \n",
    "        # Drop NaN values \n",
    "        df_anova = FRPCAfeatures_isession_toplot.dropna(subset=[plottype])\n",
    "        # Run ANOVA\n",
    "        model = smf.ols(plottype+' ~ C(bhv_name)', data=df_anova).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        # Extract and print p-value\n",
    "        p_value = anova_table[\"PR(>F)\"][0]\n",
    "        p_value_text = \"anova \"+f\"p = {p_value:.4f}\"\n",
    "        #\n",
    "        # Add text annotation to the plot\n",
    "        axs7[iplottype].text(0.5, 0.9, p_value_text, \n",
    "                              transform=axs7[iplottype].transAxes,\n",
    "                              ha=\"center\", fontsize=12, color=\"red\", fontweight=\"bold\")\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e62413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604cf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b6e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb625e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2772da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc6cbab",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately each condition\n",
    "#### for the activity aligned at the different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164cd1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "    strategy_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_allevents'])\n",
    "    strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                ichannelID = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                strategy_aligned_FR_allevents_all_dates_df = strategy_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                #\n",
    "                ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "    # act_animals_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "    # act_animals_to_ana = ['kanga']\n",
    "    act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', ]\n",
    "    # bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull',]\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "    # \n",
    "\n",
    "    # figures\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "    #\n",
    "    # 3d figure trace\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "    #\n",
    "    # 3d figure around the action, for the averaged in one session\n",
    "    fig3 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "    #\n",
    "    # 3d figure around the action, for each action\n",
    "    fig4 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond_allevents = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "        ind_cond = strategy_aligned_FR_all_dates_df['condition']==cond_ana    \n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "        ax3 = fig3.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "        ax4 = fig4.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal_allevents = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "            ind_animal = strategy_aligned_FR_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv_allevents = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "                ind_bhv = strategy_aligned_FR_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_allevents = ind_animal_allevents & ind_bhv_allevents & ind_cond_allevents\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana_allevents]\n",
    "                strategy_aligned_FR_tgt = strategy_aligned_FR_all_dates_df[ind_ana]\n",
    "\n",
    "                # separate for each dates\n",
    "                dates_to_ana = np.unique(strategy_aligned_FR_tgt['dates'])\n",
    "                ndates_ana = np.shape(dates_to_ana)[0]\n",
    "\n",
    "                for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                    date_ana = dates_to_ana[idate_ana]\n",
    "                    ind_date_allevents = strategy_aligned_FR_allevents_tgt['dates']==date_ana\n",
    "                    ind_date = strategy_aligned_FR_tgt['dates']==date_ana\n",
    "\n",
    "                    try:\n",
    "                        # get the PCA training data set\n",
    "                        #\n",
    "                        ncells = np.shape(strategy_aligned_FR_allevents_tgt[ind_date_allevents])[0]\n",
    "                        PCA_dataset_train_pre_df = pd.DataFrame(columns=['clusterID','channelID','FR_pooled','FR_allevents'])\n",
    "                        PCA_dataset_train_pre_df['clusterID'] = strategy_aligned_FR_allevents_tgt[ind_date_allevents]['clusterID']\n",
    "                        PCA_dataset_train_pre_df['channelID'] = strategy_aligned_FR_allevents_tgt[ind_date_allevents]['channelID']\n",
    "                        PCA_dataset_train_pre_df['FR_allevents'] = strategy_aligned_FR_allevents_tgt[ind_date_allevents]['FR_allevents']\n",
    "                        #\n",
    "                        for icell in np.arange(0,ncells,1):\n",
    "                            FR_ravel = np.ravel(strategy_aligned_FR_allevents_tgt[ind_date_allevents]['FR_allevents'].iloc[icell])\n",
    "                            PCA_dataset_train_pre_df['FR_pooled'].iloc[icell] = FR_ravel\n",
    "                        PCA_dataset_train = np.array(list(PCA_dataset_train_pre_df['FR_pooled']))\n",
    "                        # remove nan raw from the data set\n",
    "                        ind_nan = np.isnan(np.sum(PCA_dataset_train,axis=0))\n",
    "                        PCA_dataset_train = PCA_dataset_train[:,~ind_nan]\n",
    "\n",
    "                        # get the PCA test dataset\n",
    "                        PCA_dataset_test = np.array(list(strategy_aligned_FR_tgt[ind_date]['FR_average']))\n",
    "                        # remove nan raw from the data set\n",
    "                        ind_nan = np.isnan(np.sum(PCA_dataset_test,axis=0))\n",
    "                        PCA_dataset_test = PCA_dataset_test[:,~ind_nan]\n",
    "\n",
    "                        # run PCA\n",
    "                        pca = PCA(n_components=3)\n",
    "                        pca.fit(PCA_dataset_train.transpose())\n",
    "                        PCA_dataset_train_proj = pca.transform(PCA_dataset_train.transpose())\n",
    "                        PCA_dataset_proj = pca.transform(PCA_dataset_test.transpose())\n",
    "\n",
    "                        trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "                        xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "                        # plot PC1\n",
    "                        axs1[0,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                               label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                        axs1[1,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                               label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "                        axs1[2,icond_ana].plot(xxx_forplot,gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                               label=act_animal_ana+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                        # plot 3d PC1,2,3 trace\n",
    "                        ax2.plot(gaussian_filter1d(PCA_dataset_proj[:,0], 6),\n",
    "                                 gaussian_filter1d(PCA_dataset_proj[:,1], 6),\n",
    "                                 gaussian_filter1d(PCA_dataset_proj[:,2], 6),\n",
    "                                 label=act_animal_ana+' '+bhvname_ana,\n",
    "                                 color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "                        # plot 3d PC1,2,3 datapoint around action\n",
    "                        ind_twin = (xxx_forplot<=0.05*fps)&(xxx_forplot>=-0.2*fps)\n",
    "                        xpoint = np.nanmean(PCA_dataset_proj[ind_twin,0])\n",
    "                        ypoint = np.nanmean(PCA_dataset_proj[ind_twin,1])\n",
    "                        zpoint = np.nanmean(PCA_dataset_proj[ind_twin,2])\n",
    "                        ax3.plot(xpoint,ypoint,zpoint,'o',\n",
    "                                 label=act_animal_ana+' '+bhvname_ana,\n",
    "                                 color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "                        # run PCA on the individual action in each session\n",
    "                        if date_ana == '20240509':\n",
    "                            PCA_indivacts_df = np.array(PCA_dataset_train_pre_df['FR_allevents'])\n",
    "                            ncells = np.shape(PCA_indivacts_df)[0]\n",
    "                            ntsteps = np.shape(PCA_indivacts_df[0])[0]\n",
    "                            nacts = np.shape(PCA_indivacts_df[0])[1]\n",
    "                            PCA_indivacts = np.empty((ncells,ntsteps,nacts))\n",
    "                            for icell in np.arange(0,ncells,1):\n",
    "                                PCA_indivacts[icell,:,:] = PCA_indivacts_df[icell]\n",
    "                            #\n",
    "                            for iact in np.arange(0,nacts,6):\n",
    "                                PCA_dataset_test_iact = PCA_indivacts[:,:,iact]\n",
    "\n",
    "                                if ~np.isnan(np.sum(PCA_dataset_test_iact)):\n",
    "                                    PCA_dataset_proj_iact = pca.transform(PCA_dataset_test_iact.transpose())\n",
    "                                    ind_twin = (xxx_forplot<=0.05*fps)&(xxx_forplot>=-0.2*fps)\n",
    "                                    xpoint = np.nanmean(PCA_dataset_proj_iact[ind_twin,0])\n",
    "                                    ypoint = np.nanmean(PCA_dataset_proj_iact[ind_twin,1])\n",
    "                                    zpoint = np.nanmean(PCA_dataset_proj_iact[ind_twin,2])\n",
    "                                    if iact == 0:\n",
    "                                        ax4.plot(xpoint,ypoint,zpoint,'o',\n",
    "                                                 label=act_animal_ana+' '+bhvname_ana,\n",
    "                                                 color=bhvname_clrs[ibhvname_ana])\n",
    "                                    else:\n",
    "                                        ax4.plot(xpoint,ypoint,zpoint,'o',\n",
    "                                                 color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')   \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "        # ax2.legend() \n",
    "\n",
    "        ax3.set_xlabel('PC1')\n",
    "        ax3.set_ylabel('PC2') \n",
    "        ax3.set_zlabel('PC3')    \n",
    "        ax3.view_init(elev=30, azim=-30) \n",
    "        # ax3.view_init(elev=90, azim=-90) # PC1 and PC2\n",
    "        # ax3.view_init(elev= 0, azim=-90) # PC1 and PC3\n",
    "        # ax3.view_init(elev= 0, azim=  0) # PC2 and PC3\n",
    "        # ax3.legend() \n",
    "\n",
    "        ax4.set_xlabel('PC1')\n",
    "        ax4.set_ylabel('PC2') \n",
    "        ax4.set_zlabel('PC3')    \n",
    "        ax4.view_init(elev=30, azim=-30) \n",
    "        # ax4.view_init(elev=90, azim=-90) # PC1 and PC2\n",
    "        # ax4.view_init(elev= 0, azim=-90) # PC1 and PC3\n",
    "        # ax4.view_init(elev= 0, azim=  0) # PC2 and PC3\n",
    "        # ax4.legend() \n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_eachsession'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_eachsession'+savefile_sufix+'.pdf')\n",
    "        fig4.savefig(figsavefolder+'stretagy_aligned_PCspace_eventtimestamp_examplesession'+savefile_sufix+'.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48004e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0433ef",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately each condition\n",
    "#### for the activity aligned at the different strategies\n",
    "#### to make all conditions in the \"same\" neural space, use psedo population with the \"same\" number of neurons and concantanate neural activity across all conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0321a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "    strategy_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_allevents'])\n",
    "    strategy_aligned_FR_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','FR_average'])\n",
    "    # strategy_aligned_FR_allevents_all_dates_sepevents_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "    #                                                        'channelID','eventID','FR_ievent'])\n",
    "\n",
    "    # reorganize to a dataframes\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "\n",
    "        bhv_types = list(strategy_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "        for ibhv_type in bhv_types:\n",
    "\n",
    "            clusterIDs = list(strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "            for iclusterID in clusterIDs:\n",
    "\n",
    "                # averaged FR across events\n",
    "                ichannelID = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "                strategy_aligned_FR_all_dates_df = strategy_aligned_FR_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_average':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                # FR for individual events\n",
    "                ichannelID = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_average = strategy_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "                strategy_aligned_FR_allevents_all_dates_df = strategy_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type.split()[0],\n",
    "                                                                                        'bhv_name': ibhv_type.split()[1],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_allevents':iFR_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "\n",
    "                # nevents = np.shape(iFR_average)[1]\n",
    "                # \n",
    "                # for ievent in np.arange(0,nevents,1):\n",
    "                #     strategy_aligned_FR_allevents_all_dates_sepevents_df = strategy_aligned_FR_allevents_all_dates_sepevents_df.append({'dates': date_tgt, \n",
    "                #                                                                         'condition':task_condition,\n",
    "                #                                                                         'act_animal':ibhv_type.split()[0],\n",
    "                #                                                                         'bhv_name': ibhv_type.split()[1],\n",
    "                #                                                                         'clusterID':iclusterID,\n",
    "                #                                                                         'channelID':ichannelID,\n",
    "                #                                                                         'eventID':ievent,                \n",
    "                #                                                                         'FR_ievent':iFR_average[:,ievent],\n",
    "                #                                                                        }, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # act_animals_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "    # act_animals_to_ana = ['kanga']\n",
    "    act_animals_to_ana = ['dodson']\n",
    "    nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "    #\n",
    "    # bhv_names_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "    bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull','social_attention', ]\n",
    "    # bhv_names_to_ana = ['gaze_lead_pull', 'synced_pull',]\n",
    "    nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "    bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "    #\n",
    "    conditions_to_ana = np.unique(strategy_aligned_FR_allevents_all_dates_df['condition'])\n",
    "    nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "    # \n",
    "\n",
    "\n",
    "\n",
    "    # concatanate firing rate for individal events\n",
    "    # random sampling 500 time to create a new pseudo neural population\n",
    "    nsamples = 300\n",
    "    strategy_aligned_FR_sepevents_tgt = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                           'channelID','eventID','FR_ievent'])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond_allevents = strategy_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal_allevents = strategy_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv_allevents = strategy_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana_allevents = ind_animal_allevents & ind_bhv_allevents & ind_cond_allevents\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond\n",
    "\n",
    "                strategy_aligned_FR_allevents_tgt = strategy_aligned_FR_allevents_all_dates_df[ind_ana_allevents]\n",
    "\n",
    "                nentries = np.shape(strategy_aligned_FR_allevents_tgt)[0]\n",
    "\n",
    "                isample = 0\n",
    "\n",
    "                # randomly sample\n",
    "                while isample < nsamples:\n",
    "\n",
    "                    try:\n",
    "                        # randomly pick the entry\n",
    "                        ientry = random.randint(0, nentries-1)\n",
    "\n",
    "                        strategy_aligned_FR_ientry = strategy_aligned_FR_allevents_tgt.iloc[ientry]\n",
    "\n",
    "                        nevents = np.shape(strategy_aligned_FR_ientry['FR_allevents'])[1]\n",
    "\n",
    "                        # randomly pick the event\n",
    "                        ievent = random.randint(0, nevents-1)\n",
    "\n",
    "                        if ~np.isnan(np.sum(strategy_aligned_FR_ientry['FR_allevents'][:,ievent])):\n",
    "                            strategy_aligned_FR_sepevents_tgt = strategy_aligned_FR_sepevents_tgt.append({'dates': strategy_aligned_FR_ientry['dates'], \n",
    "                                                                                                    'condition':strategy_aligned_FR_ientry['condition'],\n",
    "                                                                                                    'act_animal':strategy_aligned_FR_ientry['act_animal'],\n",
    "                                                                                                    'bhv_name': strategy_aligned_FR_ientry['bhv_name'],\n",
    "                                                                                                    'clusterID':strategy_aligned_FR_ientry['clusterID'],\n",
    "                                                                                                    'channelID':strategy_aligned_FR_ientry['channelID'],\n",
    "                                                                                                    'eventID':ievent,                \n",
    "                                                                                                    'FR_ievent':strategy_aligned_FR_ientry['FR_allevents'][:,ievent],\n",
    "                                                                                                    }, ignore_index=True)\n",
    "                            isample = isample + 1\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # work on the sampled new data\n",
    "    strategy_aligned_PC123_cond_concat = pd.DataFrame(columns=['condition','act_animal','bhv_name',\n",
    "                                                                       'trainOrtest','PC123'])\n",
    "    for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "        act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "        ind_animal_allevents = strategy_aligned_FR_sepevents_tgt['act_animal']==act_animal_ana\n",
    "\n",
    "        for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "            bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "            ind_bhv_allevents = strategy_aligned_FR_sepevents_tgt['bhv_name']==bhvname_ana\n",
    "\n",
    "            ind_ana_allevents = ind_animal_allevents & ind_bhv_allevents\n",
    "\n",
    "            strategy_aligned_FR_sepevents_forPCA = strategy_aligned_FR_sepevents_tgt[ind_ana_allevents]\n",
    "\n",
    "            # Concatenate across conditions\n",
    "            for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "                cond_ana = conditions_to_ana[icond_ana]\n",
    "                ind_cond_allevents = strategy_aligned_FR_sepevents_forPCA['condition']==cond_ana\n",
    "\n",
    "                if icond_ana == 0:\n",
    "                    strategy_aligned_FR_conct = np.vstack(strategy_aligned_FR_sepevents_forPCA[ind_cond_allevents]['FR_ievent'])\n",
    "                else:\n",
    "                    strategy_aligned_FR_conct_new = np.vstack(strategy_aligned_FR_sepevents_forPCA[ind_cond_allevents]['FR_ievent'])\n",
    "                    strategy_aligned_FR_conct = np.hstack([strategy_aligned_FR_conct, strategy_aligned_FR_conct_new])\n",
    "\n",
    "            # run PCA \n",
    "            pca = PCA(n_components=3)\n",
    "            pca.fit(strategy_aligned_FR_conct.transpose())\n",
    "            PCA_dataset_train_proj = pca.transform(strategy_aligned_FR_conct.transpose())\n",
    "            PCA_dataset_proj = pca.transform(strategy_aligned_FR_conct.transpose())\n",
    "\n",
    "            # seperate the result for each condition\n",
    "            twinlength = int(np.shape(PCA_dataset_proj)[0]/nconds_to_ana)\n",
    "            for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "                cond_ana = conditions_to_ana[icond_ana]\n",
    "\n",
    "                PCA_dataset_train_proj_icond = PCA_dataset_train_proj[icond_ana*twinlength:(icond_ana+1)*twinlength,:]\n",
    "                PCA_dataset_proj_icond = PCA_dataset_proj[icond_ana*twinlength:(icond_ana+1)*twinlength,:]\n",
    "\n",
    "                strategy_aligned_PC123_cond_concat = strategy_aligned_PC123_cond_concat.append({'condition':cond_ana,\n",
    "                                                                                       'act_animal':act_animal_ana,\n",
    "                                                                                       'bhv_name': bhvname_ana,\n",
    "                                                                                       'trainOrtest':'training',\n",
    "                                                                                       'PC123':PCA_dataset_train_proj_icond,\n",
    "                                                                                        }, ignore_index=True)\n",
    "\n",
    "                strategy_aligned_PC123_cond_concat = strategy_aligned_PC123_cond_concat.append({'condition':cond_ana,\n",
    "                                                                                       'act_animal':act_animal_ana,\n",
    "                                                                                       'bhv_name': bhvname_ana,\n",
    "                                                                                       'trainOrtest':'testing',\n",
    "                                                                                       'PC123':PCA_dataset_proj_icond,\n",
    "                                                                                        }, ignore_index=True)\n",
    "\n",
    "\n",
    "    # figures\n",
    "    # animal_toplot = 'kanga'\n",
    "    animal_toplot = 'dodson'\n",
    "    trainortest_toplot = 'testing'\n",
    "\n",
    "    fig1, axs1 = plt.subplots(3,nconds_to_ana)\n",
    "    fig1.set_figheight(6*3)\n",
    "    fig1.set_figwidth(6*nconds_to_ana)\n",
    "    #\n",
    "    # 3d figure trace\n",
    "    fig2 = plt.figure(figsize=(6*nconds_to_ana,6))\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "\n",
    "        ax2 = fig2.add_subplot(1,nconds_to_ana,icond_ana+1,projection = '3d')\n",
    "\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond_toplot = strategy_aligned_PC123_cond_concat['condition']==cond_ana\n",
    "\n",
    "        ind_animal_toplot = strategy_aligned_PC123_cond_concat['act_animal']==animal_toplot\n",
    "        ind_traintest_toplot = strategy_aligned_PC123_cond_concat['trainOrtest']==trainortest_toplot\n",
    "\n",
    "        ind_toplot = ind_cond_toplot & ind_animal_toplot & ind_traintest_toplot\n",
    "\n",
    "        strategy_aligned_PC123_toplot = strategy_aligned_PC123_cond_concat[ind_toplot]\n",
    "\n",
    "        for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "            bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "            ind_bhv_toplot = strategy_aligned_PC123_toplot['bhv_name']==bhvname_ana\n",
    "\n",
    "            PCA_toplot = strategy_aligned_PC123_toplot[ind_bhv_toplot]['PC123']\n",
    "            PCA_toplot = np.array(PCA_toplot)[0]\n",
    "\n",
    "            trig_twins = [-4,4] # the time window to examine the spike triggered average, in the unit of s\n",
    "            xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "\n",
    "            # plot PC1\n",
    "            axs1[0,icond_ana].plot(xxx_forplot,PCA_toplot[:,0],label=animal_toplot+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "            axs1[1,icond_ana].plot(xxx_forplot,PCA_toplot[:,1],label=animal_toplot+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "            axs1[2,icond_ana].plot(xxx_forplot,PCA_toplot[:,2],label=animal_toplot+' '+bhvname_ana,color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "            # plot 3d PC1,2,3 trace\n",
    "            ax2.plot(gaussian_filter1d(PCA_toplot[:,0], 6),\n",
    "                     gaussian_filter1d(PCA_toplot[:,1], 6),\n",
    "                     gaussian_filter1d(PCA_toplot[:,2], 6),\n",
    "                     label=animal_toplot+' '+bhvname_ana,\n",
    "                     color=bhvname_clrs[ibhvname_ana])\n",
    "\n",
    "        axs1[0,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[0,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[0,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[0,icond_ana].set_title('PC1 '+cond_ana)\n",
    "        axs1[0,icond_ana].legend()      \n",
    "\n",
    "        axs1[1,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[1,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[1,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[1,icond_ana].set_title('PC2 '+cond_ana)\n",
    "        axs1[1,icond_ana].legend()    \n",
    "\n",
    "        axs1[2,icond_ana].set_xlabel('time (s)')\n",
    "        axs1[2,icond_ana].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "        axs1[2,icond_ana].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "        axs1[2,icond_ana].set_title('PC3 '+cond_ana)\n",
    "        axs1[2,icond_ana].legend()    \n",
    "\n",
    "        ax2.set_xlabel('PC1')\n",
    "        ax2.set_ylabel('PC2') \n",
    "        ax2.set_zlabel('PC3')   \n",
    "        ax2.view_init(elev=30, azim=-30) \n",
    "        ax2.set_title(cond_ana)\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FRsPCA_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig1.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_samplingForCommonSpaceAcrossConditions_'+savefile_sufix+'_PC123separate.pdf')\n",
    "        fig2.savefig(figsavefolder+'stretagy_aligned_PCspace_trajectory_samplingForCommonSpaceAcrossConditions_'+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46e2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1fea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f9ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99c78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8e71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3ab50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
