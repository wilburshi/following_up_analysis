{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### analyze the spike triggered pull and gaze ditribution\n",
    "#### the following detailed analysis focused on pull related behavioral events; with the specific focus on the partner Distance variables\n",
    "#### the pull action start events are defined based on the movement onset before each pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from dPCA import dPCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import HillClimbSearch,BicScore\n",
    "from pgmpy.base import DAG\n",
    "import networkx as nx\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam import plot_continuous_bhv_var_singlecam\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0b97",
   "metadata": {},
   "source": [
    "### function - train the dynamic bayesian network - multi time lag (3 lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_create_df_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import train_DBN_multiLag_training_only\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import graph_to_matrix\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_weighted_dags\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import get_significant_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import threshold_edges\n",
    "from ana_functions.train_DBN_multiLag_withNeuron import Modulation_Index\n",
    "from ana_functions.EfficientTimeShuffling import EfficientShuffle\n",
    "from ana_functions.AicScore import AicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b030e",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ec5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "from ana_functions.get_pull_infos import get_pull_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods 3: based on the HMM fitting\n",
    "from ana_functions.get_trial_start_frames_from_hmm_states import get_trial_start_frames_from_HMM\n",
    "from ana_functions.get_trial_start_frames_from_hmm_states import get_trial_start_frames_from_hmm_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "pull1_intv_all_dates = np.zeros((ndates,1))\n",
    "pull2_intv_all_dates = np.zeros((ndates,1))\n",
    "pull1_minintv_all_dates = np.zeros((ndates,1))\n",
    "pull2_minintv_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_infos_all_dates = dict.fromkeys(dates_list, []) # keep some useful information about pulls - time from last reward, number of preceding failed pull etc\n",
    "\n",
    "pull_rts_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pullstart_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpullstart_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpullstart_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_pullstart_aligned_FR_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvevents_pullstart_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # dummy\n",
    "    \n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_infos_all_dates  = pickle.load(f)  \n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_rts_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_rts_all_dates  = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pullstart_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/succpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpullstart_trig_events_all_dates = pickle.load(f)    \n",
    "    with open(data_saved_subfolder+'/failpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpullstart_trig_events_all_dates = pickle.load(f)    \n",
    "    \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstart_aligned_FR_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstart_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "        \n",
    "    print('all data from all dates are loaded; pull start focus')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        #\n",
    "        pull_rts_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        \n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        #\n",
    "        pulltime1 = np.array(bhv_data[(bhv_data['behavior_events']==1)]['time_points'])\n",
    "        pulltime2 = np.array(bhv_data[(bhv_data['behavior_events']==2)]['time_points'])\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        \n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        #\n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        #\n",
    "        ind_good_pull1_succ = time_point_pull1_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_succ = time_point_pull1_succ[ind_good_pull1_succ]\n",
    "        ind_good_pull2_succ = time_point_pull2_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_succ = time_point_pull2_succ[ind_good_pull2_succ]\n",
    "        #\n",
    "        ind_good_pull1_fail = time_point_pull1_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_fail = time_point_pull1_fail[ind_good_pull1_fail]\n",
    "        ind_good_pull2_fail = time_point_pull2_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_fail = time_point_pull2_fail[ind_good_pull2_fail]\n",
    "        \n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        pull_infos_all_dates[date_tgt] = pull_infos\n",
    "        \n",
    "            \n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "        # define variables and use them to find the 'onset' of pull decision\n",
    "        print('use the gaze vector speed and face mass speed to define the start of the pull decision')\n",
    "        #\n",
    "        gausKernelsize = 16\n",
    "        #\n",
    "        # clean the data\n",
    "        time_point_pull1_temp = np.array(time_point_pull1)+session_start_time\n",
    "        time_point_pull1_temp = time_point_pull1_temp[time_point_pull1_temp<totalsess_time]\n",
    "        time_point_pull2_temp = np.array(time_point_pull2)+session_start_time\n",
    "        time_point_pull2_temp = time_point_pull2_temp[time_point_pull2_temp<totalsess_time]\n",
    "        #\n",
    "        # organize the data into a time series\n",
    "        pull1_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull1_data[np.round(time_point_pull1_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        pull2_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull2_data[np.round(time_point_pull2_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        facemass1 = output_key_locations['facemass_loc_all_merge']['dodson'].transpose()\n",
    "        facemass1 = np.hstack((facemass1,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass1[:,1:]-facemass1[:,:-1])\n",
    "        speed1_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed1_data = scipy.ndimage.gaussian_filter1d(speed1_data,gausKernelsize)\n",
    "        #\n",
    "        facemass2 = output_key_locations['facemass_loc_all_merge']['scorch'].transpose()\n",
    "        facemass2 = np.hstack((facemass2,[[np.nan],[np.nan]]))\n",
    "        at1_min_at0 = (facemass2[:,1:]-facemass2[:,:-1])\n",
    "        speed2_data = np.sqrt(np.einsum('ij,ij->j', at1_min_at0, at1_min_at0))*fps \n",
    "        speed2_data = scipy.ndimage.gaussian_filter1d(speed2_data,gausKernelsize)\n",
    "        #\n",
    "        gazevect1 = np.array(output_allvectors['head_vect_all_merge']['dodson']).transpose()\n",
    "        gazevect1 = np.hstack((gazevect1, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect1[:, 1:]\n",
    "        at0 = gazevect1[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed1_data = np.full(nframes, np.nan)\n",
    "        eps = 1e-10\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed1_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed1_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed1_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed1_data[~nans])\n",
    "        anglespeed1_data = scipy.ndimage.gaussian_filter1d(anglespeed1_data, gausKernelsize)\n",
    "        #\n",
    "        gazevect2 = np.array(output_allvectors['head_vect_all_merge']['scorch']).transpose()\n",
    "        gazevect2 = np.hstack((gazevect2, [[np.nan], [np.nan]]))\n",
    "        at1 = gazevect2[:, 1:]\n",
    "        at0 = gazevect2[:, :-1] \n",
    "        nframes = np.shape(at1)[1]\n",
    "        anglespeed2_data = np.full(nframes, np.nan)\n",
    "        for iframe in np.arange(0, nframes, 1):\n",
    "            norm1 = np.linalg.norm(at1[:, iframe]) + eps\n",
    "            norm0 = np.linalg.norm(at0[:, iframe]) + eps\n",
    "            dot_val = np.dot(at1[:, iframe]/norm1, at0[:, iframe]/norm0)\n",
    "            anglespeed2_data[iframe] = np.arccos(np.clip(dot_val, -1.0, 1.0))    \n",
    "        # fill NaNs\n",
    "        nans = np.isnan(anglespeed2_data)\n",
    "        if np.any(~nans):\n",
    "            anglespeed2_data[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), anglespeed2_data[~nans])\n",
    "        anglespeed2_data = scipy.ndimage.gaussian_filter1d(anglespeed2_data, gausKernelsize)\n",
    "        \n",
    "        # find the transitional time point of angle speed and speed in IPI\n",
    "        speed1_increase = find_sharp_increases_withinIPI(pull1_data,speed1_data,session_start_time,fps)\n",
    "        anglespeed1_increase = find_sharp_increases_withinIPI(pull1_data,anglespeed1_data,session_start_time,fps)\n",
    "        # find the transitional time point using both angle speed and mass speed in IPI\n",
    "        pull1_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull1_data, speed1_data, anglespeed1_data, \n",
    "                                                                              session_start_time, fps)\n",
    "        #\n",
    "        speed2_increase = find_sharp_increases_withinIPI(pull2_data,speed2_data,session_start_time,fps)\n",
    "        anglespeed2_increase = find_sharp_increases_withinIPI(pull2_data,anglespeed2_data,session_start_time,fps)\n",
    "        # find the transitional time point using both angle speed and mass speed in IPI\n",
    "        pull2_action_onset_frames = find_sharp_increases_withinIPI_dual_speed(pull2_data, speed2_data, anglespeed2_data, \n",
    "                                                                              session_start_time, fps)\n",
    "         \n",
    "        #\n",
    "        # store the pull reaction time information\n",
    "        pull_data_points = np.where(pull1_data)[0]\n",
    "        pullonset_data_points = np.where(pull1_action_onset_frames)[0]\n",
    "        pull1_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "        pull_rts_all_dates[date_tgt][animal1] = pull1_rt\n",
    "        #\n",
    "        pull_data_points = np.where(pull2_data)[0]\n",
    "        pullonset_data_points = np.where(pull2_action_onset_frames)[0]\n",
    "        pull2_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "        pull_rts_all_dates[date_tgt][animal2] = pull2_rt\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # replace time_point_pull_xxx to the pull onset\n",
    "        time_point_pull1 = np.array(np.round(time_point_pull1,1))\n",
    "        time_point_pull2 = np.array(np.round(time_point_pull2,1))\n",
    "        time_point_pull1_succ = np.array(time_point_pull1_succ)\n",
    "        time_point_pull2_succ = np.array(time_point_pull2_succ)\n",
    "        time_point_pull1_fail = np.array(time_point_pull1_fail)\n",
    "        time_point_pull2_fail = np.array(time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1_succ_idx = np.isin(time_point_pull1,time_point_pull1_succ)\n",
    "        time_point_pull2_succ_idx = np.isin(time_point_pull2,time_point_pull2_succ)\n",
    "        time_point_pull1_fail_idx = np.isin(time_point_pull1,time_point_pull1_fail)\n",
    "        time_point_pull2_fail_idx = np.isin(time_point_pull2,time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1 = np.where(pull1_action_onset_frames)[0]/fps - session_start_time\n",
    "        time_point_pull2 = np.where(pull2_action_onset_frames)[0]/fps - session_start_time\n",
    "        #\n",
    "        time_point_pull1_succ = time_point_pull1[time_point_pull1_succ_idx]\n",
    "        time_point_pull2_succ = time_point_pull2[time_point_pull2_succ_idx]\n",
    "        time_point_pull1_fail = time_point_pull1[time_point_pull1_fail_idx]\n",
    "        time_point_pull2_fail = time_point_pull2[time_point_pull2_fail_idx]\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            print('plot self pull start triggered bhv variables')\n",
    "            \n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+cameraID+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "\n",
    "            savefig = 0\n",
    "            \n",
    "            aligntwins = 4 # 5 second\n",
    "            \n",
    "            min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "\n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                    aligntwins, savefig, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pullstart_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                        aligntwins, savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpullstart_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpullstart_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary, _, _ = plot_continuous_bhv_var_singlecam(filepath_cont_var+date_tgt+cameraID,\n",
    "                                        aligntwins, savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpullstart_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpullstart_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 0:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate; pull start focus')\n",
    "                #\n",
    "                savefig = 0\n",
    "                save_path = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents/\"+cameraID+\"/\"+\\\n",
    "                            animal1_filename+\"_\"+animal2_filename+'_'+recordedanimal+'Recorded'+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                aligntwins = 4 # 5 second\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_average_all,bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR(date_tgt,savefig,save_path, animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           aligntwins,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_pullstart_aligned_FR_all_dates[date_tgt] = bhvevents_aligned_FR_average_all\n",
    "                bhvevents_pullstart_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_rts_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_rts_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstart_trig_events_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstart_aligned_FR_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstart_aligned_FR_allevents_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstart_aligned_FR_allevents_all_dates, f) \n",
    "            \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstart_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstart_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstart_trig_events_all_dates, f) \n",
    "            \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(failpull_trig_events_all_dates['20240606'][('dannon', 'gaze_other_angle')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull1_intv_all_dates[np.isin(task_conditions,['MC','MC_withGinger','MC_withDodson','MC_withKoala','MC_withVermelho'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_infos_all_dates['20240613'].keys()\n",
    "pull_infos_all_dates.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9612c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_trig_events_all_dates['20240508'].keys()\n",
    "# pull_trig_events_all_dates['20240531'].keys()\n",
    "# pull_trig_events_all_dates['20240613'][('kanga', 'otherpull_prob')]\n",
    "# np.shape(pull_trig_events_all_dates['20240613'][('kanga', 'otherpull_prob')])\n",
    "# np.shape(pull_infos_all_dates['20240613'][('kanga', 'num_preceding_failpull')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "#### re-organized the data\n",
    "#### for the activity aligned at the different single behavioral events, mostly focus on self pull\n",
    "#### the ultamate goal is to analyze the difference in single trial and if gaze related variables related to any of them\n",
    "#### make some prilimary plot for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist','otherani_othertube_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "\n",
    "# use this variable as the main target to estimate the partner's intention \n",
    "#\n",
    "# pull_trig_events_tgtname = 'otherpull_prob' \n",
    "# pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "# pull_trig_events_tgtname = 'animal_lever_dist' \n",
    "# pull_trig_events_tgtname = 'otherani_othertube_dist' \n",
    "# pull_trig_events_tgtname = 'animal_tube_dist' \n",
    "# pull_trig_events_tgtname = 'animal_animal_dist' \n",
    "# pull_trig_events_tgtname = 'other_mass_move_speed'\n",
    "\n",
    "# change it to a list to work with \n",
    "#\n",
    "# List of pull_trig_events types to load\n",
    "pull_trig_events_tgtnames = [\n",
    "    'other_mass_move_speed', \n",
    "    'otherani_otherlever_dist',\n",
    "    'animal_animal_dist', \n",
    "    # 'mass_move_speed',\n",
    "]\n",
    "# only keep one for the further analysis\n",
    "pull_trig_events_tgtname = 'other_mass_move_speed'\n",
    "\n",
    "# Keep these as additional controls\n",
    "pull_trig_gazeprob_name = 'socialgaze_prob'\n",
    "pull_trig_otherpull_name = 'otherpull_prob'\n",
    "pull_trig_selfpull_name = 'selfpull_prob'\n",
    "pull_num_pre_failpull_name = 'num_preceding_failpull'\n",
    "pull_time_pre_reward_name = 'time_from_last_reward'\n",
    "pull_rt_name = 'pull_rt'\n",
    "\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=[\n",
    "                                                    'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                                    'succrate', 'clusterID', 'channelID', 'FR_allevents'\n",
    "                                                ])\n",
    "bhvevents_aligned_FR_all_dates_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                            'succrate', 'clusterID', 'channelID', 'FR_average'\n",
    "                                        ])\n",
    "\n",
    "for idate in np.arange(0, ndates, 1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "    succrate = succ_rate_all_dates[idate]\n",
    "    bhv_types = list(bhvevents_pullstart_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "\n",
    "    for ibhv_type in bhv_types:\n",
    "        clusterIDs = list(bhvevents_pullstart_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "\n",
    "        ibhv_type_split = ibhv_type.split()\n",
    "        if len(ibhv_type_split) == 3:\n",
    "            ibhv_type_split[1] = ibhv_type_split[1] + '_' + ibhv_type_split[2]\n",
    "\n",
    "        event_dict = {}\n",
    "        def load_event(source_dict, actor, name):\n",
    "            try:\n",
    "                return source_dict[date_tgt][(actor, name)]\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "        # determine which dataset to use\n",
    "        if ibhv_type_split[1] == 'pull':\n",
    "            source = pullstart_trig_events_all_dates\n",
    "        elif ibhv_type_split[1] == 'succpull':\n",
    "            source = succpullstart_trig_events_all_dates\n",
    "        elif ibhv_type_split[1] == 'failpull':\n",
    "            source = failpullstart_trig_events_all_dates\n",
    "        else:\n",
    "            source = None\n",
    "\n",
    "        if source is not None:\n",
    "            actor = ibhv_type_split[0]\n",
    "            for name in pull_trig_events_tgtnames:\n",
    "                event_dict[name] = load_event(source, actor, name)\n",
    "            \n",
    "            event_dict[pull_trig_gazeprob_name] = load_event(source, actor, pull_trig_gazeprob_name)\n",
    "            event_dict[pull_trig_otherpull_name] = load_event(source, actor, pull_trig_otherpull_name)\n",
    "            event_dict[pull_trig_selfpull_name] = load_event(source, actor, pull_trig_selfpull_name)\n",
    "        else:\n",
    "            for name in pull_trig_events_tgtnames + [\n",
    "                pull_trig_gazeprob_name, pull_trig_otherpull_name, pull_trig_selfpull_name\n",
    "            ]:\n",
    "                event_dict[name] = np.nan\n",
    "                \n",
    "        # for the pull information variables\n",
    "        #\n",
    "        source2 = pull_infos_all_dates\n",
    "        event_dict[pull_num_pre_failpull_name] = load_event(source2, actor, pull_num_pre_failpull_name)\n",
    "        event_dict[pull_time_pre_reward_name] = load_event(source2, actor, pull_time_pre_reward_name)\n",
    "        #\n",
    "        event_dict[pull_rt_name] = np.array(list(pull_rts_all_dates[date_tgt][actor]))\n",
    "        \n",
    "        #\n",
    "        for iclusterID in clusterIDs:\n",
    "            ichannelID = bhvevents_pullstart_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_pullstart_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "\n",
    "            row_data = {\n",
    "                'dates': date_tgt,\n",
    "                'condition': task_condition,\n",
    "                'act_animal': ibhv_type_split[0],\n",
    "                'bhv_name': ibhv_type_split[1],\n",
    "                'succrate': succrate,\n",
    "                'clusterID': iclusterID,\n",
    "                'channelID': ichannelID,\n",
    "                'FR_allevents': iFR_average\n",
    "            }\n",
    "            # Add all events into the row\n",
    "            row_data.update(event_dict)\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append(\n",
    "                row_data, ignore_index=True\n",
    "            )\n",
    "\n",
    "            # Also add to FR_average table\n",
    "            ichannelID = bhvevents_pullstart_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "            iFR_average = bhvevents_pullstart_aligned_FR_all_dates[date_tgt][ibhv_type][iclusterID]['FR_average']\n",
    "\n",
    "            bhvevents_aligned_FR_all_dates_df = bhvevents_aligned_FR_all_dates_df.append({\n",
    "                'dates': date_tgt,\n",
    "                'condition': task_condition,\n",
    "                'act_animal': ibhv_type_split[0],\n",
    "                'bhv_name': ibhv_type_split[1],\n",
    "                'succrate': succrate,\n",
    "                'clusterID': iclusterID,\n",
    "                'channelID': ichannelID,\n",
    "                'FR_average': iFR_average\n",
    "            }, ignore_index=True)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf229da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "# bhv_names_to_ana = ['failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "bhv_names_to_ana = ['pull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# \n",
    "# for dodson only\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "# 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33946e",
   "metadata": {},
   "source": [
    "### sanity check plot; focus on bhv only\n",
    "#### look at the relationship among pull_trig_events_tgtname (the variable that aim to explore other's intention), otherpull_prob (aligned at self pull), selfpull_prob, and socialgaze accumulation related quantification to see which variables correlated with self gaze accumulation the most and identify the best variables for estimating intention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c20a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # only look at pull aligned events that has no preceding self pull \n",
    "    doSingleSelfPulls = 1\n",
    "    \n",
    "    # define which time window will be analyzed\n",
    "    gaze_duration_type = 'after_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "\n",
    "    # time window time points\n",
    "    x_full = np.arange(-4, 4, 1/fps)\n",
    "    #\n",
    "    if gaze_duration_type == 'before_pull':\n",
    "        # Only use the pre-pull window (-4s to 0s)\n",
    "        pre_mask = x_full <= -0.15\n",
    "    elif gaze_duration_type == 'after_pull':\n",
    "        # Only use the post-pull window (0s to 4s)\n",
    "        pre_mask = x_full >= 0.15\n",
    "    elif gaze_duration_type == 'around_pull':\n",
    "        # the entire -4 to 4s\n",
    "        pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "    \n",
    "    # sampling interval in seconds\n",
    "    dt = 1 / fps  # sampling interval in seconds\n",
    "    \n",
    "    # initialize some dataframe for saving data\n",
    "    bhvevents_aligned_bhvonly_eachevents_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name', 'bhv_id',\n",
    "                                        ])\n",
    "    selfgaze_partnerIntention_corr_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name', 'selfgazeVarName',\n",
    "                                        ])\n",
    "    selfgaze_partnerIntention_multivar_corr_df = pd.DataFrame(columns=[\n",
    "                                            'dates', 'condition', 'act_animal', 'bhv_name',\n",
    "                                        ])\n",
    "    \n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "\n",
    "                # only look at one neuron since it's behavioral related exploration\n",
    "                for ineuron in np.arange(0,1,1):\n",
    "                # for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "                        \n",
    "                        # self gaze distribution\n",
    "                        pull_trig_gazeprob_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_trig_gazeprob_name])[0])\n",
    "                        # self pull distribution\n",
    "                        pull_trig_selfpull_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_trig_selfpull_name])[0])\n",
    "                        # other pull distribution\n",
    "                        pull_trig_otherpull_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_trig_otherpull_name])[0])\n",
    "                        # number of preceding failed pulls\n",
    "                        pull_num_pre_failpull_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_num_pre_failpull_name])[0])\n",
    "                        # time between the current pull and the previous juice\n",
    "                        pull_time_pre_reward_tgt = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [pull_time_pre_reward_name])[0])\n",
    "                        \n",
    "                        #\n",
    "                        nevents = np.shape(pull_trig_gazeprob_tgt)[0]\n",
    "                        \n",
    "                        for ievent in np.arange(0,nevents,1):\n",
    "                            \n",
    "                            # number of preceding failed pulls\n",
    "                            pull_num_pre_failpull_ievent = pull_num_pre_failpull_tgt[ievent]\n",
    "                            \n",
    "                            # time between the current pull and the previous juice\n",
    "                            pull_time_pre_reward_ievent = pull_time_pre_reward_tgt[ievent]\n",
    "                            lastreward_time = -pull_time_pre_reward_ievent\n",
    "                            \n",
    "                            #\n",
    "                            # calculate the mean self pull number\n",
    "                            #\n",
    "                            from scipy.signal import find_peaks\n",
    "                            #\n",
    "                            pull_trig_selfpull_ievent = pull_trig_selfpull_tgt[ievent,:]\n",
    "                            try:\n",
    "                                # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                                data = pull_trig_selfpull_ievent[pre_mask]\n",
    "                                peaks, _ = find_peaks(data)\n",
    "                                selfpull_num = len(peaks)\n",
    "                            except:\n",
    "                                selfpull_num = np.nan\n",
    "                            #\n",
    "                            if selfpull_num > 0:\n",
    "                                lastselfpull_time = x_full[peaks[-1]]\n",
    "                            else:\n",
    "                                lastselfpull_time =  np.nan\n",
    "                            \n",
    "                            #\n",
    "                            # calculate the mean other pull number\n",
    "                            pull_trig_otherpull_ievent = pull_trig_otherpull_tgt[ievent,:]\n",
    "                            try:\n",
    "                                # otherpull_num = np.trapz(pull_trig_otherpull_ievent[pre_mask], dx=dt)\n",
    "                                data = pull_trig_otherpull_ievent[pre_mask]\n",
    "                                peaks, _ = find_peaks(data)\n",
    "                                otherpull_num = len(peaks)\n",
    "                            except:\n",
    "                                otherpull_num = np.nan\n",
    "                                                              \n",
    "                            #\n",
    "                            # reaction time: time since last reward or last pull\n",
    "                            reaction_time = -np.nanmax([lastreward_time, lastselfpull_time])\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #\n",
    "                            # calculate the gaze accumulation level\n",
    "                            pull_trig_gazeprob_ievent = pull_trig_gazeprob_tgt[ievent,:]\n",
    "                            \n",
    "                            # \n",
    "                            # only consider gaze that's within reaction time\n",
    "                            if 0:\n",
    "                                ind_beforeRT = pull_trig_gazeprob_ievent < -reaction_time\n",
    "                                pull_trig_gazeprob_ievent[ind_beforeRT]=0     \n",
    "                            #\n",
    "                            # for the socialgaze_prob, only use the meaningful ones\n",
    "                            if 0:\n",
    "                                trace = pull_trig_gazeprob_ievent\n",
    "                                time_trace = np.arange(-4, 4, 1/30)\n",
    "                                filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                pull_trig_gazeprob_ievent = filtered_trace\n",
    "                                \n",
    "                            \n",
    "                            #\n",
    "                            try:\n",
    "                                gaze_accum = np.trapz(pull_trig_gazeprob_ievent[pre_mask], dx=dt)\n",
    "                            except:\n",
    "                                gaze_accum = np.nan\n",
    "                            # calculate the gaze start stop and duration\n",
    "                            try:\n",
    "                                first_increase_idx = np.where(np.diff(pull_trig_gazeprob_ievent) > 0)[0][0] + 1\n",
    "                                #\n",
    "                                last_decrease_idx = np.where(np.diff(pull_trig_gazeprob_ievent) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                #\n",
    "                                gazestart_time = x_full[first_increase_idx].copy()\n",
    "                                gazestop_time = x_full[last_decrease_idx].copy()\n",
    "                                #    \n",
    "                                # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                                if gaze_duration_type == 'around_pull':\n",
    "                                    gazestart_time = gazestart_time\n",
    "                                    gazestop_time = gazestop_time\n",
    "                                if gaze_duration_type == 'before_pull':\n",
    "                                    if (gazestart_time > 0):\n",
    "                                        gazestart_time = np.nan\n",
    "                                        gazestop_time = np.nan\n",
    "                                    elif (gazestop_time > 0):\n",
    "                                        gazestop_time = 0\n",
    "                                if gaze_duration_type == 'after_pull':\n",
    "                                    if (gazestop_time < 0):\n",
    "                                        gazestart_time = np.nan\n",
    "                                        gazestop_time = np.nan\n",
    "                                    elif (gazestart_time < 0):\n",
    "                                        gazestart_time = 0                                                \n",
    "                                #\n",
    "                                # if (gazestart_time == timewins[0]) | (gazestart_time == timewins[-1]):\n",
    "                                #     gazestart_time = np.nan\n",
    "                                # if (gazestop_time == timewins[0]) | (gazestop_time == timewins[-1]):\n",
    "                                #     gazestop_time = np.nan\n",
    "                                if (gazestop_time < gazestart_time):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan                           \n",
    "                            except:\n",
    "                                gazestart_time = np.nan\n",
    "                                gazestop_time = np.nan\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                                \n",
    "                            #\n",
    "                            # for variables in the pull_trig_events_tgtnames do some more detailed analysis to define metrics\n",
    "                            event_dict = {}\n",
    "                            for name in pull_trig_events_tgtnames:\n",
    "                                pull_trig_tgt_ievent = np.array(np.array(bhvevents_aligned_FR_allevents_tgt\\\n",
    "                                                          [name])[0])[ievent]\n",
    "                                \n",
    "                                # calculate the mean value\n",
    "                                tgt_mean = np.nanmean(pull_trig_tgt_ievent[pre_mask])\n",
    "                                event_dict[name+'_mean'] = tgt_mean\n",
    "                                \n",
    "                                # more detailed metric\n",
    "                                if (name == 'otherani_otherlever_dist') | \\\n",
    "                                   (name == 'animal_lever_dist') | \\\n",
    "                                   (name == 'otherani_othertube_dist') | \\\n",
    "                                   (name == 'animal_tube_dist') | \\\n",
    "                                   (name == 'animal_animal_dist') :\n",
    "\n",
    "                                    tgt_velocity = np.gradient(pull_trig_tgt_ievent, 1/fps)\n",
    "                                    #\n",
    "                                    tgt_speed = np.abs(tgt_velocity)\n",
    "                                    #\n",
    "                                    tgt_meanvelocity = np.nanmean(tgt_velocity[pre_mask])\n",
    "                                    #\n",
    "                                    tgt_meanspeed = np.nanmean(tgt_speed[pre_mask])\n",
    "\n",
    "                                    min_dist = x_full[np.argmin(pull_trig_tgt_ievent)]\n",
    "                                    max_dist = x_full[np.argmax(pull_trig_tgt_ievent)]\n",
    "                                    #\n",
    "                                    pre_min_mask = x_full <= min_dist\n",
    "\n",
    "                                    # find the time that the dramatic change starts, if could not find it, use the -4s\n",
    "                                    percentile = 95\n",
    "                                    dt = 1 / fps\n",
    "                                    abs_derivative = np.abs(np.gradient(pull_trig_tgt_ievent[x_full <= 0], dt))\n",
    "                                    threshold = np.percentile(abs_derivative, percentile)\n",
    "                                    # Start from the first time point (index 0)\n",
    "                                    idx_change = np.where(abs_derivative > threshold)[0]\n",
    "                                    if len(idx_change) > 0:\n",
    "                                        onset_idx = idx_change[0]\n",
    "                                        change_time = x_full[x_full <= 0][onset_idx]\n",
    "                                    else:\n",
    "                                        change_time = x_full[0]\n",
    "                                    #\n",
    "                                    tgt_changetime = change_time\n",
    "                                    #\n",
    "                                    post_changetime_mask = x_full >= change_time\n",
    "                                    #\n",
    "                                    # min and max after the change_time\n",
    "                                    min_dist_post_changetime = x_full[x_full>=change_time][np.argmin(pull_trig_tgt_ievent[x_full>=change_time])]\n",
    "                                    max_dist_post_changetime = x_full[x_full>=change_time][np.argmax(pull_trig_tgt_ievent[x_full>=change_time])]\n",
    "\n",
    "                                    #\n",
    "                                    # find the partner lever approaching trend\n",
    "                                    from scipy.stats import linregress\n",
    "                                    # Define time range and extract window\n",
    "                                    y_full = np.array(pull_trig_tgt_ievent)                      \n",
    "                                    #\n",
    "                                    # x_pre = x_full[pre_mask]\n",
    "                                    # y_pre = y_full[pre_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask & post_changetime_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask & post_changetime_mask]    \n",
    "                                    x_pre = x_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]\n",
    "                                    y_pre = y_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]                        \n",
    "                                    # Linear regression\n",
    "                                    slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                                    # slope = (y_pre[-1] - y_pre[0])/(x_pre[-1] - x_pre[0])\n",
    "                                    #\n",
    "                                    approaching_slope = slope\n",
    "                                \n",
    "                                    #\n",
    "                                    event_dict[name+'_speed'] = tgt_speed \n",
    "                                    event_dict[name+'_meanspeed'] = tgt_meanspeed \n",
    "                                    event_dict[name+'_velocity'] = tgt_velocity \n",
    "                                    event_dict[name+'_meanvelocity'] = tgt_meanvelocity \n",
    "                                    event_dict[name+'_max'] = max_dist \n",
    "                                    event_dict[name+'_min'] = min_dist \n",
    "                                    event_dict[name+'_changetime'] = tgt_changetime \n",
    "                                    event_dict[name+'_appoachslope'] = approaching_slope \n",
    "                                    \n",
    "                            \n",
    "                            # put the data together\n",
    "                            row_data = {'dates': date_ana, \n",
    "                                        'condition':cond_ana,\n",
    "                                        'act_animal':act_animal_ana,\n",
    "                                        'bhv_name': bhvname_ana,\n",
    "                                        'bhv_id':ievent,\n",
    "                                        'gaze_accum':gaze_accum,\n",
    "                                        'gazestart_time':gazestart_time,\n",
    "                                        'gazestop_time':gazestop_time,\n",
    "                                        'gaze_duration':gazestop_time-gazestart_time,\n",
    "                                        'selfpull_num':selfpull_num,\n",
    "                                        'otherpull_num':otherpull_num, \n",
    "                                        'prefailpull_num':pull_num_pre_failpull_ievent,\n",
    "                                        'time_since_lastreward':pull_time_pre_reward_ievent,\n",
    "                                        'reaction_time':reaction_time,\n",
    "                                       }\n",
    "                            # Add all events into the row\n",
    "                            row_data.update(event_dict)\n",
    "                            #\n",
    "                            bhvevents_aligned_bhvonly_eachevents_df = bhvevents_aligned_bhvonly_eachevents_df.append(\n",
    "                                                                       row_data, ignore_index=True)\n",
    "                            \n",
    "                        # remove events that has multiple self pulls before the aligned self pulls    \n",
    "                        if doSingleSelfPulls: \n",
    "                            ind_singlepull = bhvevents_aligned_bhvonly_eachevents_df['selfpull_num']==0\n",
    "                            bhvevents_aligned_bhvonly_eachevents_df = bhvevents_aligned_bhvonly_eachevents_df[ind_singlepull]\n",
    "    \n",
    "                        \n",
    "        \n",
    "                        #\n",
    "                        # do some regression analysis and do some plotting for each date\n",
    "                        # do some single regression\n",
    "                        import seaborn as sns\n",
    "                        from sklearn.linear_model import LinearRegression\n",
    "                        import math\n",
    "\n",
    "                        ind_cond_plot = bhvevents_aligned_bhvonly_eachevents_df['condition']==cond_ana\n",
    "                        ind_date_plot = bhvevents_aligned_bhvonly_eachevents_df['dates']==date_ana\n",
    "                        ind_ani_plot  = bhvevents_aligned_bhvonly_eachevents_df['act_animal']==act_animal_ana\n",
    "                        ind_bhv_plot  = bhvevents_aligned_bhvonly_eachevents_df['bhv_name']==bhvname_ana\n",
    "                        #\n",
    "                        ind_plot = ind_cond_plot & ind_date_plot & ind_ani_plot & ind_bhv_plot\n",
    "                        #\n",
    "                        bhvevents_aligned_bhvonly_df_toplot = bhvevents_aligned_bhvonly_eachevents_df[ind_plot]\n",
    "                        \n",
    "                        #\n",
    "                        xxx_plot_name = 'gaze_accum'\n",
    "                        # Automatically select possible Y variables\n",
    "                        excluded_keys = {'dates', 'condition', 'act_animal', 'bhv_name', 'bhv_id', \n",
    "                                        'gaze_accum', 'gaze_duration', 'gazestart_time', 'gazestop_time',}\n",
    "                        yyy_plot_names = [col for col in bhvevents_aligned_bhvonly_df_toplot.columns\n",
    "                                if col not in excluded_keys and pd.api.types.is_numeric_dtype(bhvevents_aligned_bhvonly_df_toplot[col])]\n",
    "\n",
    "                        # Prepare subplots\n",
    "                        ncols = 4\n",
    "                        nplots = len(yyy_plot_names)\n",
    "                        nrows = math.ceil(nplots / ncols)\n",
    "\n",
    "                        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5*ncols, 4*nrows))\n",
    "                        axes = axes.flatten()\n",
    "\n",
    "                        # Loop through Y variables\n",
    "                        yyy_corrs_dict = {}\n",
    "                        for i, yyy_plot_name in enumerate(yyy_plot_names):\n",
    "                            ax = axes[i]\n",
    "\n",
    "                            df_plot = bhvevents_aligned_bhvonly_df_toplot[[xxx_plot_name, yyy_plot_name]].dropna()\n",
    "                            if df_plot.empty or len(df_plot) < 2:\n",
    "                                ax.set_title(f'{yyy_plot_name} (No data)')\n",
    "                                ax.axis('off')\n",
    "                                continue\n",
    "\n",
    "                            # Linear regression using scipy\n",
    "                            x = df_plot[yyy_plot_name].values  # now independent variable\n",
    "                            y = df_plot[xxx_plot_name].values  # now dependent variable\n",
    "                            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "                            y_pred = slope * x + intercept\n",
    "\n",
    "                            # Plot\n",
    "                            sns.scatterplot(x=x, y=y, alpha=0.6, ax=ax)\n",
    "                            try:\n",
    "                                sns.lineplot(x=x, y=y_pred, color='red', ax=ax)\n",
    "                            except:\n",
    "                                print('no regression line')\n",
    "\n",
    "                            # Set title with R² and p-value\n",
    "                            ax.set_title(f'{yyy_plot_name} → {xxx_plot_name}\\nR²={r_value**2:.2f}, p={p_value:.3g}')\n",
    "                            ax.set_xlabel(yyy_plot_name)\n",
    "                            ax.set_ylabel(xxx_plot_name)\n",
    "\n",
    "                            # Store correlation\n",
    "                            yyy_corrs_dict[yyy_plot_name] = r_value\n",
    "                            \n",
    "                        # Turn off unused subplots\n",
    "                        for j in range(i+1, len(axes)):\n",
    "                            axes[j].axis('off')\n",
    "\n",
    "                        plt.suptitle(f'Regression: {xxx_plot_name} vs other features\\n{cond_ana}, {date_ana}, {act_animal_ana}, {bhvname_ana}', fontsize=16)\n",
    "                        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                        \n",
    "                        savefig = 1\n",
    "                        if savefig:\n",
    "                            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                            cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                            if not os.path.exists(figsavefolder):\n",
    "                                os.makedirs(figsavefolder)\n",
    "\n",
    "                            if not doSingleSelfPulls:\n",
    "                                fig.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                            '_and_partnerIntentionVariables_correlations.pdf')\n",
    "                            elif doSingleSelfPulls:\n",
    "                                fig.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                            '_and_partnerIntentionVariables_correlations_singleselfpulls.pdf')\n",
    "                        \n",
    "                        plt.close(fig)\n",
    "                        \n",
    "                        # organize the summarizing correlation result\n",
    "                        corr_data = {'dates': date_ana, \n",
    "                                    'condition':cond_ana,\n",
    "                                    'act_animal':act_animal_ana,\n",
    "                                    'bhv_name': bhvname_ana,\n",
    "                                    'selfgazeVarName':xxx_plot_name,    \n",
    "                                   }\n",
    "                        # Add all events into the row\n",
    "                        corr_data.update(yyy_corrs_dict)\n",
    "                        #\n",
    "                        selfgaze_partnerIntention_corr_df = selfgaze_partnerIntention_corr_df.append(\n",
    "                                                                   corr_data, ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #\n",
    "                        # Do some multiple regression\n",
    "                        # Setup variable names\n",
    "                        y_var = 'gaze_accum'\n",
    "                        excluded_keys = {'dates', 'condition', 'act_animal', 'bhv_name', 'bhv_id',\n",
    "                                         'gaze_accum', 'gaze_duration', 'gazestart_time', 'gazestop_time'}\n",
    "                        # x_vars = [col for col in bhvevents_aligned_bhvonly_df_toplot.columns\n",
    "                        #           if col not in excluded_keys and pd.api.types.is_numeric_dtype(bhvevents_aligned_bhvonly_df_toplot[col])]\n",
    "                        x_vars = [   # 'animal_animal_dist_mean',\n",
    "                                     'other_mass_move_speed_mean',\n",
    "                                     # 'otherani_otherlever_dist_mean',\n",
    "                                     # 'otherpull_num',\n",
    "                                     # 'prefailpull_num',\n",
    "                                     # 'selfpull_num',\n",
    "                                     'time_since_lastreward',\n",
    "                                     'reaction_time',\n",
    "                                    ]\n",
    "                            \n",
    "                        # Drop rows with missing values\n",
    "                        df_multivar = bhvevents_aligned_bhvonly_df_toplot[[y_var] + x_vars].dropna()\n",
    "                        if len(df_multivar) < 2:\n",
    "                            print(f'Not enough data for multivariable regression on {date_ana}, {cond_ana}, {act_animal_ana}, {bhvname_ana}')\n",
    "                        else:\n",
    "                            # Fit linear regression model\n",
    "                            X = df_multivar[x_vars].values\n",
    "                            y = df_multivar[y_var].values\n",
    "                            model = LinearRegression().fit(X, y)\n",
    "\n",
    "                            # Predictions and metrics\n",
    "                            y_pred = model.predict(X)\n",
    "                            r_value = np.corrcoef(y, y_pred)[0, 1]\n",
    "                            r_squared = model.score(X, y)\n",
    "\n",
    "                            # Compute standardized beta coefficients\n",
    "                            x_std = df_multivar[x_vars].std().values\n",
    "                            y_std = df_multivar[y_var].std()\n",
    "                            std_betas = model.coef_ * (x_std / y_std)\n",
    "\n",
    "                            # Plot predicted vs actual\n",
    "                            fig3, ax = plt.subplots(figsize=(6, 6))\n",
    "                            sns.scatterplot(x=y, y=y_pred, alpha=0.7, ax=ax)\n",
    "                            ax.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')\n",
    "                            ax.set_xlabel('Actual gaze_accum')\n",
    "                            ax.set_ylabel('Predicted gaze_accum')\n",
    "                            ax.set_title(f'Multivariable regression\\nR={r_value:.2f}, R²={r_squared:.2f}')\n",
    "                            plt.tight_layout()\n",
    "\n",
    "                            # Save figure\n",
    "                            savefig = 1\n",
    "                            if savefig:\n",
    "                                figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\" + \\\n",
    "                                                cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + '/' + date_ana + '/'\n",
    "                                if not os.path.exists(figsavefolder):\n",
    "                                    os.makedirs(figsavefolder)\n",
    "                                if not doSingleSelfPulls:\n",
    "                                    fig3.savefig(figsavefolder + bhvname_ana + '_aligned_' + y_var + '_multiRegress_predictors.pdf')\n",
    "                                elif doSingleSelfPulls:\n",
    "                                    fig3.savefig(figsavefolder + bhvname_ana + '_aligned_' + y_var + '_multiRegress_predictors_singleselfpulls.pdf')\n",
    "                            plt.close(fig3)\n",
    "\n",
    "                            # Save results\n",
    "                            multi_corr_data = {\n",
    "                                'dates': date_ana,\n",
    "                                'condition': cond_ana,\n",
    "                                'act_animal': act_animal_ana,\n",
    "                                'bhv_name': bhvname_ana,\n",
    "                                'selfgazeVarName': y_var,\n",
    "                                'R': r_value,\n",
    "                                'R_squared': r_squared\n",
    "                            }\n",
    "                            for xi, coef, std_beta in zip(x_vars, model.coef_, std_betas):\n",
    "                                multi_corr_data[f'coef_{xi}'] = coef\n",
    "                                multi_corr_data[f'std_beta_{xi}'] = std_beta\n",
    "\n",
    "                            # Append to global results dataframe\n",
    "                            selfgaze_partnerIntention_multivar_corr_df = selfgaze_partnerIntention_multivar_corr_df.append(\n",
    "                                multi_corr_data, ignore_index=True\n",
    "                            )\n",
    "    \n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    # do the correlation coeffient summary plot \n",
    "    #####\n",
    "    from scipy.stats import ttest_1samp\n",
    "\n",
    "    # examine to multivariate regression coeffient\n",
    "    domultiVar = 0\n",
    "    \n",
    "    # Generalize conditions\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond\n",
    "\n",
    "    # Apply to dataframe\n",
    "    selfgaze_partnerIntention_corr_df[\"condition_general\"] = \\\n",
    "        selfgaze_partnerIntention_corr_df[\"condition\"].apply(generalize_condition)\n",
    "    #\n",
    "    selfgaze_partnerIntention_multivar_corr_df[\"condition_general\"] = \\\n",
    "        selfgaze_partnerIntention_multivar_corr_df[\"condition\"].apply(generalize_condition)\n",
    "\n",
    "    # Melt dataframe for seaborn\n",
    "    if not domultiVar:\n",
    "        plot_df = selfgaze_partnerIntention_corr_df[['condition_general'] + yyy_plot_names].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars='condition_general',\n",
    "                                      value_vars=yyy_plot_names,\n",
    "                                      var_name='Variable',\n",
    "                                      value_name='corr coef')\n",
    "    if domultiVar:\n",
    "        std_beta_names = ['std_beta_' + name for name in x_vars]\n",
    "        plot_df = selfgaze_partnerIntention_multivar_corr_df[['condition_general'] + std_beta_names].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars='condition_general',\n",
    "                                      value_vars=std_beta_names,\n",
    "                                      var_name='Variable',\n",
    "                                      value_name='corr coef') \n",
    "    \n",
    "    \n",
    "    # if only plot certain condition\n",
    "    onlyplotMC = 1\n",
    "    if onlyplotMC:\n",
    "        plot_df_melted = plot_df_melted[plot_df_melted['condition_general']=='MC']\n",
    "    \n",
    "    # if only plot a selection of the variables\n",
    "    onlyvarisubset = 1\n",
    "    if onlyvarisubset:\n",
    "        variable_subset = ['other_mass_move_speed_mean','animal_animal_dist_mean','time_since_lastreward','reaction_time']\n",
    "        #\n",
    "        if domultiVar:\n",
    "            variable_subset = ['other_mass_move_speed_mean','time_since_lastreward']\n",
    "            variable_subset = ['std_beta_' + name for name in variable_subset]\n",
    "        #\n",
    "        ind_subset = np.isin(plot_df_melted['Variable'],variable_subset)\n",
    "        plot_df_melted = plot_df_melted[ind_subset]\n",
    "        # Set up figure\n",
    "        fig2, ax = plt.subplots(figsize=(10, 6))\n",
    "    else:\n",
    "        # Set up figure\n",
    "        fig2, ax = plt.subplots(figsize=(20, 6))\n",
    "        \n",
    "       \n",
    "\n",
    "    # Violin plot\n",
    "    sns.violinplot(data=plot_df_melted,\n",
    "                   x='condition_general',\n",
    "                   y='corr coef',\n",
    "                   hue='Variable',\n",
    "                   inner=None,\n",
    "                   palette='Set2',\n",
    "                   linewidth=1,\n",
    "                   ax=ax)\n",
    "\n",
    "    # Swarm plot\n",
    "    sns.swarmplot(data=plot_df_melted,\n",
    "                  x='condition_general',\n",
    "                  y='corr coef',\n",
    "                  hue='Variable',\n",
    "                  dodge=True,\n",
    "                  color='k',\n",
    "                  alpha=0.5,\n",
    "                  size=3,\n",
    "                  ax=ax)\n",
    "\n",
    "    # Remove duplicate legend entries\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    n_vars = len(yyy_plot_names)\n",
    "    ax.legend(handles[:n_vars],\n",
    "              labels[:n_vars],\n",
    "              title=\"Variable\",\n",
    "              bbox_to_anchor=(1.05, 1),\n",
    "              loc='upper left')\n",
    "\n",
    "    # ---- Add significance stars ---- #\n",
    "\n",
    "    # Get all condition levels and hue levels (in the same order seaborn uses them)\n",
    "    conditions = plot_df_melted[\"condition_general\"].unique()\n",
    "    variables = plot_df_melted[\"Variable\"].unique()\n",
    "    condition_order = list(plot_df_melted[\"condition_general\"].unique())\n",
    "    variable_order = list(plot_df_melted[\"Variable\"].unique())\n",
    "\n",
    "    # Build mapping for positions\n",
    "    group_spacing = 1.0\n",
    "    dodge_width = 0.8 / len(variable_order)  # spacing per hue within group\n",
    "\n",
    "    for i, cond in enumerate(condition_order):\n",
    "        for j, var in enumerate(variable_order):\n",
    "            # Filter group\n",
    "            subset = plot_df_melted[\n",
    "                (plot_df_melted[\"condition_general\"] == cond) &\n",
    "                (plot_df_melted[\"Variable\"] == var)\n",
    "            ]\n",
    "            y_vals = subset[\"corr coef\"].dropna()\n",
    "            if len(y_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            # T-test against 0\n",
    "            t_stat, p_val = ttest_1samp(y_vals, 0)\n",
    "\n",
    "            if p_val < 0.05:\n",
    "                # Compute x-position using dodge\n",
    "                x_pos = i - 0.4 + (j + 0.5) * dodge_width\n",
    "                y_max = y_vals.max()\n",
    "                ax.text(x_pos, y_max + 0.05, '*',\n",
    "                        ha='center', va='bottom', fontsize=18, color='red')\n",
    "\n",
    "    ax.set_title('Self-gaze vs Partner Intention Correlation across Conditions')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/SelfGaze_PartnerIntention_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "        \n",
    "        if not domultiVar:\n",
    "            if not doSingleSelfPulls: \n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly.pdf')\n",
    "\n",
    "            elif doSingleSelfPulls:\n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_singleselfpulls.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly_singleselfpulls.pdf')\n",
    "        \n",
    "        elif domultiVar:\n",
    "            if not doSingleSelfPulls: \n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_multivars.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly_multivars.pdf')\n",
    "\n",
    "            elif doSingleSelfPulls:\n",
    "                if not onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_singleselfpulls_multivars.pdf')\n",
    "                elif onlyplotMC:\n",
    "                    fig2.savefig(figsavefolder+bhvname_ana+'_aligned_'+xxx_plot_name+\n",
    "                                '_and_partnerIntentionVariables_correlationCoeffient_summary_MConly_singleselfpulls_multivars.pdf')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a97c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_trig_gazeprob_ievent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db83ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_bhvonly_eachevents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c48138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative sanity check plot, plot the self pull aligned traces\n",
    "\n",
    "if 0: \n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "\n",
    "                # only look at one neuron since it's behavioral related exploration\n",
    "                for ineuron in np.arange(0,1,1):\n",
    "                # for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "    \n",
    "                        #\n",
    "                        contbhv_var = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname])[0]\n",
    "                        contbhv_var = np.array(contbhv_var)\n",
    "                        # \n",
    "                        otherpull_var = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_otherpull_name])[0]\n",
    "                        otherpull_var = np.array(otherpull_var)\n",
    "                        \n",
    "                        \n",
    "                        # plot\n",
    "                        time_full = np.arange(-4, 4, 1/fps)  # 240 time points\n",
    "                        speed = np.abs(np.diff(contbhv_var, axis=1))  # shape: (17, 239)\n",
    "                        time_diff = time_full[:-1]\n",
    "\n",
    "                        # Average speed between -4s and 0s\n",
    "                        start_idx = 0\n",
    "                        end_idx = np.searchsorted(time_diff, 0)\n",
    "                        mean_speed = np.nanmean(speed[:, start_idx:end_idx], axis=1)\n",
    "\n",
    "                        # Quantile thresholds\n",
    "                        q1, q2 = np.quantile(mean_speed, [1/3, 2/3])\n",
    "\n",
    "                        # Trial indices per quantile\n",
    "                        idx1 = np.where(mean_speed <= q1)[0]\n",
    "                        idx2 = np.where((mean_speed > q1) & (mean_speed <= q2))[0]\n",
    "                        idx3 = np.where(mean_speed > q2)[0]\n",
    "\n",
    "                        # Mean and SEM function\n",
    "                        def mean_sem(data):\n",
    "                            mean = np.nanmean(data, axis=0)\n",
    "                            sem = np.nanstd(data, axis=0) / np.sqrt(data.shape[0])\n",
    "                            return mean, sem\n",
    "\n",
    "                        # Get averages\n",
    "                        cont1_mean, cont1_sem = mean_sem(contbhv_var[idx1])\n",
    "                        cont2_mean, cont2_sem = mean_sem(contbhv_var[idx2])\n",
    "                        cont3_mean, cont3_sem = mean_sem(contbhv_var[idx3])\n",
    "\n",
    "                        other1_mean, other1_sem = mean_sem(otherpull_var[idx1])\n",
    "                        other2_mean, other2_sem = mean_sem(otherpull_var[idx2])\n",
    "                        other3_mean, other3_sem = mean_sem(otherpull_var[idx3])\n",
    "\n",
    "                        # Plotting\n",
    "                        fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "                        # Left y-axis: contbhv_var\n",
    "                        ax1.plot(time_full, cont1_mean, label=pull_trig_events_tgtname+' speed Low', color='blue')\n",
    "                        ax1.fill_between(time_full, cont1_mean - cont1_sem, cont1_mean + cont1_sem, color='blue', alpha=0.2)\n",
    "\n",
    "                        ax1.plot(time_full, cont2_mean, label=pull_trig_events_tgtname+' speed Medium', color='green')\n",
    "                        ax1.fill_between(time_full, cont2_mean - cont2_sem, cont2_mean + cont2_sem, color='green', alpha=0.2)\n",
    "\n",
    "                        ax1.plot(time_full, cont3_mean, label=pull_trig_events_tgtname+' speed High', color='red')\n",
    "                        ax1.fill_between(time_full, cont3_mean - cont3_sem, cont3_mean + cont3_sem, color='red', alpha=0.2)\n",
    "\n",
    "                        ax1.set_xlabel('Time (s)')\n",
    "                        ax1.set_ylabel(pull_trig_events_tgtname)\n",
    "                        ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                        # Right y-axis: otherpull_var\n",
    "                        ax2 = ax1.twinx()\n",
    "                        ax2.plot(time_full, other1_mean, '--', \n",
    "                                 label=pull_trig_otherpull_name+' in '+pull_trig_events_tgtname+' speed Low', color='blue')\n",
    "                        ax2.fill_between(time_full, other1_mean - other1_sem, other1_mean + other1_sem, color='blue', alpha=0.1)\n",
    "\n",
    "                        ax2.plot(time_full, other2_mean, '--', \n",
    "                                 label=pull_trig_otherpull_name+' in '+pull_trig_events_tgtname+' speed Medium', color='green')\n",
    "                        ax2.fill_between(time_full, other2_mean - other2_sem, other2_mean + other2_sem, color='green', alpha=0.1)\n",
    "\n",
    "                        ax2.plot(time_full, other3_mean, '--', \n",
    "                                 label=pull_trig_otherpull_name+' in '+pull_trig_events_tgtname+' speed High', color='red')\n",
    "                        ax2.fill_between(time_full, other3_mean - other3_sem, other3_mean + other3_sem, color='red', alpha=0.1)\n",
    "\n",
    "                        ax2.set_ylabel(pull_trig_otherpull_name)\n",
    "                        ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                        # Combine legends from both axes\n",
    "                        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "                        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "                        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center left', bbox_to_anchor=(1.12, 0.5), borderaxespad=0)\n",
    "\n",
    "                        # Final touches\n",
    "                        plt.title(f\"{act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                        ax1.grid(False)\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        savefig = 0\n",
    "                        if savefig:\n",
    "                            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                            cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                            if not os.path.exists(figsavefolder):\n",
    "                                os.makedirs(figsavefolder)\n",
    "\n",
    "                            fig.savefig(figsavefolder+'OtherIntention_and_otherPull_comparison_'+bhvname_ana+'_'+\n",
    "                                        pull_trig_events_tgtname+'_'+pull_trig_otherpull_name+'.pdf')\n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc22bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfad2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f94652a",
   "metadata": {},
   "source": [
    "### sanity check plot; mean pull aligned firing rate and pull aligned gaze events (3 gaussian kernel smoothed)\n",
    "#### add the option to look at gaze accumulation over time\n",
    "#### also use this code to defined significant neurons - label neurons that significantly encode gaze accumulation before pull, this is for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:   \n",
    "    # make sure that the significance is defined based on 'pull_trig_gazeprob_name', not 'pull_trig_events_tgtname'\n",
    "    \n",
    "    # gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    gaze_duration_type = 'around_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    \n",
    "    # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "    doGazeAccum = 0\n",
    "        \n",
    "    significant_neurons_data_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                        'clusterID','significance_or_not',\n",
    "                                                        'gaze_duration_type','gaze_variable_name'])\n",
    "    \n",
    "    # load the data \n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana\n",
    "\n",
    "                # get the neurons \n",
    "                neurons_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                # Determine subplot grid (5 columns, dynamic rows)\n",
    "                ncols = 5\n",
    "                nrows = int(np.ceil(nneurons / ncols))\n",
    "\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6), constrained_layout=True)\n",
    "                axes = np.ravel(axes)  # Flatten for easy indexing\n",
    "                \n",
    "                # === New heatmap plot per date for neuron correlation over time ===\n",
    "                fig_corr, ax_corr = plt.subplots(figsize=(10, max(6, 0.3 * nneurons)))\n",
    "\n",
    "                # Store r_trace and p_trace for each neuron\n",
    "                r_traces_all_neurons = []\n",
    "                p_traces_all_neurons = []\n",
    "\n",
    "                for ineuron in np.arange(0,nneurons,1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID']==clusterID_ineuron\n",
    "\n",
    "                    ax = axes[ineuron]  # Get the subplot for this neuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date \n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                        #\n",
    "                        # load and plot bhv event ('pull') aligned FR\n",
    "                        FRs_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt['FR_allevents'])[0]\n",
    "\n",
    "                        nevents = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            FRsmoothed_allevents_ineuron = gaussian_filter1d(FRs_allevents_ineuron, sigma=6, axis=0)\n",
    "\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRsmoothed_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot the results\n",
    "                            time_trace = np.arange(-4,4,1/fps)  # Assuming time is just indices\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            ax.plot(time_trace, mean_trace, label=bhvname_ana+'(n='+str(nevents)+')', \n",
    "                                    color=bhvname_clrs[ibhvname_ana])\n",
    "                            ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                            color=bhvname_clrs[ibhvname_ana], alpha=0.3)\n",
    "                        #\n",
    "                        else:\n",
    "                            FRsmoothed_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # load and plot the pull aligned continuous bhv variables\n",
    "                        # conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_tgt[pull_trig_gazeprob_name])[0]\n",
    "                        conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                        \n",
    "                        #\n",
    "                        # calculate the gaze accumulation if the condition allows (calculate the auc)\n",
    "                        if doGazeAccum:\n",
    "                            from sklearn.metrics import auc\n",
    "                            # if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                            if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                                num_time_points = conBhv_allevents_ineuron.shape[0]\n",
    "                                num_conbhv = conBhv_allevents_ineuron.shape[1]\n",
    "                                accumulated_auc = np.zeros((num_time_points, num_conbhv))\n",
    "                                # Create a time axis (assuming equal spacing)\n",
    "                                time_ind = np.arange(num_time_points)\n",
    "                                #\n",
    "                                for i in range(num_conbhv):\n",
    "                                    for t in range(1, num_time_points):\n",
    "                                        # Calculate AUC up to the current time point for the i-th conbhv\n",
    "                                        y = conBhv_allevents_ineuron[:t+1, i]\n",
    "                                        accumulated_auc[t, i] = auc(time_ind[:t+1], y)\n",
    "                            #\n",
    "                            conBhv_allevents_ineuron = accumulated_auc\n",
    "                            \n",
    "                        # zscored the behavioral events\n",
    "                        # Flatten the data\n",
    "                        flattened = conBhv_allevents_ineuron.flatten()\n",
    "                        # Z-score the entire dataset as a single distribution\n",
    "                        flattened_z = np.full_like(flattened, np.nan)\n",
    "                        valid_mask = ~np.isnan(flattened)\n",
    "                        flattened_z[valid_mask] = st.zscore(flattened[valid_mask])\n",
    "                        # Reshape back to original shape\n",
    "                        conBhv_allevents_ineuron_z = flattened_z.reshape(conBhv_allevents_ineuron.shape)\n",
    "                        # \n",
    "                        conBhv_allevents_ineuron = conBhv_allevents_ineuron_z\n",
    "    \n",
    "                        try:\n",
    "                            nevents = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents = 0\n",
    "                        \n",
    "                        try:\n",
    "                            FRconBhv_allevents_ineuron = gaussian_filter1d(conBhv_allevents_ineuron, sigma=6, axis=0)\n",
    "                        except:\n",
    "                            FRconBhv_allevents_ineuron = np.nan\n",
    "                            \n",
    "                        # if the pull aligned FR and bhv have different number\n",
    "                        try:\n",
    "                            nevents_fr = np.shape(FRs_allevents_ineuron)[1]\n",
    "                        except:\n",
    "                            nevents_fr = 0\n",
    "                            \n",
    "                        if not  nevents_fr == nevents: \n",
    "                            print(date_ana+' mismatched number')\n",
    "                            if nevents_fr < nevents:\n",
    "                                FRconBhv_allevents_ineuron = FRconBhv_allevents_ineuron[:,0:nevents_fr]\n",
    "                            else:\n",
    "                                FRs_allevents_ineuron = FRs_allevents_ineuron[:,0:nevents]\n",
    "                            \n",
    "                        \n",
    "                        # Compute correlation coefficient between FR and behavior at each time point\n",
    "                        try:\n",
    "                            corrs = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "                            pvals = np.full(FRsmoothed_allevents_ineuron.shape[0], np.nan)\n",
    "\n",
    "                            for t in range(FRsmoothed_allevents_ineuron.shape[0]):\n",
    "                                fr_t = FRsmoothed_allevents_ineuron[t, :]\n",
    "                                bhv_t = FRconBhv_allevents_ineuron[t, :]\n",
    "\n",
    "                                valid_mask = ~np.isnan(fr_t) & ~np.isnan(bhv_t)\n",
    "                                if np.sum(valid_mask) > 5:  # Only compute if enough data points\n",
    "                                    r, p = st.pearsonr(fr_t[valid_mask], bhv_t[valid_mask])\n",
    "                                    corrs[t] = r\n",
    "                                    pvals[t] = p    \n",
    "                        except:\n",
    "                            time_trace = np.arange(-4,4,1/fps)\n",
    "                            corrs = np.full(time_trace.shape[0], np.nan)\n",
    "                            pvals = np.full(time_trace.shape[0], np.nan)\n",
    "\n",
    "\n",
    "                        r_traces_all_neurons.append(corrs)\n",
    "                        p_traces_all_neurons.append(pvals)\n",
    "\n",
    "                        # decide if this neuron is significant or not\n",
    "                        if gaze_duration_type == 'around_pull':\n",
    "                            significant_neuron = np.sum(pvals<0.01)>0\n",
    "                        elif gaze_duration_type == 'before_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace<0]<0.01)>0\n",
    "                        elif gaze_duration_type == 'after_pull':\n",
    "                            significant_neuron = np.sum(pvals[time_trace>0]<0.01)>0\n",
    "                                                \n",
    "                        #\n",
    "                        # put information about the significance \n",
    "                        if doGazeAccum:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':'gaze_accum',     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        else:\n",
    "                            significant_neurons_data_df = significant_neurons_data_df.append({'dates': date_ana, \n",
    "                                                                                    'condition':cond_ana,\n",
    "                                                                                    'act_animal':act_animal_ana,\n",
    "                                                                                    'bhv_name': bhvname_ana,\n",
    "                                                                                    'clusterID':clusterID_ineuron,\n",
    "                                                                                    'significance_or_not':significant_neuron,\n",
    "                                                                                    'gaze_duration_type':gaze_duration_type,\n",
    "                                                                                    'gaze_variable_name':pull_trig_gazeprob_name,     \n",
    "                                                                                   }, ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        if nevents > 0:\n",
    "                            # Compute mean and SEM while ignoring NaNs\n",
    "                            mean_trace = np.nanmean(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            std_trace = np.nanstd(FRconBhv_allevents_ineuron, axis=1)\n",
    "                            sem_trace = std_trace / np.sqrt(nevents)  # Standard error of the mean\n",
    "\n",
    "                            # Plot each behavior as a separate trace\n",
    "                            if doGazeAccum:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_gazeprob_name+'_AUC(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "                            else:\n",
    "                                ax.plot(time_trace, mean_trace, label='pull_trig_'+pull_trig_gazeprob_name+'(n='+str(nevents)+')', \n",
    "                                        color='#808080')\n",
    "                                ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, \n",
    "                                                color='#808080', alpha=0.3)\n",
    "\n",
    "\n",
    "                            # Create a twin axis for the correlation plot\n",
    "                            ax2 = ax.twinx()                   \n",
    "\n",
    "                            # Plot correlation coefficient trace on the right y-axis\n",
    "                            ax2.plot(time_trace, corrs, color='black', linestyle='--', label='FR–Bhv r')\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "\n",
    "                            # Highlight significant timepoints (p < 0.01) with red dots on the right y-axis\n",
    "                            significant_mask = (pvals < 0.01) & ~np.isnan(pvals)\n",
    "                            ax2.plot(time_trace[significant_mask], corrs[significant_mask], 'ro', label='p < 0.01')\n",
    "\n",
    "                            # Set the label for the right axis\n",
    "                            ax2.set_ylabel(\"Correlation (r)\", color='black')\n",
    "                            ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                            # Optionally adjust limits or formatting if necessary\n",
    "                            ax2.set_ylim(-1, 1)  # Adjust this as necessary for your data range\n",
    "\n",
    "\n",
    "                    ax.set_title(f\"Neuron {clusterID_ineuron}\")\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "                    ax.set_ylabel(\"Firing Rate (a.u.)\")\n",
    "                    # ax.set_title(act_animal_ana+' '+cond_ana+' '+date_ana+' cell#'+clusterID_ineuron)\n",
    "                    ax.legend()\n",
    "\n",
    "                # Hide empty subplots if nneurons < total grid size\n",
    "                for i in range(nneurons, len(axes)):\n",
    "                    fig.delaxes(axes[i])\n",
    "\n",
    "                # Figure title\n",
    "                fig.suptitle(f\"{act_animal_ana} {cond_ana} {date_ana}\", fontsize=14)\n",
    "\n",
    "                \n",
    "                # Convert to numpy array for heatmap\n",
    "                r_traces_all_neurons = np.array(r_traces_all_neurons)\n",
    "\n",
    "                # === Sort r_traces by the time of their first peak ===\n",
    "                peak_times = []\n",
    "                for trace in r_traces_all_neurons:\n",
    "                    if np.all(np.isnan(trace)):\n",
    "                        peak_times.append(np.inf)\n",
    "                    else:\n",
    "                        peak_idx = np.nanargmax(trace)\n",
    "                        peak_times.append(time_trace[peak_idx])\n",
    "\n",
    "                # Get sorting indices based on peak times\n",
    "                sorted_indices = np.argsort(peak_times)\n",
    "                r_traces_sorted = r_traces_all_neurons[sorted_indices, :]\n",
    "\n",
    "                # Plot heatmap of r values\n",
    "                im = ax_corr.imshow(r_traces_sorted, aspect='auto', cmap='gray_r', interpolation='none',\n",
    "                                    extent=[time_trace[0], time_trace[-1], 0, nneurons],\n",
    "                                    vmin=-0.7, vmax=0.7)\n",
    "\n",
    "                # Overlay significance as red dots\n",
    "                for i, idx in enumerate(sorted_indices):\n",
    "                    sig_times = np.where(p_traces_all_neurons[idx] < 0.01)[0]\n",
    "                    for t in sig_times:\n",
    "                        ax_corr.plot(time_trace[t], i + 0.5, 'r.', markersize=3)  # i+0.5 to center in the row\n",
    "\n",
    "                # Add vertical dashed line at time = 0\n",
    "                ax_corr.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                # Add vertical dashed line at time zero\n",
    "                ax_corr.axvline(x=0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "                ax_corr.set_xlabel(\"Time (s)\")\n",
    "                ax_corr.set_ylabel(\"Neuron (sorted by peak time)\")\n",
    "                ax_corr.set_title(f\"Neuron-wise Corr(Gaze, FR) Heatmap (Sorted): {act_animal_ana} {cond_ana} {date_ana}\")\n",
    "                cbar = fig_corr.colorbar(im, ax=ax_corr)\n",
    "                cbar.set_label('Pearson r')\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                savefig = 0\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+date_ana+'/'\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    if doGazeAccum:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_gazeprob_name+'_auc'+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_gazeprob_name+'_auc'+savefile_sufix+'.pdf')\n",
    "                    else:\n",
    "                        fig.savefig(figsavefolder+'individualneurons_meanFR_and_mean_'+bhvname_ana+'_'+\n",
    "                                     pull_trig_gazeprob_name+savefile_sufix+'.pdf')\n",
    "\n",
    "                        fig_corr.savefig(figsavefolder + 'neuron_corr_heatmap_FR_and_' + bhvname_ana+'_'+\n",
    "                                         pull_trig_gazeprob_name+savefile_sufix+'.pdf')\n",
    "\n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig)\n",
    "                plt.close(fig_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_neurons_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from scipy.integrate import cumtrapz  # Add this import\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    ind1 = bhvevents_aligned_FR_allevents_all_dates_df['condition']=='MC_withGinger'\n",
    "    ind2 = bhvevents_aligned_FR_allevents_all_dates_df['dates']=='20240808'\n",
    "    ind3 = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == 'pull'\n",
    "    ind4 = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']=='kanga'\n",
    "    conBhv_allevents_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_events_tgtname])[0]\n",
    "    conBhv_gazeprob_ineuron = np.array(bhvevents_aligned_FR_allevents_all_dates_df[ind1&ind2&ind3&ind4][pull_trig_gazeprob_name])[0]\n",
    "\n",
    "    np.shape(conBhv_gazeprob_ineuron)\n",
    "    \n",
    "    # Setup\n",
    "    plotID = 3\n",
    "    trace = conBhv_gazeprob_ineuron[plotID]\n",
    "    trace2 = conBhv_allevents_ineuron[plotID]\n",
    "    time_trace = np.arange(-4, 4, 1/30)\n",
    "\n",
    "    # Compute accumulated AUC using trapezoidal integration\n",
    "    accum_auc = cumtrapz(trace, time_trace, initial=0)\n",
    "\n",
    "    # Create plot with dual y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Plot the gaze distribution\n",
    "    color1 = 'tab:blue'\n",
    "    line1, = ax1.plot(time_trace, trace, color=color1, label='Gaze Distribution')\n",
    "    fill = ax1.fill_between(time_trace, trace, alpha=0.3, color=color1)\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Gaze Distribution', color=color1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.axvline(0, color='k', linestyle='--', linewidth=1)\n",
    "    ax1.set_title('Self pull aligned social gaze')\n",
    "\n",
    "    # Create a second y-axis for accumulated AUC\n",
    "    if 1:\n",
    "        ax2 = ax1.twinx()\n",
    "        color2 = 'tab:red'\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Patner distance to lever')\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Self distance to lever')\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Partner distance to tube')\n",
    "        line2, = ax2.plot(time_trace, trace2, color=color2, label=pull_trig_events_tgtname)\n",
    "        # line2, = ax2.plot(time_trace, trace2, color=color2, label='Self distance to tube')\n",
    "        ax2.set_ylabel('distance (a.u.)', color=color2)\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Legend: lines and manual patch for shaded area\n",
    "    legend_elements = [\n",
    "        line1,\n",
    "        Patch(facecolor=color1, alpha=0.3, label='AUC Area'),\n",
    "        line2\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_PartnerLeverDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_SelfLeverDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_PartnerTubeDist.pdf')\n",
    "        # fig.savefig(figsavefolder+'example_event_SelfGazeAccum_SelfTubeDist.pdf')\n",
    "        fig.savefig(figsavefolder+'example_event_SelfGazeAccum_'+pull_trig_events_tgtname+'.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b24c5b",
   "metadata": {},
   "source": [
    "#### run PCA on the neuron space, run different days separately for each condition\n",
    "#### for the activity aligned at the different bhv events\n",
    "#### run PCA for all bhvevent together combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "        \n",
    "    # Step 1 - run PCA separately\n",
    "    # save the simple PCA data\n",
    "    FRPCA_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','succrate','act_animal',\n",
    "                                                                'bhv_name','bhv_id','PCs',])\n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition']==cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal']==act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0,ndates_ana,1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates']==date_ana         \n",
    "\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    succrate = np.array(bhvevents_aligned_FR_allevents_tgt['succrate'])[0][0]\n",
    "                    \n",
    "                    # to better combine different bhv events, choose the same amount\n",
    "                    nbhv_topick = 50\n",
    "\n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    valid_bhvs = ~np.any(np.isnan(data_array), axis=(0, 1))  # Shape (144,)\n",
    "                    data_array = data_array[:, :, valid_bhvs]\n",
    "                    # in case no bhv events were there\n",
    "                    if np.shape(valid_bhvs)[0]==0:\n",
    "                        a, b, _ = data_array.shape\n",
    "                        data_array = np.full((a, b, 1), np.nan)\n",
    "\n",
    "                    nneurons = np.shape(data_array)[0]\n",
    "                    timepointnums = np.shape(data_array)[1]\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "                        \n",
    "                    # Randomly select bhv events with replacement, once for all neurons\n",
    "                    selected_bhvs = np.random.choice(mbhv_total, nbhv_topick, replace=True)\n",
    "                    sampled_data = data_array[:, :, selected_bhvs]\n",
    "\n",
    "                    # Reshape by flattening the last two dimensions\n",
    "                    final_array = sampled_data.reshape(nneurons, -1)\n",
    "\n",
    "                    PCA_dataset_ibv = final_array\n",
    "\n",
    "                    # combine all bhv for running PCA in the same neural space\n",
    "                    if ibhvname_ana == 0:\n",
    "                        PCA_dataset = PCA_dataset_ibv\n",
    "                    else:\n",
    "                        PCA_dataset = np.hstack([PCA_dataset,PCA_dataset_ibv])\n",
    "\n",
    "                # remove nan raw from the data set\n",
    "                # ind_nan = np.isnan(np.sum(PCA_dataset,axis=0))\n",
    "                # PCA_dataset = PCA_dataset_test[:,~ind_nan]\n",
    "                ind_nan = np.isnan(np.sum(PCA_dataset,axis=1))\n",
    "                PCA_dataset = PCA_dataset[~ind_nan,:]\n",
    "                PCA_dataset = np.transpose(PCA_dataset)\n",
    "\n",
    "                # Run PCA on this concatenated data \n",
    "                pca = PCA(n_components=3)\n",
    "                try:\n",
    "                    pca.fit(PCA_dataset)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "                totalneuronNum = np.shape(PCA_dataset)[1]\n",
    "\n",
    "                # project on the individual events\n",
    "                for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                    bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                    ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                    ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                    bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_ana]\n",
    "\n",
    "                    # get the pull triggered continuous variable of target\n",
    "                    data_array_conBhv = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname]))\n",
    "                    data_array_conBhv = np.nanmean(data_array_conBhv,axis=0)\n",
    "                    data_array_conBhv = data_array_conBhv.transpose()\n",
    "\n",
    "                    # get the pull triggered gaze distribution \n",
    "                    data_array_gazeprob = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_gazeprob_name]))\n",
    "                    data_array_gazeprob = np.nanmean(data_array_gazeprob,axis=0)\n",
    "                    data_array_gazeprob = data_array_gazeprob.transpose()\n",
    "                    \n",
    "                    # get the self and other pull variable\n",
    "                    data_array_otherpull = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_otherpull_name]))\n",
    "                    data_array_otherpull = np.nanmean(data_array_otherpull,axis=0)\n",
    "                    data_array_otherpull = data_array_otherpull.transpose()\n",
    "                    #\n",
    "                    data_array_selfpull = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_trig_selfpull_name]))\n",
    "                    data_array_selfpull = np.nanmean(data_array_selfpull,axis=0)\n",
    "                    data_array_selfpull = data_array_selfpull.transpose()\n",
    "                    \n",
    "                    # get the prefailpull number variable\n",
    "                    data_prefailpull_num = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_num_pre_failpull_name]))\n",
    "                    data_prefailpull_num = np.nanmean(data_prefailpull_num,axis=0)\n",
    "                    \n",
    "                    # get the time_since_lastreward variable\n",
    "                    data_lastreward_time = np.array(list(bhvevents_aligned_FR_allevents_tgt[pull_time_pre_reward_name]))\n",
    "                    data_lastreward_time = np.nanmean(data_lastreward_time,axis=0)\n",
    "                    \n",
    "                    # Convert list of arrays into a single NumPy array \n",
    "                    data_array = np.array(list(bhvevents_aligned_FR_allevents_tgt['FR_allevents']))  # Shape (n neuron, t time stamp, m bhv events)\n",
    "\n",
    "                    mbhv_total = np.shape(data_array)[2]\n",
    "\n",
    "                    for ibhv in np.arange(0,mbhv_total,1):\n",
    "\n",
    "                        data_ibhv = data_array[:,:,ibhv]\n",
    "\n",
    "                        #\n",
    "                        # get the pull triggered continous variables of target for individual events\n",
    "                        try:\n",
    "                            data_array_conBhv_ibhv = data_array_conBhv[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_conBhv_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        \n",
    "                        #\n",
    "                        # get the pull triggered gaze distribution\n",
    "                        try:\n",
    "                            data_array_gazeprob_ibhv = data_array_gazeprob[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_gazeprob_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                            \n",
    "                        #\n",
    "                        # for the socialgaze_prob, only use the meaningful ones\n",
    "                        if 0:\n",
    "                            if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                                trace = data_array_gazeprob_ibhv\n",
    "                                time_trace = np.arange(-4, 4, 1/30)\n",
    "                                filtered_trace = keep_closest_cluster_single_trial(trace, time_trace)\n",
    "                                data_array_gazeprob_ibhv = filtered_trace\n",
    "                               \n",
    "                        #    \n",
    "                        # get the pull triggered continous variables of target for individual events\n",
    "                        try:\n",
    "                            data_array_conBhv_ibhv = data_array_conBhv[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_conBhv_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        \n",
    "                        # get the pull triggered self and pther pull for individual events\n",
    "                        try:\n",
    "                            data_array_otherpull_ibhv = data_array_otherpull[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_otherpull_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                        #\n",
    "                        try:\n",
    "                            data_array_selfpull_ibhv = data_array_selfpull[:,ibhv]\n",
    "                        except:\n",
    "                            data_array_selfpull_ibhv = np.full((timepointnums, 1), np.nan)\n",
    "                            \n",
    "                        # get the prefailpull number variable\n",
    "                        try:\n",
    "                            data_prefailpull_num_ibhv = data_prefailpull_num[ibhv]\n",
    "                        except:\n",
    "                            data_prefailpull_num_ibhv = np.nan\n",
    "                            \n",
    "                        # get the time_since_lastreward variable\n",
    "                        try:\n",
    "                            data_lastreward_time_ibhv = data_lastreward_time[ibhv]\n",
    "                        except:\n",
    "                            data_lastreward_time_ibhv = np.nan\n",
    "                        \n",
    "                            \n",
    "                        #\n",
    "                        # for firing rate, project on the PC space    \n",
    "                        try:\n",
    "                            PCA_proj_ibhv = pca.transform(np.transpose(data_ibhv))\n",
    "                        except:\n",
    "                            PCA_proj_ibhv = np.full((timepointnums, 3), np.nan)\n",
    "\n",
    "                        FRPCA_all_sessions_allevents_sum_df = FRPCA_all_sessions_allevents_sum_df.append({'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':succrate,\n",
    "                                                                                'bhv_id':ibhv,\n",
    "                                                                                'PCs':PCA_proj_ibhv,\n",
    "                                                                                'neuronNumBeforePCA':totalneuronNum,\n",
    "                                                                                pull_trig_events_tgtname: data_array_conBhv_ibhv,\n",
    "                                                                                pull_trig_gazeprob_name: data_array_gazeprob_ibhv,\n",
    "                                                                                pull_trig_otherpull_name:data_array_otherpull_ibhv,\n",
    "                                                                                pull_trig_selfpull_name:data_array_selfpull_ibhv,\n",
    "                                                                                pull_num_pre_failpull_name:data_prefailpull_num_ibhv,\n",
    "                                                                                pull_time_pre_reward_name:data_lastreward_time_ibhv,\n",
    "                                                                                                         \n",
    "                                                                                 }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480a20f",
   "metadata": {},
   "source": [
    "#### add PCA features, gaze features, and partner distance to lever features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 for each PCA trace, calculate the length, curvature, and/or tortusity for comparison later\n",
    "# test hypothesis: 1. for testing if individual trial different was from gaze start time/stop time/gaze duration\n",
    "    \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "FRPCAfeatures_all_sessions_allevents_sum_df = pd.DataFrame(columns=['condition','session','act_animal','succrate',\n",
    "                                                                    'bhv_name','bhv_id',\n",
    "                                                                    'PClength','PCcurv','PCtort','PCspeed','PCsmoothness',\n",
    "                                                                    'PCspeed_trace','PCcurv_trace',\n",
    "                                                                    ])\n",
    "FRPCAfeatures_gazeduration_corr_all_sessions_df = pd.DataFrame(columns=['condition','session','succrate',\n",
    "                                                                        'act_animal','bhv_name',])\n",
    "\n",
    "# newly added control!!\n",
    "# only look at pull aligned events that has no preceding self pull \n",
    "doSingleSelfPulls = 1\n",
    "\n",
    "# add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "# gaze_duration_type = 'before_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "gaze_duration_type = 'after_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "# gaze_duration_type = 'around_pull' # 'around_pull', 'before_pull', 'after_pull'\n",
    "\n",
    "#\n",
    "for ianimal_ana in np.arange(0,nanimal_to_ana,1):\n",
    "    act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "    ind_animal = FRPCA_all_sessions_allevents_sum_df['act_animal']==act_animal_ana\n",
    "        \n",
    "    # get the dates\n",
    "    dates_toplot = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal]['session'])\n",
    "    ndates_toplot = np.shape(dates_toplot)[0]\n",
    "    \n",
    "\n",
    "    for icond_ana in np.arange(0,nconds_to_ana,1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = FRPCA_all_sessions_allevents_sum_df['condition']==cond_ana\n",
    "\n",
    "        # get the dates\n",
    "        dates_ana = np.unique(FRPCA_all_sessions_allevents_sum_df[ind_animal & ind_cond]['session'])\n",
    "        ndates_ana = np.shape(dates_ana)[0]\n",
    "    \n",
    "\n",
    "        for idate_ana in np.arange(0,ndates_ana,1):\n",
    "            date_ana = dates_ana[idate_ana]\n",
    "            ind_date = FRPCA_all_sessions_allevents_sum_df['session']==date_ana     \n",
    "            \n",
    "\n",
    "            for ibhvname_ana in np.arange(0,nbhvnames_to_ana,1):\n",
    "                bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                ind_bhv = FRPCA_all_sessions_allevents_sum_df['bhv_name']==bhvname_ana\n",
    "\n",
    "                ind_ana = ind_animal & ind_bhv & ind_cond & ind_date\n",
    "\n",
    "                FRPCA_allevents_toana = FRPCA_all_sessions_allevents_sum_df[ind_ana]\n",
    "\n",
    "                bhv_ids = np.array(FRPCA_allevents_toana['bhv_id'])\n",
    "                nbhvevents = np.shape(bhv_ids)[0]\n",
    "\n",
    "                for ibhv_id in np.arange(0,nbhvevents,1):\n",
    "\n",
    "                    bhv_id = bhv_ids[ibhv_id]\n",
    "                    ind_bhvid = FRPCA_allevents_toana['bhv_id'] == bhv_id\n",
    "\n",
    "                    # \n",
    "                    # count self pull after self pull intention onset, \n",
    "                    # the goal is to make sure the effect we will find later is not because of the following pulls\n",
    "                    from scipy.signal import find_peaks\n",
    "                    #\n",
    "                    pulltrig_selfpull = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_selfpull_name])[0]\n",
    "                    #\n",
    "                    x_full = np.arange(-4,4,1/fps)\n",
    "                    post_mask = x_full>0.15\n",
    "                    try:\n",
    "                        # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                        data = pulltrig_selfpull[post_mask]\n",
    "                        peaks, _ = find_peaks(data)\n",
    "                        selfpull_num = len(peaks)\n",
    "                    except:\n",
    "                        selfpull_num = np.nan\n",
    "                    \n",
    "                    #\n",
    "                    x_full = np.arange(-4, 4, 1/fps)\n",
    "                    #\n",
    "                    if gaze_duration_type == 'before_pull':\n",
    "                        # Only use the pre-pull window (-4s to 0s)\n",
    "                        pre_mask = x_full <= 0\n",
    "                    elif gaze_duration_type == 'after_pull':\n",
    "                        # Only use the post-pull window (0s to 4s)\n",
    "                        pre_mask = x_full >= 0\n",
    "                    elif gaze_duration_type == 'around_pull':\n",
    "                        # the entire -4 to 4s\n",
    "                        pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "                \n",
    "                    # \n",
    "                    # analyze some features based on 'pull_trig_events_tgtname'\n",
    "                    pulltrig_conBhv_tgt = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "                    \n",
    "                    partner_face_lever_mean = np.nanmean(pulltrig_conBhv_tgt[pre_mask])\n",
    "                    partner_face_lever_std = np.nanstd(pulltrig_conBhv_tgt[pre_mask])\n",
    "                    \n",
    "                    \n",
    "                    if (pull_trig_events_tgtname == 'otherani_otherlever_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_lever_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'otherani_othertube_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_tube_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'animal_animal_dist') | \\\n",
    "                       (pull_trig_events_tgtname == 'otherpull_prob') :\n",
    "                        \n",
    "                        partner_face_lever_velocity = np.gradient(pulltrig_conBhv_tgt, 1/fps)\n",
    "                        #\n",
    "                        partner_face_lever_speed = np.abs(partner_face_lever_velocity)\n",
    "                        #\n",
    "                        partner_face_lever_meanvelocity = np.nanmean(partner_face_lever_velocity[pre_mask])\n",
    "                        #\n",
    "                        partner_face_lever_meanspeed = np.nanmean(partner_face_lever_speed[pre_mask])\n",
    "        \n",
    "                        min_dist = np.arange(-4,4,1/fps)[np.argmin(pulltrig_conBhv_tgt)]\n",
    "                        max_dist = np.arange(-4,4,1/fps)[np.argmax(pulltrig_conBhv_tgt)]\n",
    "                        #\n",
    "                        pre_min_mask = x_full <= min_dist\n",
    "                        \n",
    "                        # find the time that the dramatic change starts, if could not find it, use the -4s\n",
    "                        percentile = 95\n",
    "                        dt = 1 / fps\n",
    "                        abs_derivative = np.abs(np.gradient(pulltrig_conBhv_tgt[x_full <= 0], dt))\n",
    "                        threshold = np.percentile(abs_derivative, percentile)\n",
    "                        # Start from the first time point (index 0)\n",
    "                        idx_change = np.where(abs_derivative > threshold)[0]\n",
    "                        if len(idx_change) > 0:\n",
    "                            onset_idx = idx_change[0]\n",
    "                            change_time = x_full[x_full <= 0][onset_idx]\n",
    "                        else:\n",
    "                            change_time = x_full[0]\n",
    "                        #\n",
    "                        partner_face_lever_changetime = change_time\n",
    "                        #\n",
    "                        post_changetime_mask = x_full >= change_time\n",
    "                        #\n",
    "                        # min and max after the change_time\n",
    "                        min_dist_post_changetime = x_full[x_full>=change_time][np.argmin(pulltrig_conBhv_tgt[x_full>=change_time])]\n",
    "                        max_dist_post_changetime = x_full[x_full>=change_time][np.argmax(pulltrig_conBhv_tgt[x_full>=change_time])]\n",
    "                        \n",
    "                        #\n",
    "                        # find the partner lever approaching trend\n",
    "                        from scipy.stats import linregress\n",
    "                        # Define time range and extract window\n",
    "                        y_full = np.array(pulltrig_conBhv_tgt)                      \n",
    "                        #\n",
    "                        # x_pre = x_full[pre_mask]\n",
    "                        # y_pre = y_full[pre_mask]\n",
    "                        # x_pre = x_full[pre_min_mask]\n",
    "                        # y_pre = y_full[pre_min_mask]\n",
    "                        # x_pre = x_full[pre_min_mask & post_changetime_mask]\n",
    "                        # y_pre = y_full[pre_min_mask & post_changetime_mask]    \n",
    "                        x_pre = x_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]\n",
    "                        y_pre = y_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]                        \n",
    "                        # Linear regression\n",
    "                        slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                        # slope = (y_pre[-1] - y_pre[0])/(x_pre[-1] - x_pre[0])\n",
    "                        #\n",
    "                        approaching_slope = slope\n",
    "                      \n",
    "                    else:\n",
    "                        partner_face_lever_velocity = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "                        #\n",
    "                        partner_face_lever_speed = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "                        \n",
    "                        partner_face_lever_meanspeed = np.nan\n",
    "                        partner_face_lever_meanvelocity = np.nan\n",
    "        \n",
    "                        min_dist = np.nan\n",
    "                        max_dist = np.nan\n",
    "                        \n",
    "                        approaching_slope = np.nan\n",
    "                        \n",
    "                        partner_face_lever_changetime = np.nan                  \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the pull triggered behavioral events\n",
    "                    #\n",
    "                    # pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_events_tgtname])[0]\n",
    "                    pulltrig_conBhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_trig_gazeprob_name])[0]\n",
    "                            \n",
    "                    # if pull_trig_events_tgtname == 'socialgaze_prob':\n",
    "                    if pull_trig_gazeprob_name == 'socialgaze_prob':\n",
    "                        # calculate the gaze start and gaze stop time, and finally gaze duration\n",
    "                        try:\n",
    "                            \n",
    "                            # Find the point of first increasing and last decrease to estimate gaze start and end\n",
    "                            #\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            #\n",
    "                            # if gaze_duration_type == 'before_pull':\n",
    "                            #     pulltrig_conBhv[timewins>0] = 0\n",
    "                            # elif gaze_duration_type == 'after_pull':\n",
    "                            #     pulltrig_conBhv[timewins<0] = 0                           \n",
    "                            \n",
    "                            if 1:\n",
    "                                first_increase_idx = np.where(np.diff(pulltrig_conBhv) > 0)[0][0] + 1\n",
    "                                #\n",
    "                                last_decrease_idx = np.where(np.diff(pulltrig_conBhv) < 0)[0][-1] + 1  # Find last decrease\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_increase_idx].copy()\n",
    "                                gazestop_time = timewins[last_decrease_idx].copy()\n",
    "                            if 0:\n",
    "                                # Find peaks\n",
    "                                peaks, _ = scipy.signal.find_peaks(pulltrig_conBhv)\n",
    "                                #\n",
    "                                # Get first and last peak\n",
    "                                first_peak = peaks[0] \n",
    "                                last_peak = peaks[-1]\n",
    "                                #\n",
    "                                gazestart_time = timewins[first_peak].copy()\n",
    "                                gazestop_time = timewins[last_peak].copy()\n",
    "                            #\n",
    "                            # change the gazestart and gazestop time based on the gaze duration definition\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                gazestart_time = gazestart_time\n",
    "                                gazestop_time = gazestop_time\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                if (gazestart_time > 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestop_time > 0):\n",
    "                                    gazestop_time = 0\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                if (gazestop_time < 0):\n",
    "                                    gazestart_time = np.nan\n",
    "                                    gazestop_time = np.nan\n",
    "                                elif (gazestart_time < 0):\n",
    "                                    gazestart_time = 0                                                \n",
    "                            #\n",
    "                            # if (gazestart_time == timewins[0]) | (gazestart_time == timewins[-1]):\n",
    "                            #     gazestart_time = np.nan\n",
    "                            # if (gazestop_time == timewins[0]) | (gazestop_time == timewins[-1]):\n",
    "                            #     gazestop_time = np.nan\n",
    "                            if (gazestop_time < gazestart_time):\n",
    "                                gazestart_time = np.nan\n",
    "                                gazestop_time = np.nan                           \n",
    "                        except:\n",
    "                            gazestart_time = np.nan\n",
    "                            gazestop_time = np.nan\n",
    "                            \n",
    "                        # calculate the gaze accumulation (use auc to estimate)\n",
    "                        try:\n",
    "                            timewins = np.arange(-4,4,1/fps) # make sure it align with the setting in the previous section\n",
    "                            dt = 1 / fps  # sampling interval in seconds\n",
    "                            #\n",
    "                            if gaze_duration_type == 'around_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv, dx=dt)\n",
    "                            if gaze_duration_type == 'before_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins<0], dx=dt)\n",
    "                            if gaze_duration_type == 'after_pull':\n",
    "                                auc = np.trapz(pulltrig_conBhv[timewins>0], dx=dt)\n",
    "                            #\n",
    "                            gaze_accum = auc\n",
    "                        #\n",
    "                        except:\n",
    "                            gaze_accum = np.nan\n",
    "                                \n",
    "                         \n",
    "                    # get the information about prefailpull_num\n",
    "                    prefailpull_num_ibhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_num_pre_failpull_name])[0]\n",
    "                    \n",
    "                    # get the information about last reward time\n",
    "                    lastreward_time_ibhv = np.array(FRPCA_allevents_toana[ind_bhvid][pull_time_pre_reward_name])[0]\n",
    "                    \n",
    "                    \n",
    "                    # \n",
    "                    # analyze the PCs \n",
    "                    FRPCA_ievent_toana = np.array(FRPCA_allevents_toana[ind_bhvid]['PCs'])[0]\n",
    "\n",
    "                    # smooth the pc trajectory\n",
    "                    if 0:\n",
    "                        FRPCA_ievent_toana = np.apply_along_axis(gaussian_filter1d, axis=0, \n",
    "                                                                 arr=FRPCA_ievent_toana, sigma=6)\n",
    "\n",
    "                    # calculate the length, curvature and tortuosity\n",
    "                    PC_traj = FRPCA_ievent_toana.copy()  # Shape (240, 3)\n",
    "                    \n",
    "                    #\n",
    "                    if gaze_duration_type == 'before_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[0:int(ntimepoints/2),:]\n",
    "                    elif gaze_duration_type == 'after_pull':\n",
    "                        ntimepoints = np.shape(PC_traj)[0]\n",
    "                        PC_traj = PC_traj[int(ntimepoints/2):,:]\n",
    "\n",
    "                    # Compute differences between consecutive points\n",
    "                    diffs = np.diff(PC_traj, axis=0)\n",
    "\n",
    "                    # Compute segment lengths\n",
    "                    segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "                    total_length = np.sum(segment_lengths)  # Arc length of trajectory\n",
    "\n",
    "                    # Compute curvature\n",
    "                    # First derivatives\n",
    "                    dX_dt = np.gradient(PC_traj[:, 0])\n",
    "                    dY_dt = np.gradient(PC_traj[:, 1])\n",
    "                    dZ_dt = np.gradient(PC_traj[:, 2])\n",
    "                    dV = np.vstack((dX_dt, dY_dt, dZ_dt)).T\n",
    "\n",
    "                    # Second derivatives\n",
    "                    d2X_dt2 = np.gradient(dX_dt)\n",
    "                    d2Y_dt2 = np.gradient(dY_dt)\n",
    "                    d2Z_dt2 = np.gradient(dZ_dt)\n",
    "                    d2V = np.vstack((d2X_dt2, d2Y_dt2, d2Z_dt2)).T\n",
    "\n",
    "                    # Curvature formula: ||dV x d2V|| / ||dV||^3\n",
    "                    cross_prod = np.cross(dV[:-1], d2V[:-1])  # Compute cross product\n",
    "                    curvature = np.linalg.norm(cross_prod, axis=1) / (np.linalg.norm(dV[:-1], axis=1) ** 3 + 1e-10)\n",
    "\n",
    "                    # Compute tortuosity: Total length / Euclidean distance between start and end\n",
    "                    euclidean_distance = np.linalg.norm(PC_traj[-1] - PC_traj[0])\n",
    "                    tortuosity = total_length / euclidean_distance if euclidean_distance > 0 else np.nan\n",
    "                    \n",
    "                    # Compute speed \n",
    "                    dt = 1.0 / fps  # Time between frames\n",
    "                    # Velocity: first derivative of position\n",
    "                    velocity = np.gradient(PC_traj, axis=0) / dt\n",
    "                    # Speed: magnitude of velocity\n",
    "                    speed = np.linalg.norm(velocity, axis=1)\n",
    "                    \n",
    "                    # Compute Smoothness - A simple way to compute trajectory smoothness is to look at the jerk \n",
    "                    # — the third derivative of position (how quickly acceleration changes), \n",
    "                    # which reflects sudden directional/velocity shifts\n",
    "                    # Acceleration: second derivative\n",
    "                    acceleration = np.gradient(velocity, axis=0) / dt\n",
    "                    # Jerk: third derivative\n",
    "                    jerk = np.gradient(acceleration, axis=0) / dt\n",
    "                    # Smoothness metric: integrated squared jerk over time\n",
    "                    squared_jerk = np.linalg.norm(jerk, axis=1) ** 2\n",
    "                    smoothness = np.sum(squared_jerk) * dt\n",
    "\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df.append({\n",
    "                                                                'condition':cond_ana,\n",
    "                                                                'act_animal':act_animal_ana,\n",
    "                                                                'bhv_name': bhvname_ana,\n",
    "                                                                'session':date_ana,\n",
    "                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                'bhv_id':ibhv_id,\n",
    "                                                                'PClength':total_length,\n",
    "                                                                'PCcurv':np.nanmean(curvature),\n",
    "                                                                'PCtort':tortuosity,\n",
    "                                                                'PCspeed':np.nanmean(speed),\n",
    "                                                                'PCsmoothness':smoothness,\n",
    "                                                                'PCspeed_trace':speed,\n",
    "                                                                'PCcurv_trace':curvature,\n",
    "                                                                'gazestart_time':gazestart_time,\n",
    "                                                                'gazestop_time':gazestop_time,\n",
    "                                                                'gaze_accum':gaze_accum,\n",
    "                                                                'neuronNumBeforePCA':np.array(FRPCA_allevents_toana[ind_bhvid]['neuronNumBeforePCA'])[0],\n",
    "                                                                'selfpull_num':selfpull_num,\n",
    "                                                                pull_trig_gazeprob_name: pulltrig_conBhv,\n",
    "                                                                pull_trig_events_tgtname: pulltrig_conBhv_tgt,\n",
    "                                                                pull_trig_events_tgtname+'_mean':partner_face_lever_mean,\n",
    "                                                                pull_trig_events_tgtname+'_std':partner_face_lever_std,\n",
    "                                                                pull_trig_events_tgtname+'_speed':partner_face_lever_speed,\n",
    "                                                                pull_trig_events_tgtname+'_meanspeed':partner_face_lever_meanspeed,\n",
    "                                                                pull_trig_events_tgtname+'_velocity':partner_face_lever_velocity,\n",
    "                                                                pull_trig_events_tgtname+'_meanvelocity':partner_face_lever_meanvelocity,\n",
    "                                                                pull_trig_events_tgtname+'_max':max_dist,\n",
    "                                                                pull_trig_events_tgtname+'_min':min_dist,\n",
    "                                                                pull_trig_events_tgtname+'_changetime':partner_face_lever_changetime,\n",
    "                                                                pull_trig_events_tgtname+'_appoachslope':approaching_slope,\n",
    "                                                                pull_num_pre_failpull_name:prefailpull_num_ibhv,\n",
    "                                                                pull_time_pre_reward_name:lastreward_time_ibhv,\n",
    "                    \n",
    "                                                                }, ignore_index=True)\n",
    "      \n",
    "        \n",
    "                # \n",
    "                # remove events that has multiple self pulls before the aligned self pulls \n",
    "                if doSingleSelfPulls:\n",
    "                    ind_good = FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num']==0\n",
    "                    FRPCAfeatures_all_sessions_allevents_sum_df = FRPCAfeatures_all_sessions_allevents_sum_df[ind_good]\n",
    "                \n",
    "               \n",
    "                # after pool all the events related data together do some plotting and calculate the correlation             \n",
    "                ind_sess_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == date_ana\n",
    "                ind_ani_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['act_animal'] == act_animal_ana\n",
    "                ind_bhv_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['bhv_name'] == bhvname_ana\n",
    "                ind_cond_toplot = FRPCAfeatures_all_sessions_allevents_sum_df['condition'] == cond_ana\n",
    "                \n",
    "                ind_toplot = ind_sess_toplot & ind_ani_toplot & ind_bhv_toplot & ind_cond_toplot\n",
    "                FRPCAfeatures_toplot = FRPCAfeatures_all_sessions_allevents_sum_df[ind_toplot]\n",
    "                \n",
    "                #\n",
    "                doNeuroPCA_plot = 1\n",
    "                #\n",
    "                if not doNeuroPCA_plot:\n",
    "                    # xxx_type = 'gaze_duration'\n",
    "                    xxx_type = 'gaze_accumulation'\n",
    "                    # xxx_type = 'gazestart_time'\n",
    "\n",
    "                    yyy_types = [pull_trig_events_tgtname+'_meanspeed',pull_trig_events_tgtname+'_max',\n",
    "                                 pull_trig_events_tgtname+'_min',pull_trig_events_tgtname+'_mean',\n",
    "                                 pull_trig_events_tgtname+'_changetime',pull_trig_events_tgtname+'_appoachslope',\n",
    "                                'gazestart_time',pull_num_pre_failpull_name,pull_time_pre_reward_name]\n",
    "                    #\n",
    "                    # yyy_types = [pull_trig_events_tgtname+'_mean',\n",
    "                    #             'gazestart_time']\n",
    "                #\n",
    "                elif doNeuroPCA_plot:\n",
    "                    # xxx_type = pull_trig_events_tgtname+'_appoachslope'\n",
    "                    # xxx_type = pull_trig_events_tgtname+'_meanspeed'\n",
    "                    # xxx_type = pull_trig_events_tgtname+'_mean'\n",
    "                    xxx_type = pull_trig_events_tgtname+'_std'\n",
    "                    # xxx_type = pull_num_pre_failpull_name\n",
    "                    # xxx_type = pull_time_pre_reward_name\n",
    "                    \n",
    "                    yyy_types = ['PCcurv','PClength','PCsmoothness','PCspeed']\n",
    "            \n",
    "                nytypes = np.shape(yyy_types)[0]\n",
    "                \n",
    "                # figures \n",
    "                fig1, axs1 = plt.subplots(nytypes+1,1)\n",
    "                fig1.set_figheight(8*nytypes)\n",
    "                fig1.set_figwidth(8*1)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                for iytype in np.arange(0,nytypes,1):\n",
    "                                        \n",
    "                    if xxx_type == 'gaze_duration':\n",
    "                        xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                        FRPCAfeatures_toplot['gaze_duration'] = xxx\n",
    "                    elif xxx_type == 'gaze_accumulation':\n",
    "                        xxx = FRPCAfeatures_toplot['gaze_accum']\n",
    "                        FRPCAfeatures_toplot['gaze_accumulation'] = xxx\n",
    "                    else:\n",
    "                        xxx = FRPCAfeatures_toplot[xxx_type]\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "\n",
    "                    ind_nan = np.isnan(xxx) | np.isnan(yyy)\n",
    "                    xxx = xxx[~ind_nan]\n",
    "                    yyy = yyy[~ind_nan]\n",
    "\n",
    "                    # Compute correlation\n",
    "                    try:\n",
    "                        r, p = st.pearsonr(xxx, yyy)\n",
    "                        # Fit regression line\n",
    "                        slope, intercept = np.polyfit(xxx, yyy, 1)\n",
    "                        x_vals = np.array([min(xxx), max(xxx)])\n",
    "                        y_vals = slope * x_vals + intercept\n",
    "                        axs1[iytype].plot(x_vals, y_vals, color='red', linestyle='--', label='Regression line')\n",
    "                    except:\n",
    "                        r, p = np.nan, np.nan\n",
    "\n",
    "                    # Scatter plot\n",
    "                    axs1[iytype].plot(xxx, yyy, 'o', label='gaze '+gaze_duration_type)\n",
    "\n",
    "                    # Title and labels\n",
    "                    axs1[iytype].set_title(bhvname_ana + ' of ' + act_animal_ana + ' in ' +\n",
    "                                 cond_ana + ' ' + date_ana + '\\n neuron #=' +\n",
    "                                 str(FRPCAfeatures_toplot['neuronNumBeforePCA'].iloc[0]), fontsize=12)\n",
    "                    axs1[iytype].set_ylabel(yyy_type, fontsize=12)\n",
    "\n",
    "                    # Add correlation text\n",
    "                    axs1[iytype].text(0.05, 0.9, f\"r = {r:.3f}\\np = {p:.3f}\", transform=axs1[iytype].transAxes, fontsize=12,\n",
    "                            verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "                    # Optional: show legend\n",
    "                    axs1[iytype].legend()\n",
    "\n",
    "                    #\n",
    "                    # plot the one without any gaze as a comparison\n",
    "                    xxx = FRPCAfeatures_toplot['gazestop_time'] - FRPCAfeatures_toplot['gazestart_time']\n",
    "                    \n",
    "                    yyy_type = yyy_types[iytype]\n",
    "                    yyy = FRPCAfeatures_toplot[yyy_type]\n",
    "                    \n",
    "                    ind_nan = (np.isnan(xxx)) & (~np.isnan(yyy))\n",
    "                    yyy = yyy[ind_nan]\n",
    "                    xxx = np.zeros(np.shape(yyy))\n",
    "                    \n",
    "                    axs1[iytype].plot(xxx, yyy, 'ro',label='no gaze '+gaze_duration_type)\n",
    "                    \n",
    "                    axs1[iytype].set_xlabel(xxx_type+' '+gaze_duration_type+' (s)',fontsize=12)  # Set xlabel only for the last subplot in the stack\n",
    "                    axs1[iytype].legend(loc='lower right')\n",
    "            \n",
    "                    # \n",
    "                    FRPCAfeatures_gazeduration_corr_all_sessions_df = FRPCAfeatures_gazeduration_corr_all_sessions_df.append({\n",
    "                                                                                'condition':cond_ana,\n",
    "                                                                                'act_animal':act_animal_ana,\n",
    "                                                                                'bhv_name': bhvname_ana,\n",
    "                                                                                'session':date_ana,\n",
    "                                                                                'succrate':np.array(FRPCA_allevents_toana[ind_bhvid]['succrate'])[0],\n",
    "                                                                                'corr_'+yyy_type+'_vs_'+xxx_type:r,\n",
    "                                                                                'pcorr_'+yyy_type+'_vs_'+xxx_type:p,\n",
    "                                                                                'gazeduration_definition':gaze_duration_type,\n",
    "                                                                               }, ignore_index=True)\n",
    "                                                               \n",
    "            \n",
    "                # add a mean trace of the partner's distance to lever as a comparison\n",
    "                # clean the dataframe if needed\n",
    "                FRPCAfeatures_toplot = FRPCAfeatures_toplot[\n",
    "                        FRPCAfeatures_toplot[pull_trig_events_tgtname].apply(\n",
    "                            lambda x: not all(np.isnan(i) for i in x if isinstance(i, float) or isinstance(i, np.floating))\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                if 1:\n",
    "                    \n",
    "                    xxx = np.arange(-4, 4, 1/fps)\n",
    "                    data = np.stack(np.array(FRPCAfeatures_toplot[pull_trig_events_tgtname]))\n",
    "                    yyy = np.nanmean(data, axis=0)\n",
    "                    sem = np.nanstd(data, axis=0) / np.sqrt(np.sum(~np.isnan(data), axis=0))  # SEM\n",
    "\n",
    "                    # Plot mean trace\n",
    "                    axs1[nytypes].plot(xxx, yyy, color='black', linewidth=2, label='Mean '+pull_trig_events_tgtname)\n",
    "\n",
    "                    # Shaded error area\n",
    "                    axs1[nytypes].fill_between(xxx, yyy - sem, yyy + sem, color='gray', alpha=0.3, label='± SEM')\n",
    "\n",
    "                    # Labels\n",
    "                    axs1[nytypes].set_xlabel('Time (s)')\n",
    "                    # axs1[nytypes].set_ylabel('Mean partner distance to lever')\n",
    "                    axs1[nytypes].set_ylabel('Mean '+pull_trig_events_tgtname)\n",
    "                    axs1[nytypes].legend()\n",
    "                \n",
    "                if 0:\n",
    "                    from matplotlib.collections import LineCollection\n",
    "                    import matplotlib.cm as cm\n",
    "                    import matplotlib.colors as colors\n",
    "\n",
    "                    # Time vector\n",
    "                    xxx = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "                    # Extract data matrix (trials x time)\n",
    "                    data = np.stack(FRPCAfeatures_toplot[pull_trig_events_tgtname].to_numpy())\n",
    "\n",
    "                    # Define the column name used for sorting colors\n",
    "                    sort_var_name = xxx_type\n",
    "                    sort_values = np.array(FRPCAfeatures_toplot[sort_var_name])\n",
    "\n",
    "                    # Normalize for color mapping\n",
    "                    norm = colors.Normalize(vmin=np.nanmin(sort_values), vmax=np.nanmax(sort_values))\n",
    "                    cmap = cm.get_cmap('Purples')  # dark to light purple\n",
    "                    colors_list = cmap(norm(sort_values))\n",
    "\n",
    "                    # Prepare line segments\n",
    "                    segments = [np.column_stack((xxx, data[i])) for i in range(data.shape[0])]\n",
    "                    lc = LineCollection(segments, colors=colors_list, linewidths=0.8, alpha=0.6)\n",
    "\n",
    "                    # Plot using LineCollection\n",
    "                    axs1[nytypes].add_collection(lc)\n",
    "                    axs1[nytypes].set_xlim(xxx.min(), xxx.max())\n",
    "                    axs1[nytypes].set_ylim(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "                    # Add colorbar\n",
    "                    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "                    sm.set_array([])\n",
    "                    cbar = plt.colorbar(sm, ax=axs1[nytypes])\n",
    "                    cbar.set_label(f'{sort_var_name} (s)')\n",
    "\n",
    "                    # Labels\n",
    "                    axs1[nytypes].set_xlabel('Time (s)')\n",
    "                    axs1[nytypes].set_ylabel('Mean ' + pull_trig_events_tgtname) \n",
    "                    \n",
    "                    \n",
    "                fig1.tight_layout()\n",
    "\n",
    "\n",
    "                savefig = 1\n",
    "                if savefig:\n",
    "                    figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                                    cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+date_ana+\"/\"\n",
    "\n",
    "                    if not os.path.exists(figsavefolder):\n",
    "                        os.makedirs(figsavefolder)\n",
    "\n",
    "                    if not doNeuroPCA_plot:\n",
    "                        fig1.savefig(figsavefolder+'bhvevents_aligned__continuousBhv_'+bhvname_ana+'_'+pull_trig_gazeprob_name+'_'+\n",
    "                                 pull_trig_events_tgtname+'_'+xxx_type+'_'+gaze_duration_type+savefile_sufix+'.pdf')\n",
    "                    elif doNeuroPCA_plot:\n",
    "                        fig1.savefig(figsavefolder+'bhvevents_aligned_PCspace_trajectory_features_and_continuousBhv_'+bhvname_ana+'_'+pull_trig_gazeprob_name+'_'+\n",
    "                                 xxx_type+'_'+gaze_duration_type+savefile_sufix+'.pdf')\n",
    "                     \n",
    "            \n",
    "                # Close the figures to avoid memory issues\n",
    "                plt.close(fig1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc12a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_trig_events_tgtname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(FRPCAfeatures_all_sessions_allevents_sum_df['selfpull_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPCAfeatures_gazeduration_corr_all_sessions_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the session-by-session correlation between PC trajectory features and gaze duration \n",
    "if 1:    \n",
    "    import itertools\n",
    "    \n",
    "    #####\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        elif cond == \"NV\" or cond.startswith(\"NV_with\"):\n",
    "            return \"NV\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    FRPCAfeatures_gazeduration_corr_all_sessions_df[\"condition_general\"] = \\\n",
    "        FRPCAfeatures_gazeduration_corr_all_sessions_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    if not doNeuroPCA_plot:\n",
    "        # corr_type = 'corr_otherani_otherlever_dist_meanspeed_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_animal_animal_dist_meanspeed_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_animal_animal_dist_mean_vs_gaze_accumulation'\n",
    "        corr_type = 'corr_animal_animal_dist_min_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_other_mass_move_speed_mean_vs_gaze_accumulation'\n",
    "        # corr_type = 'corr_other_mass_move_speed_mean_vs_gaze_accumulation'\n",
    "    elif doNeuroPCA_plot:\n",
    "        # corr_type = 'corr_PClength_vs_animal_animal_dist_appoachslope'\n",
    "        # corr_type = 'corr_PClength_vs_otherani_otherlever_dist_appoachslope'\n",
    "        # corr_type = 'corr_PClength_vs_otherani_otherlever_dist_meanspeed'\n",
    "        # corr_type = 'corr_PClength_vs_animal_lever_dist_meanspeed'\n",
    "        # corr_type = 'corr_PClength_vs_other_mass_move_speed_mean'\n",
    "        corr_type = 'corr_PClength_vs_other_mass_move_speed_std'\n",
    "        # corr_type = 'corr_PClength_vs_num_preceding_failpull'\n",
    "        # corr_type = 'corr_PClength_vs_time_from_last_reward'    \n",
    "        \n",
    "    fig2, axs2 = plt.subplots(1, 1)\n",
    "    fig2.set_figheight(5)\n",
    "    fig2.set_figwidth(12)\n",
    "\n",
    "    # Define a consistent color\n",
    "    box_color = 'skyblue'\n",
    "\n",
    "    # Plot boxplot and swarmplot\n",
    "    seaborn.violinplot(ax=axs2, data=FRPCAfeatures_gazeduration_corr_all_sessions_df,\n",
    "                x='condition_general', y=corr_type,\n",
    "                color=box_color)\n",
    "\n",
    "    seaborn.swarmplot(ax=axs2, data=FRPCAfeatures_gazeduration_corr_all_sessions_df,\n",
    "                   x='condition_general', y=corr_type,\n",
    "                   color='b', size=6)\n",
    "\n",
    "    # Rotate x-axis ticks\n",
    "    axs2.set_xticklabels(axs2.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Significance from 0 (Wilcoxon signed-rank)\n",
    "    conditions = FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'].unique()\n",
    "    y_max = FRPCAfeatures_gazeduration_corr_all_sessions_df[corr_type].max()\n",
    "    y_min = FRPCAfeatures_gazeduration_corr_all_sessions_df[corr_type].min()\n",
    "\n",
    "    y_offset = (y_max - y_min) * 0.15  # vertical spacing for significance bars\n",
    "\n",
    "    for i, cond in enumerate(conditions):\n",
    "        data = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond][corr_type].dropna()\n",
    "        if len(data) > 0 and np.any(data != 0):  # Wilcoxon requires non-zero variation\n",
    "            try:\n",
    "                stat, p = st.wilcoxon(data)\n",
    "                if p < 0.01:\n",
    "                    axs2.text(i, data.max() + y_offset, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "            except ValueError:\n",
    "                pass  # skip if data not suitable for Wilcoxon\n",
    "\n",
    "    # Pairwise comparisons (Mann–Whitney U)\n",
    "    pairs = list(itertools.combinations(range(len(conditions)), 2))\n",
    "    for j, (i1, i2) in enumerate(pairs):\n",
    "        cond1 = conditions[i1]\n",
    "        cond2 = conditions[i2]\n",
    "        data1 = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond1][corr_type].dropna()\n",
    "        data2 = FRPCAfeatures_gazeduration_corr_all_sessions_df[\n",
    "            FRPCAfeatures_gazeduration_corr_all_sessions_df['condition_general'] == cond2][corr_type].dropna()\n",
    "\n",
    "        if len(data1) > 0 and len(data2) > 0:\n",
    "            stat, p = st.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "            if p < 0.05:\n",
    "                y = max(data1.max(), data2.max()) + y_offset * (j + 2)\n",
    "                axs2.plot([i1, i2], [y, y], lw=1.5, c='black')\n",
    "                axs2.text((i1 + i2) / 2, y + y_offset * 0.2, '*', ha='center', va='bottom', fontsize=16)\n",
    "     \n",
    "    # fig2.tight_layout()\n",
    "\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/SelfGaze_PartnerIntention_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "         \n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_continuousBhv_corrSummary_'+bhvname_ana+'_'+\n",
    "                     pull_trig_gazeprob_name+'_'+pull_trig_events_tgtname+'_'+gaze_duration_type+'_'+\n",
    "                     corr_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "\n",
    "    # to see if there's correlation between correlation coefficient of PC features vs gaze duration, and succ rate \n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    # Drop rows where either column is NaN\n",
    "    subset_df = FRPCAfeatures_gazeduration_corr_all_sessions_df[['condition_general', 'succrate', corr_type]].dropna()\n",
    "\n",
    "    # Group by condition and calculate Pearson correlation and p-value\n",
    "    def compute_corr_pval(group):\n",
    "        if len(group) < 2:\n",
    "            return pd.Series({'r': None, 'p': None})  # Not enough data to correlate\n",
    "        r, p = pearsonr(group['succrate'], group[corr_type])\n",
    "        return pd.Series({'r': r, 'p': p})\n",
    "\n",
    "    grouped_corrs = subset_df.groupby('condition_general').apply(compute_corr_pval)\n",
    "\n",
    "    # Display the result\n",
    "    print(f'Correlation between {corr_type} and succrate:')\n",
    "    print(grouped_corrs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvname_ana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761373cb",
   "metadata": {},
   "source": [
    "### sanity check plot: separating the gaze accumulation intro quantiles, and plot the mean social gaze and partner_lever distance traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675edaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax2 = ax.twinx()  # Secondary y-axis\n",
    "    \n",
    "    # === Scatter plot of gaze_accum vs partner_face_lever_min ===\n",
    "    fig2, ax_scatter = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # session_example = '20250428_MC'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    # session_example = '20240523'\n",
    "    session_example = '20240606'\n",
    "    # session_example = '20240808'\n",
    "    # session_example = '20240809'\n",
    "    # session_example = '20240812'\n",
    "    # session_example = '20250415'\n",
    "    # session_example = '20250428_MC'\n",
    "\n",
    "    \n",
    "    # Filter data\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "\n",
    "    if doSingleSelfPulls:\n",
    "        shared_cols = ['condition', 'session', 'act_animal', 'bhv_name', 'bhv_id']\n",
    "        FRPCA_example_subset = FRPCA_example.merge(\n",
    "                                FRPCAfeatures_example[shared_cols].drop_duplicates(),\n",
    "                                on=shared_cols,\n",
    "                                how='inner'\n",
    "                            )\n",
    "        FRPCA_example = FRPCA_example_subset\n",
    "        \n",
    "    # Gaze durations\n",
    "    gaze_durations = np.array(FRPCAfeatures_example['gaze_accum'])\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "\n",
    "    time_trace = np.arange(-4, 4, 1/fps)\n",
    "\n",
    "    colors = ['b', 'r', 'y']\n",
    "    labels = ['low', 'mid', 'high']\n",
    "    inds = [ind_low, ind_mid, ind_high]\n",
    "\n",
    "    for ii in range(3):\n",
    "        # Social gaze trace (primary y-axis)\n",
    "        trials_socialgaze = np.stack(FRPCA_example[inds[ii]][pull_trig_gazeprob_name], axis=0)\n",
    "        mean_trace = np.nanmean(trials_socialgaze, axis=0)\n",
    "        sem_trace = np.nanstd(trials_socialgaze, axis=0) / np.sqrt(trials_socialgaze.shape[0])\n",
    "\n",
    "        ax.plot(time_trace, mean_trace, color=colors[ii], linewidth=1.5, alpha=0.5, label=pull_trig_gazeprob_name+f\" when {labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "        ax.fill_between(time_trace, mean_trace - sem_trace, mean_trace + sem_trace, color=colors[ii], alpha=0.15)\n",
    "\n",
    "        # Lever distance trace (secondary y-axis)\n",
    "        # trials_dist = np.stack(FRPCAfeatures_example[inds[ii]][pull_trig_events_tgtname+'_speed'], axis=0)\n",
    "        trials_dist = np.stack(FRPCA_example[inds[ii]][pull_trig_events_tgtname], axis=0)\n",
    "        mean_dist = np.nanmean(trials_dist, axis=0)\n",
    "        sem_dist = np.nanstd(trials_dist, axis=0) / np.sqrt(trials_dist.shape[0])\n",
    "\n",
    "        # Thicker, dashed line with markers to highlight\n",
    "        if 0:\n",
    "            #\n",
    "            ax2.plot(time_trace, mean_dist, color=colors[ii], linestyle='-', linewidth=3, marker='o', markersize=4, \n",
    "                     label=pull_trig_events_tgtname+f\" when {labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "            #\n",
    "            # ax2.plot(time_trace, mean_dist, color=colors[ii], linestyle='-', linewidth=3, marker='o', markersize=4, \n",
    "            #         label=pull_trig_events_tgtname+'_speed'+f\" when {labels[ii]} gaze accumulation ({np.sum(inds[ii])} trials)\")\n",
    "            #\n",
    "            ax2.fill_between(time_trace, mean_dist - sem_dist, mean_dist + sem_dist, color=colors[ii], alpha=0.25)\n",
    "\n",
    "    # Axis labels and title\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Social gaze probability', color='gray')\n",
    "    if 0:\n",
    "        ax2.set_ylabel(pull_trig_events_tgtname+'_speed', color='black')\n",
    "        # ax2.set_ylabel(pull_trig_events_tgtname, color='black')\n",
    "    ax.set_title(f\"{animal_exmaple} - {session_example}\")\n",
    "\n",
    "    # Handle legends (combined from both axes)\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left',bbox_to_anchor=(1.15, 1))\n",
    "    \n",
    "    \n",
    "    ## scatter plot\n",
    "    if not doNeuroPCA_plot:\n",
    "        xxx_plotname = 'gaze_accum'\n",
    "        x = FRPCAfeatures_example[xxx_plotname]\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_appoachslope'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_meanspeed'\n",
    "        yyy_plotname = pull_trig_events_tgtname+'_std'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_max'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_min'\n",
    "        # yyy_plotname = pull_trig_events_tgtname+'_changetime'\n",
    "        y = FRPCAfeatures_example[yyy_plotname]\n",
    "    #\n",
    "    elif doNeuroPCA_plot:\n",
    "        # xxx_plotname = pull_trig_events_tgtname+'_appoachslope'\n",
    "        # xxx_plotname = pull_trig_events_tgtname+'_meanspeed'\n",
    "        xxx_plotname = pull_trig_events_tgtname+'_std'\n",
    "        x = FRPCAfeatures_example[xxx_plotname]\n",
    "        yyy_plotname = 'PClength'\n",
    "        y = FRPCAfeatures_example[yyy_plotname]\n",
    "\n",
    "    # Drop NaNs\n",
    "    valid = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x_valid = np.array(x)[valid]\n",
    "    y_valid = np.array(y)[valid]\n",
    "\n",
    "    # Compute correlation\n",
    "    from scipy.stats import pearsonr\n",
    "    r_val, p_val = pearsonr(x_valid, y_valid)\n",
    "\n",
    "    # Plot scatter\n",
    "    ax_scatter.scatter(x_valid, y_valid, color='darkgreen', alpha=0.6, edgecolor='k')\n",
    "\n",
    "    # Regression line\n",
    "    slope, intercept = np.polyfit(x_valid, y_valid, 1)\n",
    "    x_line = np.linspace(np.min(x_valid), np.max(x_valid), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "    ax_scatter.plot(x_line, y_line, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Labels and title\n",
    "    if not doNeuroPCA_plot:\n",
    "        ax_scatter.set_xlabel('Gaze accumulation (s)')\n",
    "        ax_scatter.set_ylabel(yyy_plotname)\n",
    "    elif doNeuroPCA_plot:\n",
    "        ax_scatter.set_xlabel(xxx_plotname)\n",
    "        ax_scatter.set_ylabel(yyy_plotname)\n",
    "    ax_scatter.set_title(f\"Correlation: r = {r_val:.2f}, p = {p_val:.3f}\")\n",
    "\n",
    "    # Optional grid\n",
    "    ax_scatter.grid(True)\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+session_example+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'forExample_trace_between_'+pull_trig_gazeprob_name+'_and_'+pull_trig_events_tgtname+'.pdf')\n",
    "        #\n",
    "        fig2.savefig(figsavefolder+'forExample_scatter_between_'+xxx_plotname+'_and_'+yyy_plotname+'.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_socialgaze = np.stack(FRPCA_example[pull_trig_gazeprob_name], axis=0)\n",
    "mean_trace = np.nanmean(trials_socialgaze, axis=0)\n",
    "sem_trace = np.nanstd(trials_socialgaze, axis=0) / np.sqrt(trials_socialgaze.shape[0])\n",
    "plt.plot(time_trace, mean_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73692e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the single trial trajectories of an example session - compared the gaze_pull separating three quantile based on the gaze duration\n",
    "if 1:\n",
    "    # Create a 3D figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # animal_exmaple = 'dodson'\n",
    "    # session_example = '20250409'\n",
    "    # session_example = '20250428_MC'\n",
    "    \n",
    "    animal_exmaple = 'kanga'\n",
    "    # session_example = '20240523'\n",
    "    session_example = '20240606'\n",
    "    # session_example = '20240808'\n",
    "    # session_example = '20240809'\n",
    "    # session_example = '20240812'\n",
    "    # session_example = '20250415'\n",
    "    # session_example = '20250428_MC'\n",
    "\n",
    "\n",
    "    ind_example = FRPCAfeatures_all_sessions_allevents_sum_df['session'] == session_example\n",
    "    FRPCAfeatures_example = FRPCAfeatures_all_sessions_allevents_sum_df[ind_example]\n",
    "    #\n",
    "    ind_example = FRPCA_all_sessions_allevents_sum_df['session']==session_example\n",
    "    FRPCA_example = FRPCA_all_sessions_allevents_sum_df[ind_example]\n",
    "    \n",
    "    if doSingleSelfPulls:\n",
    "        shared_cols = ['condition', 'session', 'act_animal', 'bhv_name', 'bhv_id']\n",
    "        FRPCA_example_subset = FRPCA_example.merge(\n",
    "                                FRPCAfeatures_example[shared_cols].drop_duplicates(),\n",
    "                                on=shared_cols,\n",
    "                                how='inner'\n",
    "                            )\n",
    "        FRPCA_example = FRPCA_example_subset\n",
    "        \n",
    "    #\n",
    "    # xxx_type = 'gaze_accum'\n",
    "    # xxx_type = 'animal_animal_dist_appoachslope'\n",
    "    # xxx_type = 'otherani_otherlever_dist_appoachslope'\n",
    "    # xxx_type = 'otherani_otherlever_dist_meanspeed'\n",
    "    xxx_type = 'other_mass_move_speed_mean'\n",
    "\n",
    "    #\n",
    "    # gaze_durations = FRPCAfeatures_example['gazestop_time'] - FRPCAfeatures_example['gazestart_time']\n",
    "    gaze_durations = FRPCAfeatures_example[xxx_type]\n",
    "    gaze_durations = np.array(gaze_durations)\n",
    "    #\n",
    "    # Compute tertile (33rd and 67th percentiles), ignoring NaNs\n",
    "    q1, q2 = np.nanquantile(gaze_durations, [1/3, 2/3])\n",
    "    #\n",
    "    # Separate data into three groups\n",
    "    ind_low = gaze_durations <= q1\n",
    "    ind_mid = (gaze_durations > q1) & (gaze_durations <= q2)\n",
    "    ind_high = gaze_durations > q2\n",
    "    \n",
    "    for ii in np.arange(0,3,1):\n",
    "        if ii == 0:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_low]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'b'\n",
    "            traj_lab = 'low '+xxx_type+' ' + str(np.sum(ind_low))+' trials'\n",
    "        elif ii == 1:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_mid]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'r'\n",
    "            traj_lab = 'mid '+xxx_type+' ' + str(np.sum(ind_mid))+' trials'\n",
    "        elif ii == 2:\n",
    "            meanPCA_traj = np.nanmean(np.stack(FRPCA_example[ind_high]['PCs'], axis=0),axis=0)\n",
    "            traj_clr = 'y'\n",
    "            traj_lab = 'high '+xxx_type+' ' + str(np.sum(ind_high))+' trials'\n",
    "           \n",
    "        ntimepoints = np.shape(meanPCA_traj)[0]\n",
    "        \n",
    "        xxx = gaussian_filter1d(meanPCA_traj[:,0],6)\n",
    "        yyy = gaussian_filter1d(meanPCA_traj[:,1],6)\n",
    "        zzz = gaussian_filter1d(meanPCA_traj[:,2],6)\n",
    "        if gaze_duration_type == 'before_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[:int(ntimepoints/2),2],3)   \n",
    "        elif gaze_duration_type == 'after_pull':\n",
    "            xxx = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,0],3)\n",
    "            yyy = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,1],3)\n",
    "            zzz = gaussian_filter1d(meanPCA_traj[int(ntimepoints/2):,2],3)   \n",
    "            \n",
    "        ax.plot3D(xxx, yyy, zzz, color=traj_clr, linewidth=2, label=traj_lab)\n",
    "        ax.plot3D(xxx[0], yyy[0], zzz[0], color=traj_clr, marker ='o',markersize = 12)\n",
    "        ax.plot3D(xxx[-1], yyy[-1], zzz[-1], color=traj_clr, marker ='s',markersize = 12)\n",
    "\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+'/'+session_example+'/'\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'forExample_PCAtrajectory_in_'+xxx_type+'_quantiles.pdf')\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57b52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a7ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b973f7",
   "metadata": {},
   "source": [
    "## analysis with 'trial pooling' across sessions from the same condition\n",
    "### pool sessions for each task conditions together and then run PCA\n",
    "#### pool sessions based on quantiles of gaze-accumulation or gaze-length variables or partner-intention related variables  (e.g. 5 quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only analyze a subset of conditions\n",
    "# act_animals_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['act_animal'])\n",
    "act_animals_to_ana = ['kanga']\n",
    "# act_animals_to_ana = ['dodson']\n",
    "nanimal_to_ana = np.shape(act_animals_to_ana)[0]\n",
    "#\n",
    "# bhv_names_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'])\n",
    "bhv_names_to_ana = ['pull']\n",
    "# bhv_names_to_ana = ['failpull']\n",
    "# bhv_names_to_ana = ['succpull']\n",
    "# bhv_names_to_ana = ['succpull','failpull']\n",
    "nbhvnames_to_ana = np.shape(bhv_names_to_ana)[0]\n",
    "bhvname_clrs = ['r','y','g','b','c','m','#458B74','#FFC710','#FF1493','#A9A9A9','#8B4513']\n",
    "#\n",
    "# # the following analysis can only do one conditions \n",
    "# # multiple condition will be considered into one conditions for quantile and FR averaging analysis\n",
    "# conditions_to_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['condition'])\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['MC','SR',]\n",
    "###\n",
    "# For Kanga\n",
    "conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVermelho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "cond_toplot_type = 'allMC'\n",
    "\n",
    "nconds_to_ana = np.shape(conditions_to_ana)[0]\n",
    "\n",
    "doOnlySigniNeurons = 0 # define the significant neurons using the previous code\n",
    "\n",
    "# newly added control!!\n",
    "# only look at pull aligned events that has no preceding self pull \n",
    "doSingleSelfPulls = 0\n",
    "\n",
    "# only look at pull aligned events that do not have juice deliever effect from the previous pull\n",
    "doFarLastRewards = 0\n",
    "\n",
    "# only look at pulls that has large reaction time\n",
    "doLargeRTpulls = 1\n",
    "largeRTthreshold = 3 # the threshold for defining large threahold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a92ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_trig_events_tgtname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "\n",
    "if 1:\n",
    "    # load and prepare the data\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.DataFrame(columns=['condition', 'session', 'act_animal',\n",
    "                                                                                 'bhv_name', 'bhv_id', 'FR_ievent',\n",
    "                                                                                 'clusterID', 'channelID',\n",
    "                                                                                pull_trig_events_tgtname+'_mean_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_meanspeed_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_meanvelocity_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_max_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_min_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_changetime_ievent',\n",
    "                                                                                pull_trig_events_tgtname+'_appoachslope_ievent',\n",
    "                                                                                 ])\n",
    "    #\n",
    "    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.DataFrame()\n",
    "\n",
    "    # step 1 for the bhvevents_aligned_FR_allevents_all_dates_df, get and gaze-related variables and calculate features\n",
    "\n",
    "    # it's better to match this variable with the previous one\n",
    "    # add three kinds of gaze duration definition (around pull, before pull, after pull)\n",
    "    # gaze_duration_type = 'before_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    gaze_duration_type = 'after_pull'  # 'around_pull', 'before_pull', 'after_pull'\n",
    "    # xxx_type = pull_trig_events_tgtname+'_appoachslope'\n",
    "    # xxx_type = pull_trig_events_tgtname+'_meanspeed'\n",
    "    # xxx_type = pull_trig_events_tgtname+'_mean'\n",
    "    xxx_type = pull_trig_events_tgtname+'_std'\n",
    "    # xxx_type = pull_num_pre_failpull_name\n",
    "    # xxx_type = pull_time_pre_reward_name\n",
    "    \n",
    "    # special here, number of quantile to use for pooling across different days\n",
    "    num_quantiles = 3 # 10\n",
    "\n",
    "    #\n",
    "    for icond_ana in np.arange(0, nconds_to_ana, 1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_allevents_all_dates_df['condition'] == cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0, nanimal_to_ana, 1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_allevents_all_dates_df['act_animal'] == act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond]['dates'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0, ndates_ana, 1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_allevents_all_dates_df['dates'] == date_ana\n",
    "\n",
    "                # get the neurons\n",
    "                neurons_ana = np.unique(\n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0, nneurons, 1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_allevents_all_dates_df['clusterID'] == clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0, nbhvnames_to_ana, 1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_allevents_all_dates_df['bhv_name'] == bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date\n",
    "\n",
    "                        bhvevents_aligned_FR_allevents_tgt = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "                            ind_ana].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "                        if not bhvevents_aligned_FR_allevents_tgt.empty:\n",
    "                            channelID = bhvevents_aligned_FR_allevents_tgt['channelID'].iloc[0]\n",
    "\n",
    "                            #\n",
    "                            # load and plot bhv event ('pull') aligned FR\n",
    "                            FRs_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt['FR_allevents'].iloc[0])\n",
    "                            nevents_FR = np.shape(FRs_allevents_ineuron)[1]\n",
    "\n",
    "                            #\n",
    "                            # load and plot the pull aligned continuous bhv variables\n",
    "                            conBhv_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_events_tgtname].iloc[0])\n",
    "                            conBhv_allevents_ineuron = np.array(conBhv_allevents_ineuron)\n",
    "                            conBhv_allevents_ineuron = conBhv_allevents_ineuron.transpose()\n",
    "                            #\n",
    "                            nevents_bhv = np.shape(conBhv_allevents_ineuron)[1]\n",
    "                            \n",
    "                            #\n",
    "                            # load the self pull variables\n",
    "                            selfpull_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_trig_selfpull_name].iloc[0])\n",
    "                            selfpull_allevents_ineuron = np.array(selfpull_allevents_ineuron)\n",
    "                            selfpull_allevents_ineuron = selfpull_allevents_ineuron.transpose()\n",
    "                            \n",
    "                            # \n",
    "                            # load the pull reaction time information\n",
    "                            pull_rt_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_rt_name].iloc[0])\n",
    "                            \n",
    "                            # load the prefailpull number and time since the last reward\n",
    "                            prefailpullnum_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_num_pre_failpull_name].iloc[0])\n",
    "                            prefailpullnum_allevents_ineuron = np.array(prefailpullnum_allevents_ineuron)\n",
    "                            #\n",
    "                            lastrewardtime_allevents_ineuron = np.array(\n",
    "                                bhvevents_aligned_FR_allevents_tgt[pull_time_pre_reward_name].iloc[0])\n",
    "                            lastrewardtime_allevents_ineuron = np.array(lastrewardtime_allevents_ineuron)\n",
    "                            \n",
    "                            # if the pull aligned FR and bhv have different number\n",
    "                            if not nevents_FR == nevents_bhv:\n",
    "                                # print(date_ana+' mismatched number')\n",
    "                                if nevents_FR < nevents_bhv:\n",
    "                                    conBhv_allevents_ineuron = conBhv_allevents_ineuron[:, 0:nevents_FR]\n",
    "                                else:\n",
    "                                    FRs_allevents_ineuron = FRs_allevents_ineuron[:, 0:nevents_bhv]\n",
    "\n",
    "                            #\n",
    "                            nevents = np.min([nevents_FR, nevents_bhv])\n",
    "\n",
    "                            # get each bhv events\n",
    "                            for bhv_id in np.arange(0, nevents, 1):\n",
    "                                FRs_ievent_ineuron = FRs_allevents_ineuron[:, bhv_id]\n",
    "                                conBhv_ievent_ineuron = conBhv_allevents_ineuron[:, bhv_id]\n",
    "                                selfpull_ievent_ineuron = selfpull_allevents_ineuron[:, bhv_id]\n",
    "                                \n",
    "                                pull_rt_ievent_ineuron = pull_rt_allevents_ineuron[bhv_id]\n",
    "\n",
    "                                # load the prefailpull number and time since the last reward\n",
    "                                prefailpullnum_ievent_ineuron = prefailpullnum_allevents_ineuron[bhv_id]\n",
    "                                lastrewardtime_ievent_ineuron = lastrewardtime_allevents_ineuron[bhv_id]\n",
    "                                \n",
    "                                #\n",
    "                                # count self pull after self pull onset, \n",
    "                                # the idea is to make sure the effect is not becuase of the pull action\n",
    "                                from scipy.signal import find_peaks\n",
    "                                #\n",
    "                                x_full = np.arange(-4,4,1/fps)\n",
    "                                post_mask = x_full>0.15\n",
    "                                try:\n",
    "                                    # selfpull_num = np.trapz(pull_trig_selfpull_ievent[pre_mask], dx=dt)\n",
    "                                    data = selfpull_ievent_ineuron[post_mask]\n",
    "                                    peaks, _ = find_peaks(data)\n",
    "                                    selfpull_num = len(peaks)\n",
    "                                except:\n",
    "                                    selfpull_num = np.nan\n",
    "                                    \n",
    "                                    \n",
    "                                #\n",
    "                                # analyze the pull triggered behavioral events\n",
    "                                #\n",
    "                                x_full = np.arange(-4, 4, 1/fps)\n",
    "                                #\n",
    "                                if gaze_duration_type == 'before_pull':\n",
    "                                    # Only use the pre-pull window (-4s to 0s)\n",
    "                                    pre_mask = x_full <= 0\n",
    "                                    # pre_mask = (x_full <= 0) & (x_full >= -2) # manipulate the size of the time window\n",
    "                                elif gaze_duration_type == 'after_pull':\n",
    "                                    # Only use the post-pull window (0s to 4s)\n",
    "                                    # add a new cretria (0s to 2s), to account for the minimal reaction time\n",
    "                                    pre_mask = x_full >= 0\n",
    "                                    # pre_mask = (x_full <= 2)|(x_full >= 0) \n",
    "                                elif gaze_duration_type == 'around_pull':\n",
    "                                    # the entire -4 to 4s\n",
    "                                    pre_mask = (x_full <= 0)|(x_full >= 0) \n",
    "\n",
    "                                #\n",
    "                                partner_face_lever_mean = np.nanmean(conBhv_ievent_ineuron[pre_mask])\n",
    "                                partner_face_lever_std = np.nanstd(conBhv_ievent_ineuron[pre_mask])\n",
    "                                \n",
    "                                if (pull_trig_events_tgtname == 'otherani_otherlever_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'animal_lever_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'otherani_othertube_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'animal_tube_dist') | \\\n",
    "                                   (pull_trig_events_tgtname == 'animal_animal_dist'):\n",
    "\n",
    "                                    partner_face_lever_velocity = np.gradient(conBhv_ievent_ineuron, 1/fps)\n",
    "                                    #\n",
    "                                    partner_face_lever_speed = np.abs(partner_face_lever_velocity)\n",
    "                                    #\n",
    "                                    partner_face_lever_meanvelocity = np.nanmean(partner_face_lever_velocity[pre_mask])\n",
    "                                    #\n",
    "                                    partner_face_lever_meanspeed = np.nanmean(partner_face_lever_speed[pre_mask])\n",
    "\n",
    "                                    min_dist = np.arange(-4,4,1/fps)[np.argmin(conBhv_ievent_ineuron)]\n",
    "                                    max_dist = np.arange(-4,4,1/fps)[np.argmax(conBhv_ievent_ineuron)]\n",
    "                                    #\n",
    "                                    pre_min_mask = x_full <= min_dist\n",
    "\n",
    "                                    # find the time that the dramatic change starts, if could not find it, use the -4s\n",
    "                                    percentile = 95\n",
    "                                    dt = 1 / fps\n",
    "                                    abs_derivative = np.abs(np.gradient(conBhv_ievent_ineuron[x_full <= 0], dt))\n",
    "                                    threshold = np.percentile(abs_derivative, percentile)\n",
    "                                    # Start from the first time point (index 0)\n",
    "                                    idx_change = np.where(abs_derivative > threshold)[0]\n",
    "                                    if len(idx_change) > 0:\n",
    "                                        onset_idx = idx_change[0]\n",
    "                                        change_time = x_full[x_full <= 0][onset_idx]\n",
    "                                    else:\n",
    "                                        change_time = x_full[0]\n",
    "                                    #\n",
    "                                    partner_face_lever_changetime = change_time\n",
    "                                    #\n",
    "                                    post_changetime_mask = x_full >= change_time\n",
    "                                    #\n",
    "                                    # min and max after the change_time\n",
    "                                    min_dist_post_changetime = x_full[x_full>=change_time][np.argmin(conBhv_ievent_ineuron[x_full>=change_time])]\n",
    "                                    max_dist_post_changetime = x_full[x_full>=change_time][np.argmax(conBhv_ievent_ineuron[x_full>=change_time])]\n",
    "\n",
    "                                    #\n",
    "                                    # find the partner lever approaching trend\n",
    "                                    from scipy.stats import linregress\n",
    "                                    # Define time range and extract window\n",
    "                                    y_full = np.array(conBhv_ievent_ineuron)                      \n",
    "                                    #\n",
    "                                    # x_pre = x_full[pre_mask]\n",
    "                                    # y_pre = y_full[pre_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask]\n",
    "                                    # x_pre = x_full[pre_min_mask & post_changetime_mask]\n",
    "                                    # y_pre = y_full[pre_min_mask & post_changetime_mask]    \n",
    "                                    x_pre = x_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]\n",
    "                                    y_pre = y_full[post_changetime_mask & (x_full<=min_dist_post_changetime)]                        \n",
    "                                    # Linear regression\n",
    "                                    # slope, intercept, r_value, p_value, std_err = linregress(x_pre, y_pre)\n",
    "                                    slope = (y_pre[-1] - y_pre[0])/(x_pre[-1] - x_pre[0])\n",
    "                                    #\n",
    "                                    approaching_slope = slope\n",
    "\n",
    "                                else:\n",
    "                                    partner_face_lever_velocity = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "                                    #\n",
    "                                    partner_face_lever_speed = np.ones(np.shape(np.arange(-4,4,1/fps)))*np.nan\n",
    "\n",
    "                                    partner_face_lever_meanspeed = np.nan\n",
    "\n",
    "                                    min_dist = np.nan\n",
    "                                    max_dist = np.nan\n",
    "\n",
    "                                    approaching_slope = np.nan\n",
    "\n",
    "                                    partner_face_lever_changetime = np.nan\n",
    "                                \n",
    "                                #\n",
    "                                bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(\n",
    "                                    [bhvevents_aligned_FR_and_eventFeatures_all_dates_df, pd.DataFrame({\n",
    "                                        'condition': [cond_ana],\n",
    "                                        'act_animal': [act_animal_ana],\n",
    "                                        'bhv_name': [bhvname_ana],\n",
    "                                        'session': [date_ana],\n",
    "                                        'bhv_id': [bhv_id],\n",
    "                                        'clusterID': [clusterID_ineuron],\n",
    "                                        'channelID': [channelID],\n",
    "                                        'FR_ievent': [FRs_ievent_ineuron],\n",
    "                                        'selfpull_num_ievent': [selfpull_num],\n",
    "                                        'pull_rt_ievent':[pull_rt_ievent_ineuron],\n",
    "                                        pull_trig_events_tgtname:[conBhv_ievent_ineuron],\n",
    "                                        pull_trig_events_tgtname+'_mean_ievent':[partner_face_lever_mean],\n",
    "                                        pull_trig_events_tgtname+'_std_ievent':[partner_face_lever_std],\n",
    "                                        pull_trig_events_tgtname+'_speed_ievent':[partner_face_lever_speed],\n",
    "                                        pull_trig_events_tgtname+'_meanspeed_ievent':[partner_face_lever_meanspeed],\n",
    "                                        pull_trig_events_tgtname+'_velocity_ievent':[partner_face_lever_velocity],\n",
    "                                        pull_trig_events_tgtname+'_meanvelocity_ievent':[partner_face_lever_meanvelocity],\n",
    "                                        pull_trig_events_tgtname+'_max_ievent':[max_dist],\n",
    "                                        pull_trig_events_tgtname+'_min_ievent':[min_dist],\n",
    "                                        pull_trig_events_tgtname+'_changetime_ievent':[partner_face_lever_changetime],\n",
    "                                        pull_trig_events_tgtname+'_appoachslope_ievent':[approaching_slope],\n",
    "                                        pull_num_pre_failpull_name+'_ievent':prefailpullnum_ievent_ineuron,\n",
    "                                        pull_time_pre_reward_name+'_ievent':lastrewardtime_ievent_ineuron,\n",
    "                                    })], ignore_index=True)\n",
    "\n",
    "                                \n",
    "    # only look at the pulls that has no pull preceeding\n",
    "    if doSingleSelfPulls:\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['selfpull_num_ievent']==0\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "                                \n",
    "    #\n",
    "    # remove events that still has the effect from previous juice\n",
    "    if doFarLastRewards:\n",
    "        # reward should be beyond the -4s time window we are looking at, and also consider the reward effect last ~1s\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['lastreward_time_ievent']<=(-4-1)\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "                    \n",
    "    #\n",
    "    # only consider pulls that has large reaction time\n",
    "    if doLargeRTpulls:\n",
    "        # reward should be beyond the -4s time window we are looking at, and also consider the reward effect last ~1s\n",
    "        ind_good = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['pull_rt_ievent']>=largeRTthreshold\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_good]\n",
    "                   \n",
    "    #\n",
    "    # remove trial without gaze_accum\n",
    "    # ind_nan = np.isnan(bhvevents_aligned_FR_and_eventFeatures_all_dates_df['gaze_accum_ievent'])\n",
    "    # bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[~ind_nan]\n",
    "         \n",
    "    \n",
    "    # add the quantile information using the all sessions' data\n",
    "    # consider each date separately\n",
    "\n",
    "    # Create a list to store the DataFrames for each date\n",
    "    all_dates_dfs = []\n",
    "\n",
    "    # Get unique dates\n",
    "    unique_dates = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'].unique()\n",
    "\n",
    "    for date_ana in unique_dates:\n",
    "        date_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == date_ana].copy()\n",
    "\n",
    "        # separating quantiles\n",
    "        quantile_tgt = date_df[xxx_type+'_ievent'].dropna()  # Handle potential NaNs\n",
    "        n_unique = len(quantile_tgt.unique())\n",
    "        n_bins = min(num_quantiles, n_unique - 1)  # Calculate the maximum possible bins\n",
    "        if n_bins > 1:  # Only proceed if we can make at least 2 bins\n",
    "            try:\n",
    "                quantile_bins = np.nanquantile(quantile_tgt, np.linspace(0, 1, num_quantiles + 1))\n",
    "                date_df[xxx_type+'_quantile'] = pd.cut(\n",
    "                    date_df[xxx_type+'_ievent'],\n",
    "                    bins=quantile_bins,\n",
    "                    labels=False,\n",
    "                    include_lowest=True,\n",
    "                    duplicates='drop'  # Drop duplicate bin edges\n",
    "                )\n",
    "                quantile_col = xxx_type+'_quantile'\n",
    "                title_prefix = xxx_type\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Error calculating quantiles on date {date_ana}: {e}\")\n",
    "                date_df[xxx_type+'_quantile'] = np.nan\n",
    "                quantile_col = xxx_type+'_quantile'\n",
    "                title_prefix = xxx_type\n",
    "        else:\n",
    "            date_df[xxx_type+'_quantile'] = np.nan\n",
    "            quantile_col = xxx_type+'_quantile'\n",
    "            title_prefix = xxx_type\n",
    "            print(f\"Warning: Not enough distinct data for quantiles on date {date_ana}\")\n",
    "\n",
    "\n",
    "        all_dates_dfs.append(date_df)\n",
    "\n",
    "    # Concatenate the DataFrames back together\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df = pd.concat(all_dates_dfs, ignore_index=True)\n",
    "\n",
    "    # average for each neuron the firing rate of each quantile\n",
    "    if 'quantile_col' in locals():\n",
    "        # Iterate through unique (clusterID, session) pairs\n",
    "        for (cluster_id, session_id) in bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "            ['clusterID', 'session']].drop_duplicates().itertuples(index=False):\n",
    "            neuron_session_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == session_id)\n",
    "            ].copy()\n",
    "\n",
    "            for q_val in neuron_session_df[quantile_col].dropna().unique():\n",
    "                quantile_df = neuron_session_df[neuron_session_df[quantile_col] == q_val]\n",
    "                if not quantile_df.empty:\n",
    "                    all_fr_traces = np.vstack(quantile_df['FR_ievent'].tolist())\n",
    "                    mean_fr_trace = np.nanmean(all_fr_traces, axis=0)\n",
    "\n",
    "                    # Get representative metadata\n",
    "                    condition = quantile_df['condition'].iloc[0]\n",
    "                    act_animal = quantile_df['act_animal'].iloc[0]\n",
    "                    bhv_name = quantile_df['bhv_name'].iloc[0]\n",
    "                    channelID = quantile_df['channelID'].iloc[0]\n",
    "\n",
    "                    new_row = {\n",
    "                        'condition': condition,\n",
    "                        'session': session_id,\n",
    "                        'act_animal': act_animal,\n",
    "                        'bhv_name': bhv_name,\n",
    "                        'clusterID': cluster_id,\n",
    "                        'channelID': channelID,\n",
    "                        quantile_col: int(q_val),\n",
    "                        'mean_FR_trace': mean_fr_trace\n",
    "                    }\n",
    "                    bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = pd.concat(\n",
    "                        [bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df,\n",
    "                         pd.DataFrame([new_row])],\n",
    "                        ignore_index=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e562fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_and_eventFeatures_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473ff17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's for defining slope ramping feature for each neurons, and regress over the gaze accumulation or other variables\n",
    "if 1:    \n",
    "    #\n",
    "    bhveventsFeatures_and_FRslopes_all_dates_df = pd.DataFrame(columns=['condition', 'session', 'act_animal',\n",
    "                                                                         'bhv_name', 'clusterID', 'channelID',                                                                   \n",
    "                                                                                 ])\n",
    "    \n",
    "    # xvar_name = 'gaze_accum' # or 'gaze_accum', 'gaze_start', 'gaze_stop', 'selfpull_num','pull_rt'\n",
    "    # xvar_name = 'pull_rt' # or 'gaze_accum', 'gaze_start', 'gaze_stop', 'selfpull_num','pull_rt'\n",
    "    # xvar_name = 'other_mass_move_speed_mean'\n",
    "    xvar_name = 'other_mass_move_speed_std'\n",
    "    \n",
    "    for icond_ana in np.arange(0, nconds_to_ana, 1):\n",
    "        cond_ana = conditions_to_ana[icond_ana]\n",
    "        ind_cond = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['condition'] == cond_ana\n",
    "\n",
    "        for ianimal_ana in np.arange(0, nanimal_to_ana, 1):\n",
    "            act_animal_ana = act_animals_to_ana[ianimal_ana]\n",
    "            ind_animal = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['act_animal'] == act_animal_ana\n",
    "\n",
    "            # get the dates\n",
    "            dates_ana = np.unique(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_animal & ind_cond]['session'])\n",
    "            ndates_ana = np.shape(dates_ana)[0]\n",
    "\n",
    "            for idate_ana in np.arange(0, ndates_ana, 1):\n",
    "                date_ana = dates_ana[idate_ana]\n",
    "                ind_date = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session'] == date_ana\n",
    "\n",
    "                # get the neurons\n",
    "                neurons_ana = np.unique(\n",
    "                    bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_animal & ind_cond & ind_date]['clusterID'])\n",
    "                nneurons = np.shape(neurons_ana)[0]\n",
    "\n",
    "                for ineuron in np.arange(0, nneurons, 1):\n",
    "                    clusterID_ineuron = neurons_ana[ineuron]\n",
    "                    ind_neuron = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID'] == clusterID_ineuron\n",
    "\n",
    "                    for ibhvname_ana in np.arange(0, nbhvnames_to_ana, 1):\n",
    "                        bhvname_ana = bhv_names_to_ana[ibhvname_ana]\n",
    "                        ind_bhv = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['bhv_name'] == bhvname_ana\n",
    "\n",
    "                        ind_ana = ind_animal & ind_bhv & ind_cond & ind_neuron & ind_date\n",
    "\n",
    "                        bhvevents_aligned_FR_and_eventFeatures_allevents_tgt = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\n",
    "                            ind_ana].copy() \n",
    "\n",
    "                        channelID_ineuron = bhvevents_aligned_FR_and_eventFeatures_allevents_tgt['channelID'].iloc[0]\n",
    "                        \n",
    "                        min_pull_rt = np.nanmin(bhvevents_aligned_FR_and_eventFeatures_allevents_tgt['pull_rt_ievent'])\n",
    "                        \n",
    "                        #\n",
    "                        # calculate the ramping slope and other FR related features\n",
    "                        # get the firing rate\n",
    "                        xxx = np.array(list(bhvevents_aligned_FR_and_eventFeatures_allevents_tgt['FR_ievent']))\n",
    "\n",
    "                        # Time vector: from -4s to 4s (e.g., 240 points for 30Hz)\n",
    "                        time_vector = np.arange(-4,4,1/fps)\n",
    "\n",
    "                        # Step 1: Compute average firing rate and find time of max abs deviation after 0s\n",
    "                        mean_fr = np.nanmean(xxx, axis=0)\n",
    "                        post_mean_fr = mean_fr[time_vector>-0.15]\n",
    "\n",
    "                        # Find index of max absolute deviation from baseline (i.e., strongest ramp up or down)\n",
    "                        baseline = np.nanmean(mean_fr)  # optional: use entire -4 to 0s as baseline\n",
    "                        abs_dev = np.abs(mean_fr - baseline)\n",
    "                        peak_idx = np.argmax(abs_dev)\n",
    "                        #\n",
    "                        abs_dev = np.abs(post_mean_fr - baseline)\n",
    "                        postpull_peak_idx = np.argmax(abs_dev)+int(0.0*30)\n",
    "\n",
    "                        # Get corresponding index in the full time vector\n",
    "                        peak_time = time_vector[peak_idx]\n",
    "                        #\n",
    "                        postpull_peak_time = time_vector[time_vector>-0.15][postpull_peak_idx]\n",
    "\n",
    "                        \n",
    "                        # Step 2: Compute slope from 0s to peak_time or 4s for each trial\n",
    "                        #         in addition, get the firing rate at the peak time\n",
    "                        slopes = []\n",
    "                        # flexible end time or fixed end time?\n",
    "                        dofixedendtime = 1 # the new one is to the pull time\n",
    "                        #\n",
    "                        slope_start_ts = np.where(time_vector>=-0.15)[0][0] # when the time point is -0.15s\n",
    "                        #\n",
    "                        for trial_fr in xxx:\n",
    "                            # \n",
    "                            if not dofixedendtime:\n",
    "                                if peak_time > -0.15:\n",
    "                                    x = time_vector[slope_start_ts:peak_idx + 1]  # from 0s to peak_time\n",
    "                                    y = trial_fr[slope_start_ts:peak_idx + 1]\n",
    "                                else:\n",
    "                                    # zero_idx = np.where(time_vector>=0)[0][0]\n",
    "                                    # x = time_vector[:zero_idx]  # from -4s to 0\n",
    "                                    # y = trial_fr[:zero_idx]\n",
    "                                    x = time_vector[slope_start_ts:slope_start_ts+postpull_peak_idx + 1]  # from 0s to peak_time\n",
    "                                    y = trial_fr[slope_start_ts:slope_start_ts+postpull_peak_idx + 1]\n",
    "                            #\n",
    "                            elif dofixedendtime:\n",
    "                                slope_end_ts = np.where(time_vector>=min_pull_rt)[0][0]\n",
    "                                \n",
    "                                x = time_vector[slope_start_ts:slope_end_ts]  # from 0s to the peak time 4s\n",
    "                                y = trial_fr[slope_start_ts:slope_end_ts]\n",
    "                            #\n",
    "                            slope, intercept, r_value, p_value, std_err = st.linregress(x, y)\n",
    "                            slopes.append(slope)\n",
    "                        #\n",
    "                        slopes = np.array(slopes)  \n",
    "                        #\n",
    "                        # if the neuron fr is ramping down overall, flip the slopes\n",
    "                        if np.nanmean(slopes)<0:\n",
    "                            slopes = -slopes\n",
    "                            \n",
    "                        #\n",
    "                        if not dofixedendtime:\n",
    "                            if peak_time > -0.15:\n",
    "                                peakFRs = xxx[:,peak_idx]\n",
    "                            else:\n",
    "                                # zero_idx = np.where(time_vector>=0)[0][0]\n",
    "                                # peakFRs = xxx[:,zero_idx]\n",
    "                                peakFRs = xxx[:,slope_start_ts+postpull_peak_idx]\n",
    "                        # \n",
    "                        elif dofixedendtime:\n",
    "                            # peakFRs = xxx[:,-1]\n",
    "                            peakFRs = xxx[:,slope_end_ts]\n",
    "    \n",
    "                        # \n",
    "                        # get the gaze_accumulation or other variables \n",
    "                        xvar = np.array(list(bhvevents_aligned_FR_and_eventFeatures_allevents_tgt[xvar_name+'_ievent']))\n",
    "                        \n",
    "                        # get the nan index and remove then in the correlation\n",
    "                        ind_nan_slopes = np.isnan(slopes)\n",
    "                        ind_nan_peakFRs  = np.isnan(peakFRs)\n",
    "                        ind_nan_xvar = np.isnan(xvar)\n",
    "                        #\n",
    "                        ind_nan = ind_nan_slopes | ind_nan_peakFRs | ind_nan_xvar\n",
    "                        \n",
    "                        # \n",
    "                        # regression between FR slopes and xvar (e.g. gaze accumulation)\n",
    "                        slope2, intercept2, r_value2, p_value2, std_err2 = st.linregress(xvar[~ind_nan], slopes[~ind_nan])\n",
    "                        #\n",
    "                        if (r_value2 > 0.99) | (r_value2 < -0.99):\n",
    "                            r_value2 = np.nan\n",
    "                            p_value2 = np.nan\n",
    "                            \n",
    "                        #\n",
    "                        # correlation between the peak FR and xvar (e.g. gaze accumulation)\n",
    "                        slope3, intercept3, r_value3, p_value3, std_err3 = st.linregress(xvar[~ind_nan], peakFRs[~ind_nan])\n",
    "                        #\n",
    "                        if (r_value3 > 0.99) | (r_value3 < -0.99):\n",
    "                            r_value3 = np.nan\n",
    "                            p_value3 = np.nan\n",
    "                        \n",
    "                        # put the data together\n",
    "                        bhveventsFeatures_and_FRslopes_all_dates_df = pd.concat(\n",
    "                                    [bhveventsFeatures_and_FRslopes_all_dates_df, pd.DataFrame({\n",
    "                                        'condition': [cond_ana],\n",
    "                                        'act_animal': [act_animal_ana],\n",
    "                                        'bhv_name': [bhvname_ana],\n",
    "                                        'session': [date_ana],\n",
    "                                        'clusterID': [clusterID_ineuron],\n",
    "                                        'channelID': [channelID_ineuron],\n",
    "                                        'FRpeaktime': [peak_time],\n",
    "                                        'FRpeaktime_postpull': [postpull_peak_time],\n",
    "                                        'xvar_name': [xvar_name],\n",
    "                                        'corr_postpull_FRslope_and_xvar':[r_value2],\n",
    "                                        'pcorr_postpull_FRslope_and_xvar':[p_value2],\n",
    "                                        'corr_postpull_peakFR_and_xvar':[r_value3],\n",
    "                                        'pcorr_postpull_peakFR_and_xvar':[p_value3],\n",
    "                                    })], ignore_index=True)\n",
    "                        \n",
    "\n",
    "    # do some plotting\n",
    "    yvar_toplot_name = 'FRslope' # FRslope, peakFR\n",
    "    \n",
    "    #\n",
    "    # Assuming df is already your DataFrame:\n",
    "    df = bhveventsFeatures_and_FRslopes_all_dates_df.copy()\n",
    "\n",
    "    # Define significance\n",
    "    df['significance'] = df['pcorr_postpull_'+yvar_toplot_name+'_and_xvar'] < 0.05\n",
    "\n",
    "    # Drop NaNs in correlation column before analysis\n",
    "    df_clean = df.dropna(subset=['corr_postpull_'+yvar_toplot_name+'_and_xvar'])\n",
    "\n",
    "    # Define significance\n",
    "    df_clean['significance'] = df_clean['pcorr_postpull_'+yvar_toplot_name+'_and_xvar'] < 0.05\n",
    "\n",
    "    # Count neurons\n",
    "    n_sig = df_clean['significance'].sum()\n",
    "    n_nonsig = len(df_clean) - n_sig\n",
    "\n",
    "    # T-test against 0 (only on non-NaN correlations)\n",
    "    t_stat, t_pval = st.ttest_1samp(df_clean['corr_postpull_'+yvar_toplot_name+'_and_xvar'], 0)\n",
    "    meanvalue = np.nanmean(df_clean['corr_postpull_'+yvar_toplot_name+'_and_xvar'])\n",
    "\n",
    "    # Plot\n",
    "    figgg = plt.figure(figsize=(8, 5))\n",
    "    seaborn.swarmplot(\n",
    "        x='xvar_name',\n",
    "        y='corr_postpull_'+yvar_toplot_name+'_and_xvar',\n",
    "        data=df_clean,\n",
    "        hue='significance',\n",
    "        palette={True: 'red', False: 'gray'},\n",
    "        size=6\n",
    "    )\n",
    "\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.ylabel(\"Correlation with Post-pull start \"+yvar_toplot_name)\n",
    "    plt.xlabel(\"Behavioral Variable\")\n",
    "    plt.title(\"Swarm Plot of Correlations per Variable in \"+cond_toplot_type)\n",
    "\n",
    "    # Custom legend with counts\n",
    "    plt.legend(\n",
    "        title=f'Significant (p < 0.05)\\nRed: {n_sig}, Gray: {n_nonsig}',\n",
    "        # labels=['False', 'True'],  # must match unique values in significance\n",
    "        loc='best'\n",
    "    )\n",
    "\n",
    "    # Show t-test result\n",
    "    plt.text(-0.3, np.nanmax(df['corr_postpull_'+yvar_toplot_name+'_and_xvar']) * 0.9,\n",
    "             f'T-test vs 0: p = {t_pval:.3g};\\n mean = {meanvalue:.2g}', ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_gaze_quantile_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        figgg.savefig(figsavefolder+'bhvevents_aligned_'+yvar_toplot_name+'_and_'+xvar_name+'_correlation_'+bhvname_ana+'_'+\n",
    "                 pull_trig_events_tgtname+'_'+gaze_duration_type+'onset_'+savefile_sufix+'.pdf')\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9af45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01691733",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# do some big picture plot; here is to have a sense of the gaze-accumulation distribution of succ and failed pull\n",
    "# ...if the setting is set for that\n",
    "## \n",
    "if 0:\n",
    "    from scipy.stats import ks_2samp\n",
    "\n",
    "    # to make the condition more general\n",
    "    # Define the function for generalizing condition\n",
    "    def generalize_condition(cond):\n",
    "        if cond == \"MC\" or cond.startswith(\"MC_with\"):\n",
    "            return \"MC\"\n",
    "        elif cond == \"SR\" or cond.startswith(\"SR_with\"):\n",
    "            return \"SR\"\n",
    "        else:\n",
    "            return cond  # default to original condition if no match\n",
    "\n",
    "    # Apply the function to create the new column\n",
    "    bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition_general\"] = \\\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df[\"condition\"].apply(generalize_condition)\n",
    "    \n",
    "    \n",
    "    # \n",
    "    plt.figure(figsize=(20,6))\n",
    "    data_toplot = bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "    # seaborn.kdeplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "    seaborn.kdeplot(data=data_toplot,x=xxx_type+'_ievent',hue='condition_general',\n",
    "                   common_norm=False)  # don't normalize across groups\n",
    "    # seaborn.histplot(data=data_toplot,x='gaze_accum_ievent',hue='bhv_name')\n",
    "\n",
    "    # Compute group-wise quantiles (deciles from 10% to 90%)\n",
    "    # quantiles_df = data_toplot.groupby('bhv_name')['gaze_accum_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df = data_toplot.groupby('condition_general')[xxx_type+'_ievent'].quantile(np.linspace(0.1, 0.9, 9)).reset_index()\n",
    "    quantiles_df.rename(columns={xxx_type+'_ievent': 'quantile_value'}, inplace=True)\n",
    "\n",
    "    # Draw vertical lines for each quantile\n",
    "    if 0:\n",
    "        # palette = dict(zip(data_toplot['bhv_name'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "        palette = dict(zip(data_toplot['condition_general'].unique(), seaborn.color_palette()))  # color matching seaborn\n",
    "\n",
    "        for _, row in quantiles_df.iterrows():\n",
    "            plt.axvline(\n",
    "                row['quantile_value'],\n",
    "                # color=palette[row['bhv_name']],\n",
    "                color=palette[row['condition_general']],\n",
    "                linestyle='--',\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    plt.title('KDE Plot with Quantile Lines')\n",
    "    plt.show()\n",
    "    \n",
    "    # KS test: compare MC vs SR\n",
    "    if 0:\n",
    "        group_MC = data_toplot[data_toplot['condition_general'] == 'MC']['gaze_accum_ievent'].dropna()\n",
    "        group_SR = data_toplot[data_toplot['condition_general'] == 'SR']['gaze_accum_ievent'].dropna()\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(group_MC, group_SR)\n",
    "        print(f\"KS test: statistic={ks_stat:.4f}, p-value={ks_pval:.4g}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider the significant neurons based on the previous analysis\n",
    "if 1:\n",
    "    if doOnlySigniNeurons:\n",
    "        \n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df = significant_bhv_df\n",
    "\n",
    "        #\n",
    "        # Rename 'session' column in the first DataFrame to 'dates' for merging\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = bhvevents_aligned_FR_and_eventFeatures_all_dates_df.rename(columns={'session': 'dates'})\n",
    "        # Merge the DataFrames\n",
    "        merged_df = pd.merge(bhvevents_aligned_FR_and_eventFeatures_all_dates_df, significant_neurons_data_df,\n",
    "                            on=['dates', 'act_animal', 'bhv_name', 'clusterID','condition'],\n",
    "                            how='inner')\n",
    "        # Filter for significant neurons\n",
    "        significant_bhv_df = merged_df[merged_df['significance_or_not'] == True]\n",
    "        significant_bhv_df = significant_bhv_df.rename(columns={'dates':'session'})\n",
    "        bhvevents_aligned_FR_and_eventFeatures_all_dates_df = significant_bhv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98703bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_and_eventFeatures_all_dates_df\n",
    "# bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4184afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - 2:\n",
    "# do a basic plot for sanity check - mean FR across all units in the pool condition for each quantile\n",
    "if 1:\n",
    "    from scipy.integrate import cumtrapz\n",
    "\n",
    "    doQuantMeanFRs = 1\n",
    "    # only do one example session, all neurons in that session, average\n",
    "    doExampleSession = 0\n",
    "    # only do one example neuron in one example cell \n",
    "    doExampleNeuron = 0\n",
    "    \n",
    "    timewins = np.arange(-4, 4, 1/30)\n",
    "    n_timepoints = len(timewins)\n",
    "        \n",
    "    #\n",
    "    if doExampleSession:\n",
    "        # examplesess = '20240606'\n",
    "        examplesess =  '20240808'\n",
    "        #\n",
    "        if doQuantMeanFRs:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col].unique())\n",
    "        else:\n",
    "            ind = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col].unique())      \n",
    "    # \n",
    "    elif doExampleNeuron:\n",
    "        examplesess =  '20240808'\n",
    "        exampleneuron = '129'\n",
    "        # \n",
    "        # has to do not QuantMeanFRs\n",
    "        ind_1 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['session']==examplesess\n",
    "        ind_2 = bhvevents_aligned_FR_and_eventFeatures_all_dates_df['clusterID']==exampleneuron\n",
    "        quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col].unique()) \n",
    "    #\n",
    "    else:\n",
    "        if doQuantMeanFRs:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "        else:\n",
    "            quantile_values = np.sort(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col].unique())\n",
    "    \n",
    "    quantile_values = quantile_values[~np.isnan(quantile_values)]\n",
    "    \n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2 = ax.twinx()  # Secondary y-axis for AUC\n",
    "\n",
    "    for i_quantile, q_val in enumerate(quantile_values):\n",
    "    \n",
    "        #\n",
    "        if doExampleSession:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind][ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind][ind_quantile]['FR_ievent'].tolist())\n",
    "        #\n",
    "        elif doExampleNeuron:\n",
    "            ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][quantile_col] == q_val\n",
    "            fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile]['FR_ievent'].tolist())\n",
    "            gaze_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_1&ind_2][ind_quantile][pull_trig_events_tgtname].tolist())\n",
    "     \n",
    "        #\n",
    "        else:\n",
    "            if doQuantMeanFRs:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[ind_quantile]['mean_FR_trace'].tolist())\n",
    "            else:\n",
    "                ind_quantile = bhvevents_aligned_FR_and_eventFeatures_all_dates_df[quantile_col] == q_val\n",
    "                fr_traces_quantile = np.vstack(bhvevents_aligned_FR_and_eventFeatures_all_dates_df[ind_quantile]['FR_ievent'].tolist())\n",
    "\n",
    "            \n",
    "        #     \n",
    "        mean_fr = np.nanmean(fr_traces_quantile, axis=0)\n",
    "        sem_fr = np.nanstd(fr_traces_quantile, axis=0) / np.sqrt(np.sum(~np.isnan(fr_traces_quantile[:, 0]))) # Standard Error of the Mean\n",
    "        \n",
    "        ax.plot(timewins, mean_fr, label=xxx_type+f' Quantile {int(q_val)}')\n",
    "        ax.fill_between(timewins, mean_fr - sem_fr, mean_fr + sem_fr, alpha=0.3)\n",
    "\n",
    "        \n",
    "        #\n",
    "        if doExampleNeuron:\n",
    "            # Accumulated AUC for each gaze trace, then average\n",
    "            auc_gaze_all = np.array([cumtrapz(trace, timewins, initial=0) for trace in gaze_traces_quantile])\n",
    "            mean_auc_gaze = np.nanmean(auc_gaze_all, axis=0)\n",
    "            sem_auc_gaze = np.nanstd(auc_gaze_all, axis=0) / np.sqrt(np.sum(~np.isnan(auc_gaze_all[:, 0])))\n",
    "            \n",
    "           #  # Plot AUC Gaze\n",
    "           #  ax2.plot(timewins, mean_auc_gaze, linestyle='--', color='lightblue', label=f'Gaze AUC {int(q_val)}')\n",
    "           #  ax2.fill_between(timewins, mean_auc_gaze - sem_auc_gaze, mean_auc_gaze + sem_auc_gaze, alpha=0.2, color='lightblue')\n",
    "\n",
    "        \n",
    "        \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'Mean Firing Rate by {title_prefix} Quantile '+gaze_duration_type)\n",
    "    ax.axvline(0, color='k', linestyle='--', linewidth=0.8, label='Pull Onset')\n",
    "    ax.legend()\n",
    "    \n",
    "    if doExampleNeuron:\n",
    "        ax2.set_ylabel(xxx_type)\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_quantile_fig/\"\n",
    "    \n",
    "        if doExampleSession:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "        \n",
    "        if doExampleNeuron:\n",
    "            figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/\"+examplesess+\"/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        if not doExampleNeuron:\n",
    "            fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_acrossNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        elif doExampleNeuron:\n",
    "            fig.savefig(figsavefolder+'bhvevents_aligned_averaged_FR_exampleNeurons_separate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2889130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - calculate the PCA with the pooled data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import splprep, splev\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "if 1:\n",
    "    timewins = np.arange(-4, 4, 1 / 30)\n",
    "    n_timepoints = len(timewins)\n",
    "    quantile_col = xxx_type+'_quantile'  # Or 'gaze_duration_quantile'\n",
    "    y_label = 'Firing Rate (Hz)'\n",
    "    title_prefix = xxx_type  # Or 'Gaze Duration'\n",
    "    smooth_kernel_size = 6\n",
    "    n_bootstrap_iterations = 100\n",
    "    n_neurons_to_sample = 200\n",
    "    \n",
    "    # Get unique neurons\n",
    "    unique_neurons = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[['clusterID', 'session']].drop_duplicates()\n",
    "    n_neurons = len(unique_neurons)\n",
    "\n",
    "    # Get unique quantiles\n",
    "    unique_quantiles = np.sort(bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col].unique())\n",
    "    n_quantiles = len(unique_quantiles)\n",
    "\n",
    "    # Initialize the data matrix\n",
    "    data_matrix = np.empty((n_neurons, n_timepoints * n_quantiles))\n",
    "    neuron_index_lookup = {}\n",
    "\n",
    "    # Populate the data matrix\n",
    "    neuron_counter = 0\n",
    "    for neuron_row in unique_neurons.itertuples(index=False):\n",
    "        cluster_id = neuron_row.clusterID\n",
    "        session = neuron_row.session\n",
    "        neuron_index_lookup[(cluster_id, session)] = neuron_counter\n",
    "        neuron_counter += 1\n",
    "\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            # Get the mean FR trace for the current neuron and quantile\n",
    "            mean_fr_trace_df = bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['clusterID'] == cluster_id) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df['session'] == session) &\n",
    "                (bhvevents_aligned_FR_and_eventFeatures_meanFReachQuant_all_dates_df[quantile_col] == q_val)\n",
    "            ]\n",
    "\n",
    "            if not mean_fr_trace_df.empty:\n",
    "                mean_fr_trace = mean_fr_trace_df['mean_FR_trace'].values[0]  # Take the first element\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = mean_fr_trace\n",
    "            else:\n",
    "                data_matrix[neuron_index_lookup[(cluster_id, session)], i_quantile * n_timepoints:(i_quantile + 1) * n_timepoints] = np.nan  # Handle missing data\n",
    "\n",
    "    # Prepare to store results\n",
    "    all_quantile_lengths = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_quantile_curvatures = np.zeros((n_bootstrap_iterations, n_quantiles))\n",
    "    all_boot_pca_data = np.zeros((n_bootstrap_iterations, 10, n_timepoints * n_quantiles))  # Store all PCA results\n",
    "\n",
    "    # Perform Bootstrapping\n",
    "    for boot_iter in range(n_bootstrap_iterations):\n",
    "        # 1. Randomly sample neurons\n",
    "        sampled_neuron_indices = np.random.choice(n_neurons, n_neurons_to_sample, replace=True)\n",
    "        sampled_data_matrix = data_matrix[sampled_neuron_indices, :].transpose()  # Neuron dimension becomes the columns\n",
    "\n",
    "        # 2. Run PCA\n",
    "        pca = PCA(n_components=10)  # Project to 10 PCs\n",
    "        pca.fit(np.nan_to_num(sampled_data_matrix))\n",
    "        pca_data = pca.transform(np.nan_to_num(sampled_data_matrix)).transpose()  # Project and transpose\n",
    "        all_boot_pca_data[boot_iter, :, :] = pca_data\n",
    "\n",
    "        # Calculate length and curvature for each quantile\n",
    "        for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "            start_col = i_quantile * n_timepoints\n",
    "            end_col = (i_quantile + 1) * n_timepoints\n",
    "            quantile_data = pca_data[:, start_col:end_col]  # (10, n_timepoints)\n",
    "\n",
    "            # \n",
    "            if gaze_duration_type == 'around_pull':\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, :], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, :], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'before_pull':\n",
    "                ind_tgt = timewins<0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "            #\n",
    "            elif gaze_duration_type == 'after_pull':\n",
    "                ind_tgt = timewins>0\n",
    "                # Calculate Length\n",
    "                length = np.sum(np.sqrt(np.sum(np.diff(quantile_data[:10, ind_tgt], axis=1)**2, axis=0))) # Use only first 10 PCs\n",
    "                all_quantile_lengths[boot_iter, i_quantile] = length\n",
    "\n",
    "                # Calculate Curvature (using spline interpolation)\n",
    "                t = np.linspace(0, 1, n_timepoints)\n",
    "                try:\n",
    "                    spl = splprep(quantile_data[:10, ind_tgt], s=0)  # Use only first 10 PCs, s=0 for no smoothing\n",
    "                    spl_deriv2 = splev(t, spl[0], der=2)\n",
    "                    curvature = np.mean(np.sqrt(spl_deriv2[0]**2 + spl_deriv2[1]**2 + spl_deriv2[2]**2))\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = curvature\n",
    "                except:\n",
    "                    all_quantile_curvatures[boot_iter, i_quantile] = np.nan  # Handle spline fitting errors\n",
    "\n",
    "                    \n",
    "    # ---plotting---\n",
    "    # --- plotting number 1 ---\n",
    "    # Plot Length and Curvature (as before)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_lengths, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[0], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    #axes[0].boxplot(all_quantile_lengths)\n",
    "    # axes[0].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('PC Trajectory Length')\n",
    "    axes[0].set_title('Bootstrapped PC Trajectory Lengths')\n",
    "\n",
    "    # Create a Pandas DataFrame for easier plotting with Seaborn\n",
    "    df = pd.DataFrame(all_quantile_curvatures, columns=[f'Quantile {i+1}' for i in range(num_quantiles)])\n",
    "    # Melt the DataFrame to long format, which is ideal for Seaborn\n",
    "    df_melted = pd.melt(df, var_name='Quantile Group', value_name='Length')\n",
    "    # Create the swamp plot (also known as a violin plot)\n",
    "    seaborn.swarmplot(ax = axes[1], x='Quantile Group', y='Length', data=df_melted, \n",
    "                      hue='Quantile Group', palette='viridis')\n",
    "    # axes[1].boxplot(all_quantile_curvatures)\n",
    "    # axes[1].set_xticks(np.arange(1, n_quantiles + 1))\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('PC Trajectory Curvature')\n",
    "    axes[1].set_title('Bootstrapped PC Trajectory Curvatures')\n",
    "\n",
    "    # do the regression for each bootstrap iteration and then plot the average regression line\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Prepare x-axis (quantile indices)\n",
    "    quantiles = np.arange(0, n_quantiles).reshape(-1, 1)\n",
    "\n",
    "    # Store predicted regression lines from each bootstrap\n",
    "    predicted_lengths = []\n",
    "    predicted_curvatures = []\n",
    "    reg_coeff_lengths = []\n",
    "    reg_coeff_curvs = []\n",
    "\n",
    "    for i in range(n_bootstrap_iterations):\n",
    "        # Regression for length\n",
    "        y_len = all_quantile_lengths[i]\n",
    "        model_len = LinearRegression().fit(quantiles, y_len)\n",
    "        slope = model_len.coef_\n",
    "        reg_coeff_lengths.append(slope[0])\n",
    "        pred_len = model_len.predict(quantiles)\n",
    "        predicted_lengths.append(pred_len)\n",
    "\n",
    "        # Regression for curvature\n",
    "        y_curv = all_quantile_curvatures[i]\n",
    "        model_curv = LinearRegression().fit(quantiles, y_curv)\n",
    "        slope = model_curv.coef_\n",
    "        reg_coeff_curvs.append(slope[0])\n",
    "        pred_curv = model_curv.predict(quantiles)\n",
    "        predicted_curvatures.append(pred_curv)\n",
    "\n",
    "    # Convert to arrays\n",
    "    predicted_lengths = np.array(predicted_lengths)\n",
    "    predicted_curvatures = np.array(predicted_curvatures)\n",
    "\n",
    "    # Mean and 95% CI across bootstraps\n",
    "    mean_len = np.mean(predicted_lengths, axis=0)\n",
    "    ci_len_low = np.percentile(predicted_lengths, 2.5, axis=0)\n",
    "    ci_len_high = np.percentile(predicted_lengths, 97.5, axis=0)\n",
    "\n",
    "    mean_curv = np.mean(predicted_curvatures, axis=0)\n",
    "    ci_curv_low = np.percentile(predicted_curvatures, 2.5, axis=0)\n",
    "    ci_curv_high = np.percentile(predicted_curvatures, 97.5, axis=0)\n",
    "\n",
    "    # Length\n",
    "    axes[0].plot(quantiles, mean_len, color='red', label='Mean Regression')\n",
    "    axes[0].fill_between(quantiles.flatten(), ci_len_low, ci_len_high, color='red', alpha=0.3, label='95% CI')\n",
    "    axes[0].set_title('PC Trajectory Length')\n",
    "    axes[0].set_xlabel('Quantile')\n",
    "    axes[0].set_ylabel('Length')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Curvature\n",
    "    axes[1].plot(quantiles, mean_curv, color='blue', label='Mean Regression')\n",
    "    axes[1].fill_between(quantiles.flatten(), ci_curv_low, ci_curv_high, color='blue', alpha=0.3, label='95% CI')\n",
    "    axes[1].set_title('PC Trajectory Curvature')\n",
    "    axes[1].set_xlabel('Quantile')\n",
    "    axes[1].set_ylabel('Curvature')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_lengths, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the length regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "    #\n",
    "    t_stat, p_value = stats.ttest_1samp(reg_coeff_curvs, 5)\n",
    "    # Print the results\n",
    "    print('slopes of the curvature regression'+f\"T-statistic: {t_stat}\"+'; '+f\"P-value: {p_value}\")\n",
    "\n",
    "    # --- plotting number 1 end ---\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Average 3D Traces\n",
    "    fig2 = plt.figure(figsize=(10, 8))\n",
    "    ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "    mean_traces_3d = np.mean(all_boot_pca_data[:, :3, :], axis=0)  # Average across bootstrap iterations, use only first 3 PCs\n",
    "\n",
    "    for i_quantile, q_val in enumerate(unique_quantiles):\n",
    "        start_col = i_quantile * n_timepoints\n",
    "        end_col = (i_quantile + 1) * n_timepoints\n",
    "        mean_quantile_data = mean_traces_3d[:, start_col:end_col]\n",
    "\n",
    "        # Smooth the trajectory\n",
    "        smooth_x = gaussian_filter1d(mean_quantile_data[0, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_y = gaussian_filter1d(mean_quantile_data[1, ind_tgt], sigma=smooth_kernel_size)\n",
    "        smooth_z = gaussian_filter1d(mean_quantile_data[2, ind_tgt], sigma=smooth_kernel_size)\n",
    "\n",
    "        # Plot the smoothed trajectory\n",
    "        ax.plot(smooth_x, smooth_y, smooth_z, label=f'Quantile {int(q_val)}')\n",
    "\n",
    "        # Mark start and end points\n",
    "        ax.scatter(smooth_x[0], smooth_y[0], smooth_z[0], marker='o', color='k')  # Start\n",
    "        ax.scatter(smooth_x[-1], smooth_y[-1], smooth_z[-1], marker='x', color='k')  # End\n",
    "\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_zlabel('PC 3')\n",
    "    ax.set_title(f'Average 3D Trajectories in Neuron-Reduced PC Space by {title_prefix} Quantile')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\n",
    "    print(\"\\n--- Statistical Analysis (Paired t-tests with Holm-Bonferroni) ---\")\n",
    "\n",
    "    from scipy import stats\n",
    "    from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "    def perform_pairwise_paired_ttests(data, group_labels, alpha=0.01):\n",
    "        unique_groups = np.unique(group_labels)\n",
    "        n_groups = len(unique_groups)\n",
    "        p_values = []\n",
    "        comparisons = []\n",
    "\n",
    "        for i in range(n_groups):\n",
    "            for j in range(i + 1, n_groups):\n",
    "                group1_data = data[:, i]\n",
    "                group2_data = data[:, j]\n",
    "                t_stat, p_val = stats.ttest_rel(group1_data, group2_data)\n",
    "                p_values.append(p_val)\n",
    "                comparisons.append((unique_groups[i], unique_groups[j]))\n",
    "\n",
    "        reject, p_corrected, _, _ = multipletests(p_values, method='holm', alpha=alpha)\n",
    "\n",
    "        results_df = pd.DataFrame({'Comparison': comparisons,\n",
    "                                   'p_value': p_values,\n",
    "                                   'p_corrected': p_corrected,\n",
    "                                   'reject_null': reject})\n",
    "        return results_df\n",
    "\n",
    "    # Perform pairwise paired t-tests for Length\n",
    "    length_pairwise_results = perform_pairwise_paired_ttests(all_quantile_lengths, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Length:\")\n",
    "    print(length_pairwise_results)\n",
    "\n",
    "    # Perform pairwise paired t-tests for Curvature\n",
    "    curvature_pairwise_results = perform_pairwise_paired_ttests(all_quantile_curvatures, unique_quantiles)\n",
    "    print(\"\\nPairwise Paired t-tests for Curvature:\")\n",
    "    print(curvature_pairwise_results)\n",
    "    \n",
    "    \n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder+\"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartfocused_continuousBhv_partnerDistVaris/\"+\\\n",
    "                        cameraID+\"/\"+animal1_filenames[0]+\"_\"+animal2_filenames[0]+\"/FR_and_quantile_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder+'bhvevents_aligned_PCfeatures_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "        \n",
    "        fig2.savefig(figsavefolder+'bhvevents_aligned_PCtrajectory_sparate_quantiles_'+bhvname_ana+'_'+\n",
    "                     xxx_type+'_'+gaze_duration_type+'_'+cond_toplot_type+savefile_sufix+'.pdf')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d132a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sampled_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f380d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e511b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
