{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea1560b",
   "metadata": {},
   "source": [
    "### This script runs some basic bhv analysis, and detailed analysis focus on the continuous behavioral variables\n",
    "#### This script analyzed the spike triggered behavioral variables with 3D DLC-Anipose tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose import find_socialgaze_timepoint_Anipose\n",
    "from ana_functions.find_socialgaze_timepoint_Anipose_2 import find_socialgaze_timepoint_Anipose_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_Anipose import bhv_events_timepoint_Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.tracking_video_Anipose_events_demo import tracking_video_Anipose_events_demo\n",
    "from ana_functions.plot_continuous_bhv_var import plot_continuous_bhv_var\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea348f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_continuous_bhv import plot_spike_triggered_continuous_bhv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecce77",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse\n",
    "\n",
    "from ana_functions.plot_continuous_bhv_var_and_neuralFR import plot_continuous_bhv_var_and_neuralFR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d5804",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaze angle threshold\n",
    "# angle_thres = np.pi/36 # 5 degree\n",
    "# angle_thres = np.pi/18 # 10 degree\n",
    "angle_thres = np.pi/12 # 15 degree\n",
    "# angle_thres = np.pi/4 # 45 degree\n",
    "# angle_thres = np.pi/6 # 30 degree\n",
    "angle_thres_name = '15'\n",
    "\n",
    "merge_campairs = ['_Anipose'] # \"_Anipose\": this script is only for Anipose 3d reconstruction of camera 1,2,3 \n",
    "\n",
    "with_tubelever = 1 # 1: consider the location of tubes and levers, only works if using Anipose 3d (or single camera)\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "# nframes = 1*30\n",
    "nframes = 1\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 1\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                       ]\n",
    "        animal1_fixedorders = ['dodson']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 0:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "                              ] # in second\n",
    "        kilosortvers = [\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "            \n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                         4,\n",
    "                        ]\n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger',\n",
    "                               'koala','koala','koala',\n",
    "                               'vermelho','vermelho','vermelho',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Koala\",\"Koala\",\"Koala\",\n",
    "                             \"Vermelho\",\"Vermelho\",\"Vermelho\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0:\n",
    "    neural_record_conditions = ['20240509_Kanga_MC']\n",
    "    dates_list = [\"20240509\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC']\n",
    "    session_start_times = [36.0] # in second\n",
    "    kilosortvers = [4]\n",
    "    animal1_fixedorders = ['dannon']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dannon\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0:\n",
    "    neural_record_conditions = ['20240531_Dodson_MC_and_SR']\n",
    "    dates_list = [\"20240531\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_and_SR']\n",
    "    session_start_times = [0.0] # in second\n",
    "    kilosortvers = [4]\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['ginger']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Ginger\"]\n",
    "\n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "totalsess_time = 600\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()    \n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "# switch animals to make animal 1 and 2 consistent\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_succtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "pull_trig_events_errtrials_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "spike_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "# NOTE: THIS STEP will save the data to the combinedsession_Anipose folder, since they are the same\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    # dummy\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_succtrials_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_trig_events_errtrials_all_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        spike_trig_events_all_dates = pickle.load(f) \n",
    "    \n",
    "    \n",
    "    print('all data saved, loading them')\n",
    "\n",
    "        \n",
    "except:\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "\n",
    "            \n",
    "        session_start_time = session_start_times[idate]\n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        # where to save the demo video\n",
    "        withboxCorner = 1\n",
    "        video_file_dir = data_saved_folder+'/example_videos_Anipose_bhv_demo/'+animal1_filename+'_'+animal2_filename\n",
    "        if not os.path.exists(video_file_dir):\n",
    "            os.makedirs(video_file_dir)\n",
    "    \n",
    "        # folder path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        Anipose_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/anipose_cam123_3d_h5_files/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "\n",
    "        for imergepair in np.arange(0,np.shape(merge_campairs)[0],1):\n",
    "            \n",
    "            # should be only one merge type - \"Anipose\"\n",
    "            merge_campair = merge_campairs[imergepair]\n",
    "\n",
    "            # load camera tracking results\n",
    "            try:\n",
    "                # dummy\n",
    "                if reanalyze_video:\n",
    "                    print(\"re-analyze the data \",videodate_tgt)\n",
    "                    dummy\n",
    "                ## read\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'rb') as f:\n",
    "                    body_part_locs_Anipose = pickle.load(f)                 \n",
    "            except:\n",
    "                print(\"did not save data for Anipose - body part tracking \"+videodate_tgt)\n",
    "                # analyze and save\n",
    "                Anipose_h5_file = Anipose_analyzed_path +videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_anipose.h5\"\n",
    "                Anipose_h5_data = pd.read_hdf(Anipose_h5_file)\n",
    "                body_part_locs_Anipose = body_part_locs_eachpair(Anipose_h5_data)\n",
    "                with open(Anipose_analyzed_path + 'body_part_locs_Anipose.pkl', 'wb') as f:\n",
    "                    pickle.dump(body_part_locs_Anipose, f)            \n",
    "            \n",
    "            min_length = np.min(list(body_part_locs_Anipose.values())[0].shape[0])\n",
    "                    \n",
    "            # load behavioral results\n",
    "            try:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "                ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "                # \n",
    "                with open(ni_data_json[0]) as f:\n",
    "                    for line in f:\n",
    "                        ni_data=json.loads(line)   \n",
    "            except:\n",
    "                bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "                trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "                bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "                session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "                ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "                #\n",
    "                trial_record = pd.read_json(trial_record_json[0])\n",
    "                bhv_data = pd.read_json(bhv_data_json[0])\n",
    "                session_info = pd.read_json(session_info_json[0])\n",
    "                #\n",
    "                with open(ni_data_json[0]) as f:\n",
    "                    for line in f:\n",
    "                        ni_data=json.loads(line)\n",
    "\n",
    "            # get animal info\n",
    "            animal1 = session_info['lever1_animal'][0].lower()\n",
    "            animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "            # get task type and cooperation threshold\n",
    "            try:\n",
    "                coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "                tasktype = session_info[\"task_type\"][0]\n",
    "            except:\n",
    "                coop_thres = 0\n",
    "                tasktype = 1\n",
    "            tasktypes_all_dates[idate] = tasktype\n",
    "            coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "            # successful trial or not\n",
    "            succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "            succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "            succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "            # clean up the trial_record\n",
    "            warnings.filterwarnings('ignore')\n",
    "            trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "            # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "            for itrial in trial_record['trial_number']:\n",
    "                # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "                trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "            trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "            # change bhv_data time to the absolute time\n",
    "            time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "            # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "            for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "                # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "                ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "                new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "                time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "            bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "            bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "            # analyze behavior results\n",
    "            # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "            succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "\n",
    "            trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "            #\n",
    "            pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "            pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "            pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "            pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "            interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "            interpull_intv = interpull_intv[interpull_intv<10]\n",
    "            mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "            std_interpull_intv = np.nanstd(interpull_intv)\n",
    "            #\n",
    "            interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "                pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "            else:\n",
    "                pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "                pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "                \n",
    "            # load behavioral event results\n",
    "            try:\n",
    "                # dummy\n",
    "                print('load social gaze with Anipose 3d of '+date_tgt)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                    output_look_ornot = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                    output_allvectors = pickle.load(f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                    output_allangles = pickle.load(f)  \n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                    output_key_locations = pickle.load(f)\n",
    "            except:\n",
    "                print('analyze social gaze with Anipose 3d only of '+date_tgt)\n",
    "                # get social gaze information \n",
    "                output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_Anipose(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "                output_key_locations = find_socialgaze_timepoint_Anipose_2(body_part_locs_Anipose,min_length,angle_thres,with_tubelever)\n",
    "               \n",
    "                # save data\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_look_ornot, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allvectors, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_allangles, f)\n",
    "                with open(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                    pickle.dump(output_key_locations, f)\n",
    "                \n",
    "             \n",
    "            look_at_face_or_not_Anipose = output_look_ornot['look_at_face_or_not_Anipose']\n",
    "            look_at_selftube_or_not_Anipose = output_look_ornot['look_at_selftube_or_not_Anipose']\n",
    "            look_at_selflever_or_not_Anipose = output_look_ornot['look_at_selflever_or_not_Anipose']\n",
    "            look_at_othertube_or_not_Anipose = output_look_ornot['look_at_othertube_or_not_Anipose']\n",
    "            look_at_otherlever_or_not_Anipose = output_look_ornot['look_at_otherlever_or_not_Anipose']\n",
    "            # change the unit to second, and aligned to session start\n",
    "            session_start_time = session_start_times[idate]\n",
    "            look_at_face_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_face_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selflever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selflever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_selftube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_selftube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "            look_at_otherlever_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_Anipose['dodson'])[0],1)/fps - session_start_time\n",
    "            look_at_othertube_or_not_Anipose['time_in_second'] = np.arange(0,np.shape(look_at_othertube_or_not_Anipose['dodson'])[0],1)/fps - session_start_time \n",
    "\n",
    "            look_at_Anipose = {\"face\":look_at_face_or_not_Anipose,\"selflever\":look_at_selflever_or_not_Anipose,\n",
    "                               \"selftube\":look_at_selftube_or_not_Anipose,\"otherlever\":look_at_otherlever_or_not_Anipose,\n",
    "                               \"othertube\":look_at_othertube_or_not_Anipose} \n",
    "            \n",
    "            # find time point of behavioral events\n",
    "            output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_Anipose(bhv_data,look_at_Anipose)\n",
    "            time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "            time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "            oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "            oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "            mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "            mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "            timepoint_lever1 = output_time_points_levertube['time_point_lookatlever1']   \n",
    "            timepoint_lever2 = output_time_points_levertube['time_point_lookatlever2']   \n",
    "            timepoint_tube1 = output_time_points_levertube['time_point_lookattube1']   \n",
    "            timepoint_tube2 = output_time_points_levertube['time_point_lookattube2']   \n",
    "                \n",
    "            # \n",
    "            # mostly just for the sessions in which MC and SR are in the same session \n",
    "            firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "            oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "            oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "            mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "            mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "            #    \n",
    "            # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "            lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "            oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "            oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "            mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "            mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)]\n",
    "        \n",
    "            # define successful pulls and failed pulls\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "            # \n",
    "            time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                          \"pull2_succ\":time_point_pull2_succ,\n",
    "                                          \"pull1_fail\":time_point_pull1_fail,\n",
    "                                          \"pull2_fail\":time_point_pull2_fail,\n",
    "                                        }    \n",
    "            \n",
    "            # # plot behavioral events\n",
    "            if 0:\n",
    "                if np.isin(animal1,animal1_fixedorder):\n",
    "                    plot_bhv_events_levertube(date_tgt+merge_campair,animal1, animal2, session_start_time, totalsess_time, \n",
    "                                              time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2,\n",
    "                                              timepoint_lever1,timepoint_lever2,timepoint_tube1,timepoint_tube2)\n",
    "                else:\n",
    "                    plot_bhv_events_levertube(date_tgt+merge_campair,animal2, animal1, session_start_time, totalsess_time, \n",
    "                                              time_point_pull2, time_point_pull1, oneway_gaze2, oneway_gaze1, mutual_gaze2, mutual_gaze1,\n",
    "                                              timepoint_lever2,timepoint_lever1,timepoint_tube2,timepoint_tube1)\n",
    "            #\n",
    "            # save behavioral events plot\n",
    "            if 0:\n",
    "                current_dir = data_saved_folder+'/bhv_events_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                plt.savefig(data_saved_folder+\"bhv_events_Anipose/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+date_tgt+'/'+date_tgt+\"_Anipose.pdf\")\n",
    "  \n",
    "            #\n",
    "            # # old definition\n",
    "            # if np.isin(animal1,animal1_fixedorder):\n",
    "            #     owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            #     owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            #     mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            #     mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            # else:\n",
    "            #     owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            #     owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            #     mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            #     mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            #\n",
    "            # new defnition\n",
    "            # <500ms counts as one gaze, gaze number per second\n",
    "            if np.isin(animal1,animal1_fixedorder):\n",
    "                owgaze1_num_all_dates[idate] = np.sum(oneway_gaze1[1:]-oneway_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "                owgaze2_num_all_dates[idate] = np.sum(oneway_gaze2[1:]-oneway_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze1_num_all_dates[idate] = np.sum(mutual_gaze1[1:]-mutual_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze2_num_all_dates[idate] = np.sum(mutual_gaze2[1:]-mutual_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "            else:\n",
    "                owgaze1_num_all_dates[idate] = np.sum(oneway_gaze2[1:]-oneway_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "                owgaze2_num_all_dates[idate] = np.sum(oneway_gaze1[1:]-oneway_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze1_num_all_dates[idate] = np.sum(mutual_gaze2[1:]-mutual_gaze2[:-1]>=0.5)/(min_length/fps)\n",
    "                mtgaze2_num_all_dates[idate] = np.sum(mutual_gaze1[1:]-mutual_gaze1[:-1]>=0.5)/(min_length/fps)\n",
    "            \n",
    "            \n",
    "            # plot key continuous behavioral variables\n",
    "            if 1:\n",
    "                filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_Anipose/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+date_tgt+'/'\n",
    "                if not os.path.exists(filepath_cont_var):\n",
    "                    os.makedirs(filepath_cont_var)\n",
    "                \n",
    "                savefig = 0\n",
    "                pull_trig_events_summary, pull_trig_events_succtrial_summary, pull_trig_events_errtrial_summary = plot_continuous_bhv_var(filepath_cont_var+date_tgt+merge_campair,savefig, animal1, animal2, \n",
    "                                        session_start_time, min_length,succpulls_ornot,\n",
    "                                        time_point_pull1, time_point_pull2,animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                pull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "                pull_trig_events_succtrials_all_dates[date_tgt] = pull_trig_events_succtrial_summary\n",
    "                pull_trig_events_errtrials_all_dates[date_tgt] = pull_trig_events_errtrial_summary\n",
    "                    \n",
    "                \n",
    "            # analyze the events interval, especially for the pull to other and other to pull interval\n",
    "            # could be used for define time bin for DBN\n",
    "            if 1:\n",
    "                _,_,_,pullTOother_itv, otherTOpull_itv = bhv_events_interval(totalsess_time, session_start_time, time_point_pull1, time_point_pull2, \n",
    "                                                                             oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2)\n",
    "                #\n",
    "                pull_other_pool_itv = np.concatenate((pullTOother_itv,otherTOpull_itv))\n",
    "                bhv_intv_all_dates[date_tgt] = {'pull_to_other':pullTOother_itv,'other_to_pull':otherTOpull_itv,\n",
    "                                'pull_other_pooled': pull_other_pool_itv}\n",
    "        \n",
    "        \n",
    "            \n",
    "            # plot the tracking demo video\n",
    "            if 0:      \n",
    "                video_file = video_file_dir+'/'+date_tgt+'_'+animal1_filename+'_'+animal2_filename+'_anipose_bhv_demo.mp4'\n",
    "                tracking_video_Anipose_events_demo(body_part_locs_Anipose,output_look_ornot,output_allvectors,output_allangles,\n",
    "                                                   time_point_pull1,time_point_pull2,animalnames_videotrack,bodypartnames_videotrack,\n",
    "                                                   date_tgt,animal1_filename,animal2_filename,animal1,animal2,\n",
    "                                                   session_start_time,fps,nframes,video_file,withboxCorner)\n",
    "\n",
    "                \n",
    "                \n",
    "            # get the neural data\n",
    "            \n",
    "            # session starting time compared with the neural recording\n",
    "            session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "            neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "\n",
    "\n",
    "            # load channel maps\n",
    "            channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "            # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "            channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "\n",
    "            # # load spike sorting results\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "\n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "                \n",
    "\n",
    "            # calculate the firing rate\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_face_or_not_Anipose']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            # _,FR_timepoint_allclusters,FR_allclusters,FR_zscore_allclusters = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "            #                                                                                                spike_clusters_data, spike_time_data)\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "                                                                                         spike_channels_data, spike_time_data)\n",
    "            #\n",
    "            # Run PCA analysis\n",
    "            FR_zscore_allch_np_merged = np.array(pd.DataFrame(FR_zscore_allch).T)\n",
    "            FR_zscore_allch_np_merged = FR_zscore_allch_np_merged[~np.isnan(np.sum(FR_zscore_allch_np_merged,axis=1)),:]\n",
    "            # # run PCA on the entire session\n",
    "            pca = PCA(n_components=3)\n",
    "            FR_zscore_allch_PCs = pca.fit_transform(FR_zscore_allch_np_merged.T)\n",
    "            #\n",
    "            # # run PCA around the -PCAtwins to PCAtwins for each behavioral events\n",
    "            PCAtwins = 5 # 5 second\n",
    "            gaze_thresold = 0.5 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "            \n",
    "            if 0:\n",
    "                savefigs = 0 \n",
    "                PCA_around_bhv_events(FR_timepoint_allch,FR_zscore_allch_np_merged,time_point_pull1,time_point_pull2,time_point_pulls_succfail, \n",
    "                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,PCAtwins,fps,\n",
    "                              savefigs,data_saved_folder,cameraID,animal1_filename,animal2_filename,date_tgt)\n",
    "\n",
    "                    \n",
    "            # plot and PC1,2,3 traces and the continous variable traces\n",
    "            if 0:\n",
    "                print('plot the PC1,2,3 and continuous variable traces')\n",
    "                \n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_neural_and_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/\"+merge_campair+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                mintime_forplot = 50 # in the unit of second\n",
    "                maxtime_forplot = 550 # in the unit of second\n",
    "                plot_continuous_bhv_var_and_neuralFR(date_tgt,savefig,save_path, animal1, animal2, session_start_time,\n",
    "                                                     min_length, mintime_forplot, maxtime_forplot, succpulls_ornot, \n",
    "                                                     time_point_pull1, time_point_pull2, animalnames_videotrack, \n",
    "                                                     output_look_ornot, output_allvectors, output_allangles, output_key_locations, \n",
    "                                                     FR_timepoint_allch, FR_zscore_allch_PCs)\n",
    "                \n",
    "           \n",
    "            # do the spike triggered average of different bhv variables, both continuous tracking result, or the pulling and social gaze actions\n",
    "            # the goal is to get a sense for glm\n",
    "            if 1: \n",
    "                print('plot spike triggered bhv variables')\n",
    "                \n",
    "                savefig = 1\n",
    "                save_path = data_saved_folder+\"fig_for_neural_and_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/\"+merge_campair+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"+date_tgt\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                #\n",
    "                do_shuffle = 0\n",
    "                #\n",
    "                spike_trig_average_all =  plot_spike_triggered_continuous_bhv(date_tgt,savefig,save_path, animal1, animal2, session_start_time, min_length, \n",
    "                                                                              time_point_pull1, time_point_pull2, time_point_pulls_succfail,\n",
    "                                                                              oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,animalnames_videotrack, \n",
    "                                                                              output_look_ornot, output_allvectors, output_allangles, output_key_locations, \n",
    "                                                                              spike_clusters_data, spike_time_data,spike_channels_data,do_shuffle)\n",
    "                \n",
    "                spike_trig_events_all_dates[date_tgt] = spike_trig_average_all\n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    # save data\n",
    "    if 1:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_combinedsessions_Anipose'+savefile_sufix+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_succtrials_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_succtrials_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull_trig_events_errtrials_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_trig_events_errtrials_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/spike_trig_events_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(spike_trig_events_all_dates, f)    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c0fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c2654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eb65b16",
   "metadata": {},
   "source": [
    "### analyze the spike triggered behavioral variables across all dates\n",
    "### plot the tsne or PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "doPCA = 1\n",
    "doTSNE = 0\n",
    "\n",
    "spike_trig_events_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name','clusterID',\n",
    "                                                       'channelID','st_average'])\n",
    "\n",
    "# reorganize to a dataframes\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "    \n",
    "    act_animals = list(spike_trig_events_all_dates[date_tgt].keys())\n",
    "    \n",
    "    for iact_animal in act_animals:\n",
    "        \n",
    "        bhv_types = list(spike_trig_events_all_dates[date_tgt][iact_animal].keys())\n",
    "        \n",
    "        for ibhv_type in bhv_types:\n",
    "            \n",
    "            clusterIDs = list(spike_trig_events_all_dates[date_tgt][iact_animal][ibhv_type].keys())\n",
    "    \n",
    "            for iclusterID in clusterIDs:\n",
    "            \n",
    "                ichannelID = spike_trig_events_all_dates[date_tgt][iact_animal][ibhv_type][iclusterID]['ch']\n",
    "                ist_average = spike_trig_events_all_dates[date_tgt][iact_animal][ibhv_type][iclusterID]['st_average']\n",
    "\n",
    "                spike_trig_events_all_dates_df = spike_trig_events_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal': iact_animal, \n",
    "                                                                                        'bhv_name': ibhv_type,\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'st_average':ist_average,\n",
    "                                                                                       }, ignore_index=True)\n",
    "if 0:\n",
    "    # normalize st_average for each unit\n",
    "    nspikeunits = np.shape(spike_trig_events_all_dates_df)[0]\n",
    "    for ispikeunit in np.arange(0,nspikeunits,1):\n",
    "        stevent = spike_trig_events_all_dates_df['st_average'][ispikeunit]\n",
    "        stevent_norm = (stevent-np.nanmin(stevent))/(np.nanmax(stevent)-np.nanmin(stevent))\n",
    "        spike_trig_events_all_dates_df['st_average'][ispikeunit] = stevent_norm            \n",
    "        \n",
    "# only focus on the certain act animal and certain bhv_name\n",
    "# act_animals_all = ['kanga']\n",
    "# bhv_names_all = ['leverpull_prob']\n",
    "act_animals_all = np.unique(spike_trig_events_all_dates_df['act_animal'])\n",
    "bhv_names_all = np.unique(spike_trig_events_all_dates_df['bhv_name'])\n",
    "#\n",
    "nact_animals = np.shape(act_animals_all)[0]\n",
    "nbhv_names = np.shape(bhv_names_all)[0]\n",
    "\n",
    "# set for plot\n",
    "# plot all units\n",
    "fig1, axs1 = plt.subplots(nact_animals,nbhv_names)\n",
    "fig1.set_figheight(8*nact_animals)\n",
    "fig1.set_figwidth(8*nbhv_names)\n",
    "\n",
    "# plot all units but separate different days\n",
    "fig2, axs2 = plt.subplots(nact_animals,nbhv_names)\n",
    "fig2.set_figheight(8*nact_animals)\n",
    "fig2.set_figwidth(8*nbhv_names)\n",
    "\n",
    "# plot all units but seprate different channels\n",
    "fig3, axs3 = plt.subplots(nact_animals,nbhv_names)\n",
    "fig3.set_figheight(8*nact_animals)\n",
    "fig3.set_figwidth(8*nbhv_names)\n",
    "\n",
    "# plot all units but separate different conditions\n",
    "fig4, axs4 = plt.subplots(nact_animals,nbhv_names)\n",
    "fig4.set_figheight(8*nact_animals)\n",
    "fig4.set_figwidth(8*nbhv_names)\n",
    "\n",
    "# spike triggered average for different task conditions\n",
    "# to be save, prepare for five conditions\n",
    "fig6, axs6 = plt.subplots(nact_animals*5,nbhv_names)\n",
    "fig6.set_figheight(8*nact_animals*5)\n",
    "fig6.set_figwidth(8*nbhv_names)\n",
    "\n",
    "# plot all units but separate different k-mean cluster\n",
    "fig5, axs5 = plt.subplots(nact_animals,nbhv_names)\n",
    "fig5.set_figheight(8*nact_animals)\n",
    "fig5.set_figwidth(8*nbhv_names)\n",
    "\n",
    "# spike triggered average for different k-mean cluster\n",
    "# to be save, prepare for 14 clusters\n",
    "fig7, axs7 = plt.subplots(nact_animals*14,nbhv_names)\n",
    "fig7.set_figheight(8*nact_animals*14)\n",
    "fig7.set_figwidth(8*nbhv_names)\n",
    "\n",
    "#\n",
    "for ianimal in np.arange(0,nact_animals,1):\n",
    "    \n",
    "    act_animal = act_animals_all[ianimal]\n",
    "    \n",
    "    for ibhvname in np.arange(0,nbhv_names,1):\n",
    "        \n",
    "        bhv_name = bhv_names_all[ibhvname]\n",
    "        \n",
    "        ind = (spike_trig_events_all_dates_df['act_animal']==act_animal)&(spike_trig_events_all_dates_df['bhv_name']==bhv_name)\n",
    "        \n",
    "        spike_trig_events_tgt = np.vstack(list(spike_trig_events_all_dates_df[ind]['st_average']))\n",
    "        \n",
    "        \n",
    "        # k means clustering\n",
    "        # run clustering on the 15 dimension PC space (for doPCA), or the whole dataset (for doTSNE)\n",
    "        pca = PCA(n_components=15)\n",
    "        spike_trig_events_pca = pca.fit_transform(spike_trig_events_tgt)\n",
    "        tsne = TSNE(n_components=3, random_state=0)\n",
    "        spike_trig_events_tsne = tsne.fit_transform(spike_trig_events_tgt)\n",
    "        #\n",
    "        range_n_clusters = np.arange(2,15,1)\n",
    "        silhouette_avg_all = np.ones(np.shape(range_n_clusters))*np.nan\n",
    "        nkmeancls = np.shape(range_n_clusters)[0]\n",
    "        #\n",
    "        for ikmeancl in np.arange(0,nkmeancls,1):\n",
    "            n_clusters = range_n_clusters[ikmeancl]\n",
    "            #\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "            # cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "            if doPCA:\n",
    "                cluster_labels = clusterer.fit_predict(spike_trig_events_pca)\n",
    "            if doTSNE:\n",
    "                cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "                # cluster_labels = clusterer.fit_predict(spike_trig_events_tsne)\n",
    "            #\n",
    "            # The silhouette_score gives the average value for all the samples.\n",
    "            # This gives a perspective into the density and separation of the formed\n",
    "            # clusters\n",
    "            # silhouette_avg = silhouette_score(spike_trig_events_tgt, cluster_labels)\n",
    "            if doPCA:\n",
    "                silhouette_avg = silhouette_score(spike_trig_events_pca, cluster_labels)\n",
    "            if doTSNE:\n",
    "                silhouette_avg = silhouette_score(spike_trig_events_tgt, cluster_labels)\n",
    "                # silhouette_avg = silhouette_score(spike_trig_events_tsne, cluster_labels)\n",
    "            #\n",
    "            silhouette_avg_all[ikmeancl] = silhouette_avg\n",
    "        #\n",
    "        best_k_num = range_n_clusters[silhouette_avg_all==np.nanmax(silhouette_avg_all)][0]\n",
    "        #\n",
    "        clusterer = KMeans(n_clusters=best_k_num, random_state=0)\n",
    "        # kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "        if doPCA:\n",
    "            kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_pca)\n",
    "        if doTSNE:\n",
    "            kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_tgt)\n",
    "            # kmean_cluster_labels = clusterer.fit_predict(spike_trig_events_tsne)\n",
    "    \n",
    "    \n",
    "        # run PCA and TSNE     \n",
    "        pca = PCA(n_components=2)\n",
    "        tsne = TSNE(n_components=2, random_state=0)\n",
    "        #\n",
    "        spike_trig_events_pca = pca.fit_transform(spike_trig_events_tgt)\n",
    "        spike_trig_events_tsne = tsne.fit_transform(spike_trig_events_tgt)\n",
    "        \n",
    "        # plot all units\n",
    "        # plot the tsne\n",
    "        if doTSNE:\n",
    "            axs1[ianimal,ibhvname].plot(spike_trig_events_tsne[:,0],spike_trig_events_tsne[:,1],'.')\n",
    "        # plot the pca\n",
    "        if doPCA:\n",
    "            axs1[ianimal,ibhvname].plot(spike_trig_events_pca[:,0],spike_trig_events_pca[:,1],'.')\n",
    "        \n",
    "        axs1[ianimal,ibhvname].set_xticklabels([])\n",
    "        axs1[ianimal,ibhvname].set_yticklabels([])\n",
    "        axs1[ianimal,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "        \n",
    "        \n",
    "        # plot all units, but seprate different dates\n",
    "        dates_forplot = np.unique(spike_trig_events_all_dates_df[ind]['dates'])\n",
    "        for idate_forplot in dates_forplot:\n",
    "            ind_idate = list(spike_trig_events_all_dates_df[ind]['dates']==idate_forplot)\n",
    "            #\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs2[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_idate,0],spike_trig_events_tsne[ind_idate,1],\n",
    "                                        '.',label=idate_forplot)\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs2[ianimal,ibhvname].plot(spike_trig_events_pca[ind_idate,0],spike_trig_events_pca[ind_idate,1],\n",
    "                                        '.',label=idate_forplot)\n",
    "            #\n",
    "        axs2[ianimal,ibhvname].set_xticklabels([])\n",
    "        axs2[ianimal,ibhvname].set_yticklabels([])\n",
    "        axs2[ianimal,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "        axs2[ianimal,ibhvname].legend()\n",
    "        \n",
    "        \n",
    "        # plot all units, but seprate different channels\n",
    "        chs_forplot = np.unique(spike_trig_events_all_dates_df[ind]['channelID'])\n",
    "        for ich_forplot in chs_forplot:\n",
    "            ind_ich = list(spike_trig_events_all_dates_df[ind]['channelID']==ich_forplot)\n",
    "            #\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs3[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_ich,0],spike_trig_events_tsne[ind_ich,1],\n",
    "                                        '.',label=str(ich_forplot))\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs3[ianimal,ibhvname].plot(spike_trig_events_pca[ind_ich,0],spike_trig_events_pca[ind_ich,1],\n",
    "                                        '.',label=str(ich_forplot))\n",
    "            #\n",
    "        axs3[ianimal,ibhvname].set_xticklabels([])\n",
    "        axs3[ianimal,ibhvname].set_yticklabels([])\n",
    "        axs3[ianimal,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "        axs3[ianimal,ibhvname].legend()\n",
    "        \n",
    "        \n",
    "        # plot all units, but seprate different task conditions\n",
    "        cons_forplot = np.unique(spike_trig_events_all_dates_df[ind]['condition'])\n",
    "        for icon_forplot in cons_forplot:\n",
    "            ind_icon = list(spike_trig_events_all_dates_df[ind]['condition']==icon_forplot)\n",
    "            #\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs4[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_icon,0],spike_trig_events_tsne[ind_icon,1],\n",
    "                                        '.',label=icon_forplot)\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs4[ianimal,ibhvname].plot(spike_trig_events_pca[ind_icon,0],spike_trig_events_pca[ind_icon,1],\n",
    "                                        '.',label=icon_forplot)\n",
    "            #\n",
    "        axs4[ianimal,ibhvname].set_xticklabels([])\n",
    "        axs4[ianimal,ibhvname].set_yticklabels([])\n",
    "        axs4[ianimal,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "        axs4[ianimal,ibhvname].legend()\n",
    "    \n",
    "        # plot the mean spike trigger average trace across neurons in each condition\n",
    "        trig_twins = [-6,6] # the time window to examine the spike triggered average, in the unit of s\n",
    "        xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "        #\n",
    "        cons_forplot = np.unique(spike_trig_events_all_dates_df[ind]['condition'])\n",
    "        icon_ind = 0\n",
    "        for icon_forplot in cons_forplot:\n",
    "            ind_icon = list(spike_trig_events_all_dates_df[ind]['condition']==icon_forplot)\n",
    "            #\n",
    "            mean_trig_trace_icon = np.nanmean(spike_trig_events_tgt[ind_icon,:],axis=0)\n",
    "            std_trig_trace_icon = np.nanstd(spike_trig_events_tgt[ind_icon,:],axis=0)\n",
    "            sem_trig_trace_icon = np.nanstd(spike_trig_events_tgt[ind_icon,:],axis=0)/np.sqrt(np.shape(spike_trig_events_tgt[ind_icon,:])[0])\n",
    "            itv95_trig_trace_icon = 1.96*sem_trig_trace_icon\n",
    "            #\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].errorbar(xxx_forplot,mean_trig_trace_icon,yerr=itv95_trig_trace_icon,\n",
    "                                                       color='#E0E0E0',ecolor='#EEEEEE',label=icon_forplot)\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_icon-itv95_trig_trace_icon),\n",
    "                                                          np.nanmax(mean_trig_trace_icon+itv95_trig_trace_icon)],'--k')\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].set_xlabel('time (s)')\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "            axs6[ianimal*5+icon_ind,ibhvname].legend()\n",
    "            #\n",
    "            icon_ind = icon_ind + 1\n",
    "    \n",
    "    \n",
    "        # plot all units, but seprate different k-mean clusters\n",
    "        kms_forplot = np.unique(kmean_cluster_labels)\n",
    "        for ikm_forplot in kms_forplot:\n",
    "            ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "            #\n",
    "            # plot the tsne\n",
    "            if doTSNE:\n",
    "                axs5[ianimal,ibhvname].plot(spike_trig_events_tsne[ind_ikm,0],spike_trig_events_tsne[ind_ikm,1],\n",
    "                                        '.',label=str(ikm_forplot))\n",
    "            # plot the pca\n",
    "            if doPCA:\n",
    "                axs5[ianimal,ibhvname].plot(spike_trig_events_pca[ind_ikm,0],spike_trig_events_pca[ind_ikm,1],\n",
    "                                        '.',label=str(ikm_forplot))\n",
    "            #\n",
    "        axs5[ianimal,ibhvname].set_xticklabels([])\n",
    "        axs5[ianimal,ibhvname].set_yticklabels([])\n",
    "        axs5[ianimal,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "        axs5[ianimal,ibhvname].legend()\n",
    "        \n",
    "        # plot the mean spike trigger average trace across neurons in each cluster\n",
    "        trig_twins = [-6,6] # the time window to examine the spike triggered average, in the unit of s\n",
    "        xxx_forplot = np.arange(trig_twins[0]*fps,trig_twins[1]*fps,1)\n",
    "        #\n",
    "        kms_forplot = np.unique(kmean_cluster_labels)\n",
    "        for ikm_forplot in kms_forplot:\n",
    "            ind_ikm = list(kmean_cluster_labels==ikm_forplot)\n",
    "            #\n",
    "            mean_trig_trace_ikm = np.nanmean(spike_trig_events_tgt[ind_ikm,:],axis=0)\n",
    "            std_trig_trace_ikm = np.nanstd(spike_trig_events_tgt[ind_ikm,:],axis=0)\n",
    "            sem_trig_trace_ikm = np.nanstd(spike_trig_events_tgt[ind_ikm,:],axis=0)/np.sqrt(np.shape(spike_trig_events_tgt[ind_ikm,:])[0])\n",
    "            itv95_trig_trace_ikm = 1.96*sem_trig_trace_ikm\n",
    "            #\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].errorbar(xxx_forplot,mean_trig_trace_ikm,yerr=itv95_trig_trace_ikm,\n",
    "                                                          color='#E0E0E0',ecolor='#EEEEEE',label='cluster#'+str(ikm_forplot))\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].plot([0,0],[np.nanmin(mean_trig_trace_ikm-itv95_trig_trace_ikm),\n",
    "                                                             np.nanmax(mean_trig_trace_ikm+itv95_trig_trace_ikm)],'--k')\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].set_xlabel('time (s)')\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].set_xticks(np.arange(trig_twins[0]*fps,trig_twins[1]*fps,60))\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].set_xticklabels(list(map(str,np.arange(trig_twins[0],trig_twins[1],2))))\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].set_title(act_animal+'\\n'+bhv_name)\n",
    "            axs7[ianimal*14+ikm_forplot,ibhvname].legend()\n",
    "    \n",
    "    \n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder+\"fig_for_neural_and_BasicBhvAna_and_ContVariAna_Aniposelib3d_allsessions_basicEvents/\"+merge_campairs[0]+\"/\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "    if doTSNE:\n",
    "        fig1.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "        fig2.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "        fig4.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "        fig5.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "        fig6.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')        \n",
    "        fig7.savefig(figsavefolder+'spike_triggered_bhv_variables_tsne_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "\n",
    "    if doPCA:\n",
    "        fig1.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates'+savefile_sufix+'.pdf')\n",
    "        fig2.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_dates'+savefile_sufix+'.pdf')\n",
    "        fig3.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_channels'+savefile_sufix+'.pdf')\n",
    "        fig4.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_conditions'+savefile_sufix+'.pdf')\n",
    "        fig5.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_separated_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "        fig6.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_sttraces_for_conditions'+savefile_sufix+'.pdf')                           \n",
    "        fig7.savefig(figsavefolder+'spike_triggered_bhv_variables_pca_clusters_all_dates_sttraces_for_kmeanclusters'+savefile_sufix+'.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946ae83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11688c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57943305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_trig_events_all_dates_df['st_average'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_trig_trace_ikm = np.nanmean(spike_trig_events_tgt[ind_ikm,:],axis=1)\n",
    "std_trig_trace_ikm = np.nanstd(spike_trig_events_tgt[ind_ikm,:],axis=1)\n",
    "sem_trig_trace_ikm = np.nanstd(spike_trig_events_tgt[ind_ikm,:],axis=1)/np.sqrt(np.shape(spike_trig_events_tgt[ind_ikm,:])[1])\n",
    "itv95_trig_trace_ikm = 1.96*sem_trig_trace_ikm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "itv95_trig_trace_ikm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcb3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4926db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e995eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32445842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3a6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01b48a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DLC] *",
   "language": "python",
   "name": "conda-env-.conda-DLC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
