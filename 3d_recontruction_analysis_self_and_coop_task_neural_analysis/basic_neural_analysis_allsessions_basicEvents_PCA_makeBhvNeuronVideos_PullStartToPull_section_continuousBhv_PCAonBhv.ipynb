{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### Basic neural activity analysis with single camera tracking\n",
    "#### analyze the firing rate PC1,2,3\n",
    "#### making the demo videos\n",
    "#### the following detailed analysis focused on pull related behavioral events\n",
    "#### the pull action start events are defined based on the movement onset before each pull\n",
    "#### capture the entire section of neural activity from Pull Start/Onset to pull actions\n",
    "#### This is the same analysis as basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv; but change to first project all possible continuous variables on PC space, then define based on PC1 (maybe PC2 as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1786d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note:\n",
    "# need to use pyddm environment to run this to be compitable with the hddm pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from scipy.ndimage import label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace import plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace\n",
    "\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR_PullStartToPull_variedSection import plot_bhv_events_aligned_FR_PullStartToPull_variedSection\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545792d",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b43ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for projecting high D bhv variables to small PC space\n",
    "from ana_functions.singlecam_conBhv_from_highDimension_to_PCspace import get_data_for_singlecam_conBhv_from_highDimension_to_PCspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edb8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d40abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get useful information about pulls\n",
    "from ana_functions.get_pull_infos import get_pull_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b17a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gaze vector speed and face mass speed to find the pull action start time within IPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI\n",
    "from ana_functions.find_sharp_increases_withinIPI import find_sharp_increases_withinIPI_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5cdbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: find the lowest timepoint then the increase point as the pull onset\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_withinIPI\n",
    "from ana_functions.find_rising_onset_after_min_withinIPI import find_rising_onset_after_min_dual_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# if use onset of the first increase after min\n",
    "doOnsetAfterMin = 1\n",
    "if not doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = 'bhvPCA_'\n",
    "elif doOnsetAfterMin:\n",
    "    doOnsetAfterMin_suffix = 'bhvPCA_PullOnsetAfterMin_'\n",
    "\n",
    "# if use a hmm based method to find the trial start\n",
    "doHMMmethod = 0\n",
    "if doHMMmethod:\n",
    "    doOnsetAfterMin_suffix = 'HMMmethods_'\n",
    "\n",
    "    \n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     # '20231101_Dodson_withGinger_SR',\n",
    "                                     # '20231107_Dodson_withGinger_SR',\n",
    "                                     # '20231122_Dodson_withGinger_SR',\n",
    "                                     # '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                            # 'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      # \"20231101_SR\",\n",
    "                      # \"20231107_SR\",\n",
    "                      # \"20231122_SR\",\n",
    "                      # \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 # 0.00,   \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00,  \n",
    "                                 # 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         # 2, \n",
    "                         # 2, \n",
    "                         # 4, \n",
    "                         # 4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorders = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorders = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define bhv events summarizing variables     \n",
    "tasktypes_all_dates = np.zeros((ndates,1))\n",
    "coopthres_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "succ_rate_all_dates = np.zeros((ndates,1))\n",
    "interpullintv_all_dates = np.zeros((ndates,1))\n",
    "trialnum_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "owgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "owgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze1_num_all_dates = np.zeros((ndates,1))\n",
    "mtgaze2_num_all_dates = np.zeros((ndates,1))\n",
    "pull1_num_all_dates = np.zeros((ndates,1))\n",
    "pull2_num_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "pull1_intv_all_dates = np.zeros((ndates,1))\n",
    "pull2_intv_all_dates = np.zeros((ndates,1))\n",
    "pull1_minintv_all_dates = np.zeros((ndates,1))\n",
    "pull2_minintv_all_dates = np.zeros((ndates,1))\n",
    "\n",
    "bhv_intv_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pull_infos_all_dates = dict.fromkeys(dates_list, []) # keep some useful information about pulls - time from last reward, number of preceding failed pull etc\n",
    "\n",
    "pull_rts_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvtoPC1_loadings_all_dates = dict.fromkeys(dates_list, [])\n",
    "bhvPC123explained_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "succpullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "failpullstartTopull_trig_events_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "bhvevents_pullstartTopull_aligned_FR_allevents_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all data\n",
      "all data from all dates are loaded; pull start focus\n"
     ]
    }
   ],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # dummpy\n",
    "    \n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        owgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        mtgaze2_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_num_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_num_all_dates = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull1_intv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull2_intv_all_dates = pickle.load(f)\n",
    "\n",
    "    with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_infos_all_dates  = pickle.load(f)  \n",
    "        \n",
    "    with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pull_rts_all_dates  = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        tasktypes_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        coopthres_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succ_rate_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        interpullintv_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        trialnum_all_dates = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhv_intv_all_dates = pickle.load(f)\n",
    "      \n",
    "    with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        succpullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        failpullstartTopull_trig_events_all_dates = pickle.load(f) \n",
    "    \n",
    "    with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_pullstartTopull_aligned_FR_allevents_all_dates = pickle.load(f) \n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvtoPC1_loadings_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvtoPC1_loadings_all_dates  = pickle.load(f)\n",
    "        \n",
    "    with open(data_saved_subfolder+'/bhvPC123explained_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvPC123explained_all_dates  = pickle.load(f)\n",
    "        \n",
    "    print('all data from all dates are loaded; pull start focus')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "\n",
    "        #\n",
    "        pull_rts_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        \n",
    "        #\n",
    "        bhvtoPC1_loadings_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        bhvPC123explained_all_dates[date_tgt] = dict.fromkeys([animal1_fixedorder[0],animal2_fixedorder[0]],[])\n",
    "        \n",
    "        \n",
    "        # folder and file path\n",
    "        camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "        camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "        \n",
    "        # \n",
    "        try: \n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "        except:\n",
    "            singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "            bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "        tasktypes_all_dates[idate] = tasktype\n",
    "        coopthres_all_dates[idate] = coop_thres   \n",
    "\n",
    "        # successful trial or not\n",
    "        succtrial_ornot = np.array((trial_record['rewarded']>0).astype(int))\n",
    "        succpull1_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==1]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpull2_ornot = np.array((np.isin(bhv_data[bhv_data['behavior_events']==2]['trial_number'],trial_record[trial_record['rewarded']>0]['trial_number'])).astype(int))\n",
    "        succpulls_ornot = [succpull1_ornot,succpull2_ornot]\n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "\n",
    "        # analyze behavior results\n",
    "        # succ_rate_all_dates[idate] = np.sum(trial_record_clean[\"rewarded\"]>0)/np.shape(trial_record_clean)[0]\n",
    "        succ_rate_all_dates[idate] = np.sum((bhv_data['behavior_events']==3)|(bhv_data['behavior_events']==4))/np.sum((bhv_data['behavior_events']==1)|(bhv_data['behavior_events']==2))\n",
    "        trialnum_all_dates[idate] = np.shape(trial_record_clean)[0]\n",
    "        #\n",
    "        pullid = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"behavior_events\"])\n",
    "        pulltime = np.array(bhv_data[(bhv_data['behavior_events']==1) | (bhv_data['behavior_events']==2)][\"time_points\"])\n",
    "        pullid_diff = np.abs(pullid[1:] - pullid[0:-1])\n",
    "        pulltime_diff = pulltime[1:] - pulltime[0:-1]\n",
    "        interpull_intv = pulltime_diff[pullid_diff==1]\n",
    "        interpull_intv = interpull_intv[interpull_intv<10]\n",
    "        mean_interpull_intv = np.nanmean(interpull_intv)\n",
    "        std_interpull_intv = np.nanstd(interpull_intv)\n",
    "        #\n",
    "        interpullintv_all_dates[idate] = mean_interpull_intv\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2)\n",
    "        else:\n",
    "            pull1_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==2) \n",
    "            pull2_num_all_dates[idate] = np.sum(bhv_data['behavior_events']==1)\n",
    "\n",
    "        #\n",
    "        pulltime1 = np.array(bhv_data[(bhv_data['behavior_events']==1)]['time_points'])\n",
    "        pulltime2 = np.array(bhv_data[(bhv_data['behavior_events']==2)]['time_points'])\n",
    "        # \n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                pull1_intv_all_dates[idate] = np.nanmean(pulltime2[1:]-pulltime2[0:-1])\n",
    "                pull1_minintv_all_dates[idate] = np.nanmin(pulltime2[1:]-pulltime2[0:-1])\n",
    "            except:\n",
    "                pull1_intv_all_dates[idate] = np.nan\n",
    "                pull1_minintv_all_dates[idate] = np.nan\n",
    "            try:\n",
    "                pull2_intv_all_dates[idate] = np.nanmean(pulltime1[1:]-pulltime1[0:-1])\n",
    "                pull2_minintv_all_dates[idate] = np.nanmin(pulltime1[1:]-pulltime1[0:-1])\n",
    "            except:\n",
    "                pull2_intv_all_dates[idate] = np.nan\n",
    "                pull2_minintv_all_dates[idate] = np.nan\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        \n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        if 0: # old definition; not in use\n",
    "            trialnum_succ = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']>0])\n",
    "            bhv_data_succ = bhv_data[np.isin(bhv_data['trial_number'],trialnum_succ)]\n",
    "            #\n",
    "            time_point_pull1_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==1]\n",
    "            time_point_pull2_succ = bhv_data_succ[\"time_points\"][bhv_data_succ[\"behavior_events\"]==2]\n",
    "            time_point_pull1_succ = np.round(time_point_pull1_succ,1)\n",
    "            time_point_pull2_succ = np.round(time_point_pull2_succ,1)\n",
    "            #\n",
    "            trialnum_fail = np.array(trial_record_clean['trial_number'][trial_record_clean['rewarded']==0])\n",
    "            bhv_data_fail = bhv_data[np.isin(bhv_data['trial_number'],trialnum_fail)]\n",
    "            #\n",
    "            time_point_pull1_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==1]\n",
    "            time_point_pull2_fail = bhv_data_fail[\"time_points\"][bhv_data_fail[\"behavior_events\"]==2]\n",
    "            time_point_pull1_fail = np.round(time_point_pull1_fail,1)\n",
    "            time_point_pull2_fail = np.round(time_point_pull2_fail,1)\n",
    "        else:\n",
    "            # a new definition of successful and failed pulls\n",
    "            # separate successful and failed pulls\n",
    "            # step 1 all pull and juice\n",
    "            time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "            time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "            time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "            time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "            # step 2:\n",
    "            # pull 1\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "            # pull 2\n",
    "            # Find the last pull before each juice\n",
    "            successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "            # Convert to Pandas Series\n",
    "            successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "            # Find failed pulls (pulls that are not successful)\n",
    "            failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "            #\n",
    "            # step 3:\n",
    "            time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "            time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "            time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "            time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        #\n",
    "        # make sure all task code registered event, aka pulls, are within the video recording\n",
    "        ind_good_pull1 = time_point_pull1 < (totalsess_time - session_start_time)\n",
    "        time_point_pull1 = time_point_pull1[ind_good_pull1]\n",
    "        ind_good_pull2 = time_point_pull2 < (totalsess_time - session_start_time)\n",
    "        time_point_pull2 = time_point_pull2[ind_good_pull2]\n",
    "        #\n",
    "        ind_good_pull1_succ = time_point_pull1_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_succ = time_point_pull1_succ[ind_good_pull1_succ]\n",
    "        ind_good_pull2_succ = time_point_pull2_succ < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_succ = time_point_pull2_succ[ind_good_pull2_succ]\n",
    "        #\n",
    "        ind_good_pull1_fail = time_point_pull1_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull1_fail = time_point_pull1_fail[ind_good_pull1_fail]\n",
    "        ind_good_pull2_fail = time_point_pull2_fail < (totalsess_time - session_start_time)\n",
    "        time_point_pull2_fail = time_point_pull2_fail[ind_good_pull2_fail]\n",
    "        \n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        pull_infos_all_dates[date_tgt] = pull_infos\n",
    "        \n",
    "            \n",
    "        #\n",
    "        if np.isin(animal1,animal1_fixedorder):\n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "        else:            \n",
    "            owgaze1_num_all_dates[idate] = np.shape(oneway_gaze2)[0]\n",
    "            owgaze2_num_all_dates[idate] = np.shape(oneway_gaze1)[0]\n",
    "            mtgaze1_num_all_dates[idate] = np.shape(mutual_gaze2)[0]\n",
    "            mtgaze2_num_all_dates[idate] = np.shape(mutual_gaze1)[0]\n",
    "            \n",
    "\n",
    "        # define variables and use them to find the 'onset' of pull decision\n",
    "        print('project the high dimension bhv variables to PC space to define the start of the pull decision')\n",
    "        #\n",
    "        gausKernelsize = 16 # 4 or 16\n",
    "        # clean the data\n",
    "        time_point_pull1_temp = np.array(time_point_pull1)+session_start_time\n",
    "        time_point_pull1_temp = time_point_pull1_temp[time_point_pull1_temp<totalsess_time]\n",
    "        time_point_pull2_temp = np.array(time_point_pull2)+session_start_time\n",
    "        time_point_pull2_temp = time_point_pull2_temp[time_point_pull2_temp<totalsess_time]\n",
    "        #\n",
    "        # organize the data into a time series\n",
    "        pull1_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull1_data[np.round(time_point_pull1_temp*fps).astype(int)]=1\n",
    "        #\n",
    "        pull2_data = np.zeros([int(totalsess_time*fps),])\n",
    "        pull2_data[np.round(time_point_pull2_temp*fps).astype(int)]=1\n",
    "        \n",
    "        \n",
    "        # get the continuous variables\n",
    "        data_summary_twoanimals, data_summary_names = get_data_for_singlecam_conBhv_from_highDimension_to_PCspace(gausKernelsize, fps, animal1, animal2, \n",
    "                                                        animalnames_videotrack, session_start_time, \n",
    "                                                        time_point_pull1, time_point_pull2,\n",
    "                                                        time_point_juice1, time_point_juice2, \n",
    "                                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, \n",
    "                                                        output_look_ornot, output_allvectors, \n",
    "                                                        output_allangles, output_key_locations)\n",
    "        #\n",
    "        vars_toPCA_names = ['gaze_other_angle', 'gaze_tube_angle', 'gaze_lever_angle', 'animal_animal_dist',\n",
    "                            'animal_tube_dist', 'animal_lever_dist', 'mass_move_speed', 'gaze_angle_speed',]\n",
    "        #\n",
    "        indices = [data_summary_names.index(name) for name in vars_toPCA_names]\n",
    "        # \n",
    "        # reduce to smaller dimension using PCA; for animal 1\n",
    "        allbhvs_a1 = np.array(data_summary_twoanimals[animal1])[indices,:]\n",
    "        data_for_pca = allbhvs_a1.T\n",
    "        # Normalize the data (Z-score scaling)\n",
    "        # Each of the 8 variables will now have a mean of 0 and a standard deviation of 1\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data_for_pca)\n",
    "        # Initialize and run PCA\n",
    "        # We want to reduce the 8 variables to 3 principal components\n",
    "        pca = PCA(n_components=3)\n",
    "        # Fit the model and transform the data\n",
    "        principal_components = pca.fit_transform(data_scaled)\n",
    "        principal_components_transposed = principal_components.T\n",
    "        # Get the explained variance for each component\n",
    "        explained_variance_a1 = pca.explained_variance_ratio_\n",
    "        #\n",
    "        PC1_a1 = principal_components_transposed[0,:]\n",
    "        # loadings\n",
    "        PC1_a1_loadings = pca.components_[0]\n",
    "        # \n",
    "        # reduce to smaller dimension using PCA; for animal 2\n",
    "        allbhvs_a2 = np.array(data_summary_twoanimals[animal2])[indices,:]\n",
    "        data_for_pca = allbhvs_a2.T\n",
    "        # Normalize the data (Z-score scaling)\n",
    "        # Each of the 8 variables will now have a mean of 0 and a standard deviation of 1\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data_for_pca)\n",
    "        # Initialize and run PCA\n",
    "        # We want to reduce the 8 variables to 3 principal components\n",
    "        pca = PCA(n_components=3)\n",
    "        # Fit the model and transform the data\n",
    "        principal_components = pca.fit_transform(data_scaled)\n",
    "        principal_components_transposed = principal_components.T\n",
    "        # Get the explained variance for each component\n",
    "        explained_variance_a2 = pca.explained_variance_ratio_\n",
    "        #\n",
    "        PC1_a2 = principal_components_transposed[0,:]\n",
    "        # loadings\n",
    "        PC1_a2_loadings = pca.components_[0]\n",
    "        #\n",
    "        # put the PC1 loading and PC123 explained variance together and save \n",
    "        bhvtoPC1_loadings_all_dates[date_tgt][animal1] = PC1_a1_loadings\n",
    "        bhvtoPC1_loadings_all_dates[date_tgt][animal2] = PC1_a2_loadings\n",
    "        #\n",
    "        bhvPC123explained_all_dates[date_tgt][animal1] = explained_variance_a1\n",
    "        bhvPC123explained_all_dates[date_tgt][animal2] = explained_variance_a2\n",
    "        \n",
    "        #\n",
    "        # use one of the two methods; not the HMM based method\n",
    "        if not doHMMmethod:\n",
    "            if not doOnsetAfterMin:\n",
    "                # find the transitional time point of angle speed and speed in IPI\n",
    "                speed1_increase = find_sharp_increases_withinIPI(pull1_data,PC1_a1,session_start_time,fps)\n",
    "                pull1_action_onset_frames = speed1_increase\n",
    "                #\n",
    "                speed2_increase = find_sharp_increases_withinIPI(pull2_data,PC1_a2,session_start_time,fps)\n",
    "                pull2_action_onset_frames = speed2_increase\n",
    "            #\n",
    "            elif doOnsetAfterMin:\n",
    "                # find the transitional time point of angle speed and speed in IPI\n",
    "                speed1_increase = find_rising_onset_after_min_withinIPI(pull1_data,PC1_a1,session_start_time,fps)\n",
    "                pull1_action_onset_frames = speed1_increase\n",
    "                #\n",
    "                speed2_increase = find_rising_onset_after_min_withinIPI(pull2_data,PC1_a2,session_start_time,fps)\n",
    "                pull2_action_onset_frames = speed2_increase\n",
    "        #\n",
    "        elif doHMMmethod:\n",
    "            n_states = 3\n",
    "            \n",
    "            pull1_action_onset_framepoints, _ = get_trial_start_frames_from_HMM(speed1_data, anglespeed1_data, pull1_data, \n",
    "                                                                                fps, session_start_time, n_states)\n",
    "            pull1_action_onset_frames = np.isin(np.arange(len(pull1_data)), pull1_action_onset_framepoints).astype(int)\n",
    "            #\n",
    "            pull2_action_onset_framepoints, _ = get_trial_start_frames_from_HMM(speed2_data, anglespeed2_data, pull2_data, \n",
    "                                                                                fps, session_start_time, n_states)\n",
    "            pull2_action_onset_frames = np.isin(np.arange(len(pull2_data)), pull2_action_onset_framepoints).astype(int)\n",
    "\n",
    "            \n",
    "        #\n",
    "        # store the pull reaction time information\n",
    "        # temporary fix\n",
    "        try:\n",
    "            pull_data_points = np.where(pull1_data)[0]\n",
    "            pullonset_data_points = np.where(pull1_action_onset_frames)[0]\n",
    "            pull1_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "            pull_rts_all_dates[date_tgt][animal1] = pull1_rt\n",
    "            #\n",
    "            pull_data_points = np.where(pull2_data)[0]\n",
    "            pullonset_data_points = np.where(pull2_action_onset_frames)[0]\n",
    "            pull2_rt = (pull_data_points - pullonset_data_points)/fps\n",
    "            pull_rts_all_dates[date_tgt][animal2] = pull2_rt\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # replace time_point_pull_xxx to the pull onset\n",
    "        time_point_pull1 = np.array(np.round(time_point_pull1,1))\n",
    "        time_point_pull2 = np.array(np.round(time_point_pull2,1))\n",
    "        time_point_pull1_succ = np.array(time_point_pull1_succ)\n",
    "        time_point_pull2_succ = np.array(time_point_pull2_succ)\n",
    "        time_point_pull1_fail = np.array(time_point_pull1_fail)\n",
    "        time_point_pull2_fail = np.array(time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1_succ_idx = np.isin(time_point_pull1,time_point_pull1_succ)\n",
    "        time_point_pull2_succ_idx = np.isin(time_point_pull2,time_point_pull2_succ)\n",
    "        time_point_pull1_fail_idx = np.isin(time_point_pull1,time_point_pull1_fail)\n",
    "        time_point_pull2_fail_idx = np.isin(time_point_pull2,time_point_pull2_fail)\n",
    "        #\n",
    "        time_point_pull1 = np.where(pull1_action_onset_frames)[0]/fps - session_start_time\n",
    "        time_point_pull2 = np.where(pull2_action_onset_frames)[0]/fps - session_start_time\n",
    "        #\n",
    "        time_point_pull1_succ = time_point_pull1[time_point_pull1_succ_idx]\n",
    "        time_point_pull2_succ = time_point_pull2[time_point_pull2_succ_idx]\n",
    "        time_point_pull1_fail = time_point_pull1[time_point_pull1_fail_idx]\n",
    "        time_point_pull2_fail = time_point_pull2[time_point_pull2_fail_idx]\n",
    "        #\n",
    "        pull1_rt_succ = pull1_rt[time_point_pull1_succ_idx]\n",
    "        pull2_rt_succ = pull2_rt[time_point_pull2_succ_idx]\n",
    "        pull1_rt_fail = pull1_rt[time_point_pull1_fail_idx]\n",
    "        pull2_rt_fail = pull2_rt[time_point_pull2_fail_idx]\n",
    "        \n",
    "        \n",
    "        # plot key continuous behavioral variables\n",
    "        if 1:\n",
    "            print('plot self pull start triggered bhv variables')\n",
    "            \n",
    "            filepath_cont_var = data_saved_folder+'bhv_events_continuous_variables_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]+'/'+cameraID+'/'+date_tgt+'/'\n",
    "            if not os.path.exists(filepath_cont_var):\n",
    "                os.makedirs(filepath_cont_var)\n",
    "                        \n",
    "            min_length = np.shape(look_at_other_or_not_merge['dodson'])[0] # frame numbers of the video recording\n",
    "\n",
    "            # NOTE! This one used the wrong and old version of separating successful and failed \n",
    "            pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                    pull1_rt, pull2_rt, animal1, animal2, \n",
    "                                    session_start_time, min_length, succpulls_ornot, time_point_pull1, time_point_pull2, \n",
    "                                    oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                    output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "            pullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            \n",
    "            # successful pull\n",
    "            try:\n",
    "                pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                        pull1_rt_succ, pull2_rt_succ, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_succ, time_point_pull2_succ, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                succpullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                succpullstartTopull_trig_events_all_dates[date_tgt] = np.nan\n",
    "            \n",
    "            # failed pull\n",
    "            try:\n",
    "                pull_trig_events_summary = plot_continuous_bhv_var_singlecam_PullStartToPull_variedSection_highbhvDimension_to_lowPCspace(\n",
    "                                        pull1_rt_fail, pull2_rt_fail, animal1, animal2, \n",
    "                                        session_start_time, min_length, succpulls_ornot, \n",
    "                                        time_point_pull1_fail, time_point_pull2_fail, \n",
    "                                        oneway_gaze1, oneway_gaze2, mutual_gaze1, mutual_gaze2, animalnames_videotrack,\n",
    "                                        output_look_ornot, output_allvectors, output_allangles,output_key_locations)\n",
    "                failpullstartTopull_trig_events_all_dates[date_tgt] = pull_trig_events_summary\n",
    "            except:\n",
    "                failpullstartTopull_trig_events_all_dates[date_tgt] = np.nan\n",
    "                \n",
    "        \n",
    "        # session starting time compared with the neural recording\n",
    "        session_start_time_niboard_offset = ni_data['session_t0_offset'] # in the unit of second\n",
    "        try:\n",
    "            neural_start_time_niboard_offset = ni_data['trigger_ts'][0]['elapsed_time'] # in the unit of second\n",
    "        except: # for the multi-animal recording setup\n",
    "            neural_start_time_niboard_offset = next(\n",
    "                entry['timepoints'][0]['elapsed_time']\n",
    "                for entry in ni_data['trigger_ts']\n",
    "                if entry['channel_name'] == f\"{trig_channelname}\")\n",
    "        neural_start_time_session_start_offset = neural_start_time_niboard_offset-session_start_time_niboard_offset\n",
    "    \n",
    "    \n",
    "        # load channel maps\n",
    "        channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32.mat'\n",
    "        # channel_map_file = '/home/ws523/kilisort_spikesorting/Channel-Maps/Neuronexus_whitematter_2x32_kilosort4_new.mat'\n",
    "        channel_map_data = scipy.io.loadmat(channel_map_file)\n",
    "            \n",
    "        # # load spike sorting results\n",
    "        if 1:\n",
    "            print('load spike data for '+neural_record_condition)\n",
    "            if kilosortver == 2:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            elif kilosortver == 4:\n",
    "                spike_time_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_times.npy'\n",
    "                spike_time_data = np.load(spike_time_file)\n",
    "            # \n",
    "            # align the FR recording time stamps\n",
    "            spike_time_data = spike_time_data + fs_spikes*neural_start_time_session_start_offset\n",
    "            # down-sample the spike recording resolution to 30Hz\n",
    "            spike_time_data = spike_time_data/fs_spikes*fps\n",
    "            spike_time_data = np.round(spike_time_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/Kilosort/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            elif kilosortver == 4:\n",
    "                spike_clusters_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/spike_clusters.npy'\n",
    "                spike_clusters_data = np.load(spike_clusters_file)\n",
    "                spike_channels_data = np.copy(spike_clusters_data)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_maps_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_map.npy'\n",
    "                channel_maps_data = np.load(channel_maps_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/Kilosort/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            elif kilosortver == 4:\n",
    "                channel_pos_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/channel_positions.npy'\n",
    "                channel_pos_data = np.load(channel_pos_file)\n",
    "            #\n",
    "            if kilosortver == 2:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/Kilosort/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            elif kilosortver == 4:\n",
    "                clusters_info_file = neural_data_folder+neural_record_condition+'/kilosort4_6500HzNotch/cluster_info.tsv'\n",
    "                clusters_info_data = pd.read_csv(clusters_info_file,sep=\"\\t\")\n",
    "            #\n",
    "            # only get the spikes that are manually checked\n",
    "            try:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['cluster_id'].values\n",
    "            except:\n",
    "                good_clusters = clusters_info_data[(clusters_info_data.group=='good')|(clusters_info_data.group=='mua')]['id'].values\n",
    "            #\n",
    "            clusters_info_data = clusters_info_data[~pd.isnull(clusters_info_data.group)]\n",
    "            #\n",
    "            spike_time_data = spike_time_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_channels_data = spike_channels_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            spike_clusters_data = spike_clusters_data[np.isin(spike_clusters_data,good_clusters)]\n",
    "            \n",
    "            #\n",
    "            nclusters = np.shape(clusters_info_data)[0]\n",
    "            #\n",
    "            for icluster in np.arange(0,nclusters,1):\n",
    "                try:\n",
    "                    cluster_id = clusters_info_data['id'].iloc[icluster]\n",
    "                except:\n",
    "                    cluster_id = clusters_info_data['cluster_id'].iloc[icluster]\n",
    "                spike_channels_data[np.isin(spike_clusters_data,cluster_id)] = clusters_info_data['ch'].iloc[icluster]   \n",
    "            # \n",
    "            # get the channel to depth information, change 2 shanks to 1 shank \n",
    "            try:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1]*2,channel_pos_data[channel_pos_data[:,0]==1,1]*2+1])\n",
    "                # channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "            except:\n",
    "                channel_depth=np.hstack([channel_pos_data[channel_pos_data[:,0]==0,1],channel_pos_data[channel_pos_data[:,0]==31.2,1]])            \n",
    "                # channel_to_depth = np.vstack([channel_maps_data.T[0],channel_depth])\n",
    "                channel_to_depth = np.vstack([channel_maps_data.T,channel_depth])\n",
    "                channel_to_depth[1] = channel_to_depth[1]/30-64 # make the y axis consistent\n",
    "            #\n",
    "           \n",
    "            \n",
    "            # calculate the firing rate\n",
    "            # FR_kernel = 0.20 # in the unit of second\n",
    "            FR_kernel = 1/30 # in the unit of second # 1/30 same resolution as the video recording\n",
    "            # FR_kernel is sent to to be this if want to explore it's relationship with continuous trackng data\n",
    "            \n",
    "            totalsess_time_forFR = np.floor(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30)  # to match the total time of the video recording\n",
    "            _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps, FR_kernel, totalsess_time_forFR,\n",
    "                                                                                          spike_clusters_data, spike_time_data)\n",
    "            # _,FR_timepoint_allch,FR_allch,FR_zscore_allch = spike_analysis_FR_calculation(fps,FR_kernel,totalsess_time_forFR,\n",
    "            #                                                                              spike_channels_data, spike_time_data)\n",
    "            # behavioral events aligned firing rate for each unit\n",
    "            if 1: \n",
    "                print('plot event aligned firing rate; pull start focus')\n",
    "                #\n",
    "                #\n",
    "                gaze_thresold = 0.2 # min length threshold to define if a gaze is real gaze or noise, in the unit of second \n",
    "                #\n",
    "                bhvevents_aligned_FR_allevents_all = plot_bhv_events_aligned_FR_PullStartToPull_variedSection(\n",
    "                                           animal1, animal2,time_point_pull1,time_point_pull2,time_point_pulls_succfail,\n",
    "                                           oneway_gaze1,oneway_gaze2,mutual_gaze1,mutual_gaze2,gaze_thresold,totalsess_time_forFR,\n",
    "                                           pull1_rt, pull2_rt,fps,FR_timepoint_allch,FR_zscore_allch,clusters_info_data)\n",
    "                \n",
    "                bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt] = bhvevents_aligned_FR_allevents_all\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        \n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "                \n",
    "        # with open(data_saved_subfolder+'/DBN_input_data_alltypes_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        #     pickle.dump(DBN_input_data_alltypes, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/owgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/owgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(owgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/mtgaze2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(mtgaze2_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull1_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_num_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_num_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_num_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull1_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull1_intv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/pull2_minintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull2_intv_all_dates, f)\n",
    "\n",
    "        with open(data_saved_subfolder+'/tasktypes_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(tasktypes_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/coopthres_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(coopthres_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/succ_rate_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succ_rate_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/interpullintv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(interpullintv_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/trialnum_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(trialnum_all_dates, f)\n",
    "        with open(data_saved_subfolder+'/bhv_intv_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhv_intv_all_dates, f)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_infos_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_infos_all_dates, f)   \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pull_rts_all_dates, f)  \n",
    "        \n",
    "        with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(succpullstartTopull_trig_events_all_dates, f) \n",
    "        with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(failpullstartTopull_trig_events_all_dates, f) \n",
    "\n",
    "        with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates, f) \n",
    "        \n",
    "        with open(data_saved_subfolder+'/bhvtoPC1_loadings_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(bhvtoPC1_loadings_all_dates, f) \n",
    "            \n",
    "    \n",
    "    \n",
    "    # only save a subset \n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        if 1:\n",
    "            with open(data_saved_subfolder+'/bhvtoPC1_loadings_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(bhvtoPC1_loadings_all_dates, f) \n",
    "                \n",
    "            with open(data_saved_subfolder+'/bhvPC123explained_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(bhvPC123explained_all_dates, f) \n",
    "        \n",
    "        if 1:\n",
    "            with open(data_saved_subfolder+'/pull_rts_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(pull_rts_all_dates, f)  \n",
    "\n",
    "            with open(data_saved_subfolder+'/pullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(pullstartTopull_trig_events_all_dates, f) \n",
    "            with open(data_saved_subfolder+'/succpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(succpullstartTopull_trig_events_all_dates, f) \n",
    "            with open(data_saved_subfolder+'/failpullstartTopull_trig_events_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(failpullstartTopull_trig_events_all_dates, f) \n",
    "\n",
    "            with open(data_saved_subfolder+'/bhvevents_pullstartTopull_aligned_FR_allevents_all_dates_'+doOnsetAfterMin_suffix+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "                pickle.dump(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates, f) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b95393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvtoPC1_loadings_all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29868de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvPC123explained_all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d57f878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pullstartTopull_trig_events_all_dates['20250430_SR'][('dodson','self_PC1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23635e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(pull_trig_events_summary[('dodson','self_PC1')][3])\n",
    "# plt.plot(pull_trig_events_summary[('dodson','mass_move_speed')][3]/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "daa6da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull_trig_events_summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdf9a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simple sanity check plot\n",
    "if 0:\n",
    "    date_toplot = '20240808'\n",
    "    event_id = 16\n",
    "    #\n",
    "    a1 = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_toplot]['kanga pull']['8']['FR_allevents'][event_id]\n",
    "    a2 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'socialgaze_prob')][event_id]\n",
    "    a3 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'selfpull_prob')][event_id]\n",
    "    a4 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'mass_move_speed')][event_id]/500\n",
    "    a5 = pullstartTopull_trig_events_all_dates[date_toplot][('kanga', 'gaze_angle_speed')][event_id]\n",
    "\n",
    "    plt.plot(a1)\n",
    "    plt.plot(a2)\n",
    "    plt.plot(a4)\n",
    "    plt.plot(a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdde8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pullstartTopull_trig_events_all_dates[date_toplot].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e74b85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bhvPCA_PullOnsetAfterMin_'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doOnsetAfterMin_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9e615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852ade69",
   "metadata": {},
   "source": [
    "## Behavioral variables PCA summary plot\n",
    "### for the target sessions, plot the PC1,2,3's explaining percentage and the variables' loading on PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a98d933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# animal_to_ana = 'dodson'\n",
    "animal_to_ana = 'kanga'\n",
    "\n",
    "#\n",
    "# conditions_to_ana = np.unique(task_conditions)\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho',]\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'SR', 'SR_withDodson' ]\n",
    "# \n",
    "# for dodson only\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                      'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                       'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "#\n",
    "# conditions_to_ana = ['MC', \n",
    "#               'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', 'SR', 'SR_withGingerNew', 'SR_withKanga',\n",
    "#              'SR_withKoala',  ]\n",
    "\n",
    "condition_name = 'allMC'\n",
    "\n",
    "vars_toPCA_names = ['gaze_other_angle', 'gaze_tube_angle', 'gaze_lever_angle', 'animal_animal_dist',\n",
    "                    'animal_tube_dist', 'animal_lever_dist', 'mass_move_speed', 'gaze_angle_speed',]\n",
    "\n",
    "bhvtoPC1_loadings_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',])\n",
    "bhvPC123explained_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal',])\n",
    "\n",
    "#\n",
    "for idate in np.arange(0,ndates,1):\n",
    "    date_tgt = dates_list[idate]\n",
    "    task_condition = task_conditions[idate]\n",
    "\n",
    "    # only analyze the targeted conditions\n",
    "    if not np.isin(task_condition,conditions_to_ana):\n",
    "        continue\n",
    "        \n",
    "    # remove empty entries\n",
    "    # Get the specific dictionary for the date\n",
    "    loadings_dict = bhvtoPC1_loadings_all_dates[date_tgt]\n",
    "    # Create a new dictionary, keeping only items where the value has a length greater than 0\n",
    "    filtered_loadings = {key: value for key, value in loadings_dict.items() if len(value) > 0}\n",
    "    # Update the original structure with the filtered dictionary\n",
    "    bhvtoPC1_loadings_all_dates[date_tgt] = filtered_loadings\n",
    "    #\n",
    "    # Get the specific dictionary for the date\n",
    "    loadings_dict = bhvPC123explained_all_dates[date_tgt]\n",
    "    # Create a new dictionary, keeping only items where the value has a length greater than 0\n",
    "    filtered_loadings = {key: value for key, value in loadings_dict.items() if len(value) > 0}\n",
    "    # Update the original structure with the filtered dictionary\n",
    "    bhvPC123explained_all_dates[date_tgt] = filtered_loadings\n",
    "    \n",
    "    \n",
    "    #    \n",
    "    # pca loading for self animal\n",
    "    if 0:\n",
    "        bhvtoPC1_loadings_idate = bhvtoPC1_loadings_all_dates[date_tgt][animal_to_ana]\n",
    "        bhvPC123explained_idate = bhvPC123explained_all_dates[date_tgt][animal_to_ana]\n",
    "\n",
    "    # it's more important to look at pca loading for partner animal\n",
    "    animal_names = list(bhvtoPC1_loadings_all_dates[date_tgt].keys())\n",
    "    partner_ani_name = animal_names[np.where(~np.isin(animal_names,animal_to_ana))[0][0]]\n",
    "    if 1:\n",
    "        bhvtoPC1_loadings_idate = bhvtoPC1_loadings_all_dates[date_tgt][partner_ani_name]\n",
    "        bhvPC123explained_idate = bhvPC123explained_all_dates[date_tgt][partner_ani_name]\n",
    "\n",
    "    #\n",
    "    nvaris = np.shape(vars_toPCA_names)[0]\n",
    "    for ivari in np.arange(0,nvaris,1):\n",
    "        bhvtoPC1_loadings_all_dates_df = bhvtoPC1_loadings_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                            'condition': task_condition,\n",
    "                                                                            'act_animal': animal_to_ana,\n",
    "                                                                            'bhv_name': vars_toPCA_names[ivari],\n",
    "                                                                            'PC1_loading': bhvtoPC1_loadings_idate[ivari],\n",
    "                                                                            }, ignore_index=True)\n",
    "        \n",
    "    # \n",
    "    for iPC in np.arange(0,3,1):\n",
    "        bhvPC123explained_all_dates_df = bhvPC123explained_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                            'condition': task_condition,\n",
    "                                                                            'act_animal': animal_to_ana,\n",
    "                                                                            'PCnumber': iPC+1,\n",
    "                                                                            'PCexplained': bhvPC123explained_idate[iPC],\n",
    "                                                                            }, ignore_index=True)\n",
    "\n",
    "#\n",
    "bhvPC123explained_all_dates_df['cumulative_explained'] = bhvPC123explained_all_dates_df.sort_values('PCnumber') \\\n",
    "                                                                                    .groupby('dates')['PCexplained'] \\\n",
    "                                                                                    .cumsum()\n",
    "        \n",
    "# do the plot\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Create a figure with two subplots, arranged vertically (2 rows, 1 column)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
    "# --- Subplot 1: PC1 Loadings ---\n",
    "seaborn.barplot(\n",
    "    data=bhvtoPC1_loadings_all_dates_df,\n",
    "    x='bhv_name',\n",
    "    y='PC1_loading',\n",
    "    order=vars_toPCA_names,\n",
    "    # palette='viridis',\n",
    "    ax=ax1  # Specify that this plot goes on the first axis\n",
    ")\n",
    "ax1.set_title('PC1 Loadings for Each Behavior', fontsize=16)\n",
    "ax1.set_xlabel('Behavior Name', fontsize=12)\n",
    "ax1.set_ylabel('PC1 Loading', fontsize=12)\n",
    "ax1.axhline(0, color='grey', linestyle='--')\n",
    "ax1.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "# --- Subplot 2: Variance Explained by PC1, PC2, PC3 ---\n",
    "seaborn.lineplot(\n",
    "    data=bhvPC123explained_all_dates_df,\n",
    "    x='PCnumber',\n",
    "    y='cumulative_explained',\n",
    "    marker='o',\n",
    "    err_style=\"bars\",\n",
    "    errorbar=('ci', 95),\n",
    "    err_kws={'capsize': 5},  # This is the corrected way to add caps\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "# 3. Update titles and labels to reflect the new plot\n",
    "ax2.set_title('Cumulative Variance Explained by Top PCs', fontsize=16)\n",
    "ax2.set_xlabel('Number of Principal Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "# Set x-axis ticks to be integers (1, 2, 3)\n",
    "ax2.set_xticks([1, 2, 3])\n",
    "# Format the y-axis to show percentages\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax2.set_ylim(0, 1) # Set y-axis limit from 0% to 100%\n",
    "# Adjust layout to prevent plots from overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig = 1\n",
    "if savefig:\n",
    "    figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_makeBhvNeuronVideos_PullStartToPull_section_continuousBhv_PCAonBhv/\" + \\\n",
    "                    cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + \"/bhvvariables_PCA_summary_fig/\"\n",
    "\n",
    "    if not os.path.exists(figsavefolder):\n",
    "        os.makedirs(figsavefolder)\n",
    "\n",
    "    if 0:\n",
    "        plt.savefig(figsavefolder + animal_to_ana + '_in_' + condition_name +\n",
    "                '_bhvvariables_PC123explained_PC1loadings.pdf')\n",
    "    else:\n",
    "        plt.savefig(figsavefolder + animal_to_ana + '_partner_in_' + condition_name +\n",
    "                '_bhvvariables_PC123explained_PC1loadings.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d17dcc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dodson': array([-0.41005064,  0.40134345, -0.15698453, -0.44981192,  0.4999138 ,\n",
       "         0.29603008,  0.16213453,  0.28279284]),\n",
       " 'gingerNew': [],\n",
       " 'ginger': array([-0.42159477,  0.41918282,  0.17854957, -0.42678769,  0.45891646,\n",
       "         0.41588089, -0.19469522, -0.10511173])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhvtoPC1_loadings_all_dates[date_tgt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ea6915f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isin(animal_names,animal_to_ana))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031784f",
   "metadata": {},
   "source": [
    "## Organize the data\n",
    "### put all the target data together for further analysis\n",
    "### also organize and save the data for the hddm analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose one pull_trig_events type to work with\n",
    "# options: ['gaze_other_angle','gaze_tube_angle','gaze_lever_angle','animal_animal_dist',\n",
    "#           'animal_tube_dist','animal_lever_dist','othergaze_self_angle',\n",
    "#           'mass_move_speed','gaze_angle_speed','otherani_otherlever_dist',\n",
    "#           'socialgaze_prob','othergaze_prob']\n",
    "#\n",
    "# pull_trig_events_tgtname = 'otherani_otherlever_dist' \n",
    "pull_trig_events_tgtname = 'socialgaze_prob' # for testing if individual trial different was from gaze start time\n",
    "# pull_trig_events_tgtname = 'othergaze_prob' # if to test things aligned to partner's pull (in that case, the subject's gaze becomes othergaze) \n",
    "\n",
    "\n",
    "# Keep these as additional controls\n",
    "pull_trig_otherpull_name = 'otherpull_prob'\n",
    "pull_trig_selfpull_name = 'selfpull_prob'\n",
    "# pull_trig_selfspeed_name = 'mass_move_speed'\n",
    "# pull_trig_otherspeed_name = 'other_mass_move_speed'\n",
    "pull_trig_selfspeed_name = 'self_PC1'\n",
    "pull_trig_otherspeed_name = 'other_PC1'\n",
    "pull_time_pre_reward_name = 'time_from_last_reward'\n",
    "prefail_pull_num_name = 'num_preceding_failpull'\n",
    "pull_rt_name = 'pull_rt'\n",
    "\n",
    "#\n",
    "animal_to_ana = 'dodson'\n",
    "# animal_to_ana = 'kanga'\n",
    "\n",
    "#\n",
    "# conditions_to_ana = np.unique(task_conditions)\n",
    "# conditions_to_ana = ['MC',]\n",
    "# conditions_to_ana = ['SR']\n",
    "# conditions_to_ana = ['MC_DannonAuto']\n",
    "#\n",
    "# for Kanga only\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho',]\n",
    "# conditions_to_ana = ['MC', 'MC_DannonAuto', 'MC_KangaAuto', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'NV', ]\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', \n",
    "#                      'MC_withKoala', 'MC_withVermelho', 'SR', 'SR_withDodson' ]\n",
    "# \n",
    "# for dodson only\n",
    "conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                      'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "# conditions_to_ana = ['MC', 'MC_DodsonAuto_withKoala', 'MC_KoalaAuto_withKoala',\n",
    "#                       'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala',  ]\n",
    "#\n",
    "# conditions_to_ana = ['MC', \n",
    "#               'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', 'SR', 'SR_withGingerNew', 'SR_withKanga',\n",
    "#              'SR_withKoala',  ]\n",
    "\n",
    "condition_name = 'allMC'\n",
    "\n",
    "bhvevents_aligned_FR_allevents_all_dates_df = pd.DataFrame(columns=['dates','condition','act_animal','bhv_name',\n",
    "                                                                    'succrate','clusterID',\n",
    "                                                                    'channelID','FR_ievent'])\n",
    "\n",
    "try:\n",
    "\n",
    "    # dummy \n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_RawFullDatas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\\\n",
    "                  animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        bhvevents_aligned_FR_allevents_all_dates_df = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "        date_tgt = dates_list[idate]\n",
    "        task_condition = task_conditions[idate]\n",
    "        \n",
    "        # only analyze the targeted conditions\n",
    "        if not np.isin(task_condition,conditions_to_ana):\n",
    "            continue\n",
    "    \n",
    "        succrate = succ_rate_all_dates[idate]\n",
    "        \n",
    "        bhv_types = list(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt].keys())\n",
    "    \n",
    "        for ibhv_type in bhv_types:\n",
    "    \n",
    "            clusterIDs = list(bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type].keys())\n",
    "    \n",
    "            ibhv_type_split = ibhv_type.split()\n",
    "            if np.shape(ibhv_type_split)[0]==3:\n",
    "                ibhv_type_split[1] = ibhv_type_split[1]+'_'+ibhv_type_split[2]\n",
    "    \n",
    "            # only analyze targeted action animal\n",
    "            if not np.isin(ibhv_type_split[0], animal_to_ana):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # load the pull_trig_continuous_events\n",
    "            try:\n",
    "                pull_trig_events_tgt = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_events_tgtname)]\n",
    "                pull_trig_otherpull = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherpull_name)]\n",
    "                pull_trig_selfpull = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfpull_name)]\n",
    "                pull_trig_selfspeed = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_selfspeed_name)]\n",
    "                pull_trig_otherspeed = pullstartTopull_trig_events_all_dates[date_tgt][(ibhv_type_split[0],pull_trig_otherspeed_name)]\n",
    "                #\n",
    "                pull_trig_prerewardtime = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],pull_time_pre_reward_name)]))\n",
    "                prefail_pull_num = np.array(list(pull_infos_all_dates[date_tgt][(ibhv_type_split[0],prefail_pull_num_name)]))\n",
    "                #\n",
    "                pull_rt = np.array(list(pull_rts_all_dates[date_tgt][ibhv_type_split[0]]))\n",
    "                #\n",
    "                pull_outcome = np.hstack([1-(prefail_pull_num[1:]>0).astype(int),np.nan])\n",
    "            except:\n",
    "                pull_trig_events_tgt = np.nan\n",
    "                pull_trig_otherpull = np.nan\n",
    "                pull_trig_selfpull = np.nan\n",
    "                pull_trig_selfspeed = np.nan\n",
    "                pull_trig_otherspeed = np.nan\n",
    "                #\n",
    "                pull_trig_prerewardtime = np.nan\n",
    "                prefail_pull_num = np.nan\n",
    "                pull_rt = np.nan\n",
    "            \n",
    "                \n",
    "            for iclusterID in clusterIDs:   \n",
    "    \n",
    "                ichannelID = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['ch']\n",
    "                iFR_allevents = bhvevents_pullstartTopull_aligned_FR_allevents_all_dates[date_tgt][ibhv_type][iclusterID]['FR_allevents']\n",
    "    \n",
    "                #\n",
    "                nevents = np.shape([len(x) for x in pull_trig_events_tgt])[0]\n",
    "                \n",
    "                for ievent in np.arange(0,nevents,1):\n",
    "                \n",
    "                    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df.append({'dates': date_tgt, \n",
    "                                                                                        'condition':task_condition,\n",
    "                                                                                        'act_animal':ibhv_type_split[0],\n",
    "                                                                                        'bhv_name': ibhv_type_split[1],\n",
    "                                                                                        'bhv_id':ievent,\n",
    "                                                                                        'succrate':succrate[0],\n",
    "                                                                                        'clusterID':iclusterID,\n",
    "                                                                                        'channelID':ichannelID,\n",
    "                                                                                        'FR_ievent':iFR_allevents[ievent],\n",
    "                                                                                         pull_trig_events_tgtname:pull_trig_events_tgt[ievent],                          \n",
    "                                                                                         pull_trig_otherpull_name:pull_trig_otherpull[ievent],                          \n",
    "                                                                                         pull_trig_selfpull_name:pull_trig_selfpull[ievent],\n",
    "                                                                                         pull_trig_otherspeed_name:pull_trig_otherspeed[ievent],                          \n",
    "                                                                                         pull_trig_selfspeed_name:pull_trig_selfspeed[ievent],\n",
    "                                                                                                                      \n",
    "                                                                                         pull_time_pre_reward_name:pull_trig_prerewardtime[ievent],\n",
    "                                                                                         prefail_pull_num_name:prefail_pull_num[ievent],\n",
    "                                                                                         pull_rt_name:pull_rt[ievent],\n",
    "                                                                                                                      \n",
    "                                                                                         'pull_outcome': pull_outcome[ievent],\n",
    "                                                                                        }, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # save the data with other HDDM dataframes for the modeling\n",
    "    savedata = 1\n",
    "    \n",
    "    if savedata:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "            '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "                os.makedirs(data_saved_subfolder)\n",
    "    \n",
    "        with open(data_saved_subfolder+'/hddm_RawFullDatas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\\\n",
    "                  animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(bhvevents_aligned_FR_allevents_all_dates_df, f) \n",
    "\n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original dataframe for reference \n",
    "bhvevents_aligned_FR_allevents_all_dates_df_origin = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbb68c-ee11-412f-bd35-6d96b1c60fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df_origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22792829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some new columns\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['previous_pull_outcome'] = (\n",
    "        bhvevents_aligned_FR_allevents_all_dates_df['num_preceding_failpull'] == 0\n",
    "    ).astype(int)\n",
    "\n",
    "# Compute AUC for social gaze probability\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].apply(\n",
    "#     lambda x: np.trapz(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    "# )\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].apply(\n",
    "    lambda x: np.nansum(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] = \\\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc']/bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']\n",
    "\n",
    "# Mean and STD for mass_move_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['self_PC1_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['self_PC1'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['self_PC1_std'] = bhvevents_aligned_FR_allevents_all_dates_df['self_PC1'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Mean and STD for other_mass_move_speed\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_PC1_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['other_PC1'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['other_PC1_std'] = bhvevents_aligned_FR_allevents_all_dates_df['other_PC1'].apply(\n",
    "    lambda x: np.nanstd(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# firing rate mean and slope before pull (0.85s before)\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "# Compute mean firing rate\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_mean'] = bhvevents_aligned_FR_allevents_all_dates_df['FR_ievent'].apply(\n",
    "    lambda x: np.nanmean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# \n",
    "# Compute the slope, in two ways\n",
    "#\n",
    "# Make a clean copy to avoid SettingWithCopyWarning\n",
    "df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "\n",
    "# === Fixed slope before 0.85s from pull ===\n",
    "pull_margin_frames = int(0.85 * fps)\n",
    "\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        try:\n",
    "            slope, *_ = linregress(x, y)\n",
    "            return abs(slope)\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "df.loc[:, 'fr_slope'] = df['FR_ievent'].apply(compute_fr_slope)\n",
    "\n",
    "# === Peak-based slope calculation: use shortest trial ===\n",
    "\n",
    "# Prepare new columns for slopes and peak times\n",
    "df['fr_slope_peakbased'] = np.nan\n",
    "df['fr_peak_time'] = np.nan\n",
    "\n",
    "# Group by neuron\n",
    "grouped = df.groupby(['dates', 'clusterID'])\n",
    "\n",
    "for (date, cluster_id), group in tqdm(grouped, desc=\"Processing neurons\"):\n",
    "\n",
    "    traces = group['FR_ievent'].dropna().tolist()\n",
    "    if len(traces) < 2:\n",
    "        continue\n",
    "\n",
    "    # Align traces by pull (end), pad shorter trials with nan at the front\n",
    "    min_len = min(len(trace) for trace in traces)\n",
    "    aligned = [trace[-min_len:] if len(trace) >= min_len else\n",
    "               np.pad(trace, (min_len - len(trace), 0), constant_values=np.nan)\n",
    "               for trace in traces]\n",
    "\n",
    "    stacked = np.stack(aligned)  # shape: (n_trials, min_len)\n",
    "    mean_trace = np.nanmean(stacked, axis=0)\n",
    "\n",
    "    # Compute slope of mean trace (full)\n",
    "    x_full = np.arange(min_len)\n",
    "    slope_full = linregress(x_full, mean_trace).slope\n",
    "\n",
    "    # Find peak index based on slope sign\n",
    "    if slope_full >= 0:\n",
    "        peak_idx = np.nanargmax(mean_trace)\n",
    "    else:\n",
    "        peak_idx = np.nanargmin(mean_trace)\n",
    "\n",
    "    # Convert peak index to peak time relative to pull (end-aligned)\n",
    "    peak_time = (peak_idx - (min_len - 1)) / fps\n",
    "\n",
    "    # Save peak_time for all trials of this neuron\n",
    "    df.loc[(df['dates'] == date) & (df['clusterID'] == cluster_id), 'fr_peak_time'] = peak_time\n",
    "\n",
    "    # For each trial, calculate slope from trial start to the peak index (if peak_idx within trial length)\n",
    "    for idx, row in group.iterrows():\n",
    "        fr_trace = row['FR_ievent']\n",
    "        trial_len = len(fr_trace)\n",
    "        \n",
    "        # Align trial trace to end, pad front if needed\n",
    "        if trial_len < min_len:\n",
    "            padded_trace = np.pad(fr_trace, (min_len - trial_len, 0), constant_values=np.nan)\n",
    "        else:\n",
    "            padded_trace = fr_trace[-min_len:]\n",
    "\n",
    "        # Make sure peak_idx is within trial length\n",
    "        if peak_idx < len(padded_trace):\n",
    "            y = padded_trace[:peak_idx+1]\n",
    "            x = np.arange(len(y))\n",
    "            if np.isnan(y).all() or len(y) < 2:\n",
    "                slope = np.nan\n",
    "            else:\n",
    "                slope = linregress(x, y).slope\n",
    "            # Optional: store absolute slope if you want\n",
    "            slope = abs(slope) if not np.isnan(slope) else np.nan\n",
    "\n",
    "            df.loc[idx, 'fr_slope_peakbased'] = slope\n",
    "        else:\n",
    "            df.loc[idx, 'fr_slope_peakbased'] = np.nan\n",
    "\n",
    "# Save back to original dataframe\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_slope_peakbased'] = df['fr_slope_peakbased']\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_peak_time'] = df['fr_peak_time']\n",
    "bhvevents_aligned_FR_allevents_all_dates_df['fr_slope'] = df['fr_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4c4c3-6166-4206-b956-8850c27f6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_allevents_all_dates_df['fr_slope']\n",
    "yyy = bhvevents_aligned_FR_allevents_all_dates_df['fr_slope_peakbased']\n",
    "# plt.plot(xxx,yyy,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e81a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# organize the data in order to do HDDM - average across neurons for the unique trial\n",
    "##########\n",
    "\n",
    "# Step 1: Group by unique trial (date, bhv_id) and average the FR_ievent across neurons\n",
    "averaged_df = bhvevents_aligned_FR_allevents_all_dates_df.groupby(['dates', 'bhv_id'])['FR_ievent'].apply(\n",
    "    lambda traces: np.mean(np.stack(traces.to_numpy()), axis=0)\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Rename the averaged firing rate column\n",
    "averaged_df = averaged_df.rename(columns={'FR_ievent': 'FR_ievent_avg'})\n",
    "\n",
    "# Step 3: Select representative behavioral columns to merge back (drop duplicates so one per trial)\n",
    "representative_cols = [\n",
    "        'dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'bhv_id', \n",
    "        # 'mass_move_speed','other_mass_move_speed',\n",
    "        'self_PC1','other_PC1',\n",
    "        'num_preceding_failpull',  'otherpull_prob',\n",
    "        'pull_rt', 'selfpull_prob', 'socialgaze_prob', 'time_from_last_reward',\n",
    "        'previous_pull_outcome', 'pull_outcome', 'socialgaze_auc', \n",
    "        'self_PC1_mean',\n",
    "        'self_PC1_std', \n",
    "        'other_PC1_mean',\n",
    "        'other_PC1_std',\n",
    "        # 'mass_move_speed_mean',\n",
    "        # 'mass_move_speed_std', \n",
    "        # 'other_mass_move_speed_mean',\n",
    "        # 'other_mass_move_speed_std',\n",
    "]\n",
    "\n",
    "# Get one row per (dates, bhv_id) combination\n",
    "behavior_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])[representative_cols]\n",
    "\n",
    "# Step 4: Merge firing rate and behavioral data\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = pd.merge(averaged_df, behavior_df, \n",
    "                                                                    on=['dates', 'bhv_id'], how='left')\n",
    "\n",
    "# Step 5\n",
    "# Compute mean firing rate\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['fr_mean'] = \\\n",
    "      bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].apply(\n",
    "    lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Compute slope of firing rate before 0.85s prior to pull\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        slope, _, _, _, _ = linregress(x, y)\n",
    "        #\n",
    "        if slope<0:\n",
    "            slope = -slope\n",
    "        \n",
    "        return slope\n",
    "    return np.nan\n",
    "\n",
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['fr_slope'] = \\\n",
    "     bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].apply(compute_fr_slope)\n",
    "\n",
    "\n",
    "# save the data with other HDDM dataframes for the modeling\n",
    "savedata = 1\n",
    "\n",
    "if savedata:\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "    if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_datas_flexibleTW_newRTdefinition_'+doOnsetAfterMin_suffix+'withFRs_all_dates_'+\n",
    "              animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "        pickle.dump(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da385e-333c-46dd-83d5-e8bf85f7669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_alldates_mergedneurons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# load the HDDM data to get the drift diffusion v for further correlation\n",
    "##########\n",
    "\n",
    "if 1:\n",
    "    \n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_hddm'+savefile_sufix+\\\n",
    "        '/'+cameraID+'/'+animal_to_ana+'/hddm_model_fitted_combinedsession_withNeurons/'\n",
    "\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_traces_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_traces = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_stats_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_stats = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_nogaze_traces_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_nogaze_traces = pickle.load(f)\n",
    "    with open(data_saved_subfolder+'/hddm_model_fitted_nogaze_stats_'+condition_name+doOnsetAfterMin_suffix+'.pkl', 'rb') as f:\n",
    "        hddm_model_fitted_nogaze_stats = pickle.load(f)\n",
    "    \n",
    "    v_inter = hddm_model_fitted_stats['mean']['v_Intercept']\n",
    "    v_self_gaze_auc = hddm_model_fitted_stats['mean']['v_self_gaze_auc']\n",
    "    v_partner_mean_speed = hddm_model_fitted_stats['mean']['v_partner_mean_speed']\n",
    "    v_self_mean_speed = hddm_model_fitted_stats['mean']['v_self_mean_speed']\n",
    "    v_partner_speed_std = hddm_model_fitted_stats['mean']['v_partner_speed_std']\n",
    "    v_self_speed_std = hddm_model_fitted_stats['mean']['v_self_speed_std']\n",
    "\n",
    "    #\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df['predicted_v'] = v_inter + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc'] * v_self_gaze_auc + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_mean'] * v_partner_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed_std'] * v_partner_speed_std + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_mean'] * v_self_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed_std'] * v_self_speed_std \n",
    "\n",
    "    #\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['predicted_v'] = v_inter + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['socialgaze_auc'] * v_self_gaze_auc + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed_mean'] * v_partner_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed_std'] * v_partner_speed_std + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed_mean'] * v_self_mean_speed + \\\n",
    "            bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed_std'] * v_self_speed_std \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some criteria to remove trials\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['previous_pull_outcome']==1]\n",
    "\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']>3]\n",
    "# bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\\\n",
    "#                                 bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']<20]\n",
    "\n",
    "# Remove outlier pull_rt values\n",
    "# method 1\n",
    "if 0:\n",
    "    # Compute IQR bounds\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    q1 = bhv_unique_df['pull_rt'].quantile(0.25)\n",
    "    q3 = bhv_unique_df['pull_rt'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    #\n",
    "    # Filter out outliers\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "# method 2    \n",
    "if 1:\n",
    "    # Symmetrical trimming: keep central 90% of pull_rt\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_all_dates_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    lower_bound = bhv_unique_df['pull_rt'].quantile(0.05)\n",
    "    # lower_bound = 4\n",
    "    upper_bound = bhv_unique_df['pull_rt'].quantile(0.95)\n",
    "\n",
    "    # Filter the dataframe\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b923d5-396c-4dca-b998-e1ca1f8c0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at the successful pull\n",
    "if 0:\n",
    "    ind_ = bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome']==1\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df = bhvevents_aligned_FR_allevents_all_dates_df[ind_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = bhvevents_aligned_FR_allevents_all_dates_df['pull_rt']\n",
    "yyy = bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_auc']\n",
    "plt.plot(xxx,yyy,'.')\n",
    "st.pearsonr(xxx,yyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0747346",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhvevents_aligned_FR_allevents_all_dates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhvevents_aligned_FR_allevents_all_dates_df\n",
    "print(np.sum(bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome']))\n",
    "print(np.shape(bhvevents_aligned_FR_allevents_all_dates_df['pull_outcome'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same clearup for the merged dataframe\n",
    "# Remove outlier pull_rt values\n",
    "# method 1\n",
    "if 0:\n",
    "    # Compute IQR bounds\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    q1 = bhv_unique_df['pull_rt'].quantile(0.25)\n",
    "    q3 = bhv_unique_df['pull_rt'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    #\n",
    "    # Filter out outliers\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_all_dates_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "# method 2    \n",
    "if 1:\n",
    "    # Symmetrical trimming: keep central 90% of pull_rt\n",
    "    bhv_unique_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.drop_duplicates(subset=['dates', 'bhv_id'])\n",
    "    #\n",
    "    lower_bound = bhv_unique_df['pull_rt'].quantile(0.05)\n",
    "    # lower_bound = 4\n",
    "    upper_bound = bhv_unique_df['pull_rt'].quantile(0.95)\n",
    "\n",
    "    # Filter the dataframe\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[\n",
    "        (bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_rt'] >= lower_bound) &\n",
    "        (bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_rt'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7b2f3-29c3-4c68-92b4-401269a672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at the successful pull\n",
    "if 1:\n",
    "    ind_ = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['pull_outcome']==1\n",
    "    bhvevents_aligned_FR_allevents_alldates_mergedneurons_df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[ind_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7839fc-be9e-4fc6-b30c-b19dde280098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be6083-4219-4bac-a8cb-d5a41512cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = 1749\n",
    "if 0:\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['FR_ievent_avg'].loc[ind_])\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['other_mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df['socialgaze_prob'].loc[ind_]*5)\n",
    "if 0:\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['FR_ievent'].loc[ind_])\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['other_mass_move_speed'].loc[ind_]/800)\n",
    "    plt.plot(bhvevents_aligned_FR_allevents_all_dates_df['socialgaze_prob'].loc[ind_]*5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b4af3-dbf7-411e-ba8b-c0d8d919cc5d",
   "metadata": {},
   "source": [
    "#### trial wise correlation between FR trace and other continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec81fb8-6388-4f7d-9faa-8048091ef518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.stats import f_oneway, ttest_rel\n",
    "    import itertools\n",
    "    import seaborn as sns\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    betas = {var: [] for var in ['mass', 'partner', 'selfpull', 'socialgaze']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        y = df.iloc[i]['FR_ievent_avg']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['self_PC1'],\n",
    "            'partner': df.iloc[i]['other_PC1'],\n",
    "            'selfpull': df.iloc[i]['selfpull_prob'],\n",
    "            'socialgaze': df.iloc[i]['socialgaze_prob']\n",
    "        }\n",
    "    \n",
    "        if any(len(y) != len(x) for x in X_vars.values()):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            # Univariate regressions\n",
    "            for varname, x in X_vars.items():\n",
    "                model_uni = LinearRegression().fit(np.array(x).reshape(-1, 1), y)\n",
    "                r2_uni[varname].append(model_uni.score(np.array(x).reshape(-1, 1), y))\n",
    "            \n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'partner', 'selfpull', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "    \n",
    "            for idx, var in enumerate(['mass', 'partner', 'selfpull', 'socialgaze']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "            # Leave-one-out regressions\n",
    "            for idx, var in enumerate(['mass', 'partner', 'selfpull', 'socialgaze']):\n",
    "                X_reduced = np.delete(X_full, idx, axis=1)\n",
    "                X_reduced_std = StandardScaler().fit_transform(X_reduced)\n",
    "                model_reduced = LinearRegression().fit(X_reduced_std, y_std)\n",
    "                r2_reduced = model_reduced.score(X_reduced_std, y_std)\n",
    "                delta_r2[var].append(r2_full_val - r2_reduced)\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # ---------- Step 3: Format for plotting ----------\n",
    "    df_r2_uni = pd.DataFrame([\n",
    "        {'Variable': var, 'R2': r2} for var, values in r2_uni.items() for r2 in values\n",
    "    ])\n",
    "    \n",
    "    df_delta_r2 = pd.DataFrame([\n",
    "        {'Variable': var, 'Delta_R2': delta} for var, values in delta_r2.items() for delta in values\n",
    "    ])\n",
    "    \n",
    "    df_betas = pd.DataFrame([\n",
    "        {'Variable': var, 'Beta': beta} for var, values in betas.items() for beta in values\n",
    "    ])\n",
    "    \n",
    "    # ---------- Step 4: Statistical tests ----------\n",
    "    anova_r2 = f_oneway(*[df_r2_uni[df_r2_uni['Variable'] == v]['R2'] for v in df_r2_uni['Variable'].unique()])\n",
    "    anova_delta = f_oneway(*[df_delta_r2[df_delta_r2['Variable'] == v]['Delta_R2'] for v in df_delta_r2['Variable'].unique()])\n",
    "    anova_beta = f_oneway(*[df_betas[df_betas['Variable'] == v]['Beta'] for v in df_betas['Variable'].unique()])\n",
    "    \n",
    "    pairwise_results = []\n",
    "    variables = ['mass', 'partner', 'selfpull', 'socialgaze']\n",
    "    for v1, v2 in itertools.combinations(variables, 2):\n",
    "        x = df_delta_r2[df_delta_r2['Variable'] == v1]['Delta_R2'].dropna()\n",
    "        y = df_delta_r2[df_delta_r2['Variable'] == v2]['Delta_R2'].dropna()\n",
    "        min_len = min(len(x), len(y))\n",
    "        t_stat, p_val = ttest_rel(x[:min_len], y[:min_len])\n",
    "        pairwise_results.append({'Var1': v1, 'Var2': v2, 'T-stat': t_stat, 'P-value': p_val})\n",
    "    pairwise_df = pd.DataFrame(pairwise_results)\n",
    "    \n",
    "    # ---------- Step 5: Plotting ----------\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    sns.violinplot(data=df_r2_uni, x='Variable', y='R2', inner='box', ax=axes[0])\n",
    "    axes[0].set_title('Univariate R² per Variable')\n",
    "    \n",
    "    sns.violinplot(data=df_delta_r2, x='Variable', y='Delta_R2',  inner='box', ax=axes[1])\n",
    "    axes[1].set_title('ΔR² (Unique Contribution) per Variable')\n",
    "    \n",
    "    sns.violinplot(data=df_betas, x='Variable', y='Beta', inner='box', ax=axes[2])\n",
    "    axes[2].set_title('Standardized Beta Coefficients')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"regression_violin_plots.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ---------- Optional: Print stats ----------\n",
    "    print(\"ANOVA R²:\", anova_r2)\n",
    "    print(\"ANOVA ΔR²:\", anova_delta)\n",
    "    print(\"ANOVA Betas:\", anova_beta)\n",
    "    print(\"Pairwise ΔR² comparisons:\\n\", pairwise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ac641-c582-4d39-97f4-f592f0db7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test if the mean FR across neurons in each trial is just tracking the self movement, \n",
    "    # or after considering the confound of self movement, it still encode social gaze\n",
    "    \n",
    "    # nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    # df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_all_dates_df)\n",
    "    df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    \n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'socialgaze']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'socialgaze']}\n",
    "    betas = {var: [] for var in ['mass',  'socialgaze']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # y = df.iloc[i]['FR_ievent_avg']\n",
    "        y = df.iloc[i]['FR_ievent']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['self_PC1'],\n",
    "            'socialgaze': df.iloc[i]['socialgaze_prob']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "        \n",
    "            for idx, var in enumerate(['mass', 'socialgaze']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f90f4d-cddd-4a5b-bcc9-87163c515700",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test if the social gaze prob in each trial is just tracking the self movement, \n",
    "    # or after considering the confound of self movement, it still encode parnter movement\n",
    "    \n",
    "    nentries = len(bhvevents_aligned_FR_allevents_alldates_mergedneurons_df)\n",
    "    df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "    \n",
    "\n",
    "    # ---------- Step 2: Regression calculations ----------\n",
    "    r2_uni = {var: [] for var in ['mass', 'partner']}\n",
    "    r2_full = []\n",
    "    delta_r2 = {var: [] for var in ['mass', 'partner']}\n",
    "    betas = {var: [] for var in ['mass',  'partner']}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        y = df.iloc[i]['socialgaze_prob']\n",
    "        X_vars = {\n",
    "            'mass': df.iloc[i]['self_PC1'],\n",
    "            'partner': df.iloc[i]['other_PC1']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Full multivariate regression\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'partner']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            model_full = LinearRegression().fit(X_std, y_std)\n",
    "            r2_full_val = model_full.score(X_std, y_std)\n",
    "            r2_full.append(r2_full_val)\n",
    "        \n",
    "            for idx, var in enumerate(['mass', 'partner']):\n",
    "                betas[var].append(model_full.coef_[idx])\n",
    "    \n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7caf5d-f0cf-4565-adad-f38b2879779b",
   "metadata": {},
   "source": [
    "### Use multi-variable regression to define and label neurons that encode socialgaze, even after control the self movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb907f-b8cf-4de8-a48d-4539546d349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import f_oneway, ttest_rel\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#\n",
    "dates_toana = np.unique(bhvevents_aligned_FR_allevents_all_dates_df['dates'])\n",
    "ndates_toana = np.shape(dates_toana)[0]\n",
    "\n",
    "neuronEncodeSocialGaze_summary_df = pd.DataFrame(columns=['date','clusterID','neuronEncodeSocialGaze'])\n",
    "\n",
    "for idate in np.arange(0, ndates_toana,1):\n",
    "    date_toana = dates_toana[idate]\n",
    "\n",
    "    ind_idate = np.isin(bhvevents_aligned_FR_allevents_all_dates_df['dates'],date_toana)\n",
    "\n",
    "    df_allneurons_tgt = bhvevents_aligned_FR_allevents_all_dates_df[ind_idate]\n",
    "\n",
    "    neurons_toana = np.unique(df_allneurons_tgt['clusterID'])\n",
    "    nneurons_toana = np.shape(neurons_toana)[0]\n",
    "\n",
    "    for ineuron in np.arange(0,nneurons_toana,1):\n",
    "        neuron_toana = neurons_toana[ineuron]\n",
    "\n",
    "        ind_ineuron = np.isin(df_allneurons_tgt['clusterID'],neuron_toana)\n",
    "\n",
    "        df_ineuron_tgt = df_allneurons_tgt[ind_ineuron]\n",
    "\n",
    "        # get the FR traces\n",
    "        FR_allevents = np.array(df_ineuron_tgt['FR_ievent'])\n",
    "        nFR_allevents = np.shape(FR_allevents)[0]\n",
    "\n",
    "        # get the socialgaze and self movement traces\n",
    "        selfspeed_allevents = np.array(df_ineuron_tgt['self_PC1'])\n",
    "        socialgaze_allevents = np.array(df_ineuron_tgt['socialgaze_prob'])\n",
    "        nbhv_allevents = np.shape(selfspeed_allevents)[0]\n",
    "\n",
    "        # make sure the fr and bhv traces has the same size\n",
    "        if nFR_allevents < nbhv_allevents:\n",
    "            selfspeed_allevents = selfspeed_allevents[0:nFR_allevents]\n",
    "            socialgaze_allevents = socialgaze_allevents[0:nFR_allevents]\n",
    "            nevents = nFR_allevents\n",
    "        elif nFR_allevents > nbhv_allevents:\n",
    "            FR_allevents = FR_allevents[0:nbhv_allevents]\n",
    "            nevents = nbhv_allevents\n",
    "        else:\n",
    "            nevents = nFR_allevents\n",
    "\n",
    "        # remove trial that does not match\n",
    "        for ievent in np.arange(0,nevents,1):\n",
    "            ntimepoint_FR = np.shape(FR_allevents[ievent])[0]\n",
    "            ntimepoint_speed = np.shape(selfspeed_allevents[ievent])[0]\n",
    "            ntimepoint_gaze = np.shape(socialgaze_allevents[ievent])[0]\n",
    "            \n",
    "            if (ntimepoint_speed != ntimepoint_FR) |\\\n",
    "               (ntimepoint_gaze != ntimepoint_FR) |\\\n",
    "               (ntimepoint_gaze != ntimepoint_speed):\n",
    "                FR_allevents[ievent] = []\n",
    "                selfspeed_allevents[ievent] = []\n",
    "                socialgaze_allevents[ievent] = []\n",
    "        \n",
    "        # conbine all trials\n",
    "        FR_allevents_flat = np.concatenate(FR_allevents)\n",
    "        selfspeed_allevents_flat = np.concatenate(selfspeed_allevents)\n",
    "        socialgaze_allevents_flat = np.concatenate(socialgaze_allevents)\n",
    "        #\n",
    "        min_len = np.min([len(FR_allevents_flat),\n",
    "                          len(selfspeed_allevents_flat),\n",
    "                          len(socialgaze_allevents_flat)])\n",
    "        FR_allevents_flat = FR_allevents_flat[0:min_len]\n",
    "        selfspeed_allevents_flat = selfspeed_allevents_flat[0:min_len]\n",
    "        socialgaze_allevents_flat = socialgaze_allevents_flat[0:min_len]\n",
    "        \n",
    "        # multi variable regression\n",
    "        y = FR_allevents_flat\n",
    "        X_vars = {\n",
    "            'mass': selfspeed_allevents_flat,\n",
    "            'socialgaze': socialgaze_allevents_flat\n",
    "        }\n",
    "\n",
    "        # Full multivariate regression\n",
    "        try:\n",
    "            X_full = np.vstack([X_vars[v] for v in ['mass', 'socialgaze']]).T\n",
    "            X_std = StandardScaler().fit_transform(X_full)\n",
    "            y_std = (y - np.mean(y)) / np.std(y)\n",
    "            # Create a clean DataFrame for statsmodels\n",
    "            df = pd.DataFrame(X_std, columns=['mass', 'gaze'])\n",
    "            df['fr'] = y_std # Use the standardized firing rate \n",
    "            # Add a constant (intercept) to the predictors\n",
    "            df = sm.add_constant(df)\n",
    "            # Fit the Ordinary Least Squares (OLS) model\n",
    "            model_sm = sm.OLS(df['fr'], df[['const', 'mass', 'gaze']])\n",
    "            results = model_sm.fit()\n",
    "    \n",
    "            if results.pvalues['gaze'] < 0.01:\n",
    "                neuronEncodeSocialGaze = True\n",
    "            else:\n",
    "                neuronEncodeSocialGaze = False\n",
    "    \n",
    "            neuronEncodeSocialGaze_summary_df = neuronEncodeSocialGaze_summary_df.append({'date':date_toana,\n",
    "                                                                                          'clusterID':neuron_toana,\n",
    "                                                                                          'neuronEncodeSocialGaze':neuronEncodeSocialGaze,\n",
    "                                                                               }, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829f313-a110-4eb4-8752-aefb8f5660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SocialGazeEncodeNeuronRatio = np.sum(neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze'])/\\\n",
    "                              len(neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze'])\n",
    "\n",
    "print('SocialGazeEncodeNeuronRatio: '+str(SocialGazeEncodeNeuronRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db24362-5a64-4cd3-bd73-23a68113c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronEncodeSocialGaze_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3352e",
   "metadata": {},
   "source": [
    "### correlation among variables - across all trial, without carefully consider mixed effect, for general trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std',\n",
    "    # 'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "doSocialGazeEncodeNeuron = 1\n",
    "#\n",
    "if doSocialGazeEncodeNeuron:\n",
    "    goodneuron_df = neuronEncodeSocialGaze_summary_df[neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze']]\n",
    "    #\n",
    "    df1 = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    df2 = goodneuron_df\n",
    "    #\n",
    "    # 1. Rename the 'date' column in the second DataFrame to match the first\n",
    "    df2_renamed = df2.rename(columns={'date': 'dates'})\n",
    "    # 2. Perform the inner merge\n",
    "    # This keeps only the rows from df1 that have a matching ('dates', 'clusterID') pair in df2_renamed\n",
    "    filtered_df = pd.merge(df1, df2_renamed[['dates', 'clusterID']], on=['dates', 'clusterID'], how='inner')\n",
    "    #\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = filtered_df[columns_of_interest].dropna()\n",
    "\n",
    "else:\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = bhvevents_aligned_FR_allevents_all_dates_df[columns_of_interest].dropna()\n",
    "    # df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(df_clean[columns_of_interest[i]], df_clean[columns_of_interest[j]])\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "\n",
    "# FDR correction\n",
    "pvals_flat = pval_matrix[np.triu_indices(n, k=1)]\n",
    "_, pvals_corrected, _, _ = multipletests(pvals_flat, method='bonferroni')\n",
    "\n",
    "# Map corrected p-values back into full matrix\n",
    "pval_corrected_matrix = np.ones_like(pval_matrix)\n",
    "pval_corrected_matrix[np.triu_indices(n, k=1)] = pvals_corrected\n",
    "i_lower = np.tril_indices_from(pval_corrected_matrix, -1)\n",
    "pval_corrected_matrix[i_lower] = pval_corrected_matrix.T[i_lower]\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_corrected_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix with FDR-corrected Significance (p < 0.01)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acd272-8210-4383-bcdd-10668a42201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df_clean[columns_of_interest[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9381054-109e-489b-8fb1-e846734a341e",
   "metadata": {},
   "source": [
    "### correlation among variables - run each session each neuron separately, the number plotted is the mean correlation coeffient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041bb11-139c-4362-9320-b264a02405f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std',\n",
    "    # 'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "doSocialGazeEncodeNeuron = 0\n",
    "#\n",
    "if doSocialGazeEncodeNeuron:\n",
    "    goodneuron_df = neuronEncodeSocialGaze_summary_df[neuronEncodeSocialGaze_summary_df['neuronEncodeSocialGaze']]\n",
    "    #\n",
    "    df1 = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "    df2 = goodneuron_df\n",
    "    #\n",
    "    # 1. Rename the 'date' column in the second DataFrame to match the first\n",
    "    df2_renamed = df2.rename(columns={'date': 'dates'})\n",
    "    # 2. Perform the inner merge\n",
    "    # This keeps only the rows from df1 that have a matching ('dates', 'clusterID') pair in df2_renamed\n",
    "    filtered_df = pd.merge(df1, df2_renamed[['dates', 'clusterID']], on=['dates', 'clusterID'], how='inner')\n",
    "    #\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = filtered_df[columns_of_interest+['dates','clusterID']].dropna()\n",
    "\n",
    "else:\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = bhvevents_aligned_FR_allevents_all_dates_df[columns_of_interest+['dates','clusterID']].dropna()\n",
    "    # df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "dates_toana = np.unique(df_clean['dates'])\n",
    "ndates_toana = np.shape(dates_toana)[0]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "\n",
    "            corrs_temp = []\n",
    "            pvals_temp = []\n",
    "            \n",
    "            for idate in np.arange(0,ndates_toana,1):\n",
    "                \n",
    "                date_toana = dates_toana[idate]\n",
    "                \n",
    "                df_clean_idate = df_clean[np.isin(df_clean['dates'],date_toana)]\n",
    "            \n",
    "                neurons_toana = np.unique(df_clean_idate['clusterID'])\n",
    "                nneurons_toana = np.shape(neurons_toana)[0]\n",
    "            \n",
    "                for ineuron in np.arange(0,nneurons_toana,1):\n",
    "                    neuron_toana = neurons_toana[ineuron]\n",
    "\n",
    "                    try:\n",
    "                        df_clean_ineuron = df_clean_idate[np.isin(df_clean_idate['clusterID'],neuron_toana)]\n",
    "            \n",
    "                        r, p = pearsonr(df_clean_ineuron[columns_of_interest[i]], df_clean_ineuron[columns_of_interest[j]])\n",
    "                    \n",
    "                    except:\n",
    "                        r = np.nan\n",
    "                        p = np.nan\n",
    "                    \n",
    "                    corrs_temp.append(r)\n",
    "                    pvals_temp.append(p)\n",
    "\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = np.nanmean(np.unique(corrs_temp))\n",
    "            \n",
    "            corrs_temp = np.array(corrs_temp)\n",
    "            corrs_temp = corrs_temp[~np.isnan(corrs_temp)]\n",
    "            _,pttest = st.ttest_1samp(np.unique(corrs_temp),0)\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = pttest\n",
    "\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix across individual neurons (p < 0.01 means ttest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84041f9d",
   "metadata": {},
   "source": [
    "### correlation among variables - get the information of the corr coef and pvalue of each neuron's fr and bhv variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503507d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46388b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4dbacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "338a5eed-b9ac-4628-8f8f-99d71f1812f9",
   "metadata": {},
   "source": [
    "### examine the percent of neurons that significantly encode each variables, looking at the FR slope and FR mean separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9d3ac-4428-4967-8142-34cd73eeda9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Variables to correlate with fr_mean and fr_slope\n",
    "behavior_vars = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std',\n",
    "    # 'predicted_v',\n",
    "]\n",
    "\n",
    "neural_metrics = ['fr_mean', 'fr_slope']\n",
    "# neural_metrics = ['fr_mean', 'fr_slope','fr_slope_peakbased']\n",
    "\n",
    "# Store percent significance for heatmap\n",
    "results = []\n",
    "\n",
    "grouped = bhvevents_aligned_FR_allevents_all_dates_df.groupby(['dates', 'clusterID', 'condition'])\n",
    "\n",
    "for (date, cluster_id, condition), group in grouped:\n",
    "    for fr_type in neural_metrics:\n",
    "        df = group[behavior_vars + [fr_type]].dropna()\n",
    "        if len(df) >= len(behavior_vars) + 2:  # need more trials than predictors\n",
    "            X = sm.add_constant(df[behavior_vars])\n",
    "            y = df[fr_type]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            for var in behavior_vars:\n",
    "                results.append({\n",
    "                    'date': date,\n",
    "                    'clusterID': cluster_id,\n",
    "                    'condition': condition,\n",
    "                    'fr_metric': fr_type,\n",
    "                    'predictor': var,\n",
    "                    'beta': model.params.get(var, np.nan),\n",
    "                    'pval': model.pvalues.get(var, np.nan),\n",
    "                    'significant': model.pvalues.get(var, np.nan) < 0.05\n",
    "                })\n",
    "\n",
    "regression_results_df = pd.DataFrame(results)\n",
    "\n",
    "# do the heatmap plotting\n",
    "# --- Step 1: Compute % of significant neurons per predictor ---\n",
    "summary = (\n",
    "    regression_results_df.groupby(['fr_metric', 'predictor'])['significant']\n",
    "    .mean()\n",
    "    .unstack(0) * 100  # convert to percentage\n",
    ")\n",
    "\n",
    "# --- Step 2: Compute % with any significant beta ---\n",
    "any_sig_summary = (\n",
    "    regression_results_df\n",
    "    .groupby(['fr_metric', 'date', 'clusterID'])['significant']\n",
    "    .any()\n",
    "    .groupby(['fr_metric'])\n",
    "    .mean()\n",
    "    .to_frame()\n",
    "    .T * 100  # convert to percentage\n",
    ")\n",
    "any_sig_summary.index = ['any_predictor']\n",
    "\n",
    "# --- Step 3: Combine ---\n",
    "summary_with_total = pd.concat([summary, any_sig_summary])\n",
    "\n",
    "# --- Step 4: Round for annotation ---\n",
    "annot_vals = summary_with_total.round(1).astype(str) + '%'\n",
    "\n",
    "# --- Step 5: Plot ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(summary_with_total, annot=annot_vals, fmt='', cmap='YlOrRd',\n",
    "            cbar_kws={'label': '% of Neurons (p < 0.05)'}, linewidths=0.5)\n",
    "\n",
    "plt.title('Percent of Neurons Significantly Encoding Each Variable')\n",
    "plt.ylabel('Behavioral Predictor')\n",
    "plt.xlabel('Neural Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba671fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20bde7-9d78-42be-84f2-07869a65ef3a",
   "metadata": {},
   "source": [
    "### correlation among behavioral and neural variables, and this time only consider significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d6f41-0db7-4d11-b32f-2e4d1652da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "neural_features = ['fr_slope']  # 'fr_mean', 'fr_slope', 'fr_slope_peakbased'\n",
    "# neural_features = ['fr_mean']\n",
    "# predictors_to_check = [ 'predicted_v', ] # define which behavioral predictors to check significance for\n",
    "predictors_to_check =  np.unique(regression_results_df['predictor'])\n",
    "\n",
    "regression_results_df_totest = regression_results_df.copy()\n",
    "\n",
    "ind_good = (np.isin(regression_results_df_totest['fr_metric'],neural_features)) &\\\n",
    "           (np.isin(regression_results_df_totest['predictor'],predictors_to_check))\n",
    "regression_results_df_totest = regression_results_df_totest[ind_good]\n",
    "\n",
    "sig_neurons = (\n",
    "    regression_results_df_totest[regression_results_df_totest['significant']]\n",
    "    .groupby(['date', 'clusterID'])\n",
    "    .size()\n",
    "    .reset_index()[['date', 'clusterID']]\n",
    ")\n",
    "\n",
    "sig_neuron_keys = set(tuple(x) for x in sig_neurons.to_numpy())\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Filter the original DataFrames\n",
    "# -----------------------------\n",
    "bhvevents_filtered_df = bhvevents_aligned_FR_allevents_all_dates_df[\n",
    "    bhvevents_aligned_FR_allevents_all_dates_df[['dates', 'clusterID']].apply(tuple, axis=1).isin(sig_neuron_keys)\n",
    "].copy()\n",
    "\n",
    "##########\n",
    "# organize the data in order to do HDDM - average across neurons for the unique trial\n",
    "##########\n",
    "\n",
    "# Step 1: Group by unique trial (date, bhv_id) and average the FR_ievent across neurons\n",
    "averaged_df = bhvevents_filtered_df.groupby(['dates', 'bhv_id'])['FR_ievent'].apply(\n",
    "    lambda traces: np.mean(np.stack(traces.to_numpy()), axis=0)\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Rename the averaged firing rate column\n",
    "averaged_df = averaged_df.rename(columns={'FR_ievent': 'FR_ievent_avg'})\n",
    "\n",
    "# Step 3: Select representative behavioral columns to merge back (drop duplicates so one per trial)\n",
    "representative_cols = [\n",
    "    'dates', 'condition', 'act_animal', 'bhv_name', 'succrate', 'bhv_id', 'self_PC1',\n",
    "       'num_preceding_failpull', 'other_PC1', 'otherpull_prob',\n",
    "       'pull_rt', 'pull_outcome', 'selfpull_prob', 'socialgaze_prob', 'time_from_last_reward',\n",
    "       'previous_pull_outcome', 'socialgaze_auc', 'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std', # 'predicted_v',\n",
    "]\n",
    "\n",
    "# Get one row per (dates, bhv_id) combination\n",
    "behavior_df = bhvevents_filtered_df.drop_duplicates(subset=['dates', 'bhv_id'])[representative_cols]\n",
    "\n",
    "# Step 4: Merge firing rate and behavioral data\n",
    "bhvevents_filtered_mergedneurons_df = pd.merge(averaged_df, behavior_df, \n",
    "                                                on=['dates', 'bhv_id'], how='left')\n",
    "\n",
    "# Step 5\n",
    "# Compute mean firing rate\n",
    "bhvevents_filtered_mergedneurons_df['fr_mean'] = \\\n",
    "      bhvevents_filtered_mergedneurons_df['FR_ievent_avg'].apply(\n",
    "    lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else np.nan\n",
    ")\n",
    "\n",
    "# Compute slope of firing rate before 0.85s prior to pull\n",
    "def compute_fr_slope(fr_trace):\n",
    "    if isinstance(fr_trace, (list, np.ndarray)) and len(fr_trace) > pull_margin_frames:\n",
    "        y = fr_trace[:len(fr_trace) - pull_margin_frames]\n",
    "        x = np.arange(len(y))\n",
    "        slope, _, _, _, _ = linregress(x, y)\n",
    "        #\n",
    "        if slope<0:\n",
    "            slope = -slope\n",
    "        \n",
    "        return slope\n",
    "    return np.nan\n",
    "\n",
    "bhvevents_filtered_mergedneurons_df['fr_slope'] = \\\n",
    "     bhvevents_filtered_mergedneurons_df['FR_ievent_avg'].apply(compute_fr_slope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38453c46-3f5a-4ca5-85e1-6b2784a012e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Select behavioral and neural variables\n",
    "columns_of_interest = [\n",
    "    'pull_rt',\n",
    "    'num_preceding_failpull',\n",
    "    'time_from_last_reward',\n",
    "    'previous_pull_outcome',\n",
    "    'socialgaze_auc',\n",
    "    'self_PC1_mean',\n",
    "    'self_PC1_std',\n",
    "    'other_PC1_mean',\n",
    "    'other_PC1_std',\n",
    "    # 'predicted_v',\n",
    "    'fr_mean',\n",
    "    'fr_slope',\n",
    "    # 'fr_slope_peakbased',\n",
    "    # 'fr_peak_time',\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_clean = bhvevents_filtered_df[columns_of_interest].dropna()\n",
    "# df_clean = bhvevents_filtered_mergedneurons_df[columns_of_interest].dropna()\n",
    "\n",
    "\n",
    "# Compute correlation and p-values\n",
    "n = len(columns_of_interest)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "pval_matrix = np.ones((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(df_clean[columns_of_interest[i]], df_clean[columns_of_interest[j]])\n",
    "            corr_matrix[i, j] = corr_matrix[j, i] = r\n",
    "            pval_matrix[i, j] = pval_matrix[j, i] = p\n",
    "\n",
    "# FDR correction\n",
    "pvals_flat = pval_matrix[np.triu_indices(n, k=1)]\n",
    "_, pvals_corrected, _, _ = multipletests(pvals_flat, method='bonferroni')\n",
    "\n",
    "# Map corrected p-values back into full matrix\n",
    "pval_corrected_matrix = np.ones_like(pval_matrix)\n",
    "pval_corrected_matrix[np.triu_indices(n, k=1)] = pvals_corrected\n",
    "i_lower = np.tril_indices_from(pval_corrected_matrix, -1)\n",
    "pval_corrected_matrix[i_lower] = pval_corrected_matrix.T[i_lower]\n",
    "\n",
    "# Create annotation matrix with stars\n",
    "annot_matrix = np.empty((n, n), dtype=object)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        r = corr_matrix[i, j]\n",
    "        p = pval_corrected_matrix[i, j]\n",
    "        annot_matrix[i, j] = f\"{r:.2f}{'*' if i != j and p < 0.01 else ''}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "seaborn.heatmap(corr_matrix, xticklabels=columns_of_interest, yticklabels=columns_of_interest,\n",
    "            annot=annot_matrix, fmt='', cmap='coolwarm', center=0, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"})\n",
    "\n",
    "plt.title('Correlation Matrix with FDR-corrected Significance (p < 0.01) \\n only significant neurons')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3fa36-3ce4-408f-ad82-82a4e6b70d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a05a598c",
   "metadata": {},
   "source": [
    "### rescale all trials with different temporal scale to the same so it's easy to average across trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef268626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def resample_time_series_column(df, column_name, target_len):\n",
    "    \"\"\"\n",
    "    Interpolates time series in a dataframe column to a fixed length.\n",
    "    - df: DataFrame with a column containing lists/arrays of time series\n",
    "    - column_name: name of the column to interpolate (e.g., 'FR_ievent')\n",
    "    - target_len: desired length after resampling\n",
    "    Returns:\n",
    "        New column with interpolated time series.\n",
    "    \"\"\"\n",
    "    resampled_col = []\n",
    "    for ts in df[column_name]:\n",
    "        ts = np.array(ts)\n",
    "        orig_len = len(ts)\n",
    "        if orig_len < 2:\n",
    "            resampled_col.append(np.full(target_len, np.nan))  # skip or fill if too short\n",
    "            continue\n",
    "        x_orig = np.linspace(0, 1, orig_len)\n",
    "        x_new = np.linspace(0, 1, target_len)\n",
    "        interp_fn = interp1d(x_orig, ts, kind='linear', fill_value=\"extrapolate\")\n",
    "        ts_resampled = interp_fn(x_new)\n",
    "        resampled_col.append(ts_resampled)\n",
    "    return resampled_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b14c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this to each column to normalize\n",
    "# df = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df = bhvevents_filtered_mergedneurons_df.copy()\n",
    "\n",
    "try:\n",
    "    df = df.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df = df\n",
    "\n",
    "target_len = 4*fps  # choose based on average trial length (pull_rt length)\n",
    "\n",
    "for col in ['FR_ievent', 'socialgaze_prob', 'otherpull_prob', \n",
    "            'selfpull_prob','self_PC1','other_PC1']:\n",
    "    df[f'{col}_resampled'] = resample_time_series_column(df, col, target_len=target_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average trials in each quantile of the target variable\n",
    "\n",
    "quantile_tgt_name = 'other_PC1_std' # 'pull_rt' or 'socialgaze_auc' or 'other_PC1_std' or etc\n",
    "\n",
    "df[quantile_tgt_name+'_bin'] = pd.qcut(df[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Time axis\n",
    "time = np.linspace(0, 1, 120)\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over bins\n",
    "for label in ['low', 'medium', 'high']:\n",
    "        \n",
    "    group = df[df[quantile_tgt_name+'_bin'] == label]['FR_ievent_resampled'].dropna()\n",
    "\n",
    "    # Convert list of arrays to 2D matrix\n",
    "    stacked = np.stack(group.values)\n",
    "    \n",
    "    # Compute mean and SEM\n",
    "    mean_trace = np.nanmean(stacked, axis=0)\n",
    "    error_trace = sem(stacked, axis=0, nan_policy='omit')\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error_trace, mean_trace + error_trace, alpha=0.3)\n",
    "\n",
    "# Finalize plot\n",
    "plt.xlabel('Normalized Time (0 to 1)')\n",
    "plt.ylabel('Firing Rate (resampled)')\n",
    "plt.title('Firing Rate by '+quantile_tgt_name+' Quantiles')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838f563",
   "metadata": {},
   "source": [
    "### padding with NaN at the end of each trial to align at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda2390-2aaf-4344-bf1b-d1a180eced44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df_clean = bhvevents_filtered_mergedneurons_df.copy()\n",
    "\n",
    "try:\n",
    "    df_clean = df_clean.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df_clean = df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine max trial length\n",
    "\n",
    "quantile_tgt_name = 'other_PC1_std' # 'pull_rt' or 'socialgaze_auc' or 'other_PC1_std' or etc\n",
    "\n",
    "cont_vari_name = 'FR_ievent' # FR_ievent; socialgaze_prob; mass_move_speed\n",
    "\n",
    "target_len = df_clean[cont_vari_name].apply(len).max()\n",
    "\n",
    "def pad_align_start(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([ts, np.full(pad_len, np.nan)])\n",
    "    else:\n",
    "        return ts[:target_len]\n",
    "\n",
    "#\n",
    "df_clean[cont_vari_name+'_aligned'] = df_clean[cont_vari_name].apply(lambda ts: pad_align_start(ts, target_len))\n",
    "\n",
    "df_clean[quantile_tgt_name+'_bin'] = pd.qcut(df_clean[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = np.arange(target_len) / fps  # since we're aligned to trial start\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for label in ['low', 'medium', 'high']:\n",
    "    group = df_clean[df_clean[quantile_tgt_name+'_bin'] == label][cont_vari_name+'_aligned'].dropna()\n",
    "    traces = np.stack(group.values)\n",
    "\n",
    "    mean_trace = np.nanmean(traces, axis=0)\n",
    "    error = sem(traces, axis=0, nan_policy='omit')\n",
    "\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error, mean_trace + error, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (s, aligned to trial start)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.title(cont_vari_name+' Aligned to Trial Start by '+quantile_tgt_name+' Quantile')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfc0b0-465b-4704-8d8b-484959110ad2",
   "metadata": {},
   "source": [
    "### padding with NaN from the beginning to align everything to the pull event at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c043f2-c127-46e7-a569-737a83ecfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set target length to max length in FR_ievent\n",
    "\n",
    "quantile_tgt_name = 'other_PC1_std' # 'pull_rt' or 'socialgaze_auc' or 'other_PC1_std' or etc\n",
    "\n",
    "cont_vari_name = 'FR_ievent' # FR_ievent; socialgaze_prob; mass_move_speed\n",
    "\n",
    "# Compute the maximum length of trials\n",
    "target_len = df_clean[cont_vari_name].apply(len).max()\n",
    "\n",
    "def pad_align_end(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([np.full(pad_len, np.nan), ts])\n",
    "    else:\n",
    "        return ts[-target_len:]  # just in case\n",
    "\n",
    "# apply the padding\n",
    "df_clean[cont_vari_name+'_aligned'] = df_clean[cont_vari_name].apply(lambda ts: pad_align_end(ts, target_len))\n",
    "\n",
    "# Step 4: Bin pull_rt into quantiles\n",
    "df_clean[quantile_tgt_name+'_bin'] = pd.qcut(df_clean[quantile_tgt_name], q=3, labels=['low', 'medium', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6638f0f-99b3-40a8-835d-c58bbbc6bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the plotting\n",
    "time = np.linspace(-target_len / fps, 0, target_len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for label in ['low', 'medium', 'high']:\n",
    "    group = df_clean[df_clean[quantile_tgt_name+'_bin'] == label][cont_vari_name+'_aligned'].dropna()\n",
    "    traces = np.stack(group.values)\n",
    "\n",
    "    mean_trace = np.nanmean(traces, axis=0)\n",
    "    error = sem(traces, axis=0, nan_policy='omit')\n",
    "\n",
    "    plt.plot(time, mean_trace, label=label)\n",
    "    plt.fill_between(time, mean_trace - error, mean_trace + error, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (s, aligned to pull)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.title('Aligned '+cont_vari_name+' by '+quantile_tgt_name+' Quantile')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0b912-5884-4263-b91e-425d8ec30f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfe33-19d7-4351-bff0-d3bd53de1cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506d7a1-5e1e-4abe-b416-0ebe75530901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb6844-b9fc-4adc-a050-a98da178b76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46978d28-da6b-4366-ae71-bfdf53577c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0db6b6-e5eb-4dda-b5a4-c02c68bd5dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57403280-29f8-402a-916a-6a758d216386",
   "metadata": {},
   "source": [
    "### Use svm or other method to separate sucessful pull and failed pull;\n",
    "#### training based on FR or some behabioral variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# df_clean = bhvevents_aligned_FR_allevents_all_dates_df.copy()\n",
    "# df_clean = bhvevents_aligned_FR_allevents_alldates_mergedneurons_df.copy()\n",
    "df_clean = bhvevents_filtered_mergedneurons_df.copy()\n",
    "# df_clean = bhvevents_filtered_df.copy()\n",
    "\n",
    "\n",
    "# svm_tgt_name = 'FR_ievent' # FR_ievent; socialgaze_prob;\n",
    "svm_tgt_name = 'socialgaze_prob' # FR_ievent; socialgaze_prob;\n",
    "\n",
    "try:\n",
    "    df_clean = df_clean.rename(columns={'FR_ievent_avg': 'FR_ievent'})\n",
    "except:\n",
    "    df_clean = df_clean\n",
    "\n",
    "# Compute the maximum length of trials\n",
    "target_len = df_clean[svm_tgt_name].apply(len).max()\n",
    "\n",
    "def pad_align_end(ts, target_len):\n",
    "    ts = np.asarray(ts)\n",
    "    pad_len = target_len - len(ts)\n",
    "    if pad_len > 0:\n",
    "        return np.concatenate([np.full(pad_len, np.nan), ts])\n",
    "    else:\n",
    "        return ts[-target_len:]  # just in case\n",
    "\n",
    "# apply the padding\n",
    "df_clean[svm_tgt_name+'_aligned'] = df_clean[svm_tgt_name].apply(lambda ts: pad_align_end(ts, target_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_iterations = 100\n",
    "n_trials_per_class = 300\n",
    "\n",
    "# Extract\n",
    "X_full = df_clean[f'{svm_tgt_name}_aligned'].tolist()\n",
    "y_full = df_clean['pull_outcome'].values\n",
    "\n",
    "# Preprocess into 2D array (trials x time)\n",
    "max_len = max(len(x) for x in X_full)\n",
    "X = np.array([np.pad(x, (max_len - len(x), 0), constant_values=np.nan) for x in X_full])  # pad front\n",
    "y = np.array(y_full)\n",
    "\n",
    "# Collect AUCs over time\n",
    "n_timepoints = X.shape[1]\n",
    "auc_over_time = np.zeros((n_iterations, n_timepoints))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    idx_success = np.random.choice(np.where(y == 1)[0], size=n_trials_per_class, replace=True)\n",
    "    idx_fail = np.random.choice(np.where(y == 0)[0], size=n_trials_per_class, replace=True)\n",
    "    idx_sample = np.concatenate([idx_success, idx_fail])\n",
    "    \n",
    "    X_sample = X[idx_sample]\n",
    "    y_sample = y[idx_sample]\n",
    "\n",
    "    # Shuffle the samples\n",
    "    shuffle_idx = np.random.permutation(len(y_sample))\n",
    "    X_sample = X_sample[shuffle_idx]\n",
    "    y_sample = y_sample[shuffle_idx]\n",
    "\n",
    "    # Train/test split (70/30)\n",
    "    split_idx = int(0.7 * len(y_sample))\n",
    "    X_train, X_test = X_sample[:split_idx], X_sample[split_idx:]\n",
    "    y_train, y_test = y_sample[:split_idx], y_sample[split_idx:]\n",
    "\n",
    "    # Time-point-wise SVM\n",
    "    for t in range(n_timepoints):\n",
    "        # Drop NaNs for this time point\n",
    "        train_valid = ~np.isnan(X_train[:, t])\n",
    "        test_valid = ~np.isnan(X_test[:, t])\n",
    "\n",
    "        try:\n",
    "            if np.sum(train_valid) > 10 and np.sum(test_valid) > 10:\n",
    "                clf = SVC(kernel='linear', probability=True)\n",
    "                clf.fit(X_train[train_valid, t].reshape(-1, 1), y_train[train_valid])\n",
    "                y_proba = clf.predict_proba(X_test[test_valid, t].reshape(-1, 1))[:, 1]\n",
    "                auc = roc_auc_score(y_test[test_valid], y_proba)\n",
    "                auc_over_time[i, t] = auc\n",
    "            else:\n",
    "                auc_over_time[i, t] = np.nan\n",
    "        except:\n",
    "                auc_over_time[i, t] = np.nan\n",
    "\n",
    "\n",
    "# do some plotting\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "n_timepoints = auc_over_time.shape[1]\n",
    "time_axis = (np.arange(n_timepoints) - n_timepoints) / fps  # negative to 0\n",
    "\n",
    "# Compute mean and stderr\n",
    "mean_auc = np.nanmean(auc_over_time, axis=0)\n",
    "stderr_auc = np.nanstd(auc_over_time, axis=0) / np.sqrt(n_iterations)\n",
    "\n",
    "# Perform one-sample t-test against chance level (0.5)\n",
    "t_vals, p_vals = ttest_1samp(auc_over_time, popmean=0.5, axis=0, nan_policy='omit')\n",
    "\n",
    "significant = p_vals < 0.05\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_axis, mean_auc, label='Mean AUC')\n",
    "plt.fill_between(time_axis, mean_auc - stderr_auc, mean_auc + stderr_auc, alpha=0.3)\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "plt.scatter(time_axis[significant], mean_auc[significant], color='red', s=20, label='p < 0.05 (FDR)')\n",
    "plt.title(f'SVM Decoding: Predicting Pull Outcome from {svm_tgt_name}_aligned')\n",
    "plt.xlabel('Time (s, aligned to pull)')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee19f-a861-46b1-a470-895d542c40b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d734ce-fd42-434d-a65e-ebb2ea39b9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20414058-86a4-4267-b249-713b223c9090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697abb96-f779-47fc-baf6-f386196b3b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba5ade-bb09-4b20-8fae-72c3f604b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15422f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
