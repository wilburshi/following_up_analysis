{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6246b",
   "metadata": {},
   "source": [
    "### This analysis is based on the pull aligned continuous bhv variables and neural activity analysis\n",
    "### The goal of this code is to define GLM model and test the hypothesis that social gaze before pull serves as a evidence accumulation process, and test if the neural profile matches the accumulation hypothesis\n",
    "### note, the glm will be used to fit the behavioral data only, so it's different from the neuralGLM code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5fddb6e-2287-43ec-b886-695ee9c0f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # Added import for Pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from dPCA import dPCA\n",
    "\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e635923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be able to use the functions in the ana_functions under /3d_recontruction_analysis_self_and_coop_task_neural_analysis/\n",
    "sys.path.append(os.path.abspath('../3d_recontruction_analysis_self_and_coop_task_neural_analysis/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0ed4",
   "metadata": {},
   "source": [
    "### function - get body part location for each pair of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1724bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.body_part_locs_eachpair import body_part_locs_eachpair\n",
    "from ana_functions.body_part_locs_singlecam import body_part_locs_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae6f98",
   "metadata": {},
   "source": [
    "### function - align the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b03438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_align import camera_align       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494356c2",
   "metadata": {},
   "source": [
    "### function - merge the two pairs of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bda6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.camera_merge import camera_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc7f6a",
   "metadata": {},
   "source": [
    "### function - find social gaze time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f6ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.find_socialgaze_timepoint import find_socialgaze_timepoint\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam import find_socialgaze_timepoint_singlecam\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody import find_socialgaze_timepoint_singlecam_wholebody\n",
    "from ana_functions.find_socialgaze_timepoint_singlecam_wholebody_2 import find_socialgaze_timepoint_singlecam_wholebody_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7e75b",
   "metadata": {},
   "source": [
    "### function - define time point of behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfb672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_timepoint import bhv_events_timepoint\n",
    "from ana_functions.bhv_events_timepoint_singlecam import bhv_events_timepoint_singlecam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ab8c",
   "metadata": {},
   "source": [
    "### function - plot behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2ba678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_bhv_events import plot_bhv_events\n",
    "from ana_functions.plot_bhv_events_levertube import plot_bhv_events_levertube\n",
    "from ana_functions.plot_continuous_bhv_var_singlecam import plot_continuous_bhv_var_singlecam\n",
    "from ana_functions.draw_self_loop import draw_self_loop\n",
    "import matplotlib.patches as mpatches \n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c724b",
   "metadata": {},
   "source": [
    "### function - plot inter-pull interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9327925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.plot_interpull_interval import plot_interpull_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535a6",
   "metadata": {},
   "source": [
    "### function - make demo videos with skeleton and inportant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.tracking_video_singlecam_demo import tracking_video_singlecam_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_demo import tracking_video_singlecam_wholebody_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_demo import tracking_video_singlecam_wholebody_withNeuron_demo\n",
    "from ana_functions.tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_video_singlecam_wholebody_withNeuron_sepbhv_demo\n",
    "from ana_functions.tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo import tracking_frame_singlecam_wholebody_withNeuron_sepbhv_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69920a6a",
   "metadata": {},
   "source": [
    "### function - interval between all behavioral events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.bhv_events_interval import bhv_events_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631177f",
   "metadata": {},
   "source": [
    "### function - spike analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a804dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.spike_analysis_FR_calculation import spike_analysis_FR_calculation\n",
    "from ana_functions.plot_spike_triggered_singlecam_bhvevent import plot_spike_triggered_singlecam_bhvevent\n",
    "from ana_functions.plot_bhv_events_aligned_FR import plot_bhv_events_aligned_FR\n",
    "from ana_functions.plot_strategy_aligned_FR import plot_strategy_aligned_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399a64",
   "metadata": {},
   "source": [
    "### function - PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd267a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_functions.PCA_around_bhv_events import PCA_around_bhv_events\n",
    "from ana_functions.PCA_around_bhv_events_video import PCA_around_bhv_events_video\n",
    "from ana_functions.confidence_ellipse import confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b030e",
   "metadata": {},
   "source": [
    "### function - other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2ec5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for defining the meaningful social gaze (the continuous gaze distribution that is closest to the pull) \n",
    "from ana_functions.keep_closest_cluster_single_trial import keep_closest_cluster_single_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04cf9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more information for each pull: number of preceding failed pull and time since last reward/successful pull\n",
    "from ana_functions.get_pull_infos import get_pull_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d1a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.continuous_variable_glm import continuous_variable_glm\n",
    "from functions.continuous_variable_glm_shortlist_prediction import continuous_variable_glm_shortlist_prediction\n",
    "from functions.continuous_variable_create_data_forGLM import continuous_variable_create_data_forGLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819c438",
   "metadata": {},
   "source": [
    "## Analyze each session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e84fc",
   "metadata": {},
   "source": [
    "### prepare the basic behavioral data (especially the time stamps for each bhv events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9dc05cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of using gaze angle threshold, use the target rectagon to deside gaze info\n",
    "# ...need to update\n",
    "sqr_thres_tubelever = 75 # draw the square around tube and lever\n",
    "sqr_thres_face = 1.15 # a ratio for defining face boundary\n",
    "sqr_thres_body = 4 # how many times to enlongate the face box boundry to the body\n",
    "\n",
    "\n",
    "# get the fps of the analyzed video\n",
    "fps = 30\n",
    "\n",
    "# get the fs for neural recording\n",
    "fs_spikes = 20000\n",
    "fs_lfp = 1000\n",
    "\n",
    "# frame number of the demo video\n",
    "nframes = 0.5*30 # second*30fps\n",
    "# nframes = 45*30 # second*30fps\n",
    "\n",
    "# re-analyze the video or not\n",
    "reanalyze_video = 0\n",
    "redo_anystep = 0\n",
    "\n",
    "# do OFC sessions or DLPFC sessions\n",
    "do_OFC = 0\n",
    "do_DLPFC  = 1\n",
    "if do_OFC:\n",
    "    savefile_sufix = '_OFCs'\n",
    "elif do_DLPFC:\n",
    "    savefile_sufix = '_DLPFCs'\n",
    "else:\n",
    "    savefile_sufix = ''\n",
    "    \n",
    "# all the videos (no misaligned ones)\n",
    "# aligned with the audio\n",
    "# get the session start time from \"videosound_bhv_sync.py/.ipynb\"\n",
    "# currently the session_start_time will be manually typed in. It can be updated after a better method is used\n",
    "\n",
    "\n",
    "# dodson ginger\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                    '20240531_Dodson_MC',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240603_Dodson_MC_and_SR',\n",
    "                                    '20240604_Dodson_MC',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240605_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240606_Dodson_MC_and_SR',\n",
    "                                    '20240607_Dodson_SR',\n",
    "                                    '20240610_Dodson_MC',\n",
    "                                    '20240611_Dodson_SR',\n",
    "                                    '20240612_Dodson_MC',\n",
    "                                    '20240613_Dodson_SR',\n",
    "                                    '20240620_Dodson_SR',\n",
    "                                    '20240719_Dodson_MC',\n",
    "                                        \n",
    "                                    '20250129_Dodson_MC',\n",
    "                                    '20250130_Dodson_SR',\n",
    "                                    '20250131_Dodson_MC',\n",
    "                                \n",
    "            \n",
    "                                    '20250210_Dodson_SR_withKoala',\n",
    "                                    '20250211_Dodson_MC_withKoala',\n",
    "                                    '20250212_Dodson_SR_withKoala',\n",
    "                                    '20250214_Dodson_MC_withKoala',\n",
    "                                    '20250217_Dodson_SR_withKoala',\n",
    "                                    '20250218_Dodson_MC_withKoala',\n",
    "                                    '20250219_Dodson_SR_withKoala',\n",
    "                                    '20250220_Dodson_MC_withKoala',\n",
    "                                    '20250224_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250226_Dodson_MC_withKoala',\n",
    "                                    '20250227_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250228_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250304_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250305_Dodson_MC_withKoala',\n",
    "                                    '20250306_Dodson_KoalaAL_withKoala',\n",
    "                                    '20250307_Dodson_DodsonAL_withKoala',\n",
    "                                    '20250310_Dodson_MC_withKoala',\n",
    "                                    '20250312_Dodson_NV_withKoala',\n",
    "                                    '20250313_Dodson_NV_withKoala',\n",
    "                                    '20250314_Dodson_NV_withKoala',\n",
    "            \n",
    "                                    '20250401_Dodson_MC_withKanga',\n",
    "                                    '20250402_Dodson_MC_withKanga',\n",
    "                                    '20250403_Dodson_MC_withKanga',\n",
    "                                    '20250404_Dodson_SR_withKanga',\n",
    "                                    '20250407_Dodson_SR_withKanga',\n",
    "                                    '20250408_Dodson_SR_withKanga',\n",
    "                                    '20250409_Dodson_MC_withKanga',\n",
    "            \n",
    "                                    '20250415_Dodson_MC_withKanga',\n",
    "                                    # '20250416_Dodson_SR_withKanga', # has to remove from the later analysis, recording has problems\n",
    "                                    '20250417_Dodson_MC_withKanga',\n",
    "                                    '20250418_Dodson_SR_withKanga',\n",
    "                                    '20250421_Dodson_SR_withKanga',\n",
    "                                    '20250422_Dodson_MC_withKanga',\n",
    "                                    '20250422_Dodson_SR_withKanga',\n",
    "            \n",
    "                                    '20250423_Dodson_MC_withKanga',\n",
    "                                    '20250423_Dodson_SR_withKanga', \n",
    "                                    '20250424_Dodson_NV_withKanga',\n",
    "                                    '20250424_Dodson_MC_withKanga',\n",
    "                                    '20250424_Dodson_SR_withKanga',            \n",
    "                                    '20250425_Dodson_NV_withKanga',\n",
    "                                    '20250425_Dodson_SR_withKanga',\n",
    "                                    '20250428_Dodson_NV_withKanga',\n",
    "                                    '20250428_Dodson_MC_withKanga',\n",
    "                                    '20250428_Dodson_SR_withKanga',  \n",
    "                                    '20250429_Dodson_NV_withKanga',\n",
    "                                    '20250429_Dodson_MC_withKanga',\n",
    "                                    '20250429_Dodson_SR_withKanga',  \n",
    "                                    '20250430_Dodson_NV_withKanga',\n",
    "                                    '20250430_Dodson_MC_withKanga',\n",
    "                                    '20250430_Dodson_SR_withKanga',  \n",
    "            \n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',           \n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'MC',\n",
    "                            \n",
    "                            'MC_withGingerNew',\n",
    "                            'SR_withGingerNew',\n",
    "                            'MC_withGingerNew',\n",
    "            \n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'SR_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'MC_KoalaAuto_withKoala',\n",
    "                            'MC_DodsonAuto_withKoala',\n",
    "                            'MC_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "                            'NV_withKoala',\n",
    "\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            # 'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',\n",
    "            \n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga', \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',            \n",
    "                            'NV_withKanga',\n",
    "                            'SR_withKanga',\n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                            'NV_withKanga',\n",
    "                            'MC_withKanga',\n",
    "                            'SR_withKanga',  \n",
    "                          ]\n",
    "        dates_list = [\n",
    "                        '20240531',\n",
    "                        '20240603_MC',\n",
    "                        '20240603_SR',\n",
    "                        '20240604',\n",
    "                        '20240605_MC',\n",
    "                        '20240605_SR',\n",
    "                        '20240606_MC',\n",
    "                        '20240606_SR',\n",
    "                        '20240607',\n",
    "                        '20240610_MC',\n",
    "                        '20240611',\n",
    "                        '20240612',\n",
    "                        '20240613',\n",
    "                        '20240620',\n",
    "                        '20240719',\n",
    "            \n",
    "                        '20250129',\n",
    "                        '20250130',\n",
    "                        '20250131',\n",
    "            \n",
    "                        '20250210',\n",
    "                        '20250211',\n",
    "                        '20250212',\n",
    "                        '20250214',\n",
    "                        '20250217',\n",
    "                        '20250218',\n",
    "                        '20250219',\n",
    "                        '20250220',\n",
    "                        '20250224',\n",
    "                        '20250226',\n",
    "                        '20250227',\n",
    "                        '20250228',\n",
    "                        '20250304',\n",
    "                        '20250305',\n",
    "                        '20250306',\n",
    "                        '20250307',\n",
    "                        '20250310',\n",
    "                        '20250312',\n",
    "                        '20250313',\n",
    "                        '20250314',\n",
    "            \n",
    "                        '20250401',\n",
    "                        '20250402',\n",
    "                        '20250403',\n",
    "                        '20250404',\n",
    "                        '20250407',\n",
    "                        '20250408',\n",
    "                        '20250409',\n",
    "            \n",
    "                        '20250415',\n",
    "                        # '20250416',\n",
    "                        '20250417',\n",
    "                        '20250418',\n",
    "                        '20250421',\n",
    "                        '20250422',\n",
    "                        '20250422_SR',\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = [\n",
    "                            '20240531',\n",
    "                            '20240603',\n",
    "                            '20240603',\n",
    "                            '20240604',\n",
    "                            '20240605',\n",
    "                            '20240605',\n",
    "                            '20240606',\n",
    "                            '20240606',\n",
    "                            '20240607',\n",
    "                            '20240610_MC',\n",
    "                            '20240611',\n",
    "                            '20240612',\n",
    "                            '20240613',\n",
    "                            '20240620',\n",
    "                            '20240719',\n",
    "            \n",
    "                            '20250129',\n",
    "                            '20250130',\n",
    "                            '20250131',\n",
    "                            \n",
    "                            '20250210',\n",
    "                            '20250211',\n",
    "                            '20250212',\n",
    "                            '20250214',\n",
    "                            '20250217',\n",
    "                            '20250218',          \n",
    "                            '20250219',\n",
    "                            '20250220',\n",
    "                            '20250224',\n",
    "                            '20250226',\n",
    "                            '20250227',\n",
    "                            '20250228',\n",
    "                            '20250304',\n",
    "                            '20250305',\n",
    "                            '20250306',\n",
    "                            '20250307',\n",
    "                            '20250310',\n",
    "                            '20250312',\n",
    "                            '20250313',\n",
    "                            '20250314',\n",
    "            \n",
    "                            '20250401',\n",
    "                            '20250402',\n",
    "                            '20250403',\n",
    "                            '20250404',\n",
    "                            '20250407',\n",
    "                            '20250408',\n",
    "                            '20250409',\n",
    "            \n",
    "                            '20250415',\n",
    "                            # '20250416',\n",
    "                            '20250417',\n",
    "                            '20250418',\n",
    "                            '20250421',\n",
    "                            '20250422',\n",
    "                            '20250422_SR',\n",
    "            \n",
    "                            '20250423',\n",
    "                            '20250423_SR', \n",
    "                            '20250424',\n",
    "                            '20250424_MC',\n",
    "                            '20250424_SR',            \n",
    "                            '20250425',\n",
    "                            '20250425_SR',\n",
    "                            '20250428_NV',\n",
    "                            '20250428_MC',\n",
    "                            '20250428_SR',  \n",
    "                            '20250429_NV',\n",
    "                            '20250429_MC',\n",
    "                            '20250429_SR',  \n",
    "                            '20250430_NV',\n",
    "                            '20250430_MC',\n",
    "                            '20250430_SR',  \n",
    "            \n",
    "                          ] # to deal with the sessions that MC and SR were in the same session\n",
    "        session_start_times = [ \n",
    "                                0.00,\n",
    "                                340,\n",
    "                                340,\n",
    "                                72.0,\n",
    "                                60.1,\n",
    "                                60.1,\n",
    "                                82.2,\n",
    "                                82.2,\n",
    "                                35.8,\n",
    "                                0.00,\n",
    "                                29.2,\n",
    "                                35.8,\n",
    "                                62.5,\n",
    "                                71.5,\n",
    "                                54.4,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                73.5,\n",
    "                                0.00,\n",
    "                                76.1,\n",
    "                                81.5,\n",
    "                                0.00,\n",
    "            \n",
    "                                363,\n",
    "                                # 0.00,\n",
    "                                79.0,\n",
    "                                162.6,\n",
    "                                231.9,\n",
    "                                109,\n",
    "                                0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "            \n",
    "                              ] # in second\n",
    "        \n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = [ 'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0',# 'Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                              'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             \n",
    "                              ]\n",
    "        animal1_fixedorders = ['dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson',# 'dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        recordedanimals = animal1_fixedorders \n",
    "        animal2_fixedorders = ['ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger','ginger',\n",
    "                               'ginger','ginger','ginger','ginger','ginger','ginger','gingerNew','gingerNew','gingerNew',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala', 'koala',\n",
    "                               'koala', 'koala', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', # 'kanga', \n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                               'kanga', 'kanga', 'kanga', 'kanga', 'kanga',\n",
    "                              ]\n",
    "\n",
    "        animal1_filenames = [\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             'Dodson',# 'Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                             'Dodson','Dodson','Dodson','Dodson','Dodson',\n",
    "                            ]\n",
    "        animal2_filenames = [\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\"Ginger\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\", \"Koala\",\n",
    "                             \"Koala\", \"Koala\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                             'Kanga', # 'Kanga', \n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                             'Kanga', 'Kanga', 'Kanga', 'Kanga', 'Kanga',\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     '20231101_Dodson_withGinger_MC',\n",
    "                                     '20231107_Dodson_withGinger_MC',\n",
    "                                     '20231122_Dodson_withGinger_MC',\n",
    "                                     '20231129_Dodson_withGinger_MC',\n",
    "                                     '20231101_Dodson_withGinger_SR',\n",
    "                                     '20231107_Dodson_withGinger_SR',\n",
    "                                     '20231122_Dodson_withGinger_SR',\n",
    "                                     '20231129_Dodson_withGinger_SR',\n",
    "                                   ]\n",
    "        task_conditions = [\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'MC',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                            'SR',\n",
    "                          ]\n",
    "        dates_list = [\n",
    "                      \"20231101_MC\",\n",
    "                      \"20231107_MC\",\n",
    "                      \"20231122_MC\",\n",
    "                      \"20231129_MC\",\n",
    "                      \"20231101_SR\",\n",
    "                      \"20231107_SR\",\n",
    "                      \"20231122_SR\",\n",
    "                      \"20231129_SR\",      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        session_start_times = [ \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                                 0.00,   \n",
    "                                 0.00,  \n",
    "                                 0.00,  \n",
    "                                 0.00, \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                         2, \n",
    "                         2, \n",
    "                         4, \n",
    "                         4,\n",
    "                       ]\n",
    "    \n",
    "        trig_channelnames = ['Dev1/ai0']*np.shape(dates_list)[0]\n",
    "        animal1_fixedorder = ['dodson']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal1_fixedorders\n",
    "        animal2_fixedorder = ['ginger']*np.shape(dates_list)[0]\n",
    "\n",
    "        animal1_filename = [\"Dodson\"]*np.shape(dates_list)[0]\n",
    "        animal2_filename = [\"Ginger\"]*np.shape(dates_list)[0]\n",
    "\n",
    "    \n",
    "# dannon kanga\n",
    "if 1:\n",
    "    if do_DLPFC:\n",
    "        neural_record_conditions = [\n",
    "                                     '20240508_Kanga_SR',\n",
    "                                     '20240509_Kanga_MC',\n",
    "                                     '20240513_Kanga_MC',\n",
    "                                     '20240514_Kanga_SR',\n",
    "                                     '20240523_Kanga_MC',\n",
    "                                     '20240524_Kanga_SR',\n",
    "                                     '20240606_Kanga_MC',\n",
    "                                     '20240613_Kanga_MC_DannonAuto',\n",
    "                                     '20240614_Kanga_MC_DannonAuto',\n",
    "                                     '20240617_Kanga_MC_DannonAuto',\n",
    "                                     '20240618_Kanga_MC_KangaAuto',\n",
    "                                     '20240619_Kanga_MC_KangaAuto',\n",
    "                                     '20240620_Kanga_MC_KangaAuto',\n",
    "                                     '20240621_1_Kanga_NoVis',\n",
    "                                     '20240624_Kanga_NoVis',\n",
    "                                     '20240626_Kanga_NoVis',\n",
    "            \n",
    "                                     '20240808_Kanga_MC_withGinger',\n",
    "                                     '20240809_Kanga_MC_withGinger',\n",
    "                                     '20240812_Kanga_MC_withGinger',\n",
    "                                     '20240813_Kanga_MC_withKoala',\n",
    "                                     '20240814_Kanga_MC_withKoala',\n",
    "                                     '20240815_Kanga_MC_withKoala',\n",
    "                                     '20240819_Kanga_MC_withVermelho',\n",
    "                                     '20240821_Kanga_MC_withVermelho',\n",
    "                                     '20240822_Kanga_MC_withVermelho',\n",
    "            \n",
    "                                     '20250415_Kanga_MC_withDodson',\n",
    "                                     '20250416_Kanga_SR_withDodson',\n",
    "                                     '20250417_Kanga_MC_withDodson',\n",
    "                                     '20250418_Kanga_SR_withDodson',\n",
    "                                     '20250421_Kanga_SR_withDodson',\n",
    "                                     '20250422_Kanga_MC_withDodson',\n",
    "                                     '20250422_Kanga_SR_withDodson',\n",
    "            \n",
    "                                    '20250423_Kanga_MC_withDodson',\n",
    "                                    '20250423_Kanga_SR_withDodson', \n",
    "                                    '20250424_Kanga_NV_withDodson',\n",
    "                                    '20250424_Kanga_MC_withDodson',\n",
    "                                    '20250424_Kanga_SR_withDodson',            \n",
    "                                    '20250425_Kanga_NV_withDodson',\n",
    "                                    '20250425_Kanga_SR_withDodson',\n",
    "                                    '20250428_Kanga_NV_withDodson',\n",
    "                                    '20250428_Kanga_MC_withDodson',\n",
    "                                    '20250428_Kanga_SR_withDodson',  \n",
    "                                    '20250429_Kanga_NV_withDodson',\n",
    "                                    '20250429_Kanga_MC_withDodson',\n",
    "                                    '20250429_Kanga_SR_withDodson',  \n",
    "                                    '20250430_Kanga_NV_withDodson',\n",
    "                                    '20250430_Kanga_MC_withDodson',\n",
    "                                    '20250430_Kanga_SR_withDodson',  \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \"20240508\",\n",
    "                      \"20240509\",\n",
    "                      \"20240513\",\n",
    "                      \"20240514\",\n",
    "                      \"20240523\",\n",
    "                      \"20240524\",\n",
    "                      \"20240606\",\n",
    "                      \"20240613\",\n",
    "                      \"20240614\",\n",
    "                      \"20240617\",\n",
    "                      \"20240618\",\n",
    "                      \"20240619\",\n",
    "                      \"20240620\",\n",
    "                      \"20240621_1\",\n",
    "                      \"20240624\",\n",
    "                      \"20240626\",\n",
    "            \n",
    "                      \"20240808\",\n",
    "                      \"20240809\",\n",
    "                      \"20240812\",\n",
    "                      \"20240813\",\n",
    "                      \"20240814\",\n",
    "                      \"20240815\",\n",
    "                      \"20240819\",\n",
    "                      \"20240821\",\n",
    "                      \"20240822\",\n",
    "            \n",
    "                      \"20250415\",\n",
    "                      \"20250416\",\n",
    "                      \"20250417\",\n",
    "                      \"20250418\",\n",
    "                      \"20250421\",\n",
    "                      \"20250422\",\n",
    "                      \"20250422_SR\",\n",
    "            \n",
    "                        '20250423',\n",
    "                        '20250423_SR', \n",
    "                        '20250424',\n",
    "                        '20250424_MC',\n",
    "                        '20250424_SR',            \n",
    "                        '20250425',\n",
    "                        '20250425_SR',\n",
    "                        '20250428_NV',\n",
    "                        '20250428_MC',\n",
    "                        '20250428_SR',  \n",
    "                        '20250429_NV',\n",
    "                        '20250429_MC',\n",
    "                        '20250429_SR',  \n",
    "                        '20250430_NV',\n",
    "                        '20250430_MC',\n",
    "                        '20250430_SR',  \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'SR',\n",
    "                             'MC',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_DannonAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'MC_KangaAuto',\n",
    "                             'NV',\n",
    "                             'NV',\n",
    "                             'NV',   \n",
    "                            \n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withGinger',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withKoala',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "                             'MC_withVermelho',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'SR_withDodson',\n",
    "                             'MC_withDodson',\n",
    "                             'SR_withDodson',\n",
    "            \n",
    "                             'MC_withDodson',\n",
    "                            'SR_withDodson', \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',            \n",
    "                            'NV_withDodson',\n",
    "                            'SR_withDodson',\n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                            'NV_withDodson',\n",
    "                            'MC_withDodson',\n",
    "                            'SR_withDodson',  \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                 0.00,\n",
    "                                 36.0,\n",
    "                                 69.5,\n",
    "                                 0.00,\n",
    "                                 62.0,\n",
    "                                 0.00,\n",
    "                                 89.0,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 165.8,\n",
    "                                 96.0, \n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 0.00,\n",
    "                                 48.0,\n",
    "                                \n",
    "                                 59.2,\n",
    "                                 49.5,\n",
    "                                 40.0,\n",
    "                                 50.0,\n",
    "                                 0.00,\n",
    "                                 69.8,\n",
    "                                 85.0,\n",
    "                                 212.9,\n",
    "                                 68.5,\n",
    "            \n",
    "                                 363,\n",
    "                                 0.00,\n",
    "                                 79.0,\n",
    "                                 162.6,\n",
    "                                 231.9,\n",
    "                                 109,\n",
    "                                 0.00,\n",
    "            \n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,          \n",
    "                                0.00,\n",
    "                                93.0,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00,\n",
    "                                0.00, \n",
    "                                0.00,\n",
    "                                274.4,\n",
    "                                0.00,\n",
    "                              ] # in second\n",
    "        kilosortvers = list((np.ones(np.shape(dates_list))*4).astype(int))\n",
    "        \n",
    "        trig_channelnames = ['Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai0','Dev1/ai0',\n",
    "                             'Dev1/ai0','Dev1/ai0','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                             'Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9','Dev1/ai9',\n",
    "                              ]\n",
    "        \n",
    "        animal1_fixedorders = ['dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'dannon','dannon','dannon','dannon','dannon','dannon','dannon','dannon',\n",
    "                               'ginger','ginger','ginger','koala','koala','koala','vermelho','vermelho',\n",
    "                               'vermelho','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                               'dodson','dodson','dodson','dodson','dodson','dodson','dodson','dodson',\n",
    "                              ]\n",
    "        animal2_fixedorders = ['kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                               'kanga','kanga','kanga','kanga','kanga','kanga','kanga','kanga',\n",
    "                              ]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "\n",
    "        animal1_filenames = [\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\"Dannon\",\n",
    "                             \"Ginger\",\"Ginger\",\"Ginger\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\", \"Kanga\",\n",
    "                              \"Kanga\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\"Dodson\",\n",
    "                             \n",
    "                            ]\n",
    "        animal2_filenames = [\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Koala\",\"Koala\",\"Koala\",\"Vermelho\",\"Vermelho\",\n",
    "                             \"Vermelho\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                             \"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\"Kanga\",\n",
    "                            ]\n",
    "        \n",
    "    elif do_OFC:\n",
    "        # pick only five sessions for each conditions\n",
    "        neural_record_conditions = [\n",
    "                                     \n",
    "                                   ]\n",
    "        dates_list = [\n",
    "                      \n",
    "                     ]\n",
    "        videodates_list = dates_list\n",
    "        task_conditions = [\n",
    "                           \n",
    "                          ]\n",
    "        session_start_times = [ \n",
    "                                \n",
    "                              ] # in second\n",
    "        kilosortvers = [ \n",
    "\n",
    "                       ]\n",
    "    \n",
    "        animal1_fixedorders = ['dannon']*np.shape(dates_list)[0]\n",
    "        animal2_fixedorders = ['kanga']*np.shape(dates_list)[0]\n",
    "        recordedanimals = animal2_fixedorders\n",
    "        \n",
    "        animal1_filenames = [\"Dannon\"]*np.shape(dates_list)[0]\n",
    "        animal2_filenames = [\"Kanga\"]*np.shape(dates_list)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# a test case\n",
    "if 0: # kanga example\n",
    "    neural_record_conditions = ['20250415_Kanga_MC_withDodson']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withDodson']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai9']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    recordedanimals = animal2_fixedorders\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "if 0: # dodson example \n",
    "    neural_record_conditions = ['20250415_Dodson_MC_withKanga']\n",
    "    dates_list = [\"20250415\"]\n",
    "    videodates_list = dates_list\n",
    "    task_conditions = ['MC_withKanga']\n",
    "    session_start_times = [363] # in second\n",
    "    kilosortvers = [4]\n",
    "    trig_channelnames = ['Dev1/ai0']\n",
    "    animal1_fixedorders = ['dodson']\n",
    "    recordedanimals = animal1_fixedorders\n",
    "    animal2_fixedorders = ['kanga']\n",
    "    animal1_filenames = [\"Dodson\"]\n",
    "    animal2_filenames = [\"Kanga\"]\n",
    "    \n",
    "ndates = np.shape(dates_list)[0]\n",
    "\n",
    "session_start_frames = session_start_times * fps # fps is 30Hz\n",
    "\n",
    "# totalsess_time = 600\n",
    "\n",
    "# video tracking results info\n",
    "animalnames_videotrack = ['dodson','scorch'] # does not really mean dodson and scorch, instead, indicate animal1 and animal2\n",
    "bodypartnames_videotrack = ['rightTuft','whiteBlaze','leftTuft','rightEye','leftEye','mouth']\n",
    "\n",
    "\n",
    "# which camera to analyzed\n",
    "cameraID = 'camera-2'\n",
    "cameraID_short = 'cam2'\n",
    "\n",
    "considerlevertube = 1\n",
    "considertubeonly = 0\n",
    "\n",
    "# location of levers and tubes for camera 2\n",
    "# # camera 1\n",
    "# lever_locs_camI = {'dodson':np.array([645,600]),'scorch':np.array([425,435])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1350,630]),'scorch':np.array([555,345])}\n",
    "# # camera 2\n",
    "# # location of the estimiated middle of the box\n",
    "lever_locs_camI = {'dodson':np.array([1325,615]),'scorch':np.array([560,615])}\n",
    "# # location of the estimated lever\n",
    "# lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "tube_locs_camI  = {'dodson':np.array([1550,515]),'scorch':np.array([350,515])}\n",
    "# # old\n",
    "# # lever_locs_camI = {'dodson':np.array([1335,715]),'scorch':np.array([550,715])}\n",
    "# # tube_locs_camI  = {'dodson':np.array([1650,490]),'scorch':np.array([250,490])}\n",
    "# # camera 3\n",
    "# lever_locs_camI = {'dodson':np.array([1580,440]),'scorch':np.array([1296,540])}\n",
    "# tube_locs_camI  = {'dodson':np.array([1470,375]),'scorch':np.array([805,475])}\n",
    "\n",
    "\n",
    "if np.shape(session_start_times)[0] != np.shape(dates_list)[0]:\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# define glm data summarizing data set    \n",
    "glm_datas_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "glm_datas_shortlist_prediction_all_dates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "pre_data_for_GLM_alldates = dict.fromkeys(dates_list, [])\n",
    "\n",
    "# where to save the summarizing data\n",
    "data_saved_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/3d_recontruction_analysis_self_and_coop_task_data_saved/'\n",
    "\n",
    "# neural data folder\n",
    "neural_data_folder = '/gpfs/radev/pi/nandy/jadi_gibbs_data/Marmoset_neural_recording/'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbc08f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(neural_record_conditions))\n",
    "print(np.shape(task_conditions))\n",
    "print(np.shape(dates_list))\n",
    "print(np.shape(videodates_list)) \n",
    "print(np.shape(session_start_times))\n",
    "\n",
    "print(np.shape(kilosortvers))\n",
    "\n",
    "print(np.shape(trig_channelnames))\n",
    "print(np.shape(animal1_fixedorders)) \n",
    "print(np.shape(recordedanimals))\n",
    "print(np.shape(animal2_fixedorders))\n",
    "\n",
    "print(np.shape(animal1_filenames))\n",
    "print(np.shape(animal2_filenames))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93c7a76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze all dates\n",
      "load social gaze with camera-2 only of 20250415\n",
      "load the session wised data for GLM fitting\n",
      "load the result from GLM\n"
     ]
    }
   ],
   "source": [
    "# basic behavior analysis (define time stamps for each bhv events, etc)\n",
    "\n",
    "try:\n",
    "    if redo_anystep:\n",
    "        dummy\n",
    "    \n",
    "    # dummy \n",
    "    \n",
    "    #\n",
    "    print('loading all data')\n",
    "    \n",
    "    # load saved data\n",
    "    data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_glm'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "    \n",
    "    with open(data_saved_subfolder+'/glm_datas_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        glm_datas_all_dates = pickle.load(f)\n",
    "   \n",
    "    with open(data_saved_subfolder+'/glm_datas_shortlist_prediction_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        glm_datas_shortlist_prediction_all_dates = pickle.load(f)\n",
    "    \n",
    "    with open(data_saved_subfolder+'/pre_data_for_GLM_alldates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'rb') as f:\n",
    "        pre_data_for_GLM_alldates = pickle.load(f)\n",
    "        \n",
    "        \n",
    "    print('all data from all dates are loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('analyze all dates')\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "    \n",
    "        date_tgt = dates_list[idate]\n",
    "        videodate_tgt = videodates_list[idate]\n",
    "        \n",
    "        neural_record_condition = neural_record_conditions[idate]\n",
    "        \n",
    "        session_start_time = session_start_times[idate]\n",
    "        \n",
    "        kilosortver = kilosortvers[idate]\n",
    "\n",
    "        trig_channelname = trig_channelnames[idate]\n",
    "        \n",
    "        animal1_filename = animal1_filenames[idate]\n",
    "        animal2_filename = animal2_filenames[idate]\n",
    "        \n",
    "        animal1_fixedorder = [animal1_fixedorders[idate]]\n",
    "        animal2_fixedorder = [animal2_fixedorders[idate]]\n",
    "        \n",
    "        recordedanimal = recordedanimals[idate]\n",
    "        \n",
    "        # load behavioral results\n",
    "        try:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path +date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal2_filename+\"_\"+animal1_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            # \n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)   \n",
    "        except:\n",
    "            bhv_data_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/marmoset_tracking_bhv_data_from_task_code/\"+date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"/\"\n",
    "            trial_record_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_TrialRecord_\" + \"*.json\")\n",
    "            bhv_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_bhv_data_\" + \"*.json\")\n",
    "            session_info_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_session_info_\" + \"*.json\")\n",
    "            ni_data_json = glob.glob(bhv_data_path + date_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_ni_data_\" + \"*.json\")\n",
    "            #\n",
    "            trial_record = pd.read_json(trial_record_json[0])\n",
    "            bhv_data = pd.read_json(bhv_data_json[0])\n",
    "            session_info = pd.read_json(session_info_json[0])\n",
    "            #\n",
    "            with open(ni_data_json[0]) as f:\n",
    "                for line in f:\n",
    "                    ni_data=json.loads(line)\n",
    "\n",
    "        # get animal info from the session information\n",
    "        animal1 = session_info['lever1_animal'][0].lower()\n",
    "        animal2 = session_info['lever2_animal'][0].lower()\n",
    "\n",
    "        \n",
    "        # get task type and cooperation threshold\n",
    "        try:\n",
    "            coop_thres = session_info[\"pulltime_thres\"][0]\n",
    "            tasktype = session_info[\"task_type\"][0]\n",
    "        except:\n",
    "            coop_thres = 0\n",
    "            tasktype = 1\n",
    "    \n",
    "            \n",
    "        # clean up the trial_record\n",
    "        warnings.filterwarnings('ignore')\n",
    "        trial_record_clean = pd.DataFrame(columns=trial_record.columns)\n",
    "        # for itrial in np.arange(0,np.max(trial_record['trial_number']),1):\n",
    "        for itrial in trial_record['trial_number']:\n",
    "            # trial_record_clean.loc[itrial] = trial_record[trial_record['trial_number']==itrial+1].iloc[[0]]\n",
    "            trial_record_clean = trial_record_clean.append(trial_record[trial_record['trial_number']==itrial].iloc[[0]])\n",
    "        trial_record_clean = trial_record_clean.reset_index(drop = True)\n",
    "\n",
    "        # change bhv_data time to the absolute time\n",
    "        time_points_new = pd.DataFrame(np.zeros(np.shape(bhv_data)[0]),columns=[\"time_points_new\"])\n",
    "        # for itrial in np.arange(0,np.max(trial_record_clean['trial_number']),1):\n",
    "        for itrial in np.arange(0,np.shape(trial_record_clean)[0],1):\n",
    "            # ind = bhv_data[\"trial_number\"]==itrial+1\n",
    "            ind = bhv_data[\"trial_number\"]==trial_record_clean['trial_number'][itrial]\n",
    "            new_time_itrial = bhv_data[ind][\"time_points\"] + trial_record_clean[\"trial_starttime\"].iloc[itrial]\n",
    "            time_points_new[\"time_points_new\"][ind] = new_time_itrial\n",
    "        bhv_data[\"time_points\"] = time_points_new[\"time_points_new\"]\n",
    "        bhv_data = bhv_data[bhv_data[\"time_points\"] != 0]\n",
    "\n",
    "        \n",
    "        \n",
    "        # load behavioral event results\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'rb') as f:\n",
    "                output_look_ornot = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'rb') as f:\n",
    "                output_allvectors = pickle.load(f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'rb') as f:\n",
    "                output_allangles = pickle.load(f)  \n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'rb') as f:\n",
    "                output_key_locations = pickle.load(f)\n",
    "        except:   \n",
    "\n",
    "            # folder and file path\n",
    "            camera12_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera12/\"\n",
    "            camera23_analyzed_path = \"/gpfs/radev/pi/nandy/jadi_gibbs_data/VideoTracker_SocialInter/test_video_cooperative_task_3d/\"+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_camera23/\"\n",
    "            \n",
    "            # \n",
    "            try: \n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "                bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                    bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                    bodyparts_camI_camIJ = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"                \n",
    "                # get the bodypart data from files\n",
    "                bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "                video_file_original = camera12_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"\n",
    "            except:\n",
    "                singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_167500\"\n",
    "                bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_camera_withHeadchamberFeb28shuffle1_80000\"\n",
    "                    bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "                if not os.path.exists(bodyparts_camI_camIJ):\n",
    "                    singlecam_ana_type = \"DLC_dlcrnetms5_marmoset_tracking_with_middle_cameraSep1shuffle1_150000\"\n",
    "                    bodyparts_camI_camIJ = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+singlecam_ana_type+\"_el_filtered.h5\"\n",
    "            \n",
    "            # get the bodypart data from files\n",
    "            bodyparts_locs_camI = body_part_locs_singlecam(bodyparts_camI_camIJ,singlecam_ana_type,animalnames_videotrack,bodypartnames_videotrack,videodate_tgt)\n",
    "            video_file_original = camera23_analyzed_path+videodate_tgt+\"_\"+animal1_filename+\"_\"+animal2_filename+\"_\"+cameraID+\".mp4\"        \n",
    "        \n",
    "            \n",
    "            print('analyze social gaze with '+cameraID+' only of '+date_tgt)\n",
    "            # get social gaze information \n",
    "            output_look_ornot, output_allvectors, output_allangles = find_socialgaze_timepoint_singlecam_wholebody(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,\n",
    "                                                                                                                   considerlevertube,considertubeonly,sqr_thres_tubelever,\n",
    "                                                                                                                   sqr_thres_face,sqr_thres_body)\n",
    "            output_key_locations = find_socialgaze_timepoint_singlecam_wholebody_2(bodyparts_locs_camI,lever_locs_camI,tube_locs_camI,considerlevertube)\n",
    "            \n",
    "            # save data\n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            if not os.path.exists(add_date_dir):\n",
    "                os.makedirs(add_date_dir)\n",
    "            #\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_look_ornot.pkl', 'wb') as f:\n",
    "                pickle.dump(output_look_ornot, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allvectors.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allvectors, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_allangles.pkl', 'wb') as f:\n",
    "                pickle.dump(output_allangles, f)\n",
    "            with open(data_saved_folder+\"bhv_events_singlecam_wholebody/\"+animal1_fixedorder[0]+animal2_fixedorder[0]+\"/\"+cameraID+'/'+date_tgt+'/output_key_locations.pkl', 'wb') as f:\n",
    "                pickle.dump(output_key_locations, f)\n",
    "                \n",
    "\n",
    "        look_at_other_or_not_merge = output_look_ornot['look_at_other_or_not_merge']\n",
    "        look_at_tube_or_not_merge = output_look_ornot['look_at_tube_or_not_merge']\n",
    "        look_at_lever_or_not_merge = output_look_ornot['look_at_lever_or_not_merge']\n",
    "        look_at_otherlever_or_not_merge = output_look_ornot['look_at_otherlever_or_not_merge']\n",
    "        look_at_otherface_or_not_merge = output_look_ornot['look_at_otherface_or_not_merge']\n",
    "        \n",
    "        # change the unit to second and align to the start of the session\n",
    "        session_start_time = session_start_times[idate]\n",
    "        look_at_other_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_other_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_lever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_lever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_tube_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_tube_or_not_merge['dodson'])[0],1)/fps - session_start_time \n",
    "        look_at_otherlever_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherlever_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "        look_at_otherface_or_not_merge['time_in_second'] = np.arange(0,np.shape(look_at_otherface_or_not_merge['dodson'])[0],1)/fps - session_start_time\n",
    "\n",
    "        \n",
    "        # find time point of behavioral events\n",
    "        output_time_points_socialgaze ,output_time_points_levertube = bhv_events_timepoint_singlecam(bhv_data,look_at_other_or_not_merge,look_at_lever_or_not_merge,look_at_tube_or_not_merge)\n",
    "        time_point_pull1 = output_time_points_socialgaze['time_point_pull1']\n",
    "        time_point_pull2 = output_time_points_socialgaze['time_point_pull2']\n",
    "        oneway_gaze1 = output_time_points_socialgaze['oneway_gaze1']\n",
    "        oneway_gaze2 = output_time_points_socialgaze['oneway_gaze2']\n",
    "        mutual_gaze1 = output_time_points_socialgaze['mutual_gaze1']\n",
    "        mutual_gaze2 = output_time_points_socialgaze['mutual_gaze2']\n",
    "        lever_gaze1 = output_time_points_levertube['time_point_lookatlever1']\n",
    "        lever_gaze2 = output_time_points_levertube['time_point_lookatlever2']\n",
    "        # \n",
    "        # mostly just for the sessions in which MC and SR are in the same session \n",
    "        firstpulltime = np.nanmin([np.nanmin(time_point_pull1),np.nanmin(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1>(firstpulltime-15)] # 15s before the first pull (animal1 or 2) count as the active period\n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2>(firstpulltime-15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1>(firstpulltime-15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2>(firstpulltime-15)]  \n",
    "        lever_gaze1 = lever_gaze1[lever_gaze1>(firstpulltime-15)]\n",
    "        lever_gaze2 = lever_gaze2[lever_gaze2>(firstpulltime-15)]\n",
    "        #    \n",
    "        # newly added condition: only consider gaze during the active pulling time (15s after the last pull)    \n",
    "        lastpulltime = np.nanmax([np.nanmax(time_point_pull1),np.nanmax(time_point_pull2)])\n",
    "        oneway_gaze1 = oneway_gaze1[oneway_gaze1<(lastpulltime+15)]    \n",
    "        oneway_gaze2 = oneway_gaze2[oneway_gaze2<(lastpulltime+15)]\n",
    "        mutual_gaze1 = mutual_gaze1[mutual_gaze1<(lastpulltime+15)]\n",
    "        mutual_gaze2 = mutual_gaze2[mutual_gaze2<(lastpulltime+15)] \n",
    "        lever_gaze1 = lever_gaze1[lever_gaze1<(lastpulltime+15)] \n",
    "        lever_gaze2 = lever_gaze2[lever_gaze2<(lastpulltime+15)] \n",
    "            \n",
    "        # define successful pulls and failed pulls\n",
    "        # a new definition of successful and failed pulls\n",
    "        # separate successful and failed pulls\n",
    "        # step 1 all pull and juice\n",
    "        time_point_pull1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==1]\n",
    "        time_point_pull2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==2]\n",
    "        time_point_juice1 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==3]\n",
    "        time_point_juice2 = bhv_data[\"time_points\"][bhv_data[\"behavior_events\"]==4]\n",
    "        # step 2:\n",
    "        # pull 1\n",
    "        # Find the last pull before each juice\n",
    "        successful_pull1 = [time_point_pull1[time_point_pull1 < juice].max() for juice in time_point_juice1]\n",
    "        # Convert to Pandas Series\n",
    "        successful_pull1 = pd.Series(successful_pull1, index=time_point_juice1.index)\n",
    "        # Find failed pulls (pulls that are not successful)\n",
    "        failed_pull1 = time_point_pull1[~time_point_pull1.isin(successful_pull1)]\n",
    "        # pull 2\n",
    "        # Find the last pull before each juice\n",
    "        successful_pull2 = [time_point_pull2[time_point_pull2 < juice].max() for juice in time_point_juice2]\n",
    "        # Convert to Pandas Series\n",
    "        successful_pull2 = pd.Series(successful_pull2, index=time_point_juice2.index)\n",
    "        # Find failed pulls (pulls that are not successful)\n",
    "        failed_pull2 = time_point_pull2[~time_point_pull2.isin(successful_pull2)]\n",
    "        #\n",
    "        # step 3:\n",
    "        time_point_pull1_succ = np.round(successful_pull1,1)\n",
    "        time_point_pull2_succ = np.round(successful_pull2,1)\n",
    "        time_point_pull1_fail = np.round(failed_pull1,1)\n",
    "        time_point_pull2_fail = np.round(failed_pull2,1)\n",
    "        # \n",
    "        time_point_pulls_succfail = { \"pull1_succ\":time_point_pull1_succ,\n",
    "                                      \"pull2_succ\":time_point_pull2_succ,\n",
    "                                      \"pull1_fail\":time_point_pull1_fail,\n",
    "                                      \"pull2_fail\":time_point_pull2_fail,\n",
    "                                    }\n",
    "        \n",
    "        # \n",
    "        # based on time point pull and juice, define some features for each pull action\n",
    "        pull_infos = get_pull_infos(animal1, animal2, time_point_pull1, time_point_pull2, \n",
    "                                    time_point_juice1, time_point_juice2)\n",
    "        \n",
    "        # new total session time (instead of 600s) - total time of the video recording\n",
    "        totalsess_time = np.ceil(np.shape(output_look_ornot['look_at_lever_or_not_merge']['dodson'])[0]/30) \n",
    "        #\n",
    "        # remove task irrelavant period\n",
    "        if totalsess_time > (lastpulltime+session_start_time+15):\n",
    "            totalsess_time = np.ceil(lastpulltime+session_start_time+15)\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # organize variables that are required by the HDDM functions\n",
    "        # load the data first, if not process and then save the data \n",
    "        #\n",
    "        # load the data that is organized for GLM, the goal is to do the GLM with the combined dataset across session\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load the session wised data for GLM fitting')\n",
    "            \n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_glm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            \n",
    "            with open(add_date_dir+'/pre_data_for_GLM.pkl', 'rb') as f:\n",
    "                pre_data_for_GLM = pickle.load(f)\n",
    "        \n",
    "        except:\n",
    "            print('no sesison wise data saved for GLM, creating them now')\n",
    "            #\n",
    "            # MODIFICATION: Define kernel parameters here for easy adjustment\n",
    "            KERNEL_DURATION_S = 4.0  # The length of the history kernel in seconds\n",
    "            N_BASIS_FUNCS = 10       # The number of basis functions to represent the kernel\n",
    "            \n",
    "            try:\n",
    "                pre_data_for_GLM = continuous_variable_create_data_forGLM(KERNEL_DURATION_S, N_BASIS_FUNCS, fps, animal1, animal2, session_start_time,\n",
    "                                                   time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, \n",
    "                                                   mutual_gaze1, mutual_gaze2, animalnames_videotrack, \n",
    "                                                   output_look_ornot, output_allvectors, output_allangles, output_key_locations)\n",
    "            except:\n",
    "                pre_data_for_GLM = np.nan\n",
    "                \n",
    "            #\n",
    "            # save data\n",
    "            if 1:\n",
    "                current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_glm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(add_date_dir+'/pre_data_for_GLM.pkl', 'wb') as f:\n",
    "                    pickle.dump(pre_data_for_GLM, f)\n",
    "        #    \n",
    "        pre_data_for_GLM_alldates[date_tgt] = pre_data_for_GLM\n",
    "            \n",
    "        \n",
    "        # do the GLM session by session\n",
    "        try:\n",
    "            # dummy\n",
    "            print('load the result from GLM')\n",
    "            \n",
    "            current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_glm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "            add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "            \n",
    "            with open(add_date_dir+'/glm_data.pkl', 'rb') as f:\n",
    "                glm_data = pickle.load(f)\n",
    "            with open(add_date_dir+'/glm_datas_shortlist_prediction.pkl', 'rb') as f:\n",
    "                glm_datas_shortlist_prediction = pickle.load(f)\n",
    "            \n",
    "        except:\n",
    "            print('no GLM data, analyze it and save it')\n",
    "            #\n",
    "            # MODIFICATION: Define kernel parameters here for easy adjustment\n",
    "            KERNEL_DURATION_S = 4.0  # The length of the history kernel in seconds\n",
    "            N_BASIS_FUNCS = 10       # The number of basis functions to represent the kernel\n",
    "            \n",
    "            try:\n",
    "                glm_data = continuous_variable_glm(KERNEL_DURATION_S, N_BASIS_FUNCS, fps, animal1, animal2, session_start_time,\n",
    "                                                   time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, \n",
    "                                                   mutual_gaze1, mutual_gaze2, animalnames_videotrack, \n",
    "                                                   output_look_ornot, output_allvectors, output_allangles, output_key_locations)\n",
    "            except:\n",
    "                glm_data = np.nan\n",
    "                \n",
    "            try:\n",
    "                glm_datas_shortlist_prediction = continuous_variable_glm_shortlist_prediction(KERNEL_DURATION_S, N_BASIS_FUNCS, \n",
    "                                                   fps, animal1, animal2, session_start_time,\n",
    "                                                   time_point_pull1, time_point_pull2, oneway_gaze1, oneway_gaze2, \n",
    "                                                   mutual_gaze1, mutual_gaze2, animalnames_videotrack, \n",
    "                                                   output_look_ornot, output_allvectors, output_allangles, output_key_locations)\n",
    "            except:     \n",
    "                glm_datas_shortlist_prediction = np.nan\n",
    "            \n",
    "            #\n",
    "            # save data\n",
    "            if 0:\n",
    "                current_dir = data_saved_folder+'/bhv_events_singlecam_wholebody_with_glm_model/'+animal1_fixedorder[0]+animal2_fixedorder[0]\n",
    "                add_date_dir = os.path.join(current_dir,cameraID+'/'+date_tgt)\n",
    "                if not os.path.exists(add_date_dir):\n",
    "                    os.makedirs(add_date_dir)\n",
    "                #\n",
    "                with open(add_date_dir+'/glm_data.pkl', 'wb') as f:\n",
    "                    pickle.dump(glm_data, f)\n",
    "                    \n",
    "                with open(add_date_dir+'/glm_datas_shortlist_prediction.pkl', 'wb') as f:\n",
    "                    pickle.dump(glm_datas_shortlist_prediction, f)\n",
    "      \n",
    "        #    \n",
    "        glm_datas_all_dates[date_tgt] = glm_data\n",
    "        glm_datas_shortlist_prediction_all_dates[date_tgt] = glm_datas_shortlist_prediction\n",
    "        \n",
    "\n",
    "    # save data\n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_glm'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "\n",
    "        with open(data_saved_subfolder+'/glm_datas_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(glm_datas_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/glm_datas_shortlist_prediction_all_dates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(glm_datas_shortlist_prediction_all_dates, f) \n",
    "            \n",
    "        with open(data_saved_subfolder+'/pre_data_for_GLM_alldates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pre_data_for_GLM_alldates, f) \n",
    "\n",
    "    # only save a subset of data\n",
    "    if 0:\n",
    "        data_saved_subfolder = data_saved_folder+'data_saved_singlecam_wholebody_neural_and_glm'+savefile_sufix+'/'+cameraID+'/'+animal1_fixedorders[0]+animal2_fixedorders[0]+'/'\n",
    "        if not os.path.exists(data_saved_subfolder):\n",
    "            os.makedirs(data_saved_subfolder)\n",
    "            \n",
    "        with open(data_saved_subfolder+'/pre_data_for_GLM_alldates_'+animal1_fixedorders[0]+animal2_fixedorders[0]+'.pkl', 'wb') as f:\n",
    "            pickle.dump(pre_data_for_GLM_alldates, f) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the target condition\n",
    "#\n",
    "act_animal_to_ana = 'kanga'\n",
    "# act_animal_to_ana = 'dodson'\n",
    "#\n",
    "###\n",
    "# For Kanga\n",
    "conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withDodson', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson', 'MC_withVermelho', ] # MC with male\n",
    "# conditions_to_ana = ['MC_withGinger', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', ] # MC with familiar male\n",
    "# conditions_to_ana = ['MC_withGinger', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withDodson', 'MC_withVer|melho', ] # MC with unfamiliar male\n",
    "# conditions_to_ana = ['MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_DannonAuto'] # partner AL\n",
    "# conditions_to_ana = ['MC_KangaAuto'] # self AL\n",
    "# conditions_to_ana = ['NV','NV_withDodson'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withDodson','MC_withGinger', 'MC_withKoala', 'MC_withVermelho', \n",
    "#                      'SR', 'SR_withDodson',]\n",
    "###\n",
    "# For Dodson\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # all MC\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', ] # all MC\n",
    "# conditions_to_ana = ['SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ] # all SR\n",
    "# conditions_to_ana = ['MC', 'MC_withKanga', 'MC_withKoala', ] # all MC, no gingerNew\n",
    "# conditions_to_ana = ['SR', 'SR_withKanga', 'SR_withKoala', ] # all SR,  no gingerNew\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', ] # MC with female\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', ] # MC with familiar female\n",
    "# conditions_to_ana = ['MC_withKanga', 'MC_withKoala', ] # MC with unfamiliar female\n",
    "# conditions_to_ana = ['MC_KoalaAuto_withKoala'] # partner AL\n",
    "# conditions_to_ana = ['MC_DodsonAuto_withKoala'] # self AL\n",
    "# conditions_to_ana = ['NV_withKanga'] # NV\n",
    "# conditions_to_ana = ['MC', 'MC_withGingerNew', 'MC_withKanga', 'MC_withKoala', \n",
    "#                      'SR', 'SR_withGingerNew', 'SR_withKanga', 'SR_withKoala', ]\n",
    "\n",
    "cond_toplot_type = 'allMC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52753cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target data and plot the summarizing figure for fitting performance\n",
    "if 0:\n",
    "    ind_tgt = np.isin(task_conditions,conditions_to_ana)\n",
    "\n",
    "    dates_tgt = np.array(dates_list)[ind_tgt]\n",
    "    ndates = np.shape(dates_tgt)[0]\n",
    "\n",
    "    # \n",
    "    mean_beta_df_all = []\n",
    "\n",
    "    for idate in np.arange(0,ndates,1):\n",
    "\n",
    "        date_tgt = dates_tgt[idate]\n",
    "\n",
    "        try:\n",
    "            mean_beta_df_tgt = glm_datas_all_dates[date_tgt][(act_animal_to_ana,'mean_beta_df')]\n",
    "            mean_beta_df_tgt['date'] = date_tgt\n",
    "\n",
    "            mean_beta_df_all.append(mean_beta_df_tgt)\n",
    "        except:\n",
    "            continue\n",
    "    #\n",
    "    mean_beta_df_all = pd.concat(mean_beta_df_all, ignore_index=True)\n",
    "\n",
    "\n",
    "    #\n",
    "    # set up and do the plotting\n",
    "    #\n",
    "    # Set the desired variable order\n",
    "    var_names = [\n",
    "        'gaze_other_angle', 'gaze_tube_angle', 'gaze_lever_angle',\n",
    "        'animal_animal_dist', 'animal_tube_dist', 'animal_lever_dist',\n",
    "        'mass_move_speed', 'gaze_angle_speed'\n",
    "    ]    \n",
    "\n",
    "    # Ensure the necessary column is available\n",
    "    mean_beta_df_all['neg_log10_p'] = -np.log10(mean_beta_df_all['LRT_pvalue'])\n",
    "    mean_beta_df_all['is_significant'] = mean_beta_df_all['LRT_pvalue'] < 0.05\n",
    "    mean_beta_df_all['Variable'] = pd.Categorical(mean_beta_df_all['Variable'], categories=var_names, ordered=True)\n",
    "\n",
    "    # === FIGURE 1: Violin plot of -log10(p-value) ===\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(\n",
    "        data=mean_beta_df_all,\n",
    "        x='Variable',\n",
    "        y='neg_log10_p',\n",
    "        order=var_names,\n",
    "        inner='point',\n",
    "        scale='width',\n",
    "        palette='Set2'\n",
    "    )\n",
    "    plt.axhline(-np.log10(0.05), color='red', linestyle='--', label='p = 0.05')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('-log10(LRT p-value)')\n",
    "    plt.title('Figure 1: Distribution of Significance (-log10 p) Across Sessions')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === FIGURE 2: Bar plot of % sessions with significant LRT ===\n",
    "    sig_summary = mean_beta_df_all.groupby('Variable')['is_significant'].mean().reset_index()\n",
    "    sig_summary['percentage'] = sig_summary['is_significant'] * 100\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=sig_summary, x='Variable', y='percentage', palette='Set2',order=var_names,)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('% Sessions with Significant LRT (p < 0.05)')\n",
    "    plt.title('Figure 2: Frequency of Significant LRT Results per Variable')\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target data and plot the summarizing figure for fitting performance\n",
    "# focus on the prediction accuracy / roauc\n",
    "if 0:\n",
    "    ind_tgt = np.isin(task_conditions,conditions_to_ana)\n",
    "\n",
    "    dates_tgt = np.array(dates_list)[ind_tgt]\n",
    "    ndates = np.shape(dates_tgt)[0]\n",
    "    \n",
    "    # \n",
    "    glm_prediction_auc_all = pd.DataFrame(columns=['date','mean_auc_full','mean_auc_short','mean_auc_other'])\n",
    "\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "\n",
    "        date_tgt = dates_tgt[idate]\n",
    "\n",
    "        try:\n",
    "            mean_auc_full = np.nanmean(glm_datas_shortlist_prediction_all_dates[date_tgt]\\\n",
    "                                            [(act_animal_to_ana,'predictive_perf')]['auc_full'])\n",
    "            mean_auc_short = np.nanmean(glm_datas_shortlist_prediction_all_dates[date_tgt]\\\n",
    "                                            [(act_animal_to_ana,'predictive_perf')]['auc_short'])\n",
    "            mean_auc_other = np.nanmean(glm_datas_shortlist_prediction_all_dates[date_tgt]\\\n",
    "                                            [(act_animal_to_ana,'predictive_perf')]['auc_other'])\n",
    "            \n",
    "            row_data = {\n",
    "                    'date': date_tgt,\n",
    "                    'mean_auc_full': mean_auc_full,\n",
    "                    'mean_auc_short': mean_auc_short,\n",
    "                    'mean_auc_other': mean_auc_other,\n",
    "                        }\n",
    "            glm_prediction_auc_all = glm_prediction_auc_all.append(\n",
    "                                            row_data, ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    from scipy.stats import ttest_rel\n",
    "\n",
    "    # Paired t-test\n",
    "    tstat, pval = ttest_rel(glm_prediction_auc_all['mean_auc_full'], glm_prediction_auc_all['mean_auc_short'])\n",
    "\n",
    "    # Compute mean difference\n",
    "    mean_other = glm_prediction_auc_all['mean_auc_full'].mean()\n",
    "    mean_short = glm_prediction_auc_all['mean_auc_short'].mean()\n",
    "    mean_diff = mean_other - mean_short\n",
    "\n",
    "    # Melt data for violin plot\n",
    "    df_melt = glm_prediction_auc_all[['mean_auc_full', 'mean_auc_short']].melt(var_name='Model', value_name='AUC')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.violinplot(data=df_melt, x='Model', y='AUC', inner='box', palette='pastel')\n",
    "    sns.swarmplot(data=df_melt, x='Model', y='AUC', color='k', size=4)\n",
    "\n",
    "    # Annotate p-value and mean diff\n",
    "    x1, x2 = 0, 1\n",
    "    y, h, col = df_melt['AUC'].max() + 0.02, 0.01, 'k'\n",
    "    plt.plot([x1, x1, x2, x2], [y, y + h, y + h, y], lw=1.5, c=col)\n",
    "    plt.text((x1 + x2) * .5, y + h + 0.01, f\"*p = {pval:.3f}\", ha='center', va='bottom', color=col)\n",
    "    plt.text((x1 + x2) * .5, y + h + 0.04, f\"Δ = {mean_diff:.3f}\", ha='center', va='bottom', color='blue')\n",
    "\n",
    "    # Labels and formatting\n",
    "    plt.title('AUC Comparison: Other vs Short Model')\n",
    "    plt.xlim([-0.5, 1.5])\n",
    "    plt.ylim([0.6, 1.1])\n",
    "    plt.xticks([0, 1], ['Other', 'Short'])\n",
    "\n",
    "    # Save figure\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_glm_model/\" + \\\n",
    "                        cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + \"/glm_fitting_summary_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        plt.savefig(figsavefolder + act_animal_to_ana + '_in_' + cond_toplot_type +\n",
    "                    '_glm_fitting_summary_other_vs_short_figure.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the raw data together across session (in the same or tgt condition) and then run the glm\n",
    "# the goal is to test which variable contribute the most\n",
    "if 1:\n",
    "    import seaborn as sns\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from group_lasso import LogisticGroupLasso\n",
    "    \n",
    "    \n",
    "    ind_tgt = np.isin(task_conditions,conditions_to_ana)\n",
    "\n",
    "    dates_tgt = np.array(dates_list)[ind_tgt]\n",
    "    ndates = np.shape(dates_tgt)[0]\n",
    "\n",
    "    # \n",
    "    X_all_conbined = []\n",
    "    Y_all_conbined = []\n",
    "    \n",
    "    for idate in np.arange(0,ndates,1):\n",
    "\n",
    "        date_tgt = dates_tgt[idate]\n",
    "        \n",
    "        var_names = pre_data_for_GLM_alldates[date_tgt][(act_animal_to_ana,'var_names')]\n",
    "        \n",
    "        X_idate = pre_data_for_GLM_alldates[date_tgt][(act_animal_to_ana,'X_all')]\n",
    "        Y_idate = pre_data_for_GLM_alldates[date_tgt][(act_animal_to_ana,'Y')]\n",
    "        \n",
    "        # Z-score normalize per session\n",
    "        scaler = StandardScaler()\n",
    "        X_idate_zscored = scaler.fit_transform(X_idate)\n",
    "\n",
    "        # Optional: remove nan rows\n",
    "        valid_rows = ~np.isnan(X_idate_zscored).any(axis=1)\n",
    "        X_valid = X_idate_zscored[valid_rows]\n",
    "        Y_valid = Y_idate[valid_rows]\n",
    "\n",
    "        X_all_conbined.append(X_valid)\n",
    "        Y_all_conbined.append(Y_valid)\n",
    "        \n",
    "    \n",
    "    X_combined = np.vstack(X_all_conbined)\n",
    "    Y_combined = np.concatenate(Y_all_conbined)\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # BOOTSTRAP WITH LEAVE-ONE-GROUP-OUT & GROUP LASSO\n",
    "    # ==============================================================================\n",
    "\n",
    "    # 1. Setup bootstrap and results storage\n",
    "    n_bootstraps = 100\n",
    "    results = {'full_model': []}\n",
    "    for var in var_names:\n",
    "        results[f'drop_{var}'] = []\n",
    "\n",
    "    ## NEW ##\n",
    "    # Add a list to store the Group Lasso coefficients from each iteration\n",
    "    lasso_coeffs_all_runs = [] \n",
    "    # Define the feature groups once, as it's the same for all iterations\n",
    "    n_variables = len(var_names)\n",
    "    groups = np.repeat(np.arange(n_variables), repeats=N_BASIS_FUNCS)\n",
    "\n",
    "\n",
    "    # 2. Start bootstrap loop\n",
    "    for i in range(n_bootstraps):\n",
    "        print(f\"🚀 Running bootstrap iteration {i+1}/{n_bootstraps}...\")\n",
    "\n",
    "        # a. Balance dataset for this iteration\n",
    "        # (Your existing balancing code is here)\n",
    "        pos_idx = np.where(Y_combined == 1)[0]\n",
    "        neg_idx = np.where(Y_combined == 0)[0]\n",
    "        if len(pos_idx) == 0 or len(neg_idx) < len(pos_idx):\n",
    "            print(f\"Warning: Not enough samples for iteration {i+1}. Skipping.\")\n",
    "            continue\n",
    "        neg_sample_idx = np.random.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "        balanced_idx = np.concatenate([pos_idx, neg_sample_idx])\n",
    "        np.random.shuffle(balanced_idx)\n",
    "        X_balanced = X_combined[balanced_idx]\n",
    "        Y_balanced = Y_combined[balanced_idx]\n",
    "\n",
    "        # b. Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_balanced, Y_balanced, test_size=0.2, stratify=Y_balanced\n",
    "        )\n",
    "\n",
    "        # c. Normalize features based ONLY on the training set\n",
    "        scaler = StandardScaler()\n",
    "        X_train_norm = scaler.fit_transform(X_train)\n",
    "        X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "        # --- Leave-One-Out Analysis (Your existing code) ---\n",
    "        # NOTE: I've removed `class_weight='balanced'` because you are already manually balancing the data. Using both is redundant.\n",
    "\n",
    "        # d. Fit and evaluate the FULL model\n",
    "        clf_full = LogisticRegression(max_iter=1000) \n",
    "        clf_full.fit(X_train_norm, y_train)\n",
    "        y_proba_full = clf_full.predict_proba(X_test_norm)[:, 1]\n",
    "        auc_full = roc_auc_score(y_test, y_proba_full)\n",
    "        results['full_model'].append(auc_full)\n",
    "\n",
    "        # e. Loop through each variable group to perform leave-one-group-out\n",
    "        for j, var_to_drop in enumerate(var_names):\n",
    "            start_col = j * N_BASIS_FUNCS\n",
    "            end_col = start_col + N_BASIS_FUNCS\n",
    "            cols_to_drop = np.arange(start_col, end_col)\n",
    "            X_train_loo = np.delete(X_train_norm, cols_to_drop, axis=1)\n",
    "            X_test_loo = np.delete(X_test_norm, cols_to_drop, axis=1)\n",
    "\n",
    "            clf_loo = LogisticRegression(max_iter=1000)\n",
    "            clf_loo.fit(X_train_loo, y_train)\n",
    "            y_proba_loo = clf_loo.predict_proba(X_test_loo)[:, 1]\n",
    "            auc_loo = roc_auc_score(y_test, y_proba_loo)\n",
    "            results[f'drop_{var_to_drop}'].append(auc_loo)\n",
    "\n",
    "        ## NEW ## \n",
    "        # --- Group Lasso Analysis (run on the same data split) ---\n",
    "        gl_model = LogisticGroupLasso(\n",
    "            groups=groups,\n",
    "            group_reg=0.05,  # This is a key parameter to tune\n",
    "            supress_warning=True\n",
    "        )\n",
    "        gl_model.fit(X_train_norm, y_train)\n",
    "        # Store the resulting coefficients for this iteration\n",
    "        lasso_coeffs_all_runs.append(gl_model.coef_)\n",
    "\n",
    "\n",
    "    print(\"\\n✅ Bootstrap analysis complete.\")\n",
    "\n",
    "    # 3. Summarize and Plot the results\n",
    "    # --- First, analyze and plot the Leave-One-Out results (your existing code) ---\n",
    "    results_df = pd.DataFrame(results)\n",
    "    summary_stats = results_df.agg(['mean', 'std']).T\n",
    "    \n",
    "    summary_stats.rename(columns={'mean': 'mean_auc', 'std': 'std_auc'}, inplace=True)\n",
    "    mean_full_auc = summary_stats.loc['full_model', 'mean_auc']\n",
    "    summary_stats['auc_drop_from_full'] = mean_full_auc - summary_stats['mean_auc']\n",
    "\n",
    "    print(\"\\n--- 📊 Summary of Model Performance ---\")\n",
    "    print(summary_stats.sort_values(by='auc_drop_from_full', ascending=False))\n",
    "\n",
    "    # Plotting...\n",
    "    # (The plotting code from the previous answer can be used here without any changes)\n",
    "    # --- Plot 1: Mean AUC for each model ---\n",
    "    print(\"📊 Generating violin plot of model performance...\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    # Reshape the DataFrame from wide to long format for Seaborn\n",
    "    plot_df_long = results_df.melt(var_name='model', value_name='auc')\n",
    "    # Define the desired order to prevent automatic sorting\n",
    "    model_order = ['full_model'] + [f'drop_{var}' for var in var_names]\n",
    "    # Create the violin plot\n",
    "    sns.violinplot(\n",
    "        data=plot_df_long, \n",
    "        x='model', \n",
    "        y='auc', \n",
    "        order=model_order, \n",
    "        inner='quartile', # Shows the quartiles inside the violins\n",
    "        palette='viridis',\n",
    "        ax=ax\n",
    "    )\n",
    "    # Overlay individual data points for more detail\n",
    "    sns.stripplot(\n",
    "        data=plot_df_long,\n",
    "        x='model',\n",
    "        y='auc',\n",
    "        order=model_order,\n",
    "        size=2,\n",
    "        color=\"white\",\n",
    "        edgecolor='gray',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_ylabel('ROC AUC Score Distribution', fontsize=14)\n",
    "    ax.set_xlabel('Model Type', fontsize=14)\n",
    "    ax.set_title(f'Model Performance Distribution ({n_bootstraps} Bootstraps)', fontsize=16)\n",
    "    ax.axhline(y=0.5, color='black', linestyle='--', label='Chance Level (AUC = 0.5)')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- Plot 2: Feature importance with error bars ---\n",
    "    # 1. Calculate the AUC drop for each bootstrap run to get a distribution of importance scores.\n",
    "    auc_drops_df = pd.DataFrame()\n",
    "    for col in results_df.columns:\n",
    "        if 'drop_' in col:\n",
    "            auc_drops_df[col] = results_df['full_model'] - results_df[col]\n",
    "    # 2. Calculate the mean and standard deviation from these distributions.\n",
    "    mean_drops = auc_drops_df.mean()\n",
    "    std_drops = auc_drops_df.std()\n",
    "    sem_drops = std_drops/np.sqrt(n_bootstraps)\n",
    "    # 3. Create a new DataFrame for plotting.\n",
    "    importance_data = pd.DataFrame({\n",
    "        'mean_drop': mean_drops,\n",
    "        'std_drop': std_drops,\n",
    "        'sem_drop': sem_drops,\n",
    "    }).sort_values('mean_drop', ascending=True) # Sort for a clean plot\n",
    "    # 4. Create the plot.\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "    ax2.barh(\n",
    "        importance_data.index.str.replace('drop_', ''),\n",
    "        importance_data['mean_drop'],\n",
    "        # Add the standard deviation as the error bar length\n",
    "        # xerr=importance_data['std_drop'],\n",
    "        xerr=importance_data['sem_drop'],\n",
    "        capsize=4, # Adds caps to the error bars\n",
    "        color='lightgreen',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    ax2.set_xlabel('Drop in Mean AUC (Importance)', fontsize=14)\n",
    "    ax2.set_ylabel('Variable Group Removed', fontsize=14)\n",
    "    ax2.set_title('Variable Importance based on Performance Drop', fontsize=16)\n",
    "    ax2.axvline(x=0, color='grey', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ## NEW ##\n",
    "    # --- Second, analyze and plot the Group Lasso results ---\n",
    "    print(\"\\n--- 📊 Summary of Group Lasso Coefficients ---\")\n",
    "    # Average the coefficients across all bootstrap runs\n",
    "    mean_lasso_coeffs = np.mean(lasso_coeffs_all_runs, axis=0)\n",
    "\n",
    "    # Calculate the average magnitude of coefficients for each variable group\n",
    "    group_importance = []\n",
    "    for i, var_name in enumerate(var_names):\n",
    "        start_idx = i * N_BASIS_FUNCS\n",
    "        end_idx = start_idx + N_BASIS_FUNCS\n",
    "        # Use the absolute mean coefficient magnitude as the importance score\n",
    "        importance_score = np.mean(np.abs(mean_lasso_coeffs[start_idx:end_idx]))\n",
    "        group_importance.append({'variable': var_name, 'importance': importance_score})\n",
    "\n",
    "    # Create a DataFrame for easy sorting and plotting\n",
    "    lasso_importance_df = pd.DataFrame(group_importance).sort_values('importance', ascending=True)\n",
    "\n",
    "    print(lasso_importance_df)\n",
    "\n",
    "    # Plot 3: Group Lasso Feature Importance\n",
    "    fig3, ax3 = plt.subplots(figsize=(10, 8))\n",
    "    ax3.barh(\n",
    "        lasso_importance_df['variable'],\n",
    "        lasso_importance_df['importance'],\n",
    "        color='purple',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    ax3.set_xlabel('Mean Absolute Coefficient (Importance)', fontsize=14)\n",
    "    ax3.set_ylabel('Variable Group', fontsize=14)\n",
    "    ax3.set_title('Variable Importance from Group Lasso', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    savefig = 1\n",
    "    if savefig:\n",
    "        figsavefolder = data_saved_folder + \"fig_for_basic_neural_analysis_allsessions_basicEvents_PCA_Pullfocused_continuousBhv_partnerDistVaris_with_glm_model/\" + \\\n",
    "                        cameraID + \"/\" + animal1_filenames[0] + \"_\" + animal2_filenames[0] + \"/glm_fitting_summary_fig/\"\n",
    "\n",
    "        if not os.path.exists(figsavefolder):\n",
    "            os.makedirs(figsavefolder)\n",
    "\n",
    "        fig.savefig(figsavefolder + act_animal_to_ana + '_in_' + cond_toplot_type +\n",
    "                    '_glm_fitting_auc_full_model_and_leave_one_variable_out.pdf')\n",
    "        \n",
    "        fig2.savefig(figsavefolder + act_animal_to_ana + '_in_' + cond_toplot_type +\n",
    "                    '_glm_fitting_drop_in_auc_full_model_and_leave_one_variable_out.pdf')\n",
    "        \n",
    "        fig2.savefig(figsavefolder + act_animal_to_ana + '_in_' + cond_toplot_type +\n",
    "                    '_glm_fitting_group_lasso_full_model_.pdf')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate demo glm pipeline cartoon\n",
    "if 1:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.patches import FancyArrowPatch, PathPatch, ConnectionStyle, Rectangle\n",
    "    from matplotlib.path import Path\n",
    "    from scipy.interpolate import make_interp_spline\n",
    "\n",
    "    def make_raised_cosine_basis(duration_s, n_basis, dt):\n",
    "        \"\"\"\n",
    "        Generates a basis set of raised cosines.\n",
    "        \"\"\"\n",
    "        t = np.arange(0, duration_s, dt)\n",
    "        c = np.linspace(0, duration_s, n_basis)\n",
    "        width = (c[1] - c[0]) * 1.5\n",
    "        basis = []\n",
    "        for ci in c:\n",
    "            phi = (t - ci) * np.pi / width\n",
    "            b = np.cos(np.clip(phi, -np.pi, np.pi))\n",
    "            b = (b + 1) / 2\n",
    "            b[(t < ci - width/2) | (t > ci + width/2)] = 0\n",
    "            basis.append(b)\n",
    "        basis = np.stack(basis, axis=1)\n",
    "        return basis\n",
    "\n",
    "    def generate_glm_illustration():\n",
    "        \"\"\"\n",
    "        Generates and saves a PDF figure illustrating a GLM methodology, including\n",
    "        feature extraction, logistic regression, data splitting, and validation procedures.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- 1. Setup Figure and Subplots ---\n",
    "        # Increased figure height and added a row for the new cartoons\n",
    "        fig = plt.figure(figsize=(16, 14))\n",
    "        gs = fig.add_gridspec(3, 1, height_ratios=[1.5, 1, 0.8])\n",
    "        ax_top = fig.add_subplot(gs[0])\n",
    "        ax_bottom = fig.add_subplot(gs[1])\n",
    "        ax_validation = fig.add_subplot(gs[2]) # New axis for validation cartoons\n",
    "\n",
    "        fig.suptitle(\"GLM Methodology for Predicting Behavioral Events\", fontsize=22, y=0.98)\n",
    "\n",
    "        # --- 2. Top Row: Feature Extraction ---\n",
    "        ax_top.set_title(\"Step 1: Feature Extraction using Cosine Basis Kernels\", fontsize=18, pad=20)\n",
    "        ax_top.axis('off')\n",
    "\n",
    "        # A. Draw Smoother Variable Traces\n",
    "        num_vars = 8\n",
    "        time = np.linspace(-4, 0, 100)\n",
    "        for i in range(num_vars):\n",
    "            var_ax = ax_top.inset_axes([0.05 + i*0.11, 0.6, 0.1, 0.3])\n",
    "            np.random.seed(i)\n",
    "            spline = make_interp_spline(np.linspace(-4, 0, 5), np.random.rand(5))\n",
    "            var_ax.plot(time, spline(time), lw=2)\n",
    "            var_ax.set_title(f\"V{i+1}\", fontsize=12, pad=5)\n",
    "            var_ax.set_xticks([-4, 0]); var_ax.set_yticks([]); var_ax.set_xlabel(\"Time (s)\", fontsize=9)\n",
    "\n",
    "        # B. Draw Kernels using your function\n",
    "        num_kernels = 10; duration_s = 4; dt = 0.04\n",
    "        kernel_ax = ax_top.inset_axes([0.2, 0.05, 0.6, 0.3])\n",
    "        basis_kernels = make_raised_cosine_basis(duration_s, num_kernels, dt)\n",
    "        for k in range(basis_kernels.shape[1]):\n",
    "            kernel_ax.plot(np.arange(0, duration_s, dt), basis_kernels[:, k] - k * 1.1, lw=2)\n",
    "        kernel_ax.set_title(\"10 Raised Cosine Basis Kernels\", fontsize=12)\n",
    "        kernel_ax.set_yticks([]); kernel_ax.set_xticks([0, 4]); kernel_ax.set_xlabel(\"History Window (s)\", fontsize=9)\n",
    "        kernel_ax.invert_xaxis()\n",
    "\n",
    "        # C. Add connecting text and lines\n",
    "        ax_top.text(0.5, 0.45, \"Each variable trace is convolved with the 10 kernels,\\ncreating 8 x 10 = 80 features\",\n",
    "                    ha='center', va='center', fontsize=12, style='italic', color='gray')\n",
    "        pull_line = FancyArrowPatch((0.95, 0.95), (0.95, 0.55), mutation_scale=20, arrowstyle='-|>')\n",
    "        ax_top.add_patch(pull_line)\n",
    "        ax_top.text(0.96, 0.75, \"Pull Event\\n(t=0)\", va='center', fontsize=12)\n",
    "\n",
    "        # --- 3. Middle Row: Logistic Regression Model ---\n",
    "        ax_bottom.set_title(\"Step 2: Prediction using Logistic Regression\", fontsize=18, pad=20)\n",
    "        ax_bottom.axis('off')\n",
    "\n",
    "        # A. Draw model components\n",
    "        ax_bottom.text(0.2, 0.6, \"80 Weighted\\nFeatures\", ha='center', va='center', fontsize=14,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightblue\", ec=\"black\"))\n",
    "        ax_bottom.text(0.4, 0.6, \"$\\\\Sigma$\", ha='center', va='center', fontsize=30,\n",
    "                       bbox=dict(boxstyle=\"circle,pad=0.5\", fc=\"lightgreen\", ec=\"black\"))\n",
    "        sigmoid_ax = ax_bottom.inset_axes([0.55, 0.45, 0.2, 0.3])\n",
    "        x_sig = np.linspace(-6, 6, 100)\n",
    "        sigmoid_ax.plot(x_sig, 1 / (1 + np.exp(-x_sig)), color='red', lw=3)\n",
    "        sigmoid_ax.set_title(\"Sigmoid\", fontsize=12); sigmoid_ax.set_xticks([]); sigmoid_ax.set_yticks([0, 1])\n",
    "        ax_bottom.text(0.85, 0.6, \"P(Pull)\", ha='center', va='center', fontsize=14,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightcoral\", ec=\"black\"))\n",
    "\n",
    "        # B. Add arrows\n",
    "        ax_bottom.add_patch(FancyArrowPatch((0.28, 0.6), (0.35, 0.6), mutation_scale=20, arrowstyle='-|>'))\n",
    "        ax_bottom.add_patch(FancyArrowPatch((0.45, 0.6), (0.55, 0.6), mutation_scale=20, arrowstyle='-|>'))\n",
    "        ax_bottom.add_patch(FancyArrowPatch((0.75, 0.6), (0.8, 0.6), mutation_scale=20, arrowstyle='-|>'))\n",
    "\n",
    "        # --- 4. Connect Top and Bottom Rows ---\n",
    "        con = ConnectionStyle.Arc3(rad=0.3)\n",
    "        arrow = FancyArrowPatch((0.5, 0.65), (0.2, 0.45), transform=fig.transFigure, connectionstyle=con,\n",
    "                                arrowstyle=\"-|>\", mutation_scale=25, lw=2, color='purple', linestyle='dashed')\n",
    "        fig.add_artist(arrow)\n",
    "\n",
    "        # --- 5. NEW: Bottom Row for Validation Cartoons ---\n",
    "        ax_validation.set_title(\"Step 3: Model Training and Validation\", fontsize=18, pad=20)\n",
    "        ax_validation.axis('off')\n",
    "\n",
    "        # A. Cartoon for 80/20 Split\n",
    "        ax_split = ax_validation.inset_axes([0, 0, 0.45, 0.9])\n",
    "        ax_split.set_title(\"A. Train/Test Split\", fontsize=14, loc='left')\n",
    "        ax_split.axis('off')\n",
    "        ax_split.add_patch(Rectangle((0.1, 0.6), 0.8, 0.3, facecolor='gray', alpha=0.3, label='Full Dataset'))\n",
    "        ax_split.text(0.5, 0.75, \"Full Dataset\", ha='center', va='center', fontsize=12)\n",
    "        ax_split.add_patch(FancyArrowPatch((0.5, 0.6), (0.35, 0.4), mutation_scale=20, arrowstyle='-|>'))\n",
    "        ax_split.add_patch(FancyArrowPatch((0.5, 0.6), (0.65, 0.4), mutation_scale=20, arrowstyle='-|>'))\n",
    "        ax_split.add_patch(Rectangle((0.1, 0.1), 0.5, 0.3, facecolor='skyblue', label='Training Set'))\n",
    "        ax_split.text(0.35, 0.25, \"80% Training Set\\n(Model learns here)\", ha='center', va='center', fontsize=11)\n",
    "        ax_split.add_patch(Rectangle((0.7, 0.1), 0.2, 0.3, facecolor='lightcoral', label='Testing Set'))\n",
    "        ax_split.text(0.8, 0.25, \"20% Test Set\\n(Model evaluated here)\", ha='center', va='center', fontsize=11)\n",
    "\n",
    "        # B. Cartoon for Leave-One-Out\n",
    "        ax_loo = ax_validation.inset_axes([0.55, 0, 0.45, 0.9])\n",
    "        ax_loo.set_title(\"B. Leave-One-Out Importance\", fontsize=14, loc='left')\n",
    "        ax_loo.axis('off')\n",
    "        # Full Model\n",
    "        ax_loo.text(0.5, 0.8, \"Full Model\", ha='center', va='center', fontsize=12)\n",
    "        ax_loo.add_patch(Rectangle((0.2, 0.65), 0.6, 0.1, facecolor='lightgreen'))\n",
    "        ax_loo.text(0.5, 0.7, \"V1 V2 V3 ... V8\", ha='center', va='center', fontsize=10)\n",
    "        # Leave-One-Out Model\n",
    "        ax_loo.text(0.5, 0.4, \"vs.\", ha='center', va='center', fontsize=14)\n",
    "        ax_loo.text(0.5, 0.25, \"Leave-One-Out Model\", ha='center', va='center', fontsize=12)\n",
    "        ax_loo.add_patch(Rectangle((0.2, 0.1), 0.6, 0.1, facecolor='lightyellow'))\n",
    "        ax_loo.text(0.5, 0.15, \"V1 V2 V4 ... V8\", ha='center', va='center', fontsize=10)\n",
    "        ax_loo.text(0.5, 0.05, \"(e.g., V3 removed)\", ha='center', va='center', fontsize=9, style='italic')\n",
    "        ax_loo.text(0.5, -0.1, \"Large performance drop?\\n Variable was important.\", ha='center', va='center', fontsize=11)\n",
    "\n",
    "        # --- 6. Save the Figure ---\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        file_path = \"glm_method_illustration.pdf\"\n",
    "        plt.savefig(file_path)\n",
    "        print(f\"✅ Illustration saved successfully to: {file_path}\")\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        generate_glm_illustration()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e07a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ada167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb52112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170f886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e10d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368777b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1797bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b977fa4",
   "metadata": {},
   "source": [
    "## old code, not in use for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5201b-186e-4d1b-9759-962cd87a0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Helper function to define successful pulls\n",
    "def get_successful_pull_indices(juice_ts, pull_ts):\n",
    "    juice_indices = np.where(juice_ts == 1)[0]\n",
    "    pull_indices = np.where(pull_ts == 1)[0]\n",
    "    if len(juice_indices) == 0 or len(pull_indices) == 0: return np.array([], dtype=int)\n",
    "    insertion_points = np.searchsorted(pull_indices, juice_indices)\n",
    "    valid_mask = insertion_points > 0\n",
    "    successful_indices = pull_indices[insertion_points[valid_mask] - 1]\n",
    "    return np.unique(successful_indices)\n",
    "\n",
    "# Helper function to create basis functions\n",
    "def create_raised_cosine_basis(kernel_len_frames, n_bases):\n",
    "    x = np.linspace(0, np.pi, kernel_len_frames)\n",
    "    centers = np.linspace(np.pi / (2 * n_bases), np.pi - np.pi / (2 * n_bases), n_bases)\n",
    "    width = np.pi / n_bases\n",
    "    basis_funcs = np.zeros((kernel_len_frames, n_bases))\n",
    "    for i, center in enumerate(centers):\n",
    "        mask = (x > center - width) & (x < center + width)\n",
    "        basis_funcs[mask, i] = (np.cos((x[mask] - center) * np.pi / width) + 1) / 2\n",
    "    return basis_funcs\n",
    "\n",
    "# --- Function 1: Prepare the Design Matrix ---\n",
    "# --- Function 1: Prepare the Design Matrix ---\n",
    "def prepare_glm_design_matrix(animal_id, self_pull_ts, self_juice_ts, self_speed_ts, self_gaze_ts,\n",
    "                              partner_pull_ts, partner_speed_ts, fs,\n",
    "                              kernel_duration_s, n_basis_funcs):\n",
    "    \"\"\"\n",
    "    Prepares the design matrix for a GLM analysis for a single animal.\n",
    "    MODIFICATION: Replaced partner pull kernel with time since last pull for both self and partner.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Preparing Design Matrix for {animal_id} {'='*20}\")\n",
    "    \n",
    "    n_total_frames = len(self_pull_ts)\n",
    "    kernel_n_frames = int(kernel_duration_s * fs)\n",
    "\n",
    "    # 1. Pre-computation\n",
    "    successful_self_pull_indices = get_successful_pull_indices(self_juice_ts, self_pull_ts)\n",
    "    basis_functions = create_raised_cosine_basis(kernel_n_frames, n_basis_funcs)\n",
    "    \n",
    "    # MODIFICATION: Get all pull indices for recency calculation\n",
    "    all_self_pull_indices = np.where(self_pull_ts == 1)[0]\n",
    "    all_partner_pull_indices = np.where(partner_pull_ts == 1)[0]\n",
    "\n",
    "    # 2. Build the Design Matrix using efficient, vectorized operations\n",
    "    print(f\"Building GLM Design Matrix for {animal_id}...\")\n",
    "    \n",
    "    # Create the main time vector for the analysis\n",
    "    time_vector = np.arange(n_total_frames)\n",
    "    \n",
    "    # a) Vectorized \"time since last event\" calculations\n",
    "    def time_since_last_event(event_indices, time_vector):\n",
    "        if len(event_indices) == 0:\n",
    "            return np.full(len(time_vector), np.nan)\n",
    "        insertion_points = np.searchsorted(event_indices, time_vector)\n",
    "        valid_mask = insertion_points > 0\n",
    "        last_event_indices = event_indices[insertion_points[valid_mask] - 1]\n",
    "        time_since = np.full(len(time_vector), np.nan)\n",
    "        time_since[valid_mask] = (time_vector[valid_mask] - last_event_indices) / fs\n",
    "        return time_since\n",
    "\n",
    "    time_since_last_success = time_since_last_event(successful_self_pull_indices, time_vector)\n",
    "    time_since_last_self_pull = time_since_last_event(all_self_pull_indices, time_vector)\n",
    "    time_since_last_partner_pull = time_since_last_event(all_partner_pull_indices, time_vector)\n",
    "\n",
    "    # b) Create the convolved (kernel-based) predictors\n",
    "    flipped_basis = np.ascontiguousarray(basis_functions[::-1, :])\n",
    "    convolved_features = {}\n",
    "    for i in range(n_basis_funcs):\n",
    "        basis = flipped_basis[:, i]\n",
    "        convolved_features[f'self_speed_b{i}'] = np.convolve(self_speed_ts, basis, mode='same')\n",
    "        convolved_features[f'partner_speed_b{i}'] = np.convolve(partner_speed_ts, basis, mode='same')\n",
    "\n",
    "    # b) Create the simple average (rolling mean) predictor for GAZE\n",
    "    gaze_mean = pd.Series(self_gaze_ts).rolling(window=kernel_n_frames).mean().to_numpy()\n",
    "    \n",
    "    # c) Assemble the final DataFrame\n",
    "    design_matrix = pd.DataFrame(convolved_features)\n",
    "    design_matrix['time_since_last_success'] = time_since_last_success\n",
    "    design_matrix['time_since_last_self_pull'] = time_since_last_self_pull\n",
    "    design_matrix['time_since_last_partner_pull'] = time_since_last_partner_pull\n",
    "    design_matrix['response'] = self_pull_ts\n",
    "    design_matrix['self_gaze_auc'] = gaze_mean # Add the simple average predictor\n",
    "    \n",
    "    # Drop rows with NaN (from the beginning before the first events occurred) and reset index\n",
    "    design_matrix = design_matrix.dropna().reset_index(drop=True)\n",
    "    print(f\"Design matrix for {animal_id} created with {len(design_matrix)} rows.\")\n",
    "    \n",
    "    return design_matrix\n",
    "\n",
    "# --- Function 2: Fit the GLM and Return Results ---\n",
    "def fit_and_summarize_glm(design_matrix, animal_id):\n",
    "    \"\"\"\n",
    "    Fits a logistic GLM to a pre-prepared design matrix.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Fitting Logistic GLM for {animal_id} ---\")\n",
    "    if design_matrix is None or design_matrix.empty:\n",
    "        print(f\"Cannot fit model for {animal_id}, design matrix is empty.\")\n",
    "        return None\n",
    "        \n",
    "    y = design_matrix['response']\n",
    "    X = design_matrix.drop(columns=['response','self_gaze_auc'])\n",
    "    X = sm.add_constant(X, prepend=True) # Ensure const is the first column    \n",
    "\n",
    "    logit_model = sm.Logit(y, X)\n",
    "    result = logit_model.fit(disp=0)\n",
    "    \n",
    "    return result, X\n",
    "\n",
    "\n",
    "# --- Function 3: kernel recontruction ---\n",
    "# --- 3. MODIFICATION: New function to reconstruct and plot from the results object ---\n",
    "def reconstruct_and_plot_kernel(ax, glm_results, variable_name, basis_funcs, color, label):\n",
    "    \"\"\"\n",
    "    Reconstructs and plots a single kernel with its 95% CI from a fitted GLM results object.\n",
    "    \n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): The subplot axis to plot on.\n",
    "        glm_results (statsmodels.results.ResultWrapper): The fitted model object.\n",
    "        variable_name (str): The base name of the variable (e.g., 'self_speed').\n",
    "        basis_funcs (np.array): The basis function matrix used for fitting.\n",
    "        color (str): The color for the plot line.\n",
    "        label (str): The label for the plot legend.\n",
    "    \"\"\"\n",
    "    n_bases = basis_funcs.shape[1]\n",
    "    \n",
    "    # Automatically find all coefficients related to the variable (e.g., 'self_speed_b0', 'self_speed_b1', ...)\n",
    "    coef_names = [f'{variable_name}_b{i}' for i in range(n_bases)]\n",
    "    \n",
    "    # Extract coefficients and the relevant part of the covariance matrix\n",
    "    try:\n",
    "        coeffs = glm_results.params.filter(items=coef_names)\n",
    "        cov_matrix = glm_results.cov_params().loc[coef_names, coef_names]\n",
    "    except KeyError:\n",
    "        print(f\"Warning: Could not find coefficients for '{variable_name}' in the model results. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Reconstruct the kernel by multiplying the basis functions by their fitted weights\n",
    "    reconstructed_kernel = basis_funcs @ coeffs\n",
    "    \n",
    "    # --- Calculate the 95% Confidence Interval for the kernel ---\n",
    "    # This involves matrix multiplication with the covariance matrix\n",
    "    kernel_variance = np.diag(basis_funcs @ cov_matrix @ basis_funcs.T)\n",
    "    kernel_std_err = np.sqrt(kernel_variance)\n",
    "    \n",
    "    upper_ci = reconstructed_kernel + 1.96 * kernel_std_err\n",
    "    lower_ci = reconstructed_kernel - 1.96 * kernel_std_err\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    time_axis = np.linspace(-KERNEL_DURATION_S, 0, len(reconstructed_kernel))\n",
    "    ax.plot(time_axis, reconstructed_kernel, color=color, label=label, linewidth=2.5)\n",
    "    ax.fill_between(time_axis, lower_ci, upper_ci, color=color, alpha=0.2, label=f'_nolegend_')\n",
    "\n",
    "\n",
    "# \n",
    "# --- 2. Define a Reusable Function to Calculate the Partial Decision Variable ---\n",
    "def add_partial_decision_variable(glm_results, design_matrix, variable_base_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Calculates a decision variable using only the intercept and one set of kernel coefficients.\n",
    "\n",
    "    Args:\n",
    "        glm_results (statsmodels.results.ResultWrapper): The fitted model object.\n",
    "        design_matrix (pd.DataFrame): The design matrix used for fitting.\n",
    "        variable_base_name (str): The base name of the variable (e.g., 'partner_speed').\n",
    "        new_column_name (str): The name for the new column to be added.\n",
    "    \"\"\"\n",
    "    if glm_results is None or design_matrix.empty:\n",
    "        print(f\"Skipping calculation for {new_column_name}, model or data is empty.\")\n",
    "        return design_matrix\n",
    "    \n",
    "    # a) Get all fitted parameters from the results object\n",
    "    all_params = glm_results.params\n",
    "\n",
    "    # b) Isolate the specific coefficients we need\n",
    "    coef_names = [f'{variable_base_name}_b{i}' for i in range(N_BASIS_FUNCS)]\n",
    "    variable_coeffs = all_params.filter(items=coef_names)\n",
    "    intercept_coef = all_params['const']\n",
    "\n",
    "    # c) Select the corresponding feature columns from the design matrix\n",
    "    variable_features = design_matrix[coef_names]\n",
    "\n",
    "    # d) Calculate the weighted sum (dot product) and add the intercept\n",
    "    partial_dv = variable_features.dot(variable_coeffs) + intercept_coef\n",
    "    \n",
    "    # e) Add the result as a new column to the original dataframe\n",
    "    design_matrix[new_column_name] = partial_dv\n",
    "    \n",
    "    print(f\"Added column '{new_column_name}' to the dataframe.\")\n",
    "    return design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403462e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# MODIFICATION: Define kernel parameters here for easy adjustment\n",
    "KERNEL_DURATION_S = 4.0  # The length of the history kernel in seconds\n",
    "N_BASIS_FUNCS = 8        # The number of basis functions to represent the kernel\n",
    "\n",
    "# STEP A: Prepare the data for each animal, passing the new parameters\n",
    "design_matrix_a1 = prepare_glm_design_matrix(\n",
    "    animal_id='animal1',\n",
    "    self_pull_ts=pull1_data, self_juice_ts=juice1_data, self_speed_ts=speed1_data, self_gaze_ts=gaze1_data,\n",
    "    partner_pull_ts=pull2_data, partner_speed_ts=speed2_data, fs=fps,\n",
    "    kernel_duration_s=KERNEL_DURATION_S, n_basis_funcs=N_BASIS_FUNCS\n",
    ")\n",
    "\n",
    "design_matrix_a2 = prepare_glm_design_matrix(\n",
    "    animal_id='animal2',\n",
    "    self_pull_ts=pull2_data, self_juice_ts=juice2_data, self_speed_ts=speed2_data, self_gaze_ts=gaze2_data,\n",
    "    partner_pull_ts=pull1_data, partner_speed_ts=speed1_data, fs=fps,\n",
    "    kernel_duration_s=KERNEL_DURATION_S, n_basis_funcs=N_BASIS_FUNCS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8435166",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_duration_s = KERNEL_DURATION_S\n",
    "n_bases = N_BASIS_FUNCS\n",
    "kernel_n_frames = int(kernel_duration_s * fps)\n",
    "basis_functions = create_raised_cosine_basis(kernel_n_frames, n_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix_a1['time_since_last_partner_pull']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370fd5c-bdea-4dcc-b8d0-2f2e88903448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP B: Fit the models\n",
    "glm_results_a1, X1 = fit_and_summarize_glm(design_matrix_a1, 'animal1')\n",
    "glm_results_a2, X2 = fit_and_summarize_glm(design_matrix_a2, 'animal2')\n",
    "\n",
    "\n",
    "# --- MODIFICATION: Add new step to reconstruct the decision variable ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"              RECONSTRUCTING DECISION VARIABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if glm_results_a1 is not None:\n",
    "    # Use the .predict() method on the fitted model with the original data\n",
    "    # This returns the linear predictor (log-odds) for each time step\n",
    "    decision_variable_a1 = glm_results_a1.predict(X1)\n",
    "    \n",
    "    # Add it as a new column to the original design matrix\n",
    "    design_matrix_a1['decision_variable'] = decision_variable_a1\n",
    "    \n",
    "    print(\"\\n--- Animal 1 Design Matrix with new 'decision_variable' column ---\")\n",
    "    print(design_matrix_a1[['response', 'decision_variable']].head())\n",
    "\n",
    "if glm_results_a2 is not None:\n",
    "    decision_variable_a2 = glm_results_a2.predict(X2)\n",
    "    design_matrix_a2['decision_variable'] = decision_variable_a2\n",
    "    \n",
    "    print(\"\\n--- Animal 2 Design Matrix with new 'decision_variable' column ---\")\n",
    "    print(design_matrix_a2[['response', 'decision_variable']].head())\n",
    "\n",
    "print(\"\\nReconstruction complete. The design matrices have been updated.\")\n",
    "\n",
    "\n",
    "# STEP C: Inspect the final results\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"              FINAL GLM RESULTS\")\n",
    "print(\"=\"*50)\n",
    "if glm_results_a1:\n",
    "    print(\"\\n\\n--- GLM Fit Summary for Animal 1 ---\")\n",
    "    print(glm_results_a1.summary())\n",
    "if glm_results_a2:\n",
    "    print(\"\\n\\n--- GLM Fit Summary for Animal 2 ---\")\n",
    "    print(glm_results_a2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d864a3-a9f2-4e44-95bc-0e657daada5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with two subplots, side-by-side\n",
    "print(\"\\n--- Plotting Reconstructed Kernels directly from model results ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
    "\n",
    "# --- Plot for Animal 1 ---\n",
    "ax1 = axes[0]\n",
    "reconstruct_and_plot_kernel(ax1, glm_results_a1, 'self_speed', basis_functions, 'blue', 'Self Speed Kernel')\n",
    "reconstruct_and_plot_kernel(ax1, glm_results_a1, 'partner_speed', basis_functions, 'green', 'Partner Speed Kernel')\n",
    "# You could also plot the partner_pull kernel here\n",
    "# reconstruct_and_plot_kernel(ax1, glm_results_a1, 'partner_pull', basis_functions, 'purple', 'Partner Pull Kernel')\n",
    "\n",
    "ax1.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.set_title('Animal 1: Reconstructed Kernels', fontsize=16)\n",
    "ax1.set_xlabel('Time before Pull (seconds)', fontsize=12)\n",
    "ax1.set_ylabel('Kernel Weight (Influence on Pulling)', fontsize=12)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# --- Plot for Animal 2 ---\n",
    "ax2 = axes[1]\n",
    "reconstruct_and_plot_kernel(ax2, glm_results_a2, 'self_speed', basis_functions, 'blue', 'Self Speed Kernel')\n",
    "reconstruct_and_plot_kernel(ax2, glm_results_a2, 'partner_speed', basis_functions, 'green', 'Partner Speed Kernel')\n",
    "\n",
    "ax2.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_title('Animal 2: Reconstructed Kernels', fontsize=16)\n",
    "ax2.set_xlabel('Time before Pull (seconds)', fontsize=12)\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle('Comparison of GLM Kernels Across Animals (with 95% C.I.)', fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2fd88-f5a4-444e-ae5f-48d32c885d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(design_matrix_a2['response'][100:500])\n",
    "plt.plot(design_matrix_a2['decision_variable'][100:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78a93e-1c79-4bf0-b673-c51f14c7982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = design_matrix_a2['response']==1\n",
    "design_matrix_tgt = design_matrix_a2[ind_]\n",
    "\n",
    "x = design_matrix_tgt['decision_variable']\n",
    "y = design_matrix_tgt['self_gaze_auc']\n",
    "\n",
    "plt.plot(design_matrix_tgt['decision_variable'],design_matrix_tgt['self_gaze_auc'],'.')\n",
    "\n",
    "# 1. Perform the linear regression to get all statistics\n",
    "slope, intercept, r_value, p_value, std_err = st.linregress(x, y)\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dde24-abff-48c1-9b72-a5ffdfec4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Main Execution: Add the new column for each animal ---\n",
    "print(\"\\n--- Reconstructing partial decision variable for Partner Speed ---\")\n",
    "\n",
    "# For Animal 1\n",
    "design_matrix_a1 = add_partial_decision_variable(\n",
    "    glm_results=glm_results_a1,\n",
    "    design_matrix=design_matrix_a1,\n",
    "    variable_base_name='partner_speed',\n",
    "    new_column_name='decision_variable_partner_speed'\n",
    ")\n",
    "\n",
    "# For Animal 2\n",
    "design_matrix_a2 = add_partial_decision_variable(\n",
    "    glm_results=glm_results_a2,\n",
    "    design_matrix=design_matrix_a2,\n",
    "    variable_base_name='partner_speed',\n",
    "    new_column_name='decision_variable_partner_speed'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cba73-7e0c-448e-89f1-d7b952581685",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(design_matrix_a2['time_since_last_success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a4e36-71ed-4355-8612-de2dc547cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(design_matrix_a2['response'][100:1000])\n",
    "plt.plot(design_matrix_a2['decision_variable_partner_speed'][100:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = design_matrix_a2['response']==1\n",
    "design_matrix_tgt = design_matrix_a2[ind_]\n",
    "\n",
    "x = design_matrix_tgt['decision_variable_partner_speed']\n",
    "y = design_matrix_tgt['self_gaze_auc']\n",
    "\n",
    "plt.plot(design_matrix_tgt['decision_variable_partner_speed'],design_matrix_tgt['self_gaze_auc'],'.')\n",
    "\n",
    "# 1. Perform the linear regression to get all statistics\n",
    "slope, intercept, r_value, p_value, std_err = st.linregress(x, y)\n",
    "print(slope)\n",
    "print(r_value)\n",
    "print(p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84eb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ee1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6582f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
